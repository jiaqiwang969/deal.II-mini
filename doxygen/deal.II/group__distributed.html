<!-- HTML header for doxygen 1.8.13-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link rel="canonical" href="https://www.dealii.org/current/doxygen/deal.II/group__distributed.html" />
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>The deal.II Library: Parallel computing with multiple processors using distributed memory</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link rel="SHORTCUT ICON" href="deal.ico"></link>
<script type="text/javascript" src="custom.js"></script>
<meta name="author" content="The deal.II Authors <authors@dealii.org>"></meta>
<meta name="copyright" content="Copyright (C) 1998 - 2021 by the deal.II authors"></meta>
<meta name="deal.II-version" content="10.0.0-pre"></meta>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="logo200.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">
   &#160;<span id="projectnumber">Reference documentation for deal.II version 10.0.0-pre</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!--Extra macros for MathJax:-->
<div style="display:none">
\(\newcommand{\dealvcentcolon}{\mathrel{\mathop{:}}}\)
\(\newcommand{\dealcoloneq}{\dealvcentcolon\mathrel{\mkern-1.2mu}=}\)
\(\newcommand{\jump}[1]{\left[\!\left[ #1 \right]\!\right]}\)
\(\newcommand{\average}[1]{\left\{\!\left\{ #1 \right\}\!\right\}}\)
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#namespaces">Namespaces</a> &#124;
<a href="#nested-classes">Classes</a>  </div>
  <div class="headertitle">
<div class="title">Parallel computing with multiple processors using distributed memory<div class="ingroups"><a class="el" href="group__Parallel.html">Parallel computing</a></div></div>  </div>
</div><!--header-->
<div class="contents">

<p>A module discussing the use of parallelism on distributed memory clusters.  
<a href="#details">More...</a></p>
<div class="dynheader">
Collaboration diagram for Parallel computing with multiple processors using distributed memory:</div>
<div class="dyncontent">
<center><table><tr><td><div class="center"><iframe scrolling="no" frameborder="0" src="group__distributed.svg" width="360" height="67"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe>
</div>
</td></tr></table></center>
</div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="namespaces"></a>
Namespaces</h2></td></tr>
<tr class="memitem:namespaceparallel_1_1distributed"><td class="memItemLeft" align="right" valign="top"> &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceparallel_1_1distributed.html">parallel::distributed</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallel_1_1distributed_1_1CellDataTransfer.html">parallel::distributed::CellDataTransfer&lt; dim, spacedim, VectorType &gt;</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallel_1_1CellWeights.html">parallel::CellWeights&lt; dim, spacedim &gt;</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallel_1_1shared_1_1Triangulation.html">parallel::shared::Triangulation&lt; dim, spacedim &gt;</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallel_1_1distributed_1_1SolutionTransfer.html">parallel::distributed::SolutionTransfer&lt; dim, VectorType, spacedim &gt;</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation&lt; dim, spacedim &gt;</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<p>A module discussing the use of parallelism on distributed memory clusters. </p>
<dl class="section note"><dt>Note</dt><dd>The material presented here is also discussed in <a href="http://www.math.colostate.edu/~bangerth/videos.676.39.html">video lecture 39</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.html">video lecture 41</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.25.html">video lecture 41.25</a>, <a href="http://www.math.colostate.edu/~bangerth/videos.676.41.5.html">video lecture 41.5</a>. (All video lectures are also available <a href="http://www.math.colostate.edu/~bangerth/videos.html">here</a>.)</dd></dl>
<h3>Overview</h3>
<p>deal.II can use multiple machines connected via MPI to parallelize computations, in addition to the parallelization within a shared memory machine discussed in the <a class="el" href="group__threads.html">Parallel computing with multiple processors accessing shared memory</a> module. There are essentially two ways to utilize multiple machines:</p>
<ul>
<li>Each machine keeps the entire mesh and DoF handler locally, but only a share of the global matrix, sparsity pattern, and solution vector is stored on each machine.</li>
<li>The mesh and DoF handler are also distributed, i.e. each processor stores only a share of the cells and degrees of freedom. No processor has knowledge of the entire mesh, matrix, or solution, and in fact problems solved in this mode are usually so large (say, 100s of millions to billions of degrees of freedom) that no processor can or should store even a single solution vector.</li>
</ul>
<p>The first of these two options is relatively straightforward because most of the things one wants to do in a finite element program still work in essentially the same way, and handling distributed matrices, vectors, and linear solvers is something for which good external libraries such as Trilinos or PETSc exist that can make things look almost exactly the same as they would if everything was available locally. The use of this mode of parallelization is explained in the tutorial programs step-17, and step-18 and will not be discussed here in more detail.</p>
<p>The use of truly distributed meshes is somewhat more complex because it changes or makes impossible some of the things that can otherwise be done with deal.II triangulations, DoF handlers, etc. This module documents these issues with a vantage point at 50,000 ft above ground without going into too many details. All the algorithms described below are implement in classes and functions in namespace <a class="el" href="namespaceparallel_1_1distributed.html">parallel::distributed</a>.</p>
<p>One important aspect in parallel computations using MPI is that write access to matrix and vector elements requires a call to <a class="el" href="namespaceUtilities.html#a6155277fd058eddb1504f9562cb1c04d">compress()</a> after the operation is finished and before the object is used (for example read from). Also see <a class="el" href="DEALGlossary.html#GlossCompress">GlossCompress</a>.</p>
<h4>Other resources</h4>
<p>A complete discussion of the algorithms used in this namespace, as well as a thorough description of many of the terms used here, can be found in the <a class="el" href="DEALGlossary.html#distributed_paper">Distributed Computing paper</a>. In particular, the paper shows that the methods discussed in this module scale to thousands of processors and well over a billion degrees of freedom. The paper also gives a concise definition of many of the terms that are used here and in other places of the library related to distributed computing. The step-40 tutorial program shows an application of the classes and methods of this namespace to the Laplace equation, while step-55 does so for a vector-valued problem. step-32 extends the step-31 program to massively parallel computations and thereby explains the use of the topic discussed here to more complicated applications.</p>
<p>For a discussion of what we consider "scalable" programs, see <a class="el" href="DEALGlossary.html#GlossParallelScaling">this glossary entry</a>.</p>
<h4>Distributed triangulations</h4>
<p>In parallel distributed mode, objects of type <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> on each processor only store a subset of cells. In particular, the global mesh can be thought of as decomposed so that each MPI process "owns" a number of cells. The mesh each process then stores locally consists of exactly those cells that it owns, as well as one layer of <a class="el" href="DEALGlossary.html#GlossGhostCell">ghost cells</a> around the ones it locally owns, and a number of cells we call <a class="el" href="DEALGlossary.html#GlossArtificialCell">artificial</a>. The latter are cells that ensure that each processor has a mesh that has all the coarse level cells and that respects the invariant that neighboring cells can not differ by more than one level of refinement. The following pictures show such a mesh, distributed across four processors, and the collection of cells each of these processors stores locally:</p>
<table align="center">
<tr>
<td><div class="image">
<img src="distributed_mesh_0.png" alt="distributed_mesh_0.png"/>
</div>
 </td><td><div class="image">
<img src="distributed_mesh_1.png" alt="distributed_mesh_1.png"/>
</div>
  </td></tr>
<tr>
<td><div class="image">
<img src="distributed_mesh_2.png" alt="distributed_mesh_2.png"/>
</div>
 </td><td><div class="image">
<img src="distributed_mesh_3.png" alt="distributed_mesh_3.png"/>
</div>
  </td></tr>
</table>
<p>The cells are colored based on the <a class="el" href="DEALGlossary.html#GlossSubdomainId">subdomain id</a>, which identifies which processor owns a cell: turquoise for processor 0, green for processor 1, yellow for processor 2, and red for processor 3. As can be seen, each process has one layer of ghost cells around its own cells, which are correctly colored by the subdomain id that identifies the processor that owns each of these cells. Note also how each processor stores a number of artificial cells, indicated in blue, that only exist to ensure that each processor knows about all coarse grid cells and that the meshes have the 2:1 refinement property; however, in the area occupied by these artificial cells, a processor has no knowledge how refined the mesh there really is, as these are areas that are owned by other processors. As a consequence, all algorithms we will develop can only run over the locally owned cells and if necessary the ghost cells; trying to access data on any of the artificial cells is most likely an error. Note that we can determine whether we own a cell by testing that <code>cell-&gt;<a class="el" href="namespacetypes.html#a3d8ea8a4c6a58127f8c2fcc9ec74af6e">subdomain_id()</a> == triangulation.locally_owned_subdomain()</code>.</p>
<p>The "real" mesh one has to think of here is the one that would result from forming the union of cells each of the processes own, i.e. from the overlap of the turquoise, green, yellow and red areas, disregarding the blue areas.</p>
<dl class="section note"><dt>Note</dt><dd>The decomposition of this "real" mesh into the pieces stored by each processes is provided by the <a href="http://www.p4est.org">p4est</a> library. p4est stores the complete mesh in a distributed data structure called a parallel forest (thus the name). A parallel forest consists of quad-trees (in 2d) or oct-trees (in 3d) originating in each coarse mesh cell and representing the refinement structure from parent cells to their four (in 2d) or eight (in 3d) children. Internally, this parallel forest is represented by a (distributed) linear array of cells that corresponds to a depth-first traverse of each tree, and each process then stores a contiguous section of this linear array of cells. This results in partitions such as the one shown above that are not optimal in the sense that they do not minimize the length of the interface between subdomains (and consequently do not minimize the amount of communication) but that in practice are very good and can be manipulated with exceedingly fast algorithms. The efficiency of storing and manipulating cells in this way therefore often outweighs the loss in optimality of communication. (The individual subdomains resulting from this method of partitioning may also sometimes consist of disconnected parts, such as shown at the top right. However, it can be proven that each subdomain consists of at most two disconnected pieces; see C. Burstedde, J. Holke, T. Isaac: "Bounds on the number of
  discontinuities of Morton-type space-filling curves", <a href="http://arxiv.org/abs/1505.05055">arXiv 1505.05055</a>, 2017.)</dd></dl>
<h4>Distributed degree of freedom handler</h4>
<p>The <a class="el" href="classDoFHandler.html">DoFHandler</a> class builds on the <a class="el" href="classTriangulation.html">Triangulation</a> class, but it can detect whenever we actually use an object of type <a class="el" href="classparallel_1_1distributed_1_1Triangulation.html">parallel::distributed::Triangulation</a> as triangulation. In that case, it assigns global numbers for all degrees of freedom that exist, given a finite element, on the global mesh, but each processor will only know about those that are defined on locally relevant cells (i.e. cells either locally owned or that are ghost cells). Internally, the algorithm essentially works by just looping over all cells we own locally and assigning DoF indices to the degrees of freedom defined on them and, in the case of degrees of freedom at the interface between subdomains owned by different processors, that are not owned by the neighboring processor. All processors then exchange how many degrees of freedom they locally own and shift their own indices in such a way that every degree of freedom on all subdomains are uniquely identified by an index between zero and <a class="el" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">DoFHandler::n_dofs()</a> (this function returns the global number of degrees of freedom, accumulated over all processors). Note that after this step, the degrees of freedom owned by each process form a contiguous range that can, for example, be obtained by the contiguous index set returned by <a class="el" href="classDoFHandler.html#ad39fd2189568f2f6b7d557237e3372e3">DoFHandler::locally_owned_dofs()</a>. After assigning unique indices to all degrees of freedom, the <a class="el" href="classDoFHandler.html#a553ca864aaf70330d9be86bc78f36d1e">DoFHandler::distribute_dofs()</a> function then loops over all ghost cells and communicates with neighboring processors to ensure that the global indices of degrees of freedom on these ghost cells match the ones that the neighbor has assigned to them.</p>
<p>Through this scheme, we can make sure that each cell we locally own as well as all the ghost cells can be asked to yield the globally correct indices for the degrees of freedom defined on them. However, asking for degrees of freedom on artificial cells is likely going to lead to nothing good, as no information is available for these cells (in fact, it isn't even known whether these cells are active on the global mesh, or are further refined).</p>
<p>As usual, degrees of freedom can be renumbered after being enumerated, using the functions in namespace <a class="el" href="namespaceDoFRenumbering.html">DoFRenumbering</a>.</p>
<h4>Linear systems for distributed computations</h4>
<p>One thing one learns very quickly when working with very large numbers of processors is that one can not store information about every degree of freedom on each processor, even if this information is "this degree of freedom doesn't live here". An example for this is that we can create an object for a (compressed) sparsity pattern that has <a class="el" href="classDoFHandler.html#aa5b8d3c4b9deb0774dde5c2851e07e1e">DoFHandler::n_dofs()</a> rows, but for which we fill only those rows that correspond to the <a class="el" href="classDoFHandler.html#af16dc39b7ff25aaf361bc6ab440aeee7">DoFHandler::n_locally_owned_dofs()</a> locally owned degrees of freedom. The reason is simple: for the sake of example, let's assume we have 1 billion degrees of freedom distributed across 100 processors; if we even only hold 16 bytes per line in this sparsity pattern (whether we own the corresponding DoF or not), we'll need 16 GB for this object even if every single line is empty. Of course, only 10 million lines will be non-empty, for which we need 160 MB plus whatever is necessary to store the actual column indices of nonzero entries. Let's say we have a moderately complex problem with 50 entries per row, for each of which we store the column index worth 4 bytes, then we'll need 216 bytes for each of the 10 million lines that correspond to the degrees of freedom we own, for a total of 2.16 GB. And we'll need 16 bytes for each of the 990 million lines that we don't own, for a total of 15.840 GB. It is clear that this ratio doesn't become any better if we go to even higher numbers of processors.</p>
<p>The solution to this problem is to really only use any memory at all for those parts of the linear system that we own, or need for some other reason. For all other parts, we must know that they exist, but we can not set up any part of our data structure. To this end, there exists a class called <a class="el" href="classIndexSet.html">IndexSet</a> that denotes a set of indices which we care for, and for which we may have to allocate memory. The data structures for sparsity patterns, constraint matrices, matrices and vector can be initialized with these <a class="el" href="classIndexSet.html">IndexSet</a> objects to really only care for those rows or entries that correspond to indices in the index set, and not care about all others. These objects will then ask how many indices exist in the set, allocate memory for each one of them (e.g. initialize the data structures for a line of a sparsity pattern), and when you want to access data for global degree of freedom <code>i</code> you will be redirected to the result of calling <a class="el" href="classIndexSet.html#a4d924bea58d98feebf99fc714b14b7d0">IndexSet::index_within_set()</a> with index <code>i</code> instead. Accessing data for elements <code>i</code> for which <a class="el" href="classIndexSet.html#a66c79fc7f17b2eeff0f0fb757e77e0c3">IndexSet::is_element()</a> is false will yield an error.</p>
<p>The remaining question is how to identify the set of indices that correspond to degrees of freedom we need to worry about on each processor. To this end, you can use the <a class="el" href="classDoFHandler.html#ad39fd2189568f2f6b7d557237e3372e3">DoFHandler::locally_owned_dofs()</a> function to get at all the indices a processor owns. Note that this is a subset of the degrees of freedom that are defined on the locally owned cells (since some of the degrees of freedom at the interface between two different subdomains may be owned by the neighbor). This set of degrees of freedom defined on cells we own can be obtained using the function <a class="el" href="namespaceDoFTools.html#a5f745d25d2397a91dc65b9158b8d77a6">DoFTools::extract_locally_active_dofs()</a>. Finally, one sometimes needs the set of all degrees of freedom on the locally owned subdomain as well as the adjacent ghost cells. This information is provided by the <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> function.</p>
<h5>Vectors with ghost elements</h5>
<p>A typical parallel application is dealing with two different kinds of parallel vectors: vectors with ghost elements (also called ghosted vectors) and vectors without ghost elements. (Both kinds can typically be represented by the same data type, but there are of course different vector types that can each represent both flavors: for example <a class="el" href="classTrilinosWrappers_1_1MPI_1_1Vector.html">TrilinosWrappers::MPI::Vector</a>, <a class="el" href="classPETScWrappers_1_1MPI_1_1Vector.html">PETScWrappers::MPI::Vector</a>, and <a class="el" href="classBlockVector.html">BlockVector</a> objects built on these). You can find a discussion of what distinguishes these kinds of vectors in the <a class="el" href="DEALGlossary.html#GlossGhostedVector">glossary entry on ghosted vectors</a>.</p>
<p>From a usage point of view, ghosted vectors are typically used for data output, postprocessing, error estimation, input in integration. This is because in these operations, one typically needs access not only to <a class="el" href="DEALGlossary.html#GlossLocallyOwnedDof">locally owned dofs</a> but also to <a class="el" href="DEALGlossary.html#GlossLocallyActiveDof">locally active dofs</a> and sometimes to <a class="el" href="DEALGlossary.html#GlossLocallyRelevantDof">locally relevant dofs</a>, and their values may not be stored in non-ghosted vectors on the processor that needs them. The operations listed above also only require read-only access to vectors, and ghosted vectors are therefore usable in these contexts.</p>
<p>On the other hand, vectors without ghost entries are used in all other places like assembling, solving, or any other form of manipulation. These are typically write-only operations and therefore need not have read access to vector elements that may be owned by another processor.</p>
<p>You can copy between vectors with and without ghost elements (you can see this in step-40, step-55, and step-32) using operator=.</p>
<h5>Sparsity patterns</h5>
<p>At the time of writing this, the only class equipped to deal with the situation just explained is <a class="el" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a>. A version of the function <a class="el" href="classDynamicSparsityPattern.html#aa32f9f3ebad084d001349cd3ddb4074e">DynamicSparsityPattern::reinit()</a> exists that takes an <a class="el" href="classIndexSet.html">IndexSet</a> argument that indicates which lines of the sparsity pattern to allocate memory for. In other words, it is safe to create such an object that will report as its size 1 billion, but in fact only stores only as many rows as the index set has elements. You can then use the usual function <a class="el" href="group__constraints.html#gaf78e864edbfba7e0a7477457bfb96b26">DoFTools::make_sparsity_pattern</a> to build the sparsity pattern that results from assembling on the locally owned portion of the mesh. The resulting object can be used to initialize a PETSc or Trilinos matrix which support very large object sizes through completely distributed storage. The matrix can then be assembled by only looping over those cells owned by the current processor.</p>
<p>The only thing to pay attention to is for which degrees of freedom the sparsity needs to store entries. These are, in essence, the ones we could possibly store values to in the matrix upon assembly. It is clear that these are certainly the locally active degrees of freedom (which live on the cells we locally own) but through constraints, it may also be possible to write to entries that are located on ghost cells. Consequently, you need to pass the index set that results from <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> upon initializing the sparsity pattern.</p>
<h4>Constraints on degrees of freedom</h4>
<p>When creating the sparsity pattern as well as when assembling the linear system, we need to know about constraints on degrees of freedom, for example resulting from hanging nodes or boundary conditions. Like the <a class="el" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> class, the <a class="el" href="classAffineConstraints.html">AffineConstraints</a> container can also take an <a class="el" href="classIndexSet.html">IndexSet</a> upon construction that indicates for which of the possibly very large number of degrees of freedom it should actually store constraints. Unlike for the sparsity pattern, these are now only those degrees of freedom which we work on locally when assembling, namely those returned by <a class="el" href="namespaceDoFTools.html#a5f745d25d2397a91dc65b9158b8d77a6">DoFTools::extract_locally_active_dofs()</a> (a superset of the locally owned ones).</p>
<p>There are, however, situations where more complicated constraints appear in finite element programs. An example is in \(hp\) adaptive computations where degrees of freedom can be constrained against other degrees of freedom that are themselves constrained. In a case like this, in order to fully resolve this chain of constraints, it may not be sufficient to only store constraints on locally active degrees of freedom but one may also need to have constraints available on locally relevant ones. In that case, the <a class="el" href="classAffineConstraints.html">AffineConstraints</a> object needs to be initialized with the <a class="el" href="classIndexSet.html">IndexSet</a> produced by <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> .</p>
<p>In general, your program will continue to do something if you happen to not store all necessary constraints on each processor: you will just generate wrong matrix entries, but the program will not abort. This is opposed to the situation of the sparsity pattern: there, if the <a class="el" href="classIndexSet.html">IndexSet</a> passed to the <a class="el" href="classDynamicSparsityPattern.html">DynamicSparsityPattern</a> indicates that it should store too few rows of the matrix, the program will either abort when you attempt to write into matrix entries that do not exist or the matrix class will silently allocate more memory to accommodate them. As a consequence, it is useful to err on the side of caution when indicating which constraints to store and use the result of <a class="el" href="namespaceDoFTools.html#acad7e0841b9046eaafddc4c617ab1d9d">DoFTools::extract_locally_relevant_dofs()</a> rather than <a class="el" href="namespaceDoFTools.html#a5f745d25d2397a91dc65b9158b8d77a6">DoFTools::extract_locally_active_dofs()</a> . This is also affordable since the set of locally relevant degrees of freedom is only marginally larger than the set of locally active degrees of freedom. We choose this strategy in step-32, step-40, and step-55.</p>
<h4>Postprocessing</h4>
<p>Like everything else, you can only do postprocessing on cells a local processor owns. The <a class="el" href="classDataOut.html">DataOut</a> and <a class="el" href="classKellyErrorEstimator.html">KellyErrorEstimator</a> classes do this automatically: they only operate on locally owned cells without the need to do anything in particular. At least for large computations, there is also no way to merge the results of all these local computations on a single machine, i.e. each processor has to be self-sufficient. For example, each processor has to generate its own parallel output files that have to be visualized by a program that can deal with multiple input files rather than merging the results of calling <a class="el" href="classDataOut.html">DataOut</a> to a single processor before generating a single output file. The latter can be achieved, for example, using the <a class="el" href="namespaceDataOutBase.html#a3aec479936b78bd0ec2ecc3674e24584">DataOutBase::write_vtu()</a> and <a class="el" href="namespaceDataOutBase.html#ac8c87832129884d0603e3ab9ae132741">DataOutBase::write_pvtu_record()</a> functions.</p>
<p>These same considerations hold for all other postprocessing actions as well: while it is, for example, possible to compute a global energy dissipation rate by doing the computations locally and accumulating the resulting single number processor to a single number for the entire communication, it is in general not possible to do the same if the volume of data produced by every processor is significant.</p>
<p>There is one particular consideration for postprocessing, however: whatever you do on each cell a processor owns, you need access to at least all those values of the solution vector that are active on these cells (i.e. to the set of all <em>locally active degrees of freedom</em>, in the language of the <a class="el" href="DEALGlossary.html#distributed_paper">Distributed Computing paper</a>), which is a superset of the degrees of freedom this processor actually owns (because it may not own all degrees of freedom on the interface between its own cells and those cells owned by other processors). Sometimes, however, you need even more information: for example, to compute the KellyErrorIndicator results, one needs to evaluate the gradient at the interface on the current as well as its neighbor cell; the latter may be owned by another processor, so we need those degrees of freedom as well. In general, therefore, one needs access to the solution values for all degrees of freedom that are <em>locally relevant</em>. On the other hand, both of the packages we can use for parallel linear algebra (PETSc and Trilinos) as well as parallel::distributed::Vector subdivide vectors into chunks each processor owns and chunks stored on other processors. To postprocess stuff therefore means that we have to tell PETSc or Trilinos that it should also import <em>ghost elements</em>, i.e. additional vector elements of the solution vector other than the ones we own locally. For ghosted vectors, this can be achieved by using operator= with a distributed vector as argument. </p>
</div><!-- contents -->
<!-- HTML footer for doxygen 1.8.13-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
