<!-- HTML header for doxygen 1.8.13-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link rel="canonical" href="https://www.dealii.org/current/doxygen/deal.II/group__auto__symb__diff.html" />
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>The deal.II Library: Automatic and symbolic differentiation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link rel="SHORTCUT ICON" href="deal.ico"></link>
<script type="text/javascript" src="custom.js"></script>
<meta name="author" content="The deal.II Authors <authors@dealii.org>"></meta>
<meta name="copyright" content="Copyright (C) 1998 - 2021 by the deal.II authors"></meta>
<meta name="deal.II-version" content="10.0.0-pre"></meta>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="logo200.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">
   &#160;<span id="projectnumber">Reference documentation for deal.II version 10.0.0-pre</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!--Extra macros for MathJax:-->
<div style="display:none">
\(\newcommand{\dealvcentcolon}{\mathrel{\mathop{:}}}\)
\(\newcommand{\dealcoloneq}{\dealvcentcolon\mathrel{\mkern-1.2mu}=}\)
\(\newcommand{\jump}[1]{\left[\!\left[ #1 \right]\!\right]}\)
\(\newcommand{\average}[1]{\left\{\!\left\{ #1 \right\}\!\right\}}\)
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#namespaces">Namespaces</a>  </div>
  <div class="headertitle">
<div class="title">Automatic and symbolic differentiation</div>  </div>
</div><!--header-->
<div class="contents">

<p>A module dedicated to the implementation of functions and classes that relate to automatic and symbolic differentiation.  
<a href="#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="namespaces"></a>
Namespaces</h2></td></tr>
<tr class="memitem:namespaceDifferentiation"><td class="memItemLeft" align="right" valign="top"> &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceDifferentiation.html">Differentiation</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:namespaceDifferentiation_1_1AD"><td class="memItemLeft" align="right" valign="top"> &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceDifferentiation_1_1AD.html">Differentiation::AD</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:namespaceDifferentiation_1_1SD"><td class="memItemLeft" align="right" valign="top"> &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceDifferentiation_1_1SD.html">Differentiation::SD</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<p>A module dedicated to the implementation of functions and classes that relate to automatic and symbolic differentiation. </p>
<p>Below we provide a very brief introduction as to what automatic and symbolic differentiation are, what variations of these computational/numerical schemes exist, and how they are integrated within deal.II's framework. The purpose of all of these schemes is to automatically compute the derivative of functions, or approximations of it, in cases where one does not want to compute them by hand. Common examples are situations in the finite element context is where one wants to solve a nonlinear problem that is given by requiring that some residual \(F(u,\nabla u)=0\) where \(F\) is a complicated function that needs to be differentiated to apply Newton's method; and situations where one is given a parameter dependent problem \({\cal A}(q,u,\nabla u) = f\) and wants to form derivatives with regards to the parameters \(q\), for example to optimize an output functional with regards to \(q\), or for a sensitivity analysis with regards to \(q\). One should think of \(q\) as design parameters: say, the width or shape of a wing, the stiffness coefficients of a material chosen to build an object, the power sent to a device, the chemical composition of the gases sent to a burner. In all of these cases, one should think of \(F\) and \(\cal A\) as <em>complicated</em> and cumbersome to differentiate &ndash; at least when doing it by hand. A relatively simple case of a nonlinear problem that already highlights the tedium of computing derivatives by hand is shown in step-15. However, in reality, one might, for example, think about problems such as chemically reactive flows where the fluid equations have coefficients such as the density and viscosity that depend strongly and nonlinearly on the chemical composition, temperature, and pressure of the fluid at each point; and where the chemical species react with each other based on reaction coefficients that also depend nonlinearly and in complicated ways on the chemical composition, temperature, and pressure. In many cases, the exact formulas for all of these coefficients can take several lines to write out, may include exponentials and (harmonic or geometric) averages of several nonlinear terms, and/or may contain table lookup of and interpolation between data points. Just getting these terms right is difficult enough; computing derivatives of these terms is impractical in most applications and, in reality, impossible to get right. Higher derivatives are even more impossible to do without computer aid. Automatic or symbolic differentiation is a way out of this: One only has to implement the function that computes these coefficients in terms of their inputs only once, and gets the (correct!) derivatives without further coding effort (though at a non-negligible computational cost either at run time, compile time, or both).</p>
<h1><a class="anchor" id="auto_diff_1"></a>
Automatic differentiation</h1>
<p><a href="https://en.wikipedia.org/wiki/Automatic_differentiation">Automatic differentiation </a> (commonly also referred to as algorithmic differentiation), is a numerical method that can be used to "automatically" compute the first, and perhaps higher-order, derivatives of function(s) with respect to one or more input variables. Although this comes at a certain computational cost, the benefits to using such a tool may be significant. When used correctly the derivatives of often complicated functions can be computed to a very high accuracy. Although the exact accuracy achievable by these frameworks largely depends on their underlying mathematical formulation, some implementations compute with a precision on the order of machine accuracy. Note that this is different to classical numerical differentiation (using, for example, a finite difference approximation of a function by evaluating it at different points), which has an accuracy that depends on both the perturbation size as well as the chosen finite-difference scheme; the error of these methods is measurably larger than well-formulated automatic differentiation approaches.</p>
<p>Three practical examples of auto-differentiation use within a finite-element context would then be</p><ul>
<li>the quick prototyping of a new nonlinear formulation without the need to hand-compute the linearization itself,</li>
<li>automatic linearization of finite-element residuals additively formed within complex multiphysics frameworks, and</li>
<li>verification of user-implementations of linearizations for both cell-based calculations (e.g. a residual) and those based at a continuum point (e.g. tangents for nonlinear constitutive laws).</li>
</ul>
<p>There are quite a number of implementations for auto-differentiable numbers. They primarily fall into two broad categories, namely <em>source code transformation</em> and <em>operator overloading</em>. The first method generates new, compilable code based on some input function that, when executed, returns the derivatives of the input function. The second exploits the capability of <code>C++</code> operator definitions to be overloaded for custom class types. Therefore a class that represents such an auto-differentiable number can, following each mathematical operation performed on or with it, in principle evaluate and keep track of its value as well as that of its directional derivative(s). As the libraries exclusively implementing the <em>source code transformation</em> approach collectively describe highly specialized tools that are to be used as function preprocessors, they have no direct support within deal.II itself. The latter, however, represent specialized number types that can be supported through the use of template metaprogramming in the appropriate context. Given the examples presented above, this means that the <a class="el" href="classFEValues.html">FEValues</a> class (and friends), as well as the <a class="el" href="classTensor.html">Tensor</a> and <a class="el" href="classSymmetricTensor.html">SymmetricTensor</a> classes should support calculations performed with these specialized numbers. (In theory an entire program could be made differentiable. This could be useful in, for example, the sensitivity analysis of solutions with respect to input parameters. However, to date this has not been tested.)</p>
<p>Implementations of specialized frameworks based on <em>operator overloading</em> typically fall into one of three categories. In each, some customized data classes representing the floating point value of an evaluated function and its derivative(s) by</p><ol type="1">
<li>exploiting <em>dual</em>/<em>complex-step</em>/<em>hyper-dual</em> formulations (occasionally called <em>tapeless</em> methods),</li>
<li>those utilizing <em>taping</em> strategies, and</li>
<li>those using compile-time optimization through <em>expression templates</em>.</li>
</ol>
<p>To provide some tentative insight into how these various implementations might look like in practice, we offer the following generic summary of these approaches:</p><ol type="1">
<li>The first two <em>tapeless</em> approaches listed above (dual numbers and complex-step method) use some variation of a truncated Taylor series, along with a particular choice of definition for the perturbation parameter, to compute function derivatives using a finite-difference based approach. The "dual" number constitutes the accumulated directional derivatives computed simultaneously as the function values are evaluated; in the complex-step approach, the imaginary value effectively serves this purpose. The choice of the perturbation parameter determines the numerical qualities of the scheme, such as the influence of the truncation of the Taylor scheme; dual numbers do not contain any higher-order terms in their first derivative, while for the complex-step method these existent higher-order terms are neglected. It can be shown that both of these methods are not subject to subtractive cancellation errors and that, within their finite-difference scheme, they are not numerically sensitive to the internal step-size chosen for the numerical perturbation. The dual number approach thus produces exact first derivatives, while the complex-step approximation does not. The standard implementation of the dual numbers, however, cannot yield exact values for second derivatives. Hyper-dual numbers take a different view of this idea, with numbers being represented in a form similar to quaternions (i.e. carrying additional non-real components) and the derivatives being computed from a high-order truncation of the Taylor series all four components. The outcome is that, with the appropriate implementation, both first and second derivatives can be computed exactly.</li>
<li>With <em>taped</em> approaches, a specified subregion of code is selected as one for which all operations executed with active (marked) input variables are tracked and recorded in a data structure referred to as a tape. At the end of the taped region, the recorded function(s) may be reevaluated by "replaying" the tape with a different set of input variables instead of recomputing the function directly. Assuming that the taped region represents a smooth function, arbitrarily high-order derivatives of the function then can be computed by referring to the code path tracked and stored on the tape. (This could perhaps be achieved, for example, through evaluation of the function around the point of interest.) There exist strategies to deal with situations where the taped function is not smooth at the evaluated point, or if it is not analytic. Furthermore, one might need to consider the case of branched functions, where the tape is no longer sequential, but rather forks off on a different evaluation path to that due to the original recorded inputs.</li>
<li>Methods based on <a href="https://en.wikipedia.org/wiki/Expression_templates">expression templates</a> leverage the computational graph (in this case, a <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic graph (DAG)</a>), constructed from the abstract syntax tree (AST), that resolves the function output from its input values. The outermost leaves on the tree represent the independent variables or constants, and are transformed by unary operators and connected by binary operators (in the most simple case). Therefore, the operations performed on the function inputs is known at compile time, and with that the associated derivative operation can also be defined at the same time using the well-known rules of computing the derivative of an operation (such as the associativity of derivatives under addition and subtraction, the product rule, and the chain rule). The compiled output type returned by this operator need not be generic, but can rather be specialized based on the specific inputs (possibly carrying a differential history) given to that specific operator on the vertex of the DAG. In this way, a compile-time optimized set of instructions can be generated for the very specialized individual operations used to evaluate each intermediate result of the dependent function.</li>
</ol>
<p>Each of these methods, of course, has its advantages and disadvantages, and one may be more appropriate than another for a given problem that is to be solved. As the aforementioned implementational details (and others not discussed) may be hidden from the user, it may still be important to understand the implications, run-time cost, and potential limitations, of using any one of these "black-box" auto-differentiable numbers.</p>
<p>In addition to the supplied linked articles, resources used to furnish the details supplied here include:</p>
<div class="fragment"><div class="line">@InProceedings{Fike2011a,</div><div class="line">  author    = {Fike, Jeffrey A and Alonso, Juan J},</div><div class="line">  title     = {The Development of Hyper-Dual Numbers for Exact Second-Derivative Calculations},</div><div class="line">  booktitle = {49th {AIAA} Aerospace Sciences Meeting including the New Horizons Forum and Aerospace Exposition},</div><div class="line">  year      = {2011},</div><div class="line">  volume    = {886},</div><div class="line">  pages     = {124},</div><div class="line">  month     = {jan},</div><div class="line">  publisher = {American Institute of Aeronautics and Astronautics},</div><div class="line">  doi       = {10.2514/6.2011-886},</div><div class="line">}</div></div><!-- fragment --><div class="fragment"><div class="line">@Manual{Walther2009a,</div><div class="line">  title     = {Getting Started with ADOL-C},</div><div class="line">  author    = {Walther, Andrea and Griewank, Andreas},</div><div class="line">  year      = {2009},</div><div class="line">  booktitle = {Combinatorial scientific computing},</div><div class="line">  doi       = {10.1.1.210.4834},</div><div class="line">  pages     = {181--202}</div><div class="line">}</div></div><!-- fragment --><h3>Exploitation of the chain-rule</h3>
<p>In the most practical sense, any of the above categories exploit the chain-rule to compute the total derivative of a composite function. To perform this action, they typically use one of two mechanisms to compute derivatives, specifically</p><ul>
<li><em>forward-mode</em> (or <em>forward accumulation</em>) auto-differentiation, or</li>
<li><em>reverse-mode</em> (or <em>reverse accumulation</em>) auto-differentiation.</li>
</ul>
<p>As a point of interest, the <em>optimal Jacobian accumulation</em>, which performs a minimal set of computations, lies somewhere between these two limiting cases. Its computation for a general composite function remains an open problem in graph theory.</p>
<p>With the aid of the diagram below (it and some of the listed details courtesy of this <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">Wikipedia article</a>), let us think about the represention of the calculation of the function \(f (\mathbf{x}) = \sin (x_{1}) + x_{1} x_{2}\) and its derivatives:</p>
<div class="twocolumn" style="width: 80%"> <div class="parent"> <div class="img" align="center"> <div class="image">
<img src="https://upload.wikimedia.org/wikipedia/commons/a/a4/ForwardAccumulationAutomaticDifferentiation.png" alt="Forward mode automatic differentiation" width="400"/>
</div>
 </div> <div class="text" align="center"> Forward mode automatic differentiation </div> </div> <div class="parent"> <div class="img" align="center"> <div class="image">
<img src="https://upload.wikimedia.org/wikipedia/commons/a/a0/ReverseaccumulationAD.png" alt="Reverse mode automatic differentiation" width="400"/>
</div>
 </div> <div class="text" align="center"> Reverse mode automatic differentiation </div> </div> </div><p>Specifically, we will briefly describe what forward and reverse auto-differentiation are. Note that in the diagram, along the edges of the graph in text are the directional derivative of function \(w\) with respect to the \(i\)-th variable, represented by the notation \(\dot{w} = \dfrac{d w}{d x_{i}}\). The specific computations used to render the function value and its directional derivatives for this example are tabulated in the <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">source article</a>. For a second illustrative example, we refer the interested reader to <a href="http://www.columbia.edu/~ahd2125/post/2015/12/5/">this article</a>.</p>
<p>Consider first that any composite function \(f(x)\), here represented as having two independent variables, can be dissected into a composition of its elementary functions </p><p class="formulaDsp">
\[ f (\mathbf{x}) = f_{0} \circ f_{1} \circ f_{2} \circ \ldots \circ f_{n} (\mathbf{x}) \quad . \]
</p>
<p> As was previously mentioned, if each of the primitive operations \(f_{n}\) is smooth and differentiable, then the chain-rule can be universally employed to compute the total derivative of \(f\), namely \(\dfrac{d f(x)}{d \mathbf{x}}\). What distinguishes the "forward" from the "reverse" mode is how the chain-rule is evaluated, but ultimately both compute the total derivative </p><p class="formulaDsp">
\[ \dfrac{d f (\mathbf{x})}{d \mathbf{x}} = \dfrac{d f_{0}}{d f_{1}} \dfrac{d f_{1}}{d f_{2}} \dfrac{d f_{2}}{d f_{3}} \ldots \dfrac{d f_{n} (\mathbf{x})}{d \mathbf{x}} \quad . \]
</p>
<p>In forward-mode, the chain-rule is computed naturally from the "inside out". The independent variables are therefore fixed, and each sub-function \(f&#39;_{i} \vert_{f&#39;_{i+1}}\) is computed recursively and its result returned as inputs to the parent function. Encapsulating and fixing the order of operations using parentheses, this means that we compute </p><p class="formulaDsp">
\[ \dfrac{d f (\mathbf{x})}{d \mathbf{x}} = \dfrac{d f_{0}}{d f_{1}} \left( \dfrac{d f_{1}}{d f_{2}} \left(\dfrac{d f_{2}}{d f_{3}} \left(\ldots \left( \dfrac{d f_{n} (\mathbf{x})}{d \mathbf{x}} \right)\right)\right)\right) \quad . \]
</p>
<p> The computational complexity of a forward-sweep is proportional to that of the input function. However, for each directional derivative that is to be computed one sweep of the computational graph is required.</p>
<p>In reverse-mode, the chain-rule is computed somewhat unnaturally from the "outside in". The values of the dependent variables first get computed and fixed, and then the preceding differential operations are evaluated and multiplied in succession with the previous results from left to right. Again, if we encapsulate and fix the order of operations using parentheses, this implies that the reverse calculation is performed by </p><p class="formulaDsp">
\[ \dfrac{d f (\mathbf{x})}{d \mathbf{x}} = \left( \left( \left( \left( \left( \dfrac{d f_{0}}{d f_{1}} \right) \dfrac{d f_{1}}{d f_{2}} \right) \dfrac{d f_{2}}{d f_{3}} \right) \ldots \right) \dfrac{d f_{n} (\mathbf{x})}{d \mathbf{x}} \right) \quad . \]
</p>
<p> The intermediate values \(\dfrac{d f_{i-1}}{d f_{i}}\) are known as <em>adjoints</em>, which must be computed and stored as the computational graph is traversed. However, for each dependent scalar function one sweep of the computational graph renders all directional derivatives at once.</p>
<p>Overall, the efficiency of each mode is determined by the number of independent (input) variables and dependent (output) variables. If the outputs greatly exceed the inputs in number, then forward-mode can be shown to be more efficient than reverse-mode. The converse is true when the number of input variables greatly exceeds that of the output variables. This point may be used to help inform which number type is most suitable for which set of operations are to be performed using automatic differentiation. For example, in many applications for which second derivatives are to be computed it is appropriate to combine both reverse- and forward-modes. The former would then typically be used to calculate the first derivatives, and the latter the second derivatives.</p>
<h2><a class="anchor" id="auto_diff_1_1"></a>
Supported automatic differentiation libraries</h2>
<p>We currently have validated implementations for the following number types and combinations:</p>
<ul>
<li>Taped ADOL-C (n-differentiable, in theory, but internal drivers for up to second-order derivatives will be implemented)</li>
<li>Tapeless ADOL-C (once differentiable)</li>
<li>Forward-mode Sacado with dynamic memory allocation using expression templates (once differentiable)</li>
<li>Nested forward-mode Sacado using expression templates (twice differentiable)</li>
<li>Reverse-mode Sacado (once differentiable)</li>
<li>Nested reverse and dynamically-allocated forward-mode Sacado (twice differentiable, but results memory leak described in <a class="el" href="namespaceDifferentiation_1_1AD.html#acc1d0db17ec4125ac4e28f6ad2069b1c">Differentiation::AD::NumberTypes</a>)</li>
</ul>
<p>Note that in the above, "dynamic memory allocation" refers to the fact that the number of independent variables need not be specified at compile time.</p>
<p>The <a href="https://projects.coin-or.org/ADOL-C/browser/trunk/ADOL-C/doc/adolc-manual.pdf?format=raw">ADOL-C user manual</a></p>
<div class="fragment"><div class="line">@Manual{Walther2009a,</div><div class="line">  title     = {Getting Started with ADOL-C},</div><div class="line">  author    = {Walther, Andrea and Griewank, Andreas},</div><div class="line">  year      = {2009},</div><div class="line">  booktitle = {Combinatorial scientific computing},</div><div class="line">  doi       = {10.1.1.210.4834},</div><div class="line">  pages     = {181--202},</div><div class="line">  url       = {https://projects.coin-or.org/ADOL-C/browser/trunk/ADOL-C/doc/adolc-manual.pdf}</div><div class="line">}</div></div><!-- fragment --><p>provides the principle insights into their taped and tapeless implementations, and how ADOL-C can be incorporated into a user code. Some further useful resources for understanding the implementation of ADOL-C, and possibilities for how it may be used within a numerical code, include:</p>
<div class="fragment"><div class="line">@Article{Griewank1996a,</div><div class="line">  author    = {Griewank, Andreas and Juedes, David and Utke, Jean},</div><div class="line">  title     = {Algorithm 755: {ADOL-C}: a package for the automatic differentiation of algorithms written in {C/C++}},</div><div class="line">  journal   = {ACM Transactions on Mathematical Software (TOMS)},</div><div class="line">  year      = {1996},</div><div class="line">  volume    = {22},</div><div class="line">  number    = {2},</div><div class="line">  pages     = {131--167},</div><div class="line">  doi       = {10.1145/229473.229474},</div><div class="line">  publisher = {ACM}</div><div class="line">}</div></div><!-- fragment --> <div class="fragment"><div class="line">@InCollection{Bischof2008a,</div><div class="line">  author =    {Bischof, Christian and Guertler, Niels and Kowarz, Andreas and Walther, Andrea},</div><div class="line">  title =     {Parallel reverse mode automatic differentiation for OpenMP programs with ADOL-C},</div><div class="line">  booktitle = {Advances in Automatic Differentiation},</div><div class="line">  publisher = {Springer},</div><div class="line">  year =      {2008},</div><div class="line">  pages =     {163--173}</div><div class="line">}</div></div><!-- fragment --> <div class="fragment"><div class="line">@InBook{Kulshreshtha2012a,</div><div class="line">  chapter   = {Computing Derivatives in a Meshless Simulation Using Permutations in {ADOL}-C},</div><div class="line">  pages     = {321--331},</div><div class="line">  title     = {Recent Advances in Algorithmic Differentiation},</div><div class="line">  publisher = {Springer Berlin Heidelberg},</div><div class="line">  year      = {2012},</div><div class="line">  author    = {Kshitij Kulshreshtha and Jan Marburger},</div><div class="line">  editor    = {Forth S. and Hovland P. and Phipps E. and Utke J. and Walther A.},</div><div class="line">  series    = {Lecture Notes in Computational Science and Engineering},</div><div class="line">  doi       = {10.1007/978-3-642-30023-3_29},</div><div class="line">}</div></div><!-- fragment --> <div class="fragment"><div class="line">@InProceedings{Kulshreshtha2013a,</div><div class="line">  author    = {Kulshreshtha, Kshitij and Koniaeva, Alina},</div><div class="line">  title     = {Vectorizing the forward mode of ADOL-C on a GPU using CUDA},</div><div class="line">  booktitle = {13th European AD Workshop},</div><div class="line">  year      = {2013},</div><div class="line">  month     = jun</div><div class="line">}</div></div><!-- fragment --><p>Similarly, a selection of useful resources for understanding the implementation of Sacado number types (in particular, how expression templating is employed and exploited) include:</p>
<div class="fragment"><div class="line">@InCollection{Bartlett2006a,</div><div class="line">  author        = {Bartlett, R. A. and Gay, D. M. and Phipps, E. T.},</div><div class="line">  title         = {Automatic Differentiation of C++ Codes for Large-Scale Scientific Computing},</div><div class="line">  booktitle     = {International Conference on Computational Science {\textendash} {ICCS} 2006},</div><div class="line">  publisher     = {Springer Berlin Heidelberg},</div><div class="line">  year          = {2006},</div><div class="line">  editor        = {Alexandrov, V.N. and van Albada, G.D. and Sloot, P.M.A. amd Dongarra, J.},</div><div class="line">  pages         = {525--532},</div><div class="line">  doi           = {10.1007/11758549_73},</div><div class="line">  organization  = {Springer}</div><div class="line">}</div></div><!-- fragment --> <div class="fragment"><div class="line">@InBook{Gay2012a,</div><div class="line">  chapter   = {Using expression graphs in optimization algorithms},</div><div class="line">  pages     = {247--262},</div><div class="line">  title     = {Mixed Integer Nonlinear Programming},</div><div class="line">  publisher = {Springer New York},</div><div class="line">  year      = {2012},</div><div class="line">  author    = {Gay, D. M.},</div><div class="line">  editor    = {Lee, J. and Leyffer, S.},</div><div class="line">  isbn      = {978-1-4614-1927-3},</div><div class="line">  doi       = {10.1007/978-1-4614-1927-3_8}</div><div class="line">}</div></div><!-- fragment --> <div class="fragment"><div class="line">@InBook{Phipps2012a,</div><div class="line">  chapter     = {Efficient Expression Templates for Operator Overloading-based Automatic Differentiation},</div><div class="line">  pages       = {309--319},</div><div class="line">  title       = {Recent Advances in Algorithmic Differentiation},</div><div class="line">  publisher   = {Springer},</div><div class="line">  year        = {2012},</div><div class="line">  author      = {Eric Phipps and Roger Pawlowski},</div><div class="line">  editor      = {Forth S. and Hovland P. and Phipps E. and Utke J. and Walther A.},</div><div class="line">  series      = {Lecture Notes in Computational Science and Engineering},</div><div class="line">  volume      = {73},</div><div class="line">  date        = {2012-05-15},</div><div class="line">  doi         = {10.1007/978-3-642-30023-3_28},</div><div class="line">  eprint      = {1205.3506v1},</div><div class="line">  eprintclass = {cs.MS},</div><div class="line">  eprinttype  = {arXiv}</div><div class="line">}</div></div><!-- fragment --><p>The implementation of both forward- and reverse-mode Sacado numbers is quite intricate. As of Trilinos 12.12, the implementation of math operations involves a lot of preprocessor directives and macro programming. Accordingly, the code may be hard to follow and there exists no meaningful companion documentation for these classes. So, a useful resource for understanding the principle implementation of these numbers can be found at <a href="https://trilinos.org/docs/dev/packages/sacado/doc/html/classSacado_1_1Fad_1_1SimpleFad.html">this link for the Sacado::Fad::SimpleFad class</a> that outlines a reference (although reportedly inefficient) implementation of a forward-mode auto-differentiable number that does not use expression templates. (Although not explicitly stated, it would appear that the Sacado::Fad::SimpleFad class is implemented in the spirit of dual numbers.)</p>
<h2><a class="anchor" id="auto_diff_1_2"></a>
How automatic differentiation is integrated into deal.II</h2>
<p>Since the interface to each automatic differentiation library is so vastly different, a uniform internal interface to each number will be established in the near future. The goal will be to allow some driver classes (that provide the core functionality, and will later be introduced in the next section) to have a consistent mechanism to interact with different auto-differentiation libraries. Specifically, they need to be able to correctly initialize and finalize data that is to be interpreted as the dependent and independent variables of a formula.</p>
<p>A summary of the files that implement the interface to the supported auto-differentiable numbers is as follows:</p>
<ul>
<li><a class="el" href="ad__drivers_8h.html">ad_drivers.h</a>: Provides classes that act as drivers to the interface of internally supported automatic differentiation libraries. These are used internally as an intermediary to the helper classes that we provide.</li>
<li><a class="el" href="ad__helpers_8h.html">ad_helpers.h</a>: Provides a set of classes to help perform automatic differentiation in a number of different contexts. These are detailed in <a class="el" href="group__auto__symb__diff.html#auto_diff_1_3">User interface to the automatic differentiation libraries</a>.</li>
<li><a class="el" href="ad__number__types_8h.html">ad_number_types.h</a>: Introduces an enumeration (called a type code) for the auto-differentiable number combinations that will be supported by the driver classes. The rationale behind the use of this somewhat restrictive mechanism is discussed below.</li>
<li><a class="el" href="ad__number__traits_8h.html">ad_number_traits.h</a>: Declare some internal classes that are to be specialized for each auto-differentiation library and/or number type. These are subsequently used to provide a uniform interface to the classes through the NumberTraits and ADNumberTraits classes which are extensively used throughout of drivers. We also provide some mechanisms to easily query select properties of these numbers, i.e. some type traits.</li>
<li><a class="el" href="adolc__math_8h.html">adolc_math.h</a>: Extension of the ADOL-C math operations that allow these numbers to be used consistently throughout the library.</li>
<li><a class="el" href="adolc__number__types_8h.html">adolc_number_types.h</a>: Implementation of the internal classes that define how we use ADOL-C numbers.</li>
<li><a class="el" href="adolc__product__types_8h.html">adolc_product_types.h</a>: Defines some product and scalar types that allow the use of ADOL-C numbers in conjunction with the <a class="el" href="classTensor.html">Tensor</a> and <a class="el" href="classSymmetricTensor.html">SymmetricTensor</a> classes.</li>
<li><a class="el" href="sacado__math_8h.html">sacado_math.h</a>: Extension of the Sacado math operations that allow these numbers to be used consistently throughout the library.</li>
<li><a class="el" href="sacado__number__types_8h.html">sacado_number_types.h</a>: Implementation of the internal classes that define how we use the supported Sacado numbers.</li>
<li><a class="el" href="sacado__product__types_8h.html">sacado_product_types.h</a>: Defines some product and scalar types that allow the use of the supported Sacado numbers in conjunction with the <a class="el" href="classTensor.html">Tensor</a> and <a class="el" href="classSymmetricTensor.html">SymmetricTensor</a> classes.</li>
</ul>
<p>By using type codes for each supported number type, we artificially limit the type of auto-differentiable numbers that can be used within the library. This design choice is due to the fact that its not trivial to ensure that each number type is correctly initialized and that all combinations of nested (templated) types remain valid for all operations performed by the library. Furthermore, there are some lengthy functions within the library that are instantiated for the supported number types and have internal checks that are only satisfied when a auto-differentiable number, of which the library has knowledge, is used. This again ensures that the integrity of all computations is maintained. Finally, using a simple enumeration as a class template parameter ultimately makes it really easy to switch between the type used in production code with little to no further amendments required to user code.</p>
<h3><a class="anchor" id="auto_diff_1_3"></a>
User interface to the automatic differentiation libraries</h3>
<p>The deal.II library offers a unified interface to the automatic differentiation libraries that we support. To date, the helper classes have been developed for the following contexts:</p>
<ul>
<li>Classes designed to operate at the quadrature point level (or any general continuum point):<ul>
<li>ScalarFunction: <a class="el" href="namespaceDifferentiation.html">Differentiation</a> of a scalar-valued function. One typical use would be the the development of constitutive laws directly from a strain energy function.</li>
<li>VectorFunction: <a class="el" href="namespaceDifferentiation.html">Differentiation</a> of a vector-valued function. This could be used to linearize the kinematic variables of a constitutive law, or assist in solving the evolution equations of local internal variables.</li>
</ul>
</li>
<li>Classes designed to operate at the cell level:<ul>
<li>EnergyFunctional: <a class="el" href="namespaceDifferentiation.html">Differentiation</a> of a scalar-valued energy functional, such as might arise from variational formulations.</li>
<li>ResidualLinearization: <a class="el" href="namespaceDifferentiation.html">Differentiation</a> of a vector-valued finite element residual, leading to its consistent linearization.</li>
</ul>
</li>
</ul>
<p>Naturally, it is also possible for users to manage the initialization and derivative computations themselves.</p>
<p>The most up-to-date examples of how this is done using ADOL-C can be found in</p><ul>
<li>their <a href="https://projects.coin-or.org/ADOL-C/browser/trunk/ADOL-C/doc/adolc-manual.pdf?format=raw">user manual</a>,</li>
<li>their <a href="https://gitlab.com/adol-c/adol-c/tree/master/ADOL-C/examples">development repository</a>, and</li>
<li>our <a href="https://github.com/dealii/dealii/tree/master/tests/adolc">test-suite</a>,</li>
</ul>
<p>while for Sacado, illustrative examples can be found in</p><ul>
<li>their <a href="https://github.com/trilinos/Trilinos/tree/master/packages/sacado/example">development repository</a>,</li>
<li>a <a href="https://github.com/dealii/code-gallery/tree/master/Quasi_static_Finite_strain_Compressible_Elasticity">code-gallery example</a>, and</li>
<li>our <a href="https://github.com/dealii/dealii/tree/master/tests/sacado">test-suite</a>.</li>
</ul>
<h1><a class="anchor" id="symb_diff_1"></a>
Symbolic expressions and differentiation</h1>
<p><a href="https://en.wikipedia.org/wiki/Symbolic_differentiation">Symbolic differentiation</a> is, in terms of its design and usage, quite different to automatic differentiation. Underlying any symbolic library is a computer algebra system (CAS) that implements a language and collection of algorithms to manipulate symbolic (or "string-like") expressions. This is most similar, from a philosophical point of view, to how algebraic operations would be performed by hand.</p>
<p>To help better distinguish between symbolic differentiation and numerical methods like automatic differentiation, let's consider a very simple example. Suppose that the function \(f(x,y) = [2x+1]^{y}\), where \(x\) and \(y\) are variables that are independent of one another. By applying the chain-rule, the derivatives of this function are simply \(\dfrac{d f(x,y)}{d x} = 2y[2x+1]^{y-1}\) and \(\dfrac{d f(x,y)}{d y} = [2x+1]^{y} \ln(2x+1)\). These are exactly the results that you get from a CAS after defining the symbolic variables <code>x</code> and <code>y</code>, defining the symbolic expression <code>f = pow(2x+1, y)</code> and computing the derivatives <code>diff(f, x)</code> and <code>diff(f, y)</code>. At this point there is no assumption of what <code>x</code> and <code>y</code> represent; they may later be interpreted as plain (scalar) numbers, complex numbers, or something else for which the power and natural logarithm functions are well defined. Obviously this means that there is also no assumption about which point to evaluate either the expression or its derivatives. One could readily take the expression for \(\dfrac{d f(x, y)}{d x}\) and evaluate it at \(x=1, y=2.5\) and then later, with no recomputation of the derivative expression itself, evaluate it at \(x=3.25, y=-6\). In fact, the interpretation of any symbolic variable or expression, and the inter-dependencies between variables, may be defined or redefined at any point during their manipulation; this leads to a degree of flexibility in computations that cannot be matched by auto-differentiation. For example, one could perform the permanent substitution \(g(x) = \dfrac{d f(x, y)}{d x} \vert_{y=1}\) and then recompute \(g(x)\) for several different values of \(x\). One could also post-factum express an interdependency between <code>x</code> and <code>y</code>, such as \(y \rightarrow y(x) := 2x\). For such a case, this means that the initially computed derivatives \(\dfrac{d f(x, y)}{d x} \rightarrow \dfrac{\partial f(x, y(x))}{\partial x} = 2y(x) [2x+1]^{y(x)-1} = 4x[2x+1]^{2x-1}\) and \(\dfrac{d f(x, y)}{d y} \rightarrow \dfrac{\partial f(x, y(x))}{\partial y} = [2x+1]^{y(x)} \ln(2x+1) = [2x+1]^{2x} \ln(2x+1)\) truly represent partial derivatives rather than total derivatives. Of course, if such an inter-dependency was explicitly defined before the derivatives \(\dfrac{d f(x, y(x))}{d x}\) and \(\dfrac{d f(x, y(x))}{d y}\) are computed, then this could correspond to the total derivative (which is the only result that auto-differentiation is able to achieve for this example).</p>
<p>Due to the sophisticated CAS that forms the foundation of symbolic operations, the types of manipulations are not necessarily restricted to differentiation alone, but rather may span a spectrum of manipulations relevant to discrete differential calculus, topics in pure mathematics, and more. The documentation for the <a href="https://www.sympy.org/en/index.html">SymPy</a> library gives plenty of examples that highlight what a fully-fledged CAS is capable of. Through the <a class="el" href="classDifferentiation_1_1SD_1_1Expression.html">Differentiation::SD::Expression</a> class, and the associated functions in the <a class="el" href="namespaceDifferentiation_1_1SD.html">Differentiation::SD</a> namespace, we provide a wrapper to the high-performance <a href="https://github.com/symengine/symengine">SymEngine</a> symbolic manipulation library that has enriched operator overloading and a consistent interface that makes it easy and "natural" to use. In fact, this class can be used as a "drop-in" replacement for arithmetic types in many situations, transforming the operations from being numeric to symbolic in nature; this is made especially easy when classes are templated on the underlying number type. Being focused on numerical simulation of PDEs, the functionality of the CAS that is exposed within deal.II focuses on symbolic expression creation, manipulation, and differentiation.</p>
<p>The convenience wrappers to SymEngine functionality are primarily focused on manipulations that solely involve dictionary-based (i.e., something reminiscent of "string-based") operations. Although SymEngine performs these operations in an efficient manner, they are still known to be computationally expensive, especially when the operations are performed on large expressions. It should therefore be expected that the performance of the parts of code that perform differentiation, symbolic substitution, etc., <b>may</b> be a limiting factor when using this in production code. deal.II therefore provides an interface to accelerate the evaluation of lengthy symbolic expression through the <code>BatchOptimizer</code> class (itself often leveraging functionality provided by SymEngine). In particular, the <code>BatchOptimizer</code> simultaneously optimizes a collection of symbolic expressions using methods such as common subexpression elimination (CSE), as well as by generating high performance code-paths to evaluate these expressions through the use of a custom-generated <code>std::function</code> or by compiling the expression using the LLVM JIT compiler.</p>
<p>As a final note, it is important to recognize the remaining major deficiencies in deal.II's current implementation of the interface to the supported symbolic library. The level of functionality currently implemented effectively limits the use of symbolic algebra to the traditional use case (i.e. scalar and tensor algebra, as might be useful to define constitutive relations or complex functions for application as boundary conditions or source terms). In the future we will also implement classes to assist in performing assembly operations in the same spirit as that which has been done in the <a class="el" href="namespaceDifferentiation_1_1AD.html">Differentiation::AD</a> namespace.</p>
<p>A summary of the files that implement the interface to the supported symbolic differentiable numbers is as follows:</p><ul>
<li><a class="el" href="symengine__math_8h.html">symengine_math.h</a>: Implementation of math operations that allow the class that implements symbolic expressions to be used consistently throughout the library and in user code. It provides counterpart definitions for many of the math functions found in the standard namespace.</li>
<li><a class="el" href="symengine__number__traits_8h.html">symengine_number_traits.h</a>: Provides some mechanisms to easily query select properties of symbolic numbers, i.e. some type traits.</li>
<li><a class="el" href="symengine__number__types_8h.html">symengine_number_types.h</a>: Implementation of the <a class="el" href="classDifferentiation_1_1SD_1_1Expression.html">Differentiation::SD::Expression</a> class that can be used to represent scalar symbolic variables, scalar symbolic expressions, and more. This Expression class has been given a full set of operators overloaded for all mathematical and logical operations that are supported by the SymEngine library and are considered useful within the context of numerical modeling.</li>
<li><a class="el" href="symengine__optimizer_8h.html">symengine_optimizer.h</a>: Implementation of the <a class="el" href="classDifferentiation_1_1SD_1_1BatchOptimizer.html">Differentiation::SD::BatchOptimizer</a> class that can be used to accelerate (in some cases, significantly) evaluation of the symbolic expressions using an assortment of techniques.</li>
<li><a class="el" href="symengine__product__types_8h.html">symengine_product_types.h</a>: Defines some product and scalar types that allow the use of symbolic expressions in conjunction with the <a class="el" href="classTensor.html">Tensor</a> and <a class="el" href="classSymmetricTensor.html">SymmetricTensor</a> classes.</li>
<li><a class="el" href="symengine__scalar__operations_8h.html">symengine_scalar_operations.h</a>: Defines numerous operations that can be performed either on or with scalar symbolic expressions or variables. This includes (but is not limited to) the creation of scalar symbols, performing differentiation with respect to scalars, and symbolic substitution within scalar expressions.</li>
<li><a class="el" href="symengine__tensor__operations_8h.html">symengine_tensor_operations.h</a>: Defines numerous operations that can be performed either on or with tensors of symbolic expressions or variables. This includes (but is not limited to) the creation of tensors of symbols, performing differentiation with respect to tensors of symbols, differentiation of tensors of symbols, and symbolic substitution within tensor expressions.</li>
<li><a class="el" href="symengine__types_8h.html">symengine_types.h</a>: Provides aliases for some types that are commonly used within the context of symbolic computations.</li>
<li><a class="el" href="symengine__utilities_8h.html">symengine_utilities.h</a>: Provides some utility functions that are useful within the context of symbolic computations. </li>
</ul>
</div><!-- contents -->
<!-- HTML footer for doxygen 1.8.13-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
