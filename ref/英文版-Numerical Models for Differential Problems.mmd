
\title{
Numerical Models for Differential Problems
}

\author{
Alfio Quarteroni
翻译：Jiaqi
}



\textbf{Preface to the second edition}

Differential equations (DEs) are the foundation on which many mathematical models for reallife applications are built. These equations can seldom be solved in 'closed' form: in fact, the exact solution can rarely be characterized through explicit, and easily computable, mathematical formulae. Almost invariably one has to resort to appropriate numerical methods, whose scope is the approximation (or discretization) of the exact differential model and, hence, of the exact solution.

This is the second edition of a book that first appeared in $2009$. It presents in a comprehensive and self-contained way some of the most successful numerical methods for handling DEs, for their analysis and their application to classes of problems that typically show up in the applications.

Although we mostly deal with partial differential equations (PDEs), both for steady problems (in multiple space dimensions) and time-dependent problems (with one or several space variables), part of the material is specifically devoted to ordinary differential equations (ODEs) for one-dimensional boundary-value problems, especially when the discussion is interesting in itself or relevant to the PDE case.

The primary concern is on the finite-element (FE) method, which is the most popular discretization technique for engineering design and analysis. We also address other techniques, albeit to a lesser extent, such as finite differences (FD), finite volumes (FV), and spectral methods, including further $a d$-hoc methods for specific types of problems. The comparative assessment of the performance of different methods is discussed, especially when it sheds light on their mutual interplay.

We also introduce and analyze numerical strategies aimed at reducing the computational complexity of differential problems: these include operator-splitting and fractional-step methods for time discretization, preconditioning, techniques for grid adaptivity, domain decomposition (DD) methods for parallel computing, and reduced-basis (RB) methods for solving parametrized PDEs efficiently.

Besides the classical elliptic, parabolic and hyperbolic linear equations, we treat more involved model problems that arise in a host of applicative fields: linear and nonlinear conservation laws, advection-diffusion equations with dominating advection, Navier-Stokes equations, saddle-point problems and optimal-control problems.

Here is the contents' summary of the various chapters. Chapter 1 briefly surveys PDEs and their classification, while Chapter 2 introduces the main notions and theoretical results of functional analysis that are extensively used throughout the book.

In Chapter 3 we illustrate boundary-value problems for elliptic equations (in one and several dimensions), present their weak or variational formulation, treat boundary conditions and analyze well-posedness. Several examples of physical interest are introduced.

The book's first cornerstone is Chapter 4 , where we formulate Galerkin's method for the numerical discretization of elliptic boundary-value problems and analyze it in an abstract functional setting. We then introduce the Galerkin $\mathrm{FE}$ method, first in one dimension, for the reader's convenience, and then in several dimensions. We construct FE spaces and FE interpolation operators, prove stability and convergence results and derive several kinds of error estimates. Eventually, we present grid-adaptive procedures based on either a priori or a posteriori error estimates.

The numerical approximation of parabolic problems is explained in Chapter 5: we begin with semi-discrete (continuous in time) Galerkin approximations, and then consider fully-discrete approximations based on FD schemes for time discretization. For both approaches stability and convergence are proven.

Chapters 6,7 and 8 are devoted to the algorithmic features and the practical implementation of FE methods. More specifically, Chapter 6 illustrates the main techniques for grid generation, Chapter 7 surveys the basic algorithms for the solution of ill-conditioned linear algebraic systems that arise from the approximation of PDEs, and Chapter 8 presents the main operational phases of a FE code, together with a complete working example.

The basic principles underlying finite-volume methods for the approximation of diffusiontransport-reaction equations are discussed in Chapter 9. FV methods are commonly used in computational fluid dynamics owing to their intrinsic, built-in conservation properties.

Chapter 10 addresses the multi-faceted aspects of spectral methods (Galerkin, collocation, and the spectral-element method), analyzing thoroughly the reasons for their superior accuracy properties.

Galerkin discretization techniques relying on discontinuous polynomial subspaces are the subject of Chapter 12. We present, more specifically, the discontinuous Galerkin (DG) method and the mortar method, together with their use in the context of finite elements or spectral elements.

Chapter 13 focuses on singularly perturbed elliptic boundary-value problems, in particular diffusion-transport equations and diffusion-reaction equations, with small diffusion. The exact solutions to this type of problems can exhibit steep gradients in tiny subregions of the computational domains, the so-called internal or boundary layers. A great deal of attention is paid to stabilization techniques meant to prevent the on-rise of oscillatory numerical solutions. Upwinding techniques are discussed for FD approximations, and their analogy with $\mathrm{FE}$ with artificial diffusion is analyzed. We introduce and discuss other stabilization approaches in the $\mathrm{FE}$ context, as well, which lead to the sub-grid generalized Galerkin methods, the Petrov-Galerkin methods and Galerkin's Least-Squares method.

The ensuing three chapters form a thematic unit focusing on the approximation of first-order hyperbolic equations. Chapter 14 addresses classical FD methods. Stability is investigated using both the energy method and the Von Neumann analysis. Using the latter we also analyze the properties of dissipation and dispersion featured by a numerical scheme. Chapter 15 is devoted to spatial approximation by $\mathrm{FE}$ methods, including the DG methods and spectral methods. Special emphasis is put on characteristic compatibility conditions for the boundary treatment of hyperbolic systems. A very quick overview of the numerical approximation of nonlinear con- servation laws is found in Chapter 16 . Due to the relevance of this particular topic the interested reader is advised to consult the specific monographs mentioned in the references.

In Chapter 17 we discuss the Navier-Stokes equations for incompressible flows, plus their numerical approximation by FE, FV and spectral methods. A general stability and convergence theory is developed for spatial approximation of saddle-point problems, which comprises strategies for stabilization. Next we propose and analyze a number of time-discretization approaches, among which finite differences, characteristic methods, fractional-step methods and algebraic factorization techniques. Special attention is devoted to the numerical treatment of interfaces in the case of multiphase flows.

Chapter 18 discusses the issue of optimal control for elliptic PDEs. The problem is first formulated at the continuous level, where conditions of optimality are obtained using two different methods. Then we address the interplay between optimization and numerical approximation. We present several examples, some of them elementary in character, others involving physical processes of applicative relevance.

Chapter 19 regards domain-decomposition methods. These techniques are specifically devised for parallel computing and for the treatment of multiphysics' PDE problems. The families of Schwarz methods (with overlapping subdomains) and Schur methods (with disjoint subdomains) are illustrated, and their convergence properties of optimality (grid invariance) and scalability (subdomain-size invariance) studied. Several examples of domain-decomposition preconditioners are provided and tested numerically.

Finally, in Chapter 20 we introduce the reduced-basis $(\mathrm{RB})$ method for the efficient solution of PDEs. RB methods allow for the rapid and reliable evaluation of input/output relationships in which the output is expressed as a functional of a field variable that is the solution of a parametrized PDE. Parametrized PDEs model several processes relevant in applications such as steady and unsteady transfer of heat or mass, acoustics, solid and fluid mechanics, to mention a few. The input-parameter vector variously characterizes the geometric configuration of the domain, physical properties, boundary conditions or source terms. The combination with an efficient a posteriori error estimate, and the splitting between offline and online calculations, are key factors for $\mathrm{RB}$ methods to be computationally successful.

Many important topics that would have deserved a proper treatment were touched only partially (in some cases completely ignored). This depends on the desire to offer a reasonablysized textbook on one side, and our own experience on the other. The list of notable omissions includes, for instance, the approximation of equations for the structural analysis and the propagation of electromagnetic waves. Detailed studies can be found in the references' specialized literature.

This text is intended primarily for graduate students in Mathematics, Engineering, Physics and Computer Science and, more generally, for computational scientists. Each chapter is meant to provide a coherent teaching unit on a specific subject. The first eight chapters, in particular, should be regarded as a comprehensive and self-contained treatise on finite elements for elliptic and parabolic PDEs. Chapters 9-17 represent an advanced course on numerical methods for PDEs, while the last three chapters contain more subtle and sophisticated topics for the numerical solution of complex PDE problems.

This work has been used as a textbook for graduate-level courses at the Politecnico di Milano and the École Polytechnique Fédérale de Lausanne. We would like to thank the many people - students, colleagues and readers - who contributed, at various stages and in many different ways, to its preparation and to the improvement of early drafts. A (far from complete) list includes Paola Antonietti, Luca Dedè, Marco Discacciati, Luca Formaggia, Loredana Chapter 1

\section{A brief survey of partial differential equations}

The purpose of this chapter is to recall the basic concepts related to partial differential equations (PDEs, in short). For a wider coverage see [RR04, Eva98, LM68, Sal08].

\subsection{Definitions and examples}

Partial differential equations are differential equations containing derivatives of the unknown function with respect to several variables (temporal or spatial). In particular, if we denote by $u$ the unknown function in the $d+1$ independent variables $\mathbf{x}=$ $\left(x_{1}, \ldots, x_{d}\right)^{T}$ and $t$, we denote by

$$
\mathscr{P}(u, g)=F\left(\mathbf{x}, t, u, \frac{\partial u}{\partial t}, \frac{\partial u}{\partial x_{1}}, \ldots, \frac{\partial u}{\partial x_{d}}, \ldots, \frac{\partial^{p_{1}+\cdots+p_{d}+p_{t}} u}{\partial x_{1}^{p_{1}} \ldots \partial x_{d}^{p_{d}} \partial t^{p_{t}}}, g\right)=0
$$

a generic PDE, $g$ being the set of data on which the PDE depends, while $p_{1}, \ldots, p_{d}$, $p_{t} \in \mathbb{N}$

We say that (1.1) is of order $q$ if $q$ is the maximum order of the partial derivatives appearing in the equation, i.e. the maximum value taken by the integer $p_{1}+p_{2}+$ $\ldots+p_{d}+p_{t}$.

If $(1.1)$ depends linearly on the unknown $u$ and on its derivatives, the equation is said to be linear. In the particular case where the derivatives having maximal order only appear linearly (with coefficients which may depend on lower-order derivatives), the equation is said to be quasi-linear. It is said to be semi-linear when it is quasi-linear and the coefficients of the maximal order derivatives only depend on $\mathbf{x}$ and $t$, and not on the solution $u$. Finally, if the equation contains no terms which are independent of the unknown function $u$, the PDE is said to be homogeneous.

We list below some examples of PDEs frequently encountered in the applied sciences.

Example 1.1. A first-order linear equation is the transport (or advection) equation

$$
\frac{\partial u}{\partial t}+\nabla \cdot(\boldsymbol{\beta} u)=0
$$

having denoted by

$$
\nabla \cdot \mathbf{v}=\operatorname{div}(\mathbf{v})=\sum_{i=1}^{d} \frac{\partial v_{i}}{\partial x_{i}}, \quad \mathbf{v}=\left(v_{1}, \ldots, v_{d}\right)^{T}
$$

the divergence operator. Integrated on a region $\Omega \subset \mathbb{R}^{d},(1.2)$ expresses the mass conservation of a material system (a continuous media) occupying the region $\Omega$. The $u$ variable is the system's density, while $\boldsymbol{\beta}(\mathbf{x}, \mathbf{t})$ is the velocity of a particle in the system that occupies position $\mathbf{x}$ at time $t$.

Example 1.2. Linear second-order equations include: the potential equation

$$
-\Delta u=f,
$$

that describes the diffusion of a fluid in a homogeneous and isotropic region $\Omega \subset \mathbb{R}^{d}$, but also the vertical displacement of an elastic membrane; the heat (or diffusion) equation

$$
\frac{\partial u}{\partial t}-\Delta u=f
$$

the wave equation

$$
\frac{\partial^{2} u}{\partial t^{2}}-\Delta u=0
$$

We have denoted by

$$
\Delta u=\sum_{i=1}^{d} \frac{\partial^{2} u}{\partial x_{i}^{2}}
$$

the Laplace operator (Laplacian).

Example 1.3. An example of a quasi-linear first-order equation is Burgers' equation

$$
\frac{\partial u}{\partial t}+u \frac{\partial u}{\partial x_{1}}=0
$$

while its variant obtained by adding a second-order perturbation

$$
\frac{\partial u}{\partial t}+u \frac{\partial u}{\partial x_{1}}=\varepsilon \frac{\partial^{2} u}{\partial x_{1}^{2}}, \quad \varepsilon>0
$$

is an example of a semi-linear equation.

Another second-order, non-linear equation, is

$$
\left(\frac{\partial^{2} u}{\partial x_{1}^{2}}\right)^{2}+\left(\frac{\partial^{2} u}{\partial x_{2}^{2}}\right)^{2}=f
$$

A function $u=u\left(x_{1}, \ldots, x_{d}, t\right)$ is said to be a solution (or a particular integral) of (1.1) if it makes (1.1) an identity once it is replaced in (1.1) together with all of its derivatives. The set of all solutions of $(1.1)$ is called the general integral of $(1.1)$.

Example 1.4. The transport equation in the one-dimensional case,

$$
\frac{\partial u}{\partial t}-\frac{\partial u}{\partial x_{1}}=0
$$

admits a general integral of the form $u=w\left(x_{1}+t\right), w$ being a sufficiently regular arbitrary function (see Exercise 2). Similarly, the one-dimensional wave equation

$$
\frac{\partial^{2} u}{\partial t^{2}}-\frac{\partial^{2} u}{\partial x_{1}^{2}}=0
$$

admits as a general integral

$$
u\left(x_{1}, t\right)=w_{1}\left(x_{1}+t\right)+w_{2}\left(x_{1}-t\right)
$$

$w_{1}$ and $w_{2}$ being two sufficiently regular arbitrary functions (see Exercise 3 ).

Example 1.5. Let us consider the one-dimensional heat equation

$$
\frac{\partial u}{\partial t}-\frac{\partial^{2} u}{\partial x_{1}^{2}}=0
$$

for $0<x_{1}<1$ and $t>0$, with boundary conditions

$$
u(0, t)=u(1, t)=0, \quad t>0
$$

and initial condition $\left.u\right|_{t=0}=u_{0}$. Its solution is

$$
u\left(x_{1}, t\right)=\sum_{j=1}^{\infty} u_{0, j} e^{-(j \pi)^{2} t} \sin \left(j \pi x_{1}\right)
$$

where $u_{0}=u_{\mid t=0}$ is the initial datum and

$$
u_{0, j}=2 \int_{0}^{1} u_{0}\left(x_{1}\right) \sin \left(j \pi x_{1}\right) d x_{1}, \quad j=1,2, \ldots
$$

\subsection{Numerical solution}

In general, it is not possible to obtain a solution of (1.1) in closed (explicit) form. Indeed, the available analytical integration methods (such as the technique of separation of variables) are of limited applicability. On the other hand, even in the case where a general integral is known, it is not guaranteed that a particular integral may be determined. Indeed, in order to obtain the latter, it will be necessary to assign appropriate conditions on $u$ (and/or its derivatives) at the boundary of the domain $\Omega$. Besides, from the examples provided it is evident that the general integral depends on a number of arbitrary functions (and not on arbitrary constants, as it happens for ordinary differential equations), so that the imposition of the boundary conditions will result in the solution of mathematical problems that are generally rather involved.

Thus, from a theoretical point of view, the analysis of a given PDE is often bound to investigating existence, uniqueness, and, possibly, regularity of its solutions, but lacks practical tools for their actual determination.

It follows that it is extremely important to have numerical methods at one's disposal, that allow to construct an approximation $u_{N}$ of the exact solution $u$ and to evaluate (in some suitable norm) the error $u_{N}-u$ when substituting to the exact solution $u$ the approximate solution $u_{N}$. In general, $N \geq 1$ is a positive integer that denotes the (finite) dimension of the approximate problem. Schematically, we will obtain the following situation:

We have denoted by $g_{N}$ an approximation of the set of data $g$ on which the PDE depends, and with $\mathscr{P}_{N}$ the new functional relation characterizing the approximated problem. For simplicity, one writes $u=u(g)$ and $u_{N}=u_{N}\left(g_{N}\right)$.

We will present several numerical methods starting from Chap. 4. Here, we only recall their main features. A numerical method is convergent if

$$
\left\|u-u_{N}\right\| \rightarrow 0 \quad \text { as } N \rightarrow \infty
$$

for a given norm. More precisely, we have convergence if and only if

$$
\begin{aligned}
\forall \varepsilon>0, \exists N_{0}=N_{0}(\varepsilon)>0, \exists \delta=\delta\left(N_{0}, \varepsilon\right): \forall N>N_{0}, \forall g_{N} \text { such that }\left\|g-g_{N}\right\|<\delta \\
&\left\|u(g)-u_{N}\left(g_{N}\right)\right\| \leq \varepsilon
\end{aligned}
$$

(The norm used for the data is not necessarily the same as that used for the solutions.) A direct verification of the convergence of a numerical method may not be easy. A verification of its consistency and stability properties is recommendable, instead. A numerical method is said to be consistent if

$$
\mathscr{P}_{N}(u, g) \rightarrow 0 \text { as } N \rightarrow \infty
$$

and strongly consistent (or fully consistent) if

$$
\mathscr{P}_{N}(u, g)=0 \quad \forall N \geq 1
$$

Notice that (1.9) can be equivalently formulated as

$$
\mathscr{P}_{N}(u, g)-\mathscr{P}(u, g) \rightarrow 0 \text { as } N \rightarrow \infty
$$

This expresses the property that $\mathscr{P}_{N}$ (the approximated PDE) "tends" to $\mathscr{P}$ (the exact one) as $N \rightarrow \infty$. Instead, we say that a numerical method is stable if to small perturbations to the data correspond small perturbations to the solution. More precisely,

$$
\forall \varepsilon>0, \exists \delta=\delta(\varepsilon)>0: \forall \delta g_{N}:\left\|\delta g_{N}\right\|<\delta \Rightarrow\left\|\delta u_{N}\right\| \leq \varepsilon, \forall N \geq 1
$$

$u_{N}+\delta u_{N}$ being the solution of the perturbed problem

$$
\mathscr{P}_{N}\left(u_{N}+\delta u_{N}, g_{N}+\delta g_{N}\right)=0
$$

(See also [QSS07, Chap. 2] for an in-depth coverage.)

The fundamental result, often quoted as the Lax-Richtmyer equivalence theorem, finally guarantees that

Theorem 1.1. If a method is consistent and stable, then it is convergent.

Other important properties will obviously influence the choice of a numerical method, such as its convergence rate (i.e. the order with respect to $1 / N$ with which the error tends to zero) and its computational cost, that is the computation time and memory required to implement such method on the computer.

\subsection{PDE Classification}

Partial differential equations can be classified into three different families: elliptic, parabolic and hyperbolic equations, for each of which appropriate specific numerical methods will be considered. For the sake of brevity, here we will limit ourselves to the case of a linear second-order PDE, with constant coefficients, of the form $L u=G$,

$$
L u=A \frac{\partial^{2} u}{\partial x_{1}^{2}}+B \frac{\partial^{2} u}{\partial x_{1} \partial x_{2}}+C \frac{\partial^{2} u}{\partial x_{2}^{2}}+D \frac{\partial u}{\partial x_{1}}+E \frac{\partial u}{\partial x_{2}}+F u
$$

with assigned function $G$ and $A, B, C, D, E, F \in \mathbb{R}$. (Notice that any of the $x_{i}$ variables could represent the temporal variable.) In that case, the classification is carried out based on the sign of the discriminant, $\triangle=B^{2}-4 A C$. In particular:

$$
\begin{array}{ll}
\text { if } \triangle<0 & \text { the equation is said to be elliptic, } \\
\text { if } \triangle=0 & \text { the equation is said to be parabolic, } \\
\text { if } \triangle>0 & \text { the equation is said to be hyperbolic. }
\end{array}
$$

Example 1.6. The wave equation (1.8) is hyperbolic, while the potential equation (1.3) is elliptic. An example of a parabolic problem is given by the heat equation (1.4), but also by the following diffusion-transport equation

$$
\frac{\partial u}{\partial t}-\mu \Delta u+\nabla \cdot(\beta u)=0
$$

where the constant $\mu>0$ and the vector field $\boldsymbol{\beta}$ are given. The criterion introduced above makes the classification depend on the sole coefficients of the highest derivatives and is justified via the following argument. As the reader will recall, the quadratic algebraic equation

$$
A x_{1}^{2}+B x_{1} x_{2}+C x_{2}^{2}+D x_{1}+E x_{2}+F=G
$$

represents a hyperbola, a parabola or an ellipse in the Cartesian plane $\left(x_{1}, x_{2}\right)$ depending whether $\triangle$ is positive, null or negative. This parallel motivates the name assigned to the three classes of partial derivative operators.

Let us investigate the difference between the three classes more attentively. Let us suppose, without this being restrictive, that $D, E, F$ and $G$ be null. We look for a change of variables of the form

$$
\xi=\alpha x_{2}+\beta x_{1}, \quad \eta=\gamma x_{2}+\delta x_{1}
$$

with $\alpha, \beta, \gamma$ and $\delta$ to be chosen so that $L u$ becomes a multiple of $\partial^{2} u / \partial \xi \partial \eta$. Since

$$
\begin{aligned}
&L u=\left(A \beta^{2}+B \alpha \beta+C \alpha^{2}\right) \frac{\partial^{2} u}{\partial \xi^{2}} \\
&+(2 A \beta \delta+B(\alpha \delta+\beta \gamma)+2 C \alpha \gamma) \frac{\partial^{2} u}{\partial \xi \partial \eta}+\left(A \delta^{2}+B \gamma \delta+C \gamma^{2}\right) \frac{\partial^{2} u}{\partial \eta^{2}}
\end{aligned}
$$

we need to require that

$$
A \beta^{2}+B \alpha \beta+C \alpha^{2}=0, \quad A \delta^{2}+B \gamma \delta+C \gamma^{2}=0
$$

If $A=C=0$, the trivial trasformation $\xi=x_{2}, \eta=x_{1}$ (for instance) provides $L u$ in the desired form.

Let us then suppose that $A$ or $C$ be not null. It is not restrictive to suppose $A \neq 0$. Then, if $\alpha \neq 0$ and $\gamma \neq 0$, we can divide the first equation of $(1.14)$ by $\alpha^{2}$ and the second one by $\gamma^{2}$. We find two identical quadratic equations for the ratios $\beta / \alpha$ and $\delta / \gamma$. By solving them, we have

$$
\frac{\beta}{\alpha}=\frac{1}{2 A}[-B \pm \sqrt{\triangle}], \quad \frac{\delta}{\gamma}=\frac{1}{2 A}[-B \pm \sqrt{\triangle}]
$$

In order for the transformation (1.12) to be non-singular, the quotients $\beta / \alpha$ and $\delta / \gamma$ must be different. We must therefore take the positive sign in one case, and the negative sign in the other. Moreover, we must assume $\triangle>0$. If $\triangle$ were indeed null, the two fractions would still be coincident, while if $\triangle$ were negative none of the two fractions could be real. To conclude, we can take the following values as coefficients of transformation (1.12):

$$
\alpha=\gamma=2 A, \quad \beta=-B+\sqrt{\triangle}, \quad \delta=-B-\sqrt{\triangle}
$$

Correspondingly, (1.12) becomes

$$
\xi=2 A x_{2}+[-B+\sqrt{\triangle}] x_{1}, \quad \eta=2 A x_{2}+[-B-\sqrt{\triangle}] x_{1}
$$

and, after the transformation, the original differential problem $L u=0$ becomes

$$
L u=-4 A \triangle \frac{\partial^{2} u}{\partial \xi \partial \eta}=0
$$

(For ease of notation, we still denote by $u$ the transformed solution and by $L$ the transformed differential operator.) The case $A=0$ and $C \neq 0$ can be treated in a similar way by taking $\xi=x_{1}, \eta=x_{2}-(C / B) x_{1}$.

To conclude, the original term $L u$ can become a multiple of $\partial^{2} u / \partial \xi \partial \eta$ based on the transformation (1.12) if and only if $\triangle>0$, and in such case, as we have anticipated, the problem is said to be hyperbolic. It is easy to verify that the general solution of problem (1.15) is

$$
u=p(\xi)+q(\eta)
$$

$p$ and $q$ being arbitrary differentiable functions in one variable. The lines $\xi=$ constant and $\eta=$ constant are said to be the characteristics of $L$ and are characterized by the fact that on these lines, the functions $p$ and $q$, respectively, remain constant. In particular, possible discontinuities of the solution $u$ propagate along the characteristic lines (this will be shown in more detail in Chap. 14). Indeed, if $A \neq 0$, by identifying $x_{1}$ with $t$ and $x_{2}$ with $x$, the transformation

$$
x^{\prime}=x-\frac{B}{2 A} t, \quad t^{\prime}=t
$$

transforms the hyperbolic operator $L$ such that

$$
L u=A \frac{\partial^{2} u}{\partial t^{2}}+B \frac{\partial^{2} u}{\partial t \partial x}+C \frac{\partial^{2} u}{\partial x^{2}}
$$

in a multiple of the wave operator $L$ such that

$$
L u=\frac{\partial^{2} u}{\partial t^{2}}-c^{2} \frac{\partial^{2} u}{\partial x^{2}}, \text { with } c^{2}=\triangle / 4 A^{2}
$$

The latter is the wave operator in a coordinate system moving with velocity $-B / 2 A$. The characteristic lines of the wave operator are the lines verifying

$$
\left(\frac{d t}{d x}\right)^{2}=\frac{1}{c^{2}}
$$

that is

$$
\frac{d t}{d x}=\frac{1}{c} \quad \text { and } \quad \frac{d t}{d x}=-\frac{1}{c} .
$$

When $\triangle=0$, as previously stated $L$ is parabolic. In this case there exists only one value of $\beta / \alpha$ in corrispondence of which the coefficient of $\partial^{2} u / \partial \xi^{2}$ in $(1.13)$ becomes zero: precisely, $\beta / \alpha=-B /(2 A)$. On the other hand, since $B /(2 A)=2 C / B$, this choice also implies that the coefficient of $\partial^{2} u / \partial \xi \partial \eta$ becomes zero. Hence, the change of variables

$$
\xi=2 A x_{2}-B x_{1}, \quad \eta=x_{1}
$$

transforms the original problem $L u=0$ into the following

$$
L u=A \frac{\partial^{2} u}{\partial \eta^{2}}=0
$$

the general solution of which has the form

$$
u=p(\xi)+\eta q(\xi)
$$

A parabolic operator therefore has only one family of characteristics, precisely $\xi=$ constant. The discontinuities in the derivatives of $u$ propagate along such characteristic lines.

Finally, if $\triangle<0$ (elliptic operators) there does not exist any choice of $\beta / \alpha$ or $\delta / \gamma$ that makes the coefficients $\partial^{2} u / \partial \xi^{2}$ and $\partial^{2} u / \partial \eta^{2}$ null. However, the transformation

$$
\xi=\frac{2 A x_{2}-B x_{1}}{\sqrt{-\triangle}}, \quad \eta=x_{1}
$$

transforms $L u=0$ into

$$
L u=A\left(\frac{\partial^{2} u}{\partial \xi^{2}}+\frac{\partial^{2} u}{\partial \eta^{2}}\right)=0
$$

i.e. a multiple of the potential equation. The latter has therefore no family of characteristic lines.

\subsubsection{Quadratic form associated to a PDE}

We can associate to equation (1.11) the so-called principal symbol $S^{p}$ defined by

$$
S^{p}(\mathbf{x}, \mathbf{q})=-A(\mathbf{x}) q_{1}^{2}-B(\mathbf{x}) q_{1} q_{2}-C(\mathbf{x}) q_{2}^{2}
$$

This quadratic form can be represented in matrix form as follows:

$$
S^{p}(\mathbf{x}, \mathbf{q})=\mathbf{q}^{T}\left[\begin{array}{cc}
-A(\mathbf{x}) & -\frac{1}{2} B(\mathbf{x}) \\
-\frac{1}{2} B(\mathbf{x}) & -C(\mathbf{x})
\end{array}\right] \mathbf{q}
$$

A quadratic form is said to be definite if all of the eigenvalues of its associated matrix have the same sign (either positive or negative); it is indefinite if the matrix has eigenvalues of both signs; it is degenerate if the matrix is singular.

It can then be said that equation (1.11) is elliptic if its quadratic form (1.16) is definite (positive or negative), hyperbolic if it is indefinite, and parabolic if it is degenerate. The matrices associated to the potential equation (1.3), the (one-dimensional) heat equation (1.4) and the wave equation (1.5) are given respectively by

$$
\left[\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right],\left[\begin{array}{ll}
0 & 0 \\
0 & 1
\end{array}\right] \text { and }\left[\begin{array}{cc}
-1 & 0 \\
0 & 1
\end{array}\right]
$$

and are positive definite in the first case, singular in the second case, and indefinite in the third case.

\subsection{Exercises}

1. Classify the following equations based on their order and linearity:

$$
\begin{aligned}
&\text { (a) }\left[1+\left(\frac{\partial u}{\partial x_{1}}\right)^{2}\right] \frac{\partial^{2} u}{\partial x_{2}^{2}}-2 \frac{\partial u}{\partial x_{1}} \frac{\partial u}{\partial x_{2}} \frac{\partial^{2} u}{\partial x_{1} \partial x_{2}}+\left[1+\left(\frac{\partial u}{\partial x_{2}}\right)^{2}\right] \frac{\partial^{2} u}{\partial x_{1}^{2}}=0 \\
&\text { (b) } \rho \frac{\partial^{2} u}{\partial t^{2}}+K \frac{\partial^{4} u}{\partial x_{1}^{4}}=f \\
&\text { (c) }\left(\frac{\partial u}{\partial x_{1}}\right)^{2}+\left(\frac{\partial u}{\partial x_{2}}\right)^{2}=f
\end{aligned}
$$

[Solution: $(a)$ quasi-linear, second-order; it is Plateau's equation which governs, under appropriate hypotheses, the plane motion of a fluid. The $u$ appearing in the equation is the so-called kinetic potential; $(b)$ linear, fourth-order. It is the vibrating rod equation, $\rho$ is the rod's density, while $K$ is a positive quantity that depends on the geometrical properties of the rod itself; $(c)$ non-linear, first-order.]

2. Reduce the one-dimensional transport equation (1.7) to an equation of the form $\partial w / \partial y=0$, having set $y=x_{1}-t$, and obtain that $u=w\left(x_{1}+t\right)$ is a solution of the original equation.

[Solution: operate the substitution of variables $z=x_{1}+t, y=x_{1}-t, u\left(x_{1}, t\right)=$ $w(y, z)$. In such way $\partial u / \partial x_{1}=\partial w / \partial z+\partial w / \partial y$, where $\partial u / \partial t=\partial w / \partial z-\partial w / \partial y$, and thus $-2 \partial w / \partial y=0$. Note at this point that the equation obtained thereby admits a solution $w(y, z)$ that does not depend on $y$ and, using the original variables, we get $\left.u=w\left(x_{1}+t\right) .\right]$

3. Prove that the wave equation

$$
\frac{\partial^{2} u}{\partial t^{2}}-c^{2} \frac{\partial^{2} u}{\partial x_{1}^{2}}=0
$$

with constant $c$, admits as a solution $u\left(x_{1}, t\right)=w_{1}\left(x_{1}+c t\right)+w_{2}\left(x_{1}-c t\right), w_{1}$ and $w_{2}$ being two sufficiently regular arbitrary functions. [Solution: proceed as in Exercise 2 , by applying the substitution of variables $y=$ $x_{1}+c t, z=x_{1}-c t$ and setting $\left.u\left(x_{1}, t\right)=w(y, z) .\right]$

4. Verify that the Korteveg-de-Vries equation

$$
\frac{\partial u}{\partial t}+\beta \frac{\partial u}{\partial x_{1}}+\alpha \frac{\partial^{3} u}{\partial x_{1}^{3}}=0
$$

admits a general integral of the form $u=a \cos \left(k x_{1}-\omega t\right)$ with an appropriate $\omega$ to be determined, and $a, \beta$ and $\alpha$ being assigned constants. This equation describes the position $u$ of a fluid with respect to a reference position, in the presence of long wave propagation.

[Solution: the given $u$ satisfies the equation only if $\omega=k \beta-\alpha k^{3}$.]

5. Consider the equation

$$
x_{1}^{2} \frac{\partial^{2} u}{\partial x_{1}^{2}}-x_{2}^{2} \frac{\partial^{2} u}{\partial x_{2}^{2}}=0
$$

with $x_{1} x_{2} \neq 0$. Classify it and determine its characteristic lines.

6. Consider the generic second-order semi-linear differential equation

$$
a\left(x_{1}, x_{2}\right) \frac{\partial^{2} u}{\partial x_{1}^{2}}+2 b\left(x_{1}, x_{2}\right) \frac{\partial^{2} u}{\partial x_{1} \partial x_{2}}+c\left(x_{1}, x_{2}\right) \frac{\partial^{2} u}{\partial x_{2}^{2}}+f(u, \nabla u)=0
$$

where $\nabla u=\left(\frac{\partial u}{\partial x_{1}}, \frac{\partial u}{\partial x_{2}}\right)^{T}$ is the gradient of $u$. Write the equation of its characteristic lines and deduce from it the classification of the proposed equation, by distinguishing the different cases.

7. Set $r(\mathbf{x})=|\mathbf{x}|=\left(x_{1}^{2}+x_{2}^{2}\right)^{1 / 2}$ and define $u(\mathbf{x})=\ln (r(\mathbf{x})), \mathbf{x} \in \mathbb{R}^{2} \backslash\{\mathbf{0}\}$. Verify that

$$
\Delta u(\mathbf{x})=0, \quad \mathbf{x} \in \Omega
$$

where $\Omega$ is any given open set such that $\bar{\Omega} \subset \mathbb{R}^{2} \backslash\{\mathbf{0}\}$. [Solution: observe that

$$
\left.\frac{\partial^{2} u}{\partial x_{i}^{2}}=\frac{1}{r^{2}}\left(1-\frac{2 x_{i}^{2}}{r^{2}}\right), \quad i=1,2 .\right]
$$



\section{Elements of functional analysis}

In this chapter we recall a number of concepts used extensively in this textbook: functionals and bilinear forms, distributions, Sobolev spaces, $\mathrm{L}^{p}$ spaces. For a more indepth reading, the reader can refer to e.g. [Sal08],[Yos74], [Bre86], [LM68], [Ada75].

\subsection{Functionals and bilinear forms}

The functional is often denoted as $F(v)=\langle F, v\rangle$, an expression called duality or crochet. A functional is said to be linear if it is linear with respect to its argument, that is if

$$
F(\lambda v+\mu w)=\lambda F(v)+\mu F(w) \quad \forall \lambda, \mu \in \mathbb{R}, \forall v, w \in V
$$

A linear functional is bounded if there is a constant $C>0$ such that

$$
|F(v)| \leq C\|v\|_{V} \quad \forall v \in V .
$$

A linear and bounded functional on a Banach space (i.e. a normed and complete space) is also continuous. We then define the space $V^{\prime}$, called dual of $V$, as the set of linear and bounded functionals on $V$, that is

$$
V^{\prime}=\{F: V \mapsto \mathbb{R} \text { such that } F \text { is linear and bounded }\}
$$

and we equip it with the norm $\|\cdot\|_{V^{\prime}}$ defined as

$$
\|F\|_{V^{\prime}}=\sup _{v \in V \backslash\{0\}} \frac{|F(v)|}{\|v\|_{V}} .
$$

The constant $C$ appearing in $(2.1)$ is greater than or equal to $\|F\|_{V^{\prime}}$

The following theorem, called identification or representation theorem ([Yos74]), holds.

If $H$ is a Hilbert space, its dual space $H^{\prime}$ of linear and bounded functionals on $H$ is a Hilbert space too. Moreover, thanks to Theorem $2.1$, there exists a bijective and isometric (i.e. norm-preserving) transformation $f \leftrightarrow x_{f}$ between $H^{\prime}$ and $H$ thanks to which $H^{\prime}$ and $H$ can be identified. We can denote this transformation as follows:

$$
\begin{array}{ll}
\Lambda_{H}: H \rightarrow H^{\prime}, & x \rightarrow f_{x}=\Lambda_{H} x \\
\Lambda_{H}^{-1}: H^{\prime} \rightarrow H, & f \rightarrow x_{f}=\Lambda_{H}^{-1} x
\end{array}
$$

We now introduce the notion of fom.

A form is called:

bilinear if it is linear with respect to both its arguments, i.e. if:

$$
\begin{array}{ll}
a(\lambda u+\mu w, v)=\lambda a(u, v)+\mu a(w, v) & \forall \lambda, \mu \in \mathbb{R}, \forall u, v, w \in V \\
a(u, \lambda w+\mu v)=\lambda a(u, v)+\mu a(u, w) & \forall \lambda, \mu \in \mathbb{R}, \forall u, v, w \in V
\end{array}
$$

continuous if there exists a constant $M>0$ such that

$$
|a(u, v)| \leq M\|u\|_{V}\|v\|_{V} \quad \forall u, v \in V
$$

symmetric if

$$
a(u, v)=a(v, u) \quad \forall u, v \in V ;
$$

positive (or positive definite) if

$$
a(v, v)>0 \quad \forall v \in V
$$

coercive if there exists a constant $\alpha>0$ such that

$$
a(v, v) \geq \alpha\|v\|_{V}^{2} \quad \forall v \in V
$$

Definition 2.3. Let $X$ and $Y$ be two Hilbert spaces. We say that $X$ is contained in $Y$ with continuous injection if there exists a constant $C$ such that $\|w\|_{Y} \leq C\|w\|_{X}$ $\forall w \in X$. Moreover $X$ is dense in $Y$ if each element belonging to $Y$ can be obtained as the limit, in the $\|\cdot\|_{Y}$ norm, of a sequence of elements of $X$.

Given two Hilbert spaces $V$ and $H$, such that $V \subset H$, the injection of $V$ in $H$ is continuous and moreover $V$ is dense in $H$. Then, upon identification of $H$ and $H^{\prime}$,

$$
V \subset H \simeq H^{\prime} \subset V^{\prime}
$$

For elliptic problems, the spaces $V$ and $H$ will typically be chosen respectively as $H^{1}(\Omega)$ (or one of its subspaces, $H_{0}^{1}(\Omega)$ or $\left.H_{\Gamma_{D}}^{1}(\Omega)\right)$ and $L^{2}(\Omega)$, see Chap. $3$.

Definition 2.4. A linear and bounded (hence continuous) operator $\mathscr{T}$ between two functional spaces $X$ and $Y$ is an isomorphism if it maps bijectively the elements of the spaces $X$ and $Y$ and its inverse $\mathscr{T}^{-1}$ exists. If also $X \subset Y$ holds, such isomorphism is called canonical.

\subsection{Differentiation in linear spaces}

In this section, we briefly report the notions of differentiability and differentiation for applications on linear functional spaces; for a further analysis of this topic, as well as an extension of such notions to more general cases, see [KF89].

Let us begin by considering the notion of strong (or Fréchet) differential:

Definition 2.5. Let $X$ and $Y$ be two normed linear spaces and $F$ an application

of $X$ in $Y$, defined on an open set $E \subset X ;$ such application is called differentiable We call the expression $L_{x} h$ (or $\left.L_{x}[h]\right)$, which generates an element in $Y$ for each We call the expression $L_{x} h$ (or $\left.L_{x}[h]\right)$, which generates an element in $Y$ for each
$h \in X$, strong differential (or Fréchet differential) of the application $F$ at $x \in E ;$ the
operator $L_{x}$ is called strong derivative of the application $F$ at $x$ and is generally
denoted as $F^{\prime}(x)$, that is $F^{\prime}(x)=L_{x}$.

From the definition, we deduce that a differentiable application in $x$ is also continuous in $x$. We list below some properties deriving from this definition:

- if $F(x)=$ constant, then $F^{\prime}(x)$ is the null operator, that is $L_{x}[h]=0 \forall h \in X$;

- the strong derivative of a continuous linear application $F(x)$ is the application itself, that is $F^{\prime}(x)=F(x)$;

- given two continuous applications $F$ and $G$ of $X$ in $Y$, if these are differentiable at $x_{0}$, so are the applications $F+G$ and $\alpha F$, for all $\alpha \in \mathbb{R}$, and we have:

$$
\begin{gathered}
(F+G)^{\prime}\left(x_{0}\right)=F^{\prime}\left(x_{0}\right)+G^{\prime}\left(x_{0}\right), \\
(\alpha F)^{\prime}\left(x_{0}\right)=\alpha F^{\prime}\left(x_{0}\right) .
\end{gathered}
$$

Consider now the following definition of weak (or Gâteaux) differential: norm of the space $Y$. If the weak differential $D F(x, h)$ is linear (in general it is

Note that if an application $F$ has a strong derivative, then it also admits a weak derivative, coinciding with the strong one; the converse instead is not generally true. However, the following theorem holds (see [KF89]): Theorem 2.2. If on a neighbourhood $U\left(x_{0}\right)$ of $x_{0}$ there exists a weak derivative $F_{G}^{\prime}(x)$ of the application $F$ and such derivative is a function of $x$ on such neigh-

\subsection{Elements of distributions}

In this section we want to recall the main definitions regarding the theory of distributions and Sobolev spaces, useful for a better comprehension of the subjects introduced in the textbook. For a more in-depth treatment, see, e.g., the monographs [Bre86], [Ada75] and [LM68].

Let $\Omega$ be an open set of $\mathbb{R}^{n}$ and $f: \Omega \mapsto \mathbb{R}$.

Definition 2.7. By support of a function $f$ we mean the closure of the set where the function itself takes values different from zero, that is

A function $f: \Omega \mapsto \mathbb{R}$ is said to have a compact support in $\Omega$ if there exists a compact set ${ }^{1} K \subset \Omega$ such that $\operatorname{supp} f \subset K$.

At this point, we can provide the following definition:

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-019.jpg?height=143&width=720&top_left_y=767&top_left_x=97)

We introduce the multi-index notation for the derivatives. Let $\boldsymbol{\alpha}=\left(\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n}\right)$ be an n-tuple of non-negative integers (called multi-index) and let $f: \Omega \mapsto \mathbb{R}$ be a function defined on $\Omega \subset \mathbb{R}^{n}$. We will use the following notation

$$
D^{\alpha} f(\mathbf{x})=\frac{\partial|\alpha| f(\mathbf{x})}{\partial x_{1}^{\alpha_{1}} \partial x_{2}^{\alpha_{2}} \ldots \partial x_{n}^{\alpha_{n}}}
$$

$|\alpha|=\alpha_{1}+\alpha_{2}+\ldots+\alpha_{n}$ being the length of the multi-index coinciding with the order of differentiation of $f$.

${ }^{1}$ With $\Omega \subset \mathbb{R}^{n}$, a compact set is a closed and bounded set. In the space $\mathscr{D}(\Omega)$ we can introduce the following notion of convergence:

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-020.jpg?height=246&width=722&top_left_y=160&top_left_x=97)

We are now able to define the space of distributions on $\Omega$ :

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-020.jpg?height=270&width=722&top_left_y=463&top_left_x=97)

The action of a distribution $T \in \mathscr{D}^{\prime}(\Omega)$ on a function $\phi \in \mathscr{D}(\Omega)$ will always be denoted via the identity pairing $\langle T, \phi\rangle$.

Example 2.1. Let a be a point of the set $\Omega$. The Dirac delta relative to point a is the distribution $\delta_{\mathrm{a}}$ defined by the following relation

$$
\left\langle\delta_{\mathbf{a}}, \phi\right\rangle=\phi(\mathbf{a}) \quad \forall \phi \in \mathscr{D}(\Omega)
$$

For another example, see Exercise 4. Also in $\mathscr{D}^{\prime}(\Omega)$ we introduce a notion of convergence:

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-020.jpg?height=156&width=722&top_left_y=1068&top_left_x=97)



\subsubsection{Square-integrable functions}

We consider the space of square-integrable functions on $\Omega \subset \mathbb{R}^{n}$,

$$
\mathrm{L}^{2}(\Omega)=\left\{f: \Omega \mapsto \mathbb{R} \text { such that } \int_{\Omega}(f(\mathbf{x}))^{2} d \Omega<+\infty\right\}
$$

More precisely, $L^{2}(\Omega)$ is a space of equivalence classes of measurable functions, the equivalence relation to be intended as follows: $v$ is equivalent to $w$ if and only if $v$ and $w$ are equal almost everywhere, i.e. they differ at most on a subset of $\Omega$ with zero measure. The expression "almost everywhere in $\Omega$ " (in short, a.e. in $\Omega$ ) means exactly "for all the $\mathbf{x} \in \Omega$, except for a zero-measure set, at most".

The space $L^{2}(\Omega)$ is a Hilbert space whose scalar product is

$$
(f, g)_{\mathrm{L}^{2}(\Omega)}=\int_{\Omega} f(\mathbf{x}) g(\mathbf{x}) d \Omega
$$

The norm in $L^{2}(\Omega)$ is the one induced by this scalar product, i.e.

$$
\|f\|_{\mathrm{L}^{2}(\Omega)}=\sqrt{(f, f)_{\mathrm{L}^{2}(\Omega)}} .
$$

To each function $f \in \mathrm{L}^{2}(\Omega)$ we associate a distribution $T_{f} \in \mathscr{D}^{\prime}(\Omega)$ defined in the following way

$$
\left\langle T_{f}, \phi\right\rangle=\int_{\Omega} f(\mathbf{x}) \phi(\mathbf{x}) d \Omega \quad \forall \phi \in \mathscr{D}(\Omega)
$$

The following result holds:

Due to the latter, it is possible to prove that the correspondence $f \rightarrow T_{f}$ is injective, thus we can identify $\mathrm{L}^{2}(\Omega)$ with a subset of $\mathscr{D}^{\prime}(\Omega)$, writing

$$
\mathrm{L}^{2}(\Omega) \subset \mathscr{D}^{\prime}(\Omega)
$$

Example 2.2. Let $\Omega=\mathbb{R}$ and let us denote by $\chi_{[a, b]}(x)$ the characteristic function of the interval $[a, b]$, defined as

$$
\chi_{[a, b]}(x)= \begin{cases}1 & \text { if } x \in[a, b] \\ 0 & \text { otherwise }\end{cases}
$$

Let us then consider the sequence of functions $f_{n}(x)=\frac{n}{2} \chi_{[-1 / n, 1 / n]}(x)$ (see Fig. 2.1). 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-022.jpg?height=226&width=640&top_left_y=112&top_left_x=135)

Fig. 2.1. The characteristic function of the interval $[-1 / n, 1 / n]$ (left) and the triangular function $f_{n}$ (right)

We want to verify that the sequence $\left\{T_{f_{n}}\right\}$ of the distributions associated to the former converges to the distribution $\delta_{0}$, i.e. the Dirac delta relative to the origin. As a matter of fact, for each function $\phi \in \mathscr{D}(\Omega)$, we have

$$
\left\langle T_{f_{n}}, \phi\right\rangle=\int_{\mathbb{R}} f_{n}(x) \phi(x) d x=\frac{n}{2} \int_{-1 / n}^{1 / n} \phi(x) d x=\frac{n}{2}[\Phi(1 / n)-\Phi(-1 / n)]
$$

$\Phi$ being a primitive of $\phi$. If we now set $h=1 / n$, we can write

$$
\left\langle T_{f n}, \phi\right\rangle=\frac{\Phi(h)-\Phi(-h)}{2 h}
$$

When $n \rightarrow \infty, h \rightarrow 0$ and thus, following the definition of derivative, we have

$$
\frac{\Phi(h)-\Phi(-h)}{2 h} \rightarrow \Phi^{\prime}(0)
$$

By construction $\Phi^{\prime}=\phi$, and therefore

$$
\left\langle T_{f_{n}}, \phi\right\rangle \rightarrow \phi(0)=\left\langle\delta_{0}, \phi\right\rangle,
$$

having used the definition of $\delta_{0}$ (see Example 2.1).

The same limit can be obtained by taking a sequence of triangular functions (see Fig. 2.1) or Gaussian functions, instead of rectangular ones (provided that they still have unit integral).

Finally, we point out that in the usual metrics, such sequences converge to a function which is null almost everywhere.

\subsubsection{Differentiation in the sense of distributions}

Let $\Omega \subset \mathbb{R}^{n}$ and $T \in \mathscr{D}^{\prime}(\Omega)$. Its derivatives $\frac{\partial T}{\partial x_{i}}$ in the sense of distributions are distributions defined in the following way

$$
\left\langle\frac{\partial T}{\partial x_{i}}, \phi\right\rangle=-\left\langle T, \frac{\partial \phi}{\partial x_{i}}\right\rangle \quad \forall \phi \in \mathscr{D}(\Omega), \quad i=1, \ldots, n
$$


![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-023.jpg?height=228&width=664&top_left_y=117&top_left_x=122)

Fig. 2.2. The Heaviside function (left). On the right, the function of Example $2.6$ with $k=1 / 3$. Note that this function tends to infinity at the origin

In a similar way, we define derivatives of arbitrary order. Precisely, for each multiindex $\boldsymbol{\alpha}=\left(\alpha_{1}, \alpha_{2}, \ldots, \alpha_{n}\right)$, we have that $D^{\alpha} T$ is a new distribution defined as

$$
\left\langle D^{\alpha} T, \phi\right\rangle=(-1)^{|\alpha|}\left\langle T, D^{\alpha} \phi\right\rangle \quad \forall \phi \in \mathscr{D}(\Omega)
$$

Example 2.3. The Heaviside function on $\mathbb{R}$ (see Fig. 2.2) is defined as

$$
H(x)= \begin{cases}1 & \text { if } x>0 \\ 0 & \text { if } x \leq 0\end{cases}
$$

The derivative of the distribution $T_{H}$ associated to the latter is the Dirac distribution relative to the origin (see Example 2.1); upon identifying the function $H$ with the associated distribution $T_{H}$, we will then write

$$
\frac{d H}{d x}=\delta_{0}
$$

Differentiation in the context of distributions enjoys some important properties that do not hold in the more restricted context of differentiation for functions in classical terms.

Property 2.1. The set $\mathscr{D}^{\prime}(\Omega)$ is closed with respect to the differentiation opera-

tion (in the sense of distributions), that is each distribution is differentiable infinitely many times and its distributional derivatives are themselves distributions.

Property 2.2. Differentiation in $\mathscr{D}^{\prime}(\Omega)$ is a continuous operation, in the sense We finally note that differentiation in the sense of distributions is an extension of the classical differentiation of functions. Indeed, if a function $f$ is differentiable with continuity (in classical sense) on $\Omega$, then the derivative of the distribution $T_{f}$ corresponding to $f$ coincides with the distribution $T_{f^{\prime}}$ corresponding to the classical derivative $f^{\prime}$ of $f($ see Exercise 7 ).

We will invariably identify a function $f$ of $\mathrm{L}^{2}(\Omega)$ with the corresponding distribution $T_{f}$ of $\mathscr{D}^{\prime}(\Omega)$, writing $f$ in place of $T_{f}$. Similarly, when we talk about derivatives, we will always refer to the latter in the sense of distributions.

\subsection{Sobolev spaces}

In Sect. 2.3.1 we have noted that the functions of $\mathrm{L}^{2}(\Omega)$ are particular distributions. However, this does not guarantee that their derivatives (in the sense of distributions) are still functions of $L^{2}(\Omega)$, as shown in the following example.

Example 2.4. Let $\Omega \subset \mathbb{R}$ and let $[a, b] \subset \Omega$. Then, the characteristic function of the interval $[a, b]$ (see Example 2.2) belongs to $\mathrm{L}^{2}(\Omega)$, while its distributional derivative $d \chi_{[a, b]} / d x=\delta_{a}-\delta_{b}($ see Example $2.3)$ does not.

It is therefore reasonable to introduce the following spaces:

Definition 2.12. Let $\Omega$ be an open set of $\mathbb{R}^{n}$ and $k$ be a positive integer. We call Sobolev space of order $k$ on $\Omega$ the space formed by the totality of functions of

It follows, obviously, that $\mathrm{H}^{k+1}(\Omega) \subset \mathrm{H}^{k}(\Omega)$ for each $k \geq 0$ and this inclusion is continuous. The space $\mathrm{L}^{2}(\Omega)$ is sometimes denoted by $\mathrm{H}^{0}(\Omega)$.

The Sobolev spaces $\mathrm{H}^{k}(\Omega)$ are Hilbert spaces with respect to the following scalar product

$$
(f, g)_{k}=\sum_{|\boldsymbol{\alpha}| \leq k} \int_{\Omega}\left(D^{\alpha} f\right)\left(D^{\alpha} g\right) d \Omega
$$

from which descend the norms

$$
\|f\|_{k}=\|f\|_{\mathrm{H}^{k}(\Omega)}=\sqrt{(f, f)_{k}}=\sqrt{\sum_{|\alpha| \leq k} \int_{\Omega}\left(D^{\alpha} f\right)^{2} d \Omega}
$$

Finally, we define the seminorms

$$
|f|_{k}=|f|_{\mathrm{H}^{k}(\Omega)}=\sqrt{\sum_{|\boldsymbol{\alpha}|=k} \int_{\Omega}\left(D^{\alpha} f\right)^{2} d \Omega}
$$

so that $(2.11)$ becomes

$$
\|f\|_{\mathrm{H}^{k}(\Omega)}=\sqrt{\sum_{m=0}^{k}|f|_{\mathrm{H}^{m}(\Omega)}^{2}}
$$

Example 2.5. If $n=1$ and $k=1$ we have:

$$
\begin{aligned}
&(f, g)_{1}=(f, g)_{\mathrm{H}^{1}(\Omega)}=\int_{\Omega} f g d \Omega+\int_{\Omega} f^{\prime} g^{\prime} d \Omega \\
&\|f\|_{1}=\|f\|_{\mathrm{H}^{1}(\Omega)}=\sqrt{\int_{\Omega} f^{2} d \Omega+\int_{\Omega} f^{\prime 2} d \Omega}=\sqrt{\|f\|_{\mathrm{L}^{2}(\Omega)}^{2}+\left\|f^{\prime}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}} \\
&\qquad\left.f\right|_{1}=|f|_{\mathrm{H}^{1}(\Omega)}=\sqrt{\int_{\Omega}\left(f^{\prime}\right)^{2} d \Omega}=\left\|f^{\prime}\right\|_{\mathrm{L}^{2}(\Omega)} .
\end{aligned}
$$

\subsubsection{Regularity of the spaces $\mathbf{H}^{k}(\Omega)$}

We now want to relate the fact that a function belongs to a space $\mathrm{H}^{k}(\Omega)$ with its continuity properties.

Example 2.6. Let $\Omega=B(0,1) \subset \mathbb{R}^{2}$ be the ball centered at the origin and of radius $1$. Then the function

$$
f\left(x_{1}, x_{2}\right)=\left|\ln \frac{1}{\sqrt{x_{1}^{2}+x_{2}^{2}}}\right|^{k}
$$

belongs to $\mathrm{H}^{1}(\Omega)$ when $0<k<1 / 2$, see Fig. $2.2$ (right). It develops a singularity at the origin and therefore it is neither continuous nor bounded. A similar conclusion can be drawn for

$$
f\left(x_{1}, x_{2}\right)=\ln \left(-\ln \left(x_{1}^{2}+x_{2}^{2}\right)\right)
$$

this time with $\Omega=B(0,1 / 2) \subset \mathbb{R}^{2}$.

Not all of the functions of $\mathrm{H}^{1}(\Omega)$ are therefore continuous if $\Omega$ is an open set of $\mathbb{R}^{2}$ (or $\mathbb{R}^{3}$ ). In general, the following result holds:

In particular, in one spatial dimension $(n=1)$, the functions of $\mathrm{H}^{1}(\Omega)$ are continuous (they are indeed absolutely continuous, see [Sal08] and [Bre86]), while in two or three dimensions they are not necessarily so. Instead, the functions of $\mathrm{H}^{2}(\Omega)$ are always continuous for $n=1,2,3$. 

\subsubsection{The space $\mathrm{H}_{0}^{1}(\Omega)$}

If $\Omega$ is bounded, the space $\mathscr{D}(\Omega)$ is not dense in $\mathrm{H}^{1}(\Omega)$. We can then give the following definition:

Definition 2.13. We denote by $\mathrm{H}_{0}^{1}(\Omega)$ the closure of $\mathscr{D}(\Omega)$ in $\mathrm{H}^{1}(\Omega)$.

The functions of $\mathrm{H}_{0}^{1}(\Omega)$ enjoy the following properties:

Property 2.4 (Poincaré inequality). Let $\Omega$ be a bounded set in $\mathbb{R}^{n}$; then there exists a constant $C_{\Omega}$ such that

Proof. $\Omega$ being bounded, we can always find a sphere $S_{D}=\{\mathbf{x}:|\mathbf{x}-\mathbf{g}|<D\}$ with centre $\mathbf{g}$ and radius $D>0$, containing $\Omega$. Since $\mathscr{D}(\Omega)$ is dense in $\mathrm{H}_{0}^{1}(\Omega)$ it is sufficient to prove the inequality for a function $u \in \mathscr{D}(\Omega)$. ( In the general case where $v \in \mathrm{H}_{0}^{1}(\Omega)$ it will suffice to build a sequence $u_{i} \in \mathscr{D}(\Omega), i=1,2, \ldots$ converging to $v$ in the norm of $\mathrm{H}^{1}(\Omega)$, apply the inequality to the terms of the sequence and pass to the limit.) Integrating by parts and exploiting the fact that $\operatorname{div}(\mathbf{x}-\mathbf{g})=n$,

$$
\begin{aligned}
&\|u\|_{\mathrm{L}^{2}(\Omega)}^{2}=n^{-1} \int_{\Omega} n \cdot|u(\mathbf{x})|^{2} d \Omega=-n^{-1} \int_{\Omega}(\mathbf{x}-\mathbf{g}) \cdot \nabla\left(|u(\mathbf{x})|^{2}\right) d \Omega \\
&\begin{array}{c}
=-2 n^{-1} \int_{\Omega}(\mathbf{x}-\mathbf{g}) \cdot[u(\mathbf{x}) \nabla u(\mathbf{x})] d \Omega \leq 2 n^{-1}\|\mathbf{x}-\mathbf{g}\|_{L^{\infty}(\Omega)}\|u\|_{\mathrm{L}^{2}(\Omega)}\|u\|_{\mathrm{H}^{1}(\Omega)} \\
\leq 2 n^{-1} D\|u\|_{\mathrm{L}^{2}(\Omega)}\|u\|_{\mathrm{H}^{1}(\Omega)}
\end{array}
\end{aligned}
$$

As an immediate consequence, we have that:

Property 2.5. The seminorm $|v|_{H^{1}(\Omega)}$ is a norm on the space $\mathrm{H}_{0}^{1}(\Omega)$ that turns
out to be equivalent to the norm $\|y\|_{\text {. } 1}$ (n)

Property 2.5. The seminorm $|v|_{H^{1}(\Omega)}$ is a norm on the space $\mathrm{H}_{0}^{1}(\Omega)$ that turns
out to be equivalent to the norm $\|v\|_{H^{1}(\Omega)}$.

Proof. We recall that two norms, $\|\cdot\|$ and $\|\cdot\| \mid$, are said to be equivalent if there exist two positive constants $c_{1}$ and $c_{2}$, such that

$$
c_{1}\left\|\left|v\|\mid \leq\| v\left\|\leq c_{2}\right\| v\|\| \quad \forall v \in V\right.\right.
$$

As $\|v\|_{1}=\sqrt{|v|_{1}^{2}+\|v\|_{0}^{2}}$ it is evident that $|v|_{1} \leq\|v\|_{1}$. Conversely, exploiting Property $2.4$,

$$
\|v\|_{1}=\sqrt{|v|_{1}^{2}+\|v\|_{0}^{2}} \leq \sqrt{|v|_{1}^{2}+C_{\Omega}^{2}|v|_{1}^{2}} \leq C_{\Omega}^{*}|v|_{1}
$$

from which we deduce the equivalence of the two norms.

In a similar way, we define the spaces $\mathrm{H}_{0}^{k}(\Omega)$ as the closure of $\mathscr{D}(\Omega)$ in $\mathrm{H}^{k}(\Omega)$. 

\subsubsection{Trace operators}

Let $\Omega$ be a domain of $\mathbb{R}^{n}$. By that we mean:

- an open bounded interval if $n=1$;

- an open bounded connected set, with a sufficiently regular boundary $\partial \Omega$. For instance, a polygon if $n=2$ (i.e. a domain whose boundary is a finite union of segments), or a polyhedron if $n=3$ (i.e. a domain whose boundary is a finite union of polygons).

Let $v$ be an element of $\mathrm{H}^{1}(\Omega):$ the remarks formulated in Sect. $2.4 .1$ show that it is not simple to define the "value" of $v$ on the boundary of $\Omega$, a value that we will call the trace of $v$ on $\partial \Omega$. We exploit the following result:

Theorem 2.3. Let $\Omega$ be a domain of $\mathbb{R}^{n}$ provided with a "sufficiently regular" continuity of $\gamma_{0}$ implies that there exists a constant $C>0$ such that

Owing to this result, Dirichlet boundary conditions make sense when seeking solutions $v$ in $\mathrm{H}^{k}(\Omega)$, with $k \geq 1$, provided we interpret the boundary value in the sense of the trace.

Remark 2.1. The trace operator $\gamma_{\Gamma}$ is not surjective on $\mathrm{L}^{2}(\Gamma)$. In particular, the set of functions of $L^{2}(\Gamma)$ which are traces of functions of $H^{1}(\Omega)$ constitutes a subspace of $\mathrm{L}^{2}(\Gamma)$ denoted by $\mathrm{H}^{1 / 2}(\Gamma)$ and characterized by intermediate regularity properties between those of $\mathrm{L}^{2}(\Gamma)$ and those of $\mathrm{H}^{1}(\Gamma)$. More generally, for every $k \geq 1$ there exists a unique linear and continuous application $\gamma_{0}: \mathrm{H}^{k}(\Omega) \mapsto \mathrm{H}^{k-1 / 2}(\Gamma)$ such that $\gamma_{0} v=v_{\Gamma}$ for each $v \in \mathrm{H}^{k}(\Omega) \cap C^{0}(\bar{\Omega})$.

The trace operators allow for an interesting characterization of the previously defined space $\mathrm{H}_{0}^{1}(\Omega)$. Indeed, we have the following property: In other words, $\mathrm{H}_{0}^{1}(\Omega)$ is formed by the functions of $\mathrm{H}^{1}(\Omega)$ having null trace on the boundary.

\subsection{The spaces $\mathrm{L}^{\infty}(\Omega)$ and $\mathrm{L}^{p}(\Omega)$, with $1 \leq p<\infty$}

The space $L^{2}(\Omega)$ can be generalized in the following way: for each real number $p$ with $1 \leq p<\infty$ we can define the following space of (equivalence classes of) measurable functions

$$
\mathrm{L}^{p}(\Omega)=\left\{v: \Omega \mapsto \mathbb{R} \text { such that } \int_{\Omega}|v(\mathbf{x})|^{p} d \Omega<\infty\right\}
$$

whose norm is given by

$$
\|v\|_{\mathrm{L}^{p}(\Omega)}=\left(\int_{\Omega}|v(\mathbf{x})|^{p} d \Omega\right)^{1 / p}
$$

Furthermore, we define the space

$$
\mathrm{L}_{l o c}^{1}(\Omega)=\left\{f: \Omega \rightarrow \mathbb{R} \text { such that }\left.f\right|_{K} \in \mathrm{L}^{1}(K) \text { for each compact set } K \subset \Omega\right\}
$$

If $1 \leq p<\infty$, then $\mathscr{D}(\Omega)$ is dense in $\mathrm{L}^{p}(\Omega)$.

In the case where $p=\infty$, we define $\mathrm{L}^{\infty}(\Omega)$ to be the space of functions that are bounded a.e. in $\Omega$. Its norm is defined as follows

$$
\begin{aligned}
\|v\|_{L^{\infty}(\Omega)} &=\inf \{C \in \mathbb{R}:|v(x)| \leq C, \text { a.e. in } \Omega\} \\
&=\sup \{|v(x)|, \text { a.e. in } \Omega\}
\end{aligned}
$$

For $1 \leq p \leq \infty$, the spaces $\mathrm{L}^{p}(\Omega)$, provided with the norm $\|\cdot\|_{\mathrm{L}^{p}(\Omega)}$, are Banach spaces.

We recall the Hölder inequality: given $v \in \mathrm{L}^{p}(\Omega)$ and $w \in \mathrm{L}^{p^{\prime}}(\Omega)$ with $1 \leq p \leq \infty$ and $\frac{1}{p}+\frac{1}{p^{\prime}}=1$, then $v w \in \mathrm{L}^{1}(\Omega)$ and

$$
\int_{\Omega}|v(\mathbf{x}) w(\mathbf{x})| d \Omega \leq\|v\|_{L^{p}(\Omega)}\|w\|_{L^{\prime}(\Omega)}
$$

The index $p^{\prime}$ is called conjugate of $p$.

If $1<p<\infty$, then $\mathrm{L}^{p}(\Omega)$ is a reflexive space: this means that any linear and continuous form $\varphi: \mathrm{L}^{p}(\Omega) \rightarrow \mathbb{R}$ can be identified to an element of $\mathrm{L}^{p^{\prime}}(\Omega)$, i.e. there exists a unique $g \in \mathrm{L}^{p^{\prime}}(\Omega)$ such that

$$
\varphi(f)=\int_{\Omega} f(\mathbf{x}) g(\mathbf{x}) d \Omega \quad \forall f \in \mathrm{L}^{p}(\Omega)
$$

If $p=2$, then $p^{\prime}=2$, so the Hölder inequality becomes

$$
(v, w)_{\mathrm{L}^{2}(\Omega)} \leq\|v\|_{\mathrm{L}^{2}(\Omega)}\|w\|_{\mathrm{L}^{2}(\Omega)} \quad \forall v, w \in \mathrm{L}^{2}(\Omega)
$$

As such, it is known as Cauchy-Schwarz inequality. Moreover, the following inequality holds

$$
\|v w\|_{\mathrm{L}^{2}(\Omega)} \leq\|v\|_{\mathrm{L}^{4}(\Omega)}\|w\|_{\mathrm{L}^{4}(\Omega)} \quad \forall v, w \in \mathrm{L}^{4}(\Omega)
$$

If $\Omega \subset \mathbb{R}^{n}$ is a bounded domain, for $1 \leq p \leq q \leq \infty$ we have

$$
\mathrm{L}^{q}(\Omega) \subset \mathrm{L}^{p}(\Omega) \subset \mathrm{L}^{1}(\Omega) \subset \mathrm{L}_{l o c}^{1}(\Omega)
$$

If $\Omega$ is unbounded, we always have

$$
\mathrm{L}^{p}(\Omega) \subset \mathrm{L}_{l o c}^{1}(\Omega) \quad \forall p \geq 1
$$

Moreover, if $\Omega \subset \mathbb{R}^{n}$ and for $n>1$ the boundary $\partial \Omega$ is polygonal (more generally, it is Lipschitz continuous), we have the following continuous inclusions:

if $0<2 s<n$ then $\mathrm{H}^{s}(\Omega) \subset \mathrm{L}^{q}(\Omega) \forall q$ such that $1 \leq q \leq q^{*}$ with $q^{*}=2 n /(n-2 s)$

if $2 s=n \quad$ then $\mathrm{H}^{s}(\Omega) \subset \mathrm{L}^{q}(\Omega) \forall q$ such that $1 \leq q<\infty$;

if $2 s>n \quad$ then $\mathrm{H}^{s}(\Omega) \subset C^{0}(\bar{\Omega})$.

Finally, we introduce the Sobolev space $W^{k, p}(\Omega)$, with $k$ a non-negative integer and $1 \leq p \leq \infty$, as the space of functions $v \in L^{p}(\Omega)$ such that all the distributional derivatives of $v$ of order up to $k$ are in $L^{p}(\Omega)$

$$
W^{k, p}(\Omega)=\left\{v \in L^{p}(\Omega): D^{\alpha} v \in L^{p}(\Omega)\right.
$$

for each non-negative multi-index $\boldsymbol{\alpha}$ such that $|\boldsymbol{\alpha}| \leq k\}$.

For $1 \leq p<\infty$ this is a Banach space with norm

$$
\|v\|_{W^{k, p}(\Omega)}=\left(\sum_{|\alpha| \leq k}\left\|D^{\alpha} v\right\|_{L^{p}(\Omega)}^{p}\right)^{1 / p}
$$

Its seminorm $|v|_{W^{k, p}(\Omega)}$ is defined similarly, provided we sum over multi-integers $\alpha$ such that $|\boldsymbol{\alpha}|=k$. Note that, for $k=0, W^{k, p}(\Omega)=L^{p}(\Omega)$ and that, for $p=2, W^{k, 2}(\Omega)=H^{k}(\Omega)$.

\subsection{Adjoint operators of a linear operator}

Let $X$ and $Y$ be two Banach spaces and $\mathscr{L}(X, Y)$ be the space of linear and bounded operators from $X$ to $Y$. Given $L \in \mathscr{L}(X, Y)$, the adjoint (or coniugate) operator of $L$ is the operator $L^{\prime}: Y^{\prime} \rightarrow X^{\prime}$ defined by

$$
{ }_{X^{\prime}}\left\langle L^{\prime} f, x\right\rangle_{X}={ }_{Y^{\prime}}\langle f, L x\rangle_{Y} \quad \forall f \in Y^{\prime}, x \in X
$$

$L^{\prime}$ is a linear and bounded operator between $Y^{\prime}$ and $X^{\prime}$, that is $L^{\prime} \in \mathscr{L}\left(Y^{\prime}, X^{\prime}\right)$, moreover $\left\|L^{\prime}\right\|_{\mathscr{L}\left(Y^{\prime}, X^{\prime}\right)}=\|L\|_{\mathscr{L}(X, Y)}$, where we have set

$$
\|L\|_{\mathscr{L}(X, Y)}=\sup _{x \in X \atop x \neq 0} \frac{\|L x\|_{Y}}{\|x\|_{X}}
$$

In the case where $X$ and $Y$ are two Hilbert spaces, an additional adjoint operator, $L^{T}: Y \rightarrow X$, called transpose of $L$, can be introduced. It is defined by

$$
\left(L^{T} y, x\right)_{X}=(y, L x)_{Y} \quad \forall x \in X, y \in Y
$$

Here, $(\cdot, \cdot)_{X}$ denotes the scalar product of $X$, while $(\cdot, \cdot)_{Y}$ denotes the scalar product of $Y$. The above definition can be explained as follows: for any given element $y \in Y$, the real-valued function $x \rightarrow(y, L x)_{Y}$ is linear and continuous, hence it defines an element of $X^{\prime}$. By Riesz's theorem (Theorem 2.1) there exists an element $x$ of $X$, which we name $L^{T} y$, that satisfies $(2.22)$. Such operator belongs to $\mathscr{L}(Y, X)$ (that is, it is linear and bounded from $Y$ to $X$ ) and moreover

$$
\left\|L^{T}\right\|_{\mathscr{L}(Y, X)}=\|L\|_{\mathscr{L}(X, Y)}
$$

Thus, in the case where $X$ and $Y$ are two Hilbert spaces, we have two notions of adjoint operator, $L^{\prime}$ and $L^{T}$. The relationship between the two operators is

$$
\Lambda_{X} L^{T}=L^{\prime} \Lambda_{Y}
$$

$\Lambda_{X}$ and $\Lambda_{Y}$ being Riesz's canonical isomorphisms from $X$ to $X^{\prime}$ and from $Y$ to $Y^{\prime}$, respectively $($ see $(2.5))$. Indeed, $\forall x \in X, y \in Y$,

$$
{ }_{X^{\prime}}\left\langle\Lambda_{X} L^{T} y, x\right\rangle_{X}=\left(L^{T} y, x\right)_{X}=(y, L x)_{Y}={Y^{\prime}}\left\langle\Lambda_{Y} y, L x\right\rangle_{Y}={ }_{X^{\prime}}\left\langle L^{\prime} \Lambda_{Y} y, x\right\rangle_{X}
$$

The identity ( $2.24)$ can be equivalently expressed by stating that the diagramme in Fig. $2.3$ is commutative.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-030.jpg?height=273&width=277&top_left_y=923&top_left_x=315)

Fig. 2.3. The adjoint operators $L^{T}$ and $L^{\prime}$ of the operator $\mathrm{L}$ 

\subsection{Spaces of time-dependent functions}

When considering space-time functions $v(\mathbf{x}, t), \mathbf{x} \in \Omega \subset \mathbb{R}^{n}, n \geq 1, t \in(0, T), T>0$, it is natural to introduce the functional space

$$
\begin{aligned}
&L^{q}\left(0, T ; W^{k, p}(\Omega)\right)= \\
&\left\{v:(0, T) \rightarrow W^{k, p}(\Omega) \text { such that } v \text { is measurable and } \int_{0}^{T}\|v(t)\|_{W^{k, p}(\Omega)}^{q} d t<\infty\right\}
\end{aligned}
$$

where $k \geq 0$ is a non-negative integer, $1 \leq q<\infty, 1 \leq p \leq \infty$, endowed with the norm

$$
\|v\|_{L^{q}\left(0, T ; W^{k, p}(\Omega)\right)}=\left(\int_{0}^{T}\|v(t)\|_{W^{k, p}(\Omega)}^{q} d t\right)^{1 / q}
$$

For every $t \in(0, T)$ we have used the shorthand notation $v(t)$ to indicate the function:

$$
v(t): \Omega \rightarrow \mathbb{R}, v(t)(\mathbf{x})=v(\mathbf{x}, t) \quad \forall \mathbf{x} \in \Omega
$$

The spaces $L^{\infty}\left(0, T: W^{k, p}(\Omega)\right)$ and $C^{0}\left([0, T] ; W^{k, p}(\Omega)\right)$ are defined in a similar way.

When dealing with time-dependent initial-boundary value problems, the following result can be useful to derive a-priori estimates and stability inequalities.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-031.jpg?height=528&width=723&top_left_y=717&top_left_x=96)

A discrete counterpart of this lemma, useful when dealing with fully discrete (in space and time) approximations of initial-boundary value problems, is the following

For the proof of these two lemmas, see, e.g., [QV94, Chap. 1].

\section{$2.8$ Exercises}

1. Let $\Omega=(0,1)$ and, for $\alpha>0, f(x)=x^{-\alpha}$. For which $\alpha$ do we have $f \in \mathrm{L}^{p}(\Omega)$,

$1 \leq p<\infty$ ? Is there an $\alpha>0$ for which $f \in \mathrm{L}^{\infty}(\Omega) ?$

2. Let $\Omega=\left(0, \frac{1}{2}\right)$ and $f(x)=\frac{1}{x(\ln x)^{2}}$. Show that $f \in \mathrm{L}^{1}(\Omega)$.

3. Prove for which $\alpha \in \mathbb{R}$ we have that $f \in \mathrm{L}_{l o c}^{1}(0,1)$, with $f(x)=x^{-\alpha}$.

4. Let $u \in \mathrm{L}_{\text {loc }}^{1}(\Omega)$. Define $T_{u} \in \mathscr{D}^{\prime}(\Omega)$ as follows

$$
\left\langle T_{u}, \varphi\right\rangle=\int_{\Omega} \varphi(\mathbf{x}) u(\mathbf{x}) d \Omega \quad \forall \varphi \in \mathscr{D}(\Omega)
$$

Verify that $T_{u}$ is indeed a distribution and that the application $u \rightarrow T_{u}$ is injective. We can therefore identify $u$ with $T_{u}$ and conclude by observing that $\mathrm{L}_{l o c}^{1}(\Omega) \subset$ $D^{\prime}(\Omega)$.

5. Show that the function defined as follows:

$$
\begin{aligned}
&f(x)=e^{1 /\left(x^{2}-1\right)} \text { if } x \in(-1,1) \\
&f(x)=0 \text { if } x \in]-\infty,-1] \cup[1,+\infty[
\end{aligned}
$$

belongs to $\mathscr{D}(\mathbb{R})$. 6. Prove that for the function $f$ defined in $(2.12)$ we have

$$
\|f\|_{\mathrm{H}^{1}(\Omega)}^{2}=2 \pi \int_{0}^{r}|\log s|^{2 k} s d s+2 \pi k^{2} \int_{0}^{r} \frac{1}{s}|\log s|^{2 k-2} d s
$$

hence $f$ belongs to $\mathrm{H}^{1}(\Omega)$ for every $0<k<\frac{1}{2}$.

7. Let $\varphi \in C^{1}(-1,1)$. Show that the derivative $\frac{d \varphi}{d x}$ computed in the classical sense is equal to $\frac{d \varphi}{d x}$ computed in the sense of distributions, after observing that $C^{0}(-1,1) \subset$ $\mathrm{L}_{l o c}^{1}(-1,1) \subset \mathscr{D}^{\prime}(-1,1)$.

8. Prove that if $\Omega=(a, b)$ the Poincaré inequality $(2.13)$ holds with $C_{\Omega}=(b-$ a) $/ \sqrt{2}$.

[Solution: observe that the Cauchy-Schwarz inequality implies

$$
v(x)=\int_{a}^{x} v^{\prime}(t) d t \leq\left(\int_{a}^{x}\left[v^{\prime}(t)\right]^{2} d t\right)^{1 / 2}\left(\int_{a}^{x} 1 d t\right)^{1 / 2} \leq \sqrt{x-a}\left\|v^{\prime}\right\|_{\mathrm{L}^{2}(a, b)}
$$

whence

$$
\left.\|v\|_{L^{2}(a, b)}^{2} \leq\left\|v^{\prime}\right\|_{L^{2}(a, b)}^{2} \int_{a}^{b}(x-a) d x\right] .
$$

Chapter 3

\section{Elliptic equations}

This chapter is devoted to the introduction of elliptic problems and to their weak formulation. Although our introduction is quite basic, the complete novice to functional analysis is invited to consult Chapter 2 before reading it.

For the sake of simplicity, we will focus primarily on one-dimensional and twodimensional problems. However, the generalization to three-dimensional problems is (almost always) straightforward.

\subsection{An elliptic problem example: the Poisson equation}

Consider a domain $\Omega \subset \mathbb{R}^{2}$, i.e. an open bounded and connected set, and let $\partial \Omega$ be its boundary. We denote by $\mathbf{x}$ the spatial variable pair $\left(x_{1}, x_{2}\right)$. The problem under examination is

$$
-\Delta u=f \quad \text { in } \Omega
$$

where $f=f(\mathbf{x})$ is a given function and the symbol $\Delta$ denotes the Laplacian operator (1.6) in two dimensions. (3.1) is an elliptic, linear, non-homogeneous (if $f \neq 0$ ) second-order equation. We call (3.1) the strong formulation of the Poisson equation. We also recall that, in the case where $f=0$, equation (3.1) is known as the Laplace equation.

Physically, $u$ can represent the vertical displacement of an elastic membrane due to the application of a force with intensity equal to $f$, or the electric potential distribution due to an electric charge with density $f$.

To obtain a unique solution, suitable boundary conditions must be added to $(3.1)$, that is we need information about the behaviour of the solution $u$ at the domain boundary $\partial \Omega$. For instance, the value of the displacement $u$ on the boundary can be assigned

$$
u=g \quad \text { on } \partial \Omega
$$

where $g$ is a given function, and in such case we will talk about a Dirichlet problem. The case where $g=0$ is said to be homogeneous.

Alternatively, the value of the normal derivative of $u$ can be imposed

$$
\nabla u \cdot \mathbf{n}=\frac{\partial u}{\partial n}=h \quad \text { on } \partial \Omega
$$

n being the outward unit normal vector on $\partial \Omega$ and $h$ an assigned function. The associated problem is called a Neumann problem and corresponds, in the case of the membrane problem, to imposing the traction at the boundary of the membrane itself. Once again, the case $h=0$ is said to be homogeneous.

Finally, different types of conditions can be assigned to different portions of the boundary of the computational domain $\Omega$. For instance, supposing that $\partial \Omega=\Gamma_{D} \cup \Gamma_{N}$ with $\stackrel{0}{\Gamma}_{D} \cap \stackrel{\circ}{\Gamma}_{N}=\emptyset$, the following conditions can be imposed:

$$
\begin{cases}u=g & \text { on } \Gamma_{D} \\ \frac{\partial u}{\partial n}=h & \text { on } \Gamma_{N}\end{cases}
$$

The notation $\Gamma^{\circ}$ has been used to indicate the interior of $\Gamma$. In such a case, the associated problem is said to be mixed.

Also in the case of homogeneous Dirichlet problems where $f$ is a continuous function in $\bar{\Omega}$ (the closure of $\Omega$ ), it is not guaranteed that problem $(3.1),(3.2)$ admits a regular solution. For instance, if $\Omega=(0,1) \times(0,1)$ and $f=1, u$ may not belong to the space $C^{2}(\bar{\Omega})$. Indeed, if it were so, we would have

$$
-\Delta u(0,0)=-\frac{\partial^{2} u}{\partial x_{1}^{2}}(0,0)-\frac{\partial^{2} u}{\partial x_{2}^{2}}(0,0)=0
$$

as the boundary conditions would imply that $u\left(x_{1}, 0\right)=u\left(0, x_{2}\right)=0$ for all $x_{1}, x_{2}$ belonging to $[0,1]$. Hence $u$ could not satisfy equation (3.1), that is

$$
-\Delta u=1 \quad \text { in }(0,1) \times(0,1)
$$

What can be learned from this counterexample is that, even if $f \in C^{0}(\bar{\Omega})$, it makes no sense in general to look for a solution $u \in C^{2}(\bar{\Omega})$ to problem (3.1), (3.2), while one has greater probabilities to find a solution $u \in C^{2}(\Omega) \cap C^{0}(\bar{\Omega})$ (a larger space than $\left.C^{2}(\bar{\Omega}) !\right)$

We are therefore interested in finding an alternative formulation to the strong one, also because, as we will see in the following section, the latter does not allow the treatment of some physically significant cases. For instance, it is not guaranteed that, in the presence of non-smooth data, the physical solution lies in the space $C^{2}(\Omega) \cap$ $C^{0}(\bar{\Omega})$, and not even that it lies in $C^{1}(\Omega) \cap C^{0}(\bar{\Omega})$.

\subsection{The Poisson problem in the one-dimensional case}

Our first step is the introduction of the weak formulation of a simple boundary-value problem in one dimension. 

\subsubsection{Homogeneous Dirichlet problem}

Let us consider the homogeneous Dirichlet problem in the one-dimensional interval $\Omega=(0,1)$

$$
\begin{cases}-u^{\prime \prime}(x)=f(x), & 0<x<1 \\ u(0)=0, & u(1)=0\end{cases}
$$

This problem governs, for instance, the equilibrium configuration of an elastic string with tension equal to one, fixed at the endpoints, in a small displacement configuration and subject to a transversal force with intensity $f$. The overall force acting on the section $(0, x)$ of the string is

$$
F(x)=\int_{0}^{x} f(t) d t
$$

The function $u$ describes the vertical displacement of the string relative to the resting position $u=0$

The strong formulation (3.3) is in general inadequate. If we consider, for instance, the case where the elastic string is subject to a charge concentrated in one or more points (in such case $f$ can be represented via Dirac distributions), the physical solution exists and is continuous, but not differentiable. Fig. $3.1$ shows the case of a unit charge concentrated only in the point $x=0.5$ (left) and in the two points $x=0.4$ and $x=0.6$ (right). These functions cannot be solutions of $(3.3)$, as the latter would require the
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-036.jpg?height=420&width=652&top_left_y=708&top_left_x=120)

Fig. 3.1. We display on the left the equilibrium configuration of the string corresponding to the unit charge concentrated in $x=0.5$, represented in the upper part of the figure. On the right we display the one corresponding to two unit charges concentrated in $x=0.4$ and $x=0.6$, also represented in the upper part of the figure 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-037.jpg?height=402&width=292&top_left_y=112&top_left_x=298)

Fig. 3.2. Displacement relative to the discontinuous charge represented in the upper part of the figure

solution to have a continuous second derivative. Similar considerations hold in the case where $f$ is a piecewise constant function. For instance, in the case represented in Fig. $3.2$ of a null load, except for the interval $[0.4,0.6]$ where it is equal to $-1$, the analytical solution is only of class $C^{1}([0,1])$, since it is given by

$$
u(x)= \begin{cases}-\frac{1}{10} x & \text { for } x \in[0,0.4] \\ \frac{1}{2} x^{2}-\frac{1}{2} x+\frac{2}{25} & \text { for } x \in[0.4,0.6] \\ -\frac{1}{10}(1-x) & \text { for } x \in[0.6,1] .\end{cases}
$$

A formulation of the problem alternative to the strong one is therefore necessary to allow reducing the order of the derivation required for the unknown solution $u$. We move from a second-order differential problem to a first-order one in integral form, which is called the weak formulation of the differential problem.

To this end, we operate a sequence of formal transformations of $(3.3)$, without worrying at this stage whether all the operations appearing in it are allowed. We start by multiplying equation (3.3) by a (so far arbitrary) test function $v$ and integrating on the interval $(0,1)$,

$$
-u^{\prime \prime} v=f v \Rightarrow-\int_{0}^{1} u^{\prime \prime} v d x=\int_{0}^{1} f v d x
$$

We integrate by parts the first integral, with the purpose of eliminating the second derivative, in order to impose a lower regularity on the solution. We find

$$
-\int_{0}^{1} u^{\prime \prime} v d x=\int_{0}^{1} u^{\prime} v^{\prime} d x-\left[u^{\prime} v\right]_{0}^{1}
$$

Since $u$ is known at the boundary, we can consider only test functions which vanish at the endpoints of the interval, hence the contribution of the boundary terms vanishes. In this way, the equation becomes

$$
\int_{0}^{1} u^{\prime} v^{\prime} d x=\int_{0}^{1} f v d x
$$

The test function space $V$ must therefore be such that if $v \in V$ then $v(0)=v(1)=0$. Note that the solution $u$, being null at the boundary and having the same requirements of regularity as the test functions, will also be sought in the same space $V$.

There remains to specify the regularity requirements which must be satisfied by the space $V$, so that all the operations introduced make sense. Evidently, if $u$ and $v$ belonged to $C^{1}([0,1])$, we would have $u^{\prime}, v^{\prime} \in C^{0}([0,1])$ and therefore the integral appearing in the left-hand side of (3.4) would make sense. However, the examples in Fig. $3.1$ tell us that the physical solutions might not be continuously differentiable: we must therefore require a lower regularity. Moreover, even when $f \in C^{0}([0,1])$, there is no garantee that the problem admits solutions in the space

$$
V=\left\{v \in C^{1}([0,1]): v(0)=v(1)=0\right\}
$$

As a matter of fact, when provided with the scalar product

$$
[u, v]_{1}=\int_{0}^{1} u^{\prime} v^{\prime} d x
$$

this space is not complete, that is, not all of the Cauchy sequences with values in $V$ converge to an element of $V$. (Verify as an exercise that (3.6) is indeed a scalar product.)

Let us then proceed as follows. We recall the definition of the spaces $\mathrm{L}^{p}$ of functions whose $p$-th power is Lebesgue integrable. For $1 \leq p<\infty$, these are defined as follows (see Sect. 2.5):

$$
\mathrm{L}^{p}(0,1)=\left\{v:(0,1) \mapsto \mathbb{R} \text { such that } \quad\|v\|_{L^{p}(0,1)}=\left(\int_{0}^{1}|v(x)|^{p} d x\right)^{1 / p}<+\infty\right\}
$$

For the integral $\int_{0}^{1} u^{\prime} v^{\prime} d x$ to be well defined, the minimum requirement on $u^{\prime}$ and $v^{\prime}$ is that the product $u^{\prime} v^{\prime}$ lies in $\mathrm{L}^{1}(0,1)$. To this end, the following property holds: This result is a direct consequence of the Cauchy-Schwarz inequality $(2.17)$ :

$$
\left|\int_{0}^{1} \varphi(x) \psi(x) d x\right| \leq\|\varphi\|_{\mathrm{L}^{2}(0,1)}\|\psi\|_{\mathrm{L}^{2}(0,1)}
$$

where

$$
\|\varphi\|_{\mathrm{L}^{2}(0,1)}=\sqrt{\int_{\Omega}|\varphi(x)|^{2} d x}
$$

is the norm of $\varphi$ in $\mathrm{L}^{2}(0,1)$. Since $\|\varphi\|_{\mathrm{L}^{2}(0,1)},\|\psi\|_{\mathrm{L}^{2}(0,1)}<\infty$ by hypothesis, this proves that there also exists a (finite) integral of $\varphi \psi$.

In order for the integrals appearing in (3.4) to make sense, functions, as well as their derivatives, must be square integrable. We therefore define the Sobolev space

$$
\mathrm{H}^{1}(0,1)=\left\{v \in \mathrm{L}^{2}(0,1): v^{\prime} \in \mathrm{L}^{2}(0,1)\right\}
$$

The derivative must be interpreted in the sense of distributions (see Sect. $2.3)$. We then choose as $V$ the following subspace of $\mathrm{H}^{1}(0,1)$

$$
\mathrm{H}_{0}^{1}(0,1)=\left\{v \in \mathrm{H}^{1}(0,1): v(0)=v(1)=0\right\}
$$

constituted by the functions of $\mathrm{H}^{1}(0,1)$ that are null at the endpoints of the interval. If we suppose $f \in \mathrm{L}^{2}(0,1)$, the integral on the right-hand side of $(3.4)$ also makes sense. Problem (3.3) is then reduced to the following problem

$$
\text { find } u \in V=\mathrm{H}_{0}^{1}(0,1): \int_{0}^{1} u^{\prime} v^{\prime} d x=\int_{0}^{1} f v d x \quad \forall v \in V
$$

Remark 3.1. The space $\mathrm{H}_{0}^{1}(0,1)$ is the closure, with respect to the scalar product (3.6), of the space defined in (3.5).

The functions of $\mathrm{H}^{\mathrm{I}}(0,1)$ are not necessarily differentiable in a traditional sense, that is $\mathrm{H}^{1}(0,1) \not \subset C^{1}([0,1])$. For instance, functions that are piecewise continuous on a partition of the interval $(0,1)$ with derivatives that do not match at all endpoints of the partition belong to $\mathrm{H}^{\mathrm{I}}(0,1)$ but not to $C^{\mathrm{l}}([0,1])$. Hence, also continuous but not differentiable solutions of the previous examples are considered. The weak problem (3.9) turns out to be equivalent to a variational problem, due to the following result:

$$
\begin{aligned}
&\text { Theorem 3.1. The problem } \\
&\text { is equivalent to problem (3.9), in the sense that } u \text { is a solution of }(3.9) \text { if and only } \\
&\text { if } u \text { is a solution of }(3.10) \text {. }
\end{aligned}
$$

Proof. Suppose that $u$ is a solution of the variational problem $(3.10)$. Then, setting $v=u+\delta w$, with $\delta \in \mathbb{R}$, we have that

$$
J(u) \leq J(u+\delta w) \quad \forall w \in V
$$

The function $\psi(\delta)=J(u+\delta w)$ is a quadratic function in $\delta$ with minimum reached for $\delta=0$. Thus,

$$
\left.\psi^{\prime}(\delta)\right|_{\delta=0}=\left.\frac{\partial J(u+\delta w)}{\partial \delta}\right|_{\delta=0}=0
$$

From the definition of derivative we have

$$
\frac{\partial J(u+\delta w)}{\partial \delta}=\lim _{\delta \rightarrow 0} \frac{J(u+\delta w)-J(u)}{\delta} \quad \forall w \in V
$$

Let us consider the term $J(u+\delta w)$ :

$$
\begin{aligned}
J(u+\delta w) &=\frac{1}{2} \int_{0}^{1}\left[(u+\delta w)^{\prime}\right]^{2} d x-\int_{0}^{1} f(u+\delta w) d x \\
&=\frac{1}{2} \int_{0}^{1}\left[u^{\prime 2}+\delta^{2} w^{\prime 2}+2 \delta u^{\prime} w^{\prime}\right] d x-\int_{0}^{1} f u d x-\int_{0}^{1} f \delta w d x \\
&=J(u)+\frac{1}{2} \int_{0}^{1}\left[\delta^{2} w^{\prime 2}+2 \delta u^{\prime} w^{\prime}\right] d x-\int_{0}^{1} f \delta w d x
\end{aligned}
$$

Hence,

$$
\frac{J(u+\delta w)-J(u)}{\delta}=\frac{1}{2} \int_{0}^{1}\left[\delta w^{\prime 2}+2 u^{\prime} w^{\prime}\right] d x-\int_{0}^{1} f w d x
$$

Passing to the limit for $\delta \rightarrow 0$ and setting to 0 , we obtain

$$
\int_{0}^{1} u^{\prime} w^{\prime} d x-\int_{0}^{1} f w d x=0 \quad \forall w \in V
$$

that is, $u$ satisfies the weak problem (3.9).

Conversely, if $u$ is a solution of $(3.9)$, by setting $v=\delta w$, we have in particular that

$$
\int_{0}^{1} u^{\prime} \delta w^{\prime} d x-\int_{0}^{1} f \delta w d x=0
$$

and therefore

$$
\begin{aligned}
J(u+\delta w)=\frac{1}{2} \int_{0}^{1}\left[(u+\delta w)^{\prime}\right]^{2} d x-\int_{0}^{1} f(u+\delta w) d x \\
=& \frac{1}{2} \int_{0}^{1} u^{\prime 2} d x-\int_{0}^{1} f u d x+\int_{0}^{1} u^{\prime} \delta w^{\prime} d x-\int_{0}^{1} f \delta w d x+\frac{1}{2} \int_{0}^{1} \delta^{2} w^{\prime 2} d x \\
&=J(u)+\frac{1}{2} \int_{0}^{1} \delta^{2} w^{\prime 2} d x
\end{aligned}
$$

Since

$$
\frac{1}{2} \int_{0}^{1} \delta^{2} w^{\prime 2} d x \geq 0 \quad \forall w \in V, \forall \delta \in \mathbb{R}
$$

we deduce that

$$
J(u) \leq J(v) \quad \forall v \in V
$$

that is $u$ also satisfies the variational problem $(3.10)$.

Remark 3.2 (Principle of virtual work). Let us consider again the problem of studying the configuration assumed by a string fixed at the endpoints and subject to a forcing term of intensity $f$, described by equation $(3.3)$. We indicate with $v$ an admissible displacement of the string (that is a null displacement at the endpoints) from the equilibrium position $u$. Equation (3.9), expressing the equality between the work performed by the internal forces and by the external forces in correspondence to the displacement $v$, is nothing but the principle of virtual work of mechanics. Moreover, as in our case there exists a potential (indeed, $J(w)$ defined in (3.10) expresses the global potential energy corresponding to the configuration $w$ of the system), the principle of virtual work establishes that any displacement allowed by the equilibrium configuration causes an increment of the system's potential energy. In this sense, Theorem $3.1$ states that the weak solution is also the one minimizing the potential energy. 

\subsubsection{Non-homogeneous Dirichlet problem}

In the non-homogeneous case the boundary conditions in (3.3) are replaced by

$$
u(0)=g_{0}, \quad u(1)=g_{1}
$$

$g_{0}$ and $g_{1}$ being two assigned values.

We can reduce to the homogeneous case by noticing that if $u$ is a solution of the non-homogeneous problem, then the function $\stackrel{\circ}{u}=u-\left[(1-x) g_{0}+x g_{1}\right]$ is a solution of the corresponding homogeneous problem (3.3). The function $R_{g}=(1-x) g_{0}+x g_{1}$ is said lifting (or extension, or prolongation) of the boundary data.

\subsubsection{Neumann Problem}

Let us now consider the following Neumann problem

$$
\begin{cases}-u^{\prime \prime}+\sigma u=f, & 0<x<1 \\ u^{\prime}(0)=h_{0}, & u^{\prime}(1)=h_{1}\end{cases}
$$

$\sigma$ being a positive function and $h_{0}, h_{1}$ two real numbers. We observe that in the case where $\sigma=0$ the solution of this problem would not be unique, being defined up to an additive constant. By applying the same procedure followed in the case of the Dirichlet problem, that is by multiplying the equation by a test function $v$, integrating on the interval $(0,1)$ and applying the formula of integration by parts, we get the equation

$$
\int_{0}^{1} u^{\prime} v^{\prime} d x+\int_{0}^{1} \sigma u v d x-\left[u^{\prime} v\right]_{0}^{1}=\int_{0}^{1} f v d x
$$

Let us suppose $f \in \mathrm{L}^{2}(0,1)$ and $\sigma \in \mathrm{L}^{\infty}(0,1)$, that is that $\sigma$ is a bounded function almost everywhere (a.e.) on $(0,1)$ (see (2.14)). The boundary term is known from the Neumann conditions. On the other hand, the unknown $u$ is not known at the boundary in this case, hence it must not be required that $v$ is null at the boundary. The weak formulation of the Neumann problem is therefore: find $u \in \mathrm{H}^{\mathrm{I}}(0,1)$ such that

$$
\int_{0}^{1} u^{\prime} v^{\prime} d x+\int_{0}^{1} \sigma u v d x=\int_{0}^{1} f v d x+h_{1} v(1)-h_{0} v(0) \quad \forall v \in \mathrm{H}^{1}(0,1)
$$

In the homogeneous case $h_{0}=h_{1}=0$, the weak problem is characterized by the same equation as the Dirichlet case, but the space $V$ of test functions is now $\mathrm{H}^{1}(0,1)$ instead of $\mathrm{H}_{0}^{1}(0,1)$.

\subsubsection{Mixed homogeneous problem}

Analogous considerations hold for the mixed homogeneous problem, that is when we have a homogeneous Dirichlet condition at one endpoint and a homogeneous Neu- mann condition at the other,

$$
\begin{cases}-u^{\prime \prime}+\sigma u=f, & 0<x<1 \\ u(0)=0, & u^{\prime}(1)=0\end{cases}
$$

In such case it must be required that the test functions are null in $x=0$. Setting $\Gamma_{D}=$ $\{0\}$ and defining

$$
\mathrm{H}_{\Gamma_{D}}^{1}(0,1)=\left\{v \in \mathrm{H}^{1}(0,1): v(0)=0\right\}
$$

the weak formulation of problem (3.12) is: find $u \in \mathrm{H}_{\Gamma_{D}}^{1}(0,1)$ such that

$$
\int_{0}^{1} u^{\prime} v^{\prime} d x+\int_{0}^{1} \sigma u v d x=\int_{0}^{1} f v d x \quad \forall v \in \mathrm{H}_{\Gamma_{D}}^{1}(0,1)
$$

with $f \in \mathrm{L}^{2}(0,1)$ and $\sigma \in \mathrm{L}^{\infty}(0,1)$. The formulation is once again the same as in the homogeneous Dirichlet problem, however the space where to find the solution changes.

\subsubsection{Mixed (or Robin) boundary conditions}

Finally, consider the following problem

$$
\begin{cases}-u^{\prime \prime}+\sigma u=f, & 0<x<1 \\ u(0)=0, & u^{\prime}(1)+\gamma u(1)=r\end{cases}
$$

where $\gamma>0$ and $r$ are two assigned constants.

Also in this case, we will use test functions that are null at $x=0$, the value of $u$ being thereby known. As opposed to the Neumann case, the boundary term for $x=1$, deriving from the integration by parts, no longer provides a known quantity, but a term proportional to the unknown $u$. As a matter of fact, we have

$$
-\left[u^{\prime} v\right]_{0}^{1}=-r v(1)+\gamma u(1) v(1)
$$

The weak formulation is therefore: find $u \in \mathrm{H}_{\Gamma_{D}}^{1}(0,1)$ such that

$$
\int_{0}^{1} u^{\prime} v^{\prime} d x+\int_{0}^{1} \sigma u v d x+\gamma u(1) v(1)=\int_{0}^{1} f v d x+r v(1) \quad \forall v \in \mathrm{H}_{\Gamma_{D}}^{1}(0,1)
$$

A boundary condition that is a linear combination between the value of $u$ and the value of its first derivative is called Robin (or Newton, or third-type) condition.

\subsection{The Poisson problem in the two-dimensional case}

In this section, we consider the boundary-value problems associated to the Poisson equation in the two-dimensional case. 

\subsubsection{The homogeneous Dirichlet problem}

The problem consists in finding $u$ such that

$$
\begin{cases}-\Delta u=f & \text { in } \Omega \\ u=0 & \text { on } \partial \Omega\end{cases}
$$

where $\Omega \subset \mathbb{R}^{2}$ is a bounded domain with boundary $\partial \Omega$. We proceed in a similar way as for the one-dimensional case. By multiplying the differential equation in (3.13) by an arbitrary function $v$ and integrating on $\Omega$, we find

$$
-\int_{\Omega} \Delta u v d \Omega=\int_{\Omega} f v d \Omega
$$

At this point, it is necessary to apply the multi-dimensional analogue of the one-dimensional formula of integration by parts. This can be obtained by applying the divergence (Gauss) theorem by which

$$
\int_{\Omega} \operatorname{div}(\mathbf{a}) d \Omega=\int_{\partial \Omega} \mathbf{a} \cdot \mathbf{n} d \gamma
$$

$\mathbf{a}(\mathbf{x})=\left(a_{1}(\mathbf{x}), a_{2}(\mathbf{x})\right)^{T}$ being a sufficiently regular vector-valued function and $\mathbf{n}(\mathbf{x})=$ $\left(n_{1}(\mathbf{x}), n_{2}(\mathbf{x})\right)^{T}$ the outward unit normal vector on $\partial \Omega$. If we apply (3.14) first to the function $\mathbf{a}=(\varphi \psi, 0)^{T}$ and then to $\mathbf{a}=(0, \varphi \psi)^{T}$, we get the relations

$$
\int_{\Omega} \frac{\partial \varphi}{\partial x_{i}} \psi d \Omega=-\int_{\Omega} \varphi \frac{\partial \psi}{\partial x_{i}} d \Omega+\int_{\partial \Omega} \varphi \psi n_{i} d \gamma, \quad i=1,2
$$

Note also that if we take $\mathbf{a}=\mathbf{b} \varphi$, where $\mathbf{b}$ and $\varphi$ are respectively a vector and a scalar field, then (3.14) yields

$$
\int_{\Omega} \varphi \operatorname{div} \mathbf{b} d \Omega=-\int_{\Omega} \mathbf{b} \cdot \nabla \varphi d \Omega+\int_{\partial \Omega} \mathbf{b} \cdot \mathbf{n} \varphi d \gamma
$$

which is called Green formula for the divergence operator.

We exploit (3.15) by keeping into account the fact that $\Delta u=\operatorname{div} \nabla u=\sum_{i=1}^{2} \frac{\partial}{\partial x_{i}}\left(\frac{\partial u}{\partial x_{i}}\right)$. Supposing that all the integrals make sense, we find

$$
\begin{aligned}
-\int_{\Omega} \Delta u v d \Omega &=-\sum_{i=1}^{2} \int_{\Omega} \frac{\partial}{\partial x_{i}}\left(\frac{\partial u}{\partial x_{i}}\right) v d \Omega \\
&=\sum_{i=1}^{2} \int_{\Omega} \frac{\partial u}{\partial x_{i}} \frac{\partial v}{\partial x_{i}} d \Omega-\sum_{i=1}^{2} \int_{\partial \Omega} \frac{\partial u}{\partial x_{i}} v n_{i} d \gamma \\
&=\int_{\Omega} \sum_{i=1}^{2} \frac{\partial u}{\partial x_{i}} \frac{\partial v}{\partial x_{i}} d \Omega-\int_{\partial \Omega}\left(\sum_{i=1}^{2} \frac{\partial u}{\partial x_{i}} n_{i}\right) v d \gamma
\end{aligned}
$$

We obtain the following relation, called Green formula for the Laplacian

$$
-\int_{\Omega} \Delta u v d \Omega=\int_{\Omega} \nabla u \cdot \nabla v d \Omega-\int_{\partial \Omega} \frac{\partial u}{\partial n} v d \gamma
$$

Similarly to the one-dimensional case, the homogeneous Dirichlet problem will lead us to choose test functions that vanish at the boundary, and, consequently, the boundary term that appears in (3.17) will in turn vanish.

Taking this into account, we get the following weak formulation for problem (3.13)

$$
\text { find } \mathrm{u} \in \mathrm{H}_{0}^{1}(\Omega): \int_{\Omega} \nabla u \cdot \nabla v d \Omega=\int_{\Omega} f v d \Omega \quad \forall v \in \mathrm{H}_{0}^{1}(\Omega)
$$

$f$ being a function of $\mathrm{L}^{2}(\Omega)$ and having set

$$
\begin{aligned}
&\mathrm{H}^{1}(\Omega)=\left\{v: \Omega \rightarrow \mathbb{R} \text { such that } v \in \mathrm{L}^{2}(\Omega), \frac{\partial v}{\partial x_{i}} \in \mathrm{L}^{2}(\Omega), i=1,2\right\} \\
&\mathrm{H}_{0}^{1}(\Omega)=\left\{v \in \mathrm{H}^{1}(\Omega): v=0 \text { on } \partial \Omega\right\}
\end{aligned}
$$

The derivatives must be understood in the sense of distributions and the condition $v=0$ on $\partial \Omega$ in the sense of the traces (see Chap. 2 ). In particular, we observe that if $u, v \in \mathrm{H}_{0}^{1}(\Omega)$, then $\nabla u, \nabla v \in\left[\mathrm{L}^{2}(\Omega)\right]^{2}$ and therefore $\nabla u \cdot \nabla v \in \mathrm{L}^{1}(\Omega)$. The latter property is obtained by applying the following inequality

$$
\left|\int_{\Omega} \nabla u \cdot \nabla v d \Omega\right| \leq\|\nabla u\|_{\mathrm{L}^{2}(\Omega)}\|\nabla v\|_{\mathrm{L}^{2}(\Omega)}
$$

a direct consequence of the Cauchy-Schwarz inequality $(2.17)$.

Hence, the integral appearing in the left side of $(3.18)$ is perfectly meaningful, and so is the one appearing at the right.

Similarly to the one-dimensional case, it can be shown also in the two-dimensional case that problem (3.18) is equivalent to the following variational problem

$$
\text { find } u \in V:\left\{\begin{array}{l}
J(u)=\inf _{v \in V} J(v), \text { with } \\
J(v)=\frac{1}{2} \int_{\Omega}|\nabla v|^{2} d \Omega-\int_{\Omega} f v d \Omega
\end{array}\right.
$$

having set $V=\mathrm{H}_{0}^{1}(\Omega)$.

We can rewrite the weak formulation (3.18) in a more compact way by introducing the following form

$$
a: V \times V \rightarrow \mathbb{R}, \quad a(u, v)=\int_{\Omega} \nabla u \cdot \nabla v d \Omega
$$

and the following functional

$$
F: V \rightarrow \mathbb{R}, \quad F(v)=\int_{\Omega} f v d \Omega
$$

(functionals and forms are introduced in Chap. 2).

Problem (3.18) therefore becomes:

$$
\text { find } u \in V: \quad a(u, v)=F(v) \quad \forall v \in V \text {. }
$$

We notice that $a(\cdot, \cdot)$ is a bilinear form (that is, linear in to both its arguments), while $F$ is a linear functional. Then

$$
|F(v)| \leq\|f\|_{\mathrm{L}^{2}(\Omega)}\|v\|_{\mathrm{L}^{2}(\Omega)} \leq\|f\|_{\mathrm{L}^{2}(\Omega)}\|v\|_{\mathrm{H}^{1}(\Omega)}
$$

Consequently, $F$ is also bounded. Following definition (2.2), its norm is bounded by $\|F\|_{V^{\prime}} \leq\|f\|_{\mathrm{L}^{2}(\Omega)}$. Consequently, $F$ belongs to $V^{\prime}$, the dual space of $V$, that is the set of linear and continuous functionals defined on $V$ (see Sect. 2.1).

\subsubsection{Equivalence, in the sense of distributions, between weak and strong form of the Dirichlet problem}

We want to prove that the equations of problem (3.13) are actually satisfied by the weak solution, albeit only in the sense of distributions.

To this end, we consider the weak formulation (3.18). Let $\mathscr{D}(\Omega)$ now be the space of functions that are infinitely differentiable and with compact support in $\Omega$ (see Chap. 2 ). We recall that $\mathscr{D}(\Omega) \subset \mathrm{H}_{0}^{1}(\Omega)$. Hence, by choosing $v=\varphi \in \mathscr{D}(\Omega)$ in (3.18), we have

$$
\int_{\Omega} \nabla u \cdot \nabla \varphi d \Omega=\int_{\Omega} f \varphi d \Omega \quad \forall \varphi \in \mathscr{D}(\Omega)
$$

By applying Green's formula (3.17) to the left-hand side of $(3.20)$, we find

$$
-\int_{\Omega} \Delta u \varphi d \Omega+\int_{\partial \Omega} \frac{\partial u}{\partial n} \varphi d \gamma=\int_{\Omega} f \varphi d \Omega \quad \forall \varphi \in \mathscr{D}(\Omega)
$$

where the integrals are to be understood via duality, that is:

$$
\begin{aligned}
-\int_{\Omega} \Delta u \varphi d \Omega &=\mathscr{P}^{\prime}(\Omega)\langle-\Delta u, \varphi\rangle_{\mathscr{D}(\Omega)} \\
\int_{\partial \Omega} \frac{\partial u}{\partial n} \varphi d \gamma &=\mathscr{P}^{\prime}(\partial \Omega)\left\langle\frac{\partial u}{\partial n}, \varphi\right\rangle \mathscr{D}(\partial \Omega)
\end{aligned}
$$

Since $\varphi \in \mathscr{D}(\Omega)$, the boundary integral is null, so that

$$
\mathscr{D}^{\prime}(\Omega)\langle-\Delta u-f, \varphi\rangle_{\mathscr{P}(\Omega)}=0 \quad \forall \varphi \in \mathscr{D}(\Omega),
$$

which corresponds to saying that $-\Delta u-f$ is the null distribution, that is

$$
-\Delta u=f \quad \text { in } \mathscr{D}^{\prime}(\Omega) .
$$

The differential equation (3.13) is therefore verified, as long as we intend the derivatives in the sense of distributions and we interpret the equality between $-\Delta u$ and $f$ not in a pointwise sense, but in the sense of distributions (and thus almost everywhere in $\Omega$ ). Finally, the fact that $u$ vanishes on the boundary (in the sense of traces) is a direct consequence of $u$ being in $\mathrm{H}_{0}^{1}(\Omega)$.

\subsubsection{The problem with mixed, non homogeneous conditions}

The problem we want to solve is now the following

$$
\begin{cases}-\Delta u=f & \text { in } \Omega \\ u=g & \text { on } \Gamma_{D} \\ \frac{\partial u}{\partial n}=\phi & \text { on } \Gamma_{N}\end{cases}
$$

where $\Gamma_{D}$ and $\Gamma_{N}$ yield a partition of $\partial \Omega$, that is $\Gamma_{D} \cup \Gamma_{N}=\partial \Omega, \stackrel{\circ}{\Gamma_{D}} \cap \Gamma_{N}=\emptyset$ (see Fig. 3.3).

In the case of the Neumann problem, where $\Gamma_{D}=\emptyset$, the data $f$ and $\phi$ must verify the following compatibility condition

$$
-\int_{\partial \Omega} \phi d \gamma=\int_{\Omega} f d \Omega
$$

in order for the problem to have a solution. Condition (3.22) is deduced by integrating the differential equation in (3.21) and applying the divergence theorem (3.14)

$$
-\int_{\Omega} \Delta u d \Omega=-\int_{\Omega} \operatorname{div}(\nabla u) d \Omega=-\int_{\partial \Omega} \frac{\partial u}{\partial n} d \gamma
$$

Moreover, we observe that also in the case of the Neumann problem, the solution is defined only up to an additive constant. In order to have uniqueness it would be sufficient, for example, to find a function with null average in $\Omega$.

Let us now suppose that $\Gamma_{D} \neq \emptyset$ in order to ensure the uniqueness of the solution to the strong problem without conditions of compatibility on the data. Let us also suppose that $f \in \mathrm{L}^{2}(\Omega), g \in \mathrm{H}^{1 / 2}\left(\Gamma_{D}\right)$ and $\phi \in \mathrm{L}^{2}\left(\Gamma_{N}\right)$, having denoted by $\mathrm{H}^{1 / 2}\left(\Gamma_{D}\right)$ the space of functions of $L^{2}\left(\Gamma_{D}\right)$ that are traces of functions of $\mathrm{H}^{1}(\Omega)$ (see Sect. 2.4.3).

By Green's formula (3.17) we obtain from (3.21)

$$
\int_{\Omega} \nabla u \cdot \nabla v d \Omega-\int_{\partial \Omega} \frac{\partial u}{\partial n} v d \gamma=\int_{\Omega} f v d \Omega
$$



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-048.jpg?height=239&width=309&top_left_y=118&top_left_x=301)

Fig. 3.3. The computational domain $\Omega$

We recall that $\partial u / \partial n=\phi$ on $\Gamma_{N}$, and by exploiting the additivity of integrals, (3.23) becomes

$$
\int_{\Omega} \nabla u \cdot \nabla v d \Omega-\int_{\Gamma_{D}} \frac{\partial u}{\partial n} v d \gamma-\int_{\Gamma_{N}} \phi v d \gamma=\int_{\Omega} f v d \Omega
$$

By forcing the test function $v$ to vanish on $\Gamma_{D}$, the first boundary integral appearing in (3.24) vanishes. The mixed problem therefore admits the following weak formulation

$$
\text { find } u \in V_{g}: \quad \int_{\Omega} \nabla u \cdot \nabla v d \Omega=\int_{\Omega} f v d \Omega+\int_{\Gamma_{N}} \phi v d \gamma \quad \forall v \in V,
$$

having denoted by $V$ the space

$$
V=\mathrm{H}_{\Gamma_{D}}^{1}(\Omega)=\left\{v \in \mathrm{H}^{1}(\Omega):\left.v\right|_{\Gamma_{D}}=0\right\},
$$

and having set

$$
V_{g}=\left\{v \in \mathrm{H}^{1}(\Omega):\left.v\right|_{\Gamma_{D}}=g\right\}
$$

The formulation $(3.25)$ is not satisfactory, not only because the choice of spaces is "asymmetrical" $\left(v \in V\right.$, while $\left.u \in V_{g}\right)$, but mainly because $V_{g}$ is an affine manifold, but not a subspace of $\mathrm{H}^{1}(\Omega)$ (indeed, it is not true that linear combinations of elements of $V_{g}$ are still elements of $V_{g}$ ).

We then proceed similarly to what we saw in Sect. 3.2.2. We suppose to know a function $R_{g}$, called lifting of the boundary data, such that

$$
R_{g} \in \mathrm{H}^{1}(\Omega),\left.\quad R_{g}\right|_{\Gamma_{D}}=g
$$

Furthermore, we suppose that such lifting are continuous, i.e. that

$$
\exists C>0:\left\|R_{g}\right\|_{\mathrm{H}^{1}(\Omega)} \leq C\|g\|_{\mathrm{H}^{1 / 2}\left(\Gamma_{D}\right)} \forall g \in \mathrm{H}^{1 / 2}\left(\Gamma_{D}\right)
$$

We set $\stackrel{\circ}{u}=u-R_{g}$ and we begin by observing that $\left.\stackrel{u}\right|_{\Gamma_{D}}=\left.u\right|_{\Gamma_{D}}-\left.R_{g}\right|_{\Gamma_{D}}=0$, that is $\stackrel{u}{u} \in \mathrm{H}_{\Gamma_{D}}^{1}(\Omega)$. Moreover, since $\nabla u=\nabla \stackrel{\circ}{u}+\nabla R_{g}$, problem ( $\left.3.25\right)$ becomes

$$
\text { find } \stackrel{\circ}{u} \in \mathrm{H}_{\Gamma_{D}}^{1}(\Omega): \quad a(\stackrel{\circ}, v)=F(v) \quad \forall v \in \mathrm{H}_{\Gamma_{D}}^{1}(\Omega)
$$

having defined the bilinear form $a(\cdot, \cdot)$ as in (3.19), while the linear functional $F$ now takes the form

$$
F(v)=\int_{\Omega} f v d \Omega+\int_{\Gamma_{N}} \phi v d \gamma-\int_{\Omega} \nabla R_{g} \cdot \nabla v d \Omega
$$

The problem is now symmetrical since the space where the (new) unknown solution is sought coincides with the test function space.

The Dirichlet conditions are said to be essential as they are imposed explicitly in the functional space in which the problem is set.

The Neumann conditions are instead said to be natural, as they are satisfied implicitly by the solution of the problem (to this end, see Sect. 3.3.4). This difference in treatment has important ripercussions on the approximate problems.

Remark 3.3. The reduction of the problem to a "symmetrical" form allows to obtain a linear system with a symmetric matrix when solving the problem numerically (for instance via the finite elements method).

Remark 3.4. Building a lifting $R_{g}$ of a boundary function with an arbitrary form can turn out to be problematic. Such task is simpler in the context of a numerical approximation, where one generally builds a lifting of an approximation of the function $g$ (see Chap. 4).

\subsubsection{Equivalence, in the sense of distributions, between weak and strong form of the Neumann problem}

Let us consider the nonhomogeneous Neumann problem

$$
\begin{cases}-\Delta u+\sigma u=f & \text { in } \Omega \\ \frac{\partial u}{\partial n}=\phi & \text { on } \partial \Omega\end{cases}
$$

where $\sigma$ is a positive constant or, more generally, a function $\sigma \in \mathrm{L}^{\infty}(\Omega)$ such that $\sigma(\mathbf{x}) \geq \alpha_{0}$ a.e. in $\Omega$, for a well-chosen constant $\alpha_{0}>0$. Let us also suppose that $f \in \mathrm{L}^{2}(\Omega)$ and that $\phi \in \mathrm{L}^{2}(\partial \Omega)$. By proceeding as in Sect. 3.3.3, the following weak formulation can be derived:

$$
\begin{aligned}
&\text { find } u \in \mathrm{H}^{1}(\Omega): \\
&\int_{\Omega} \nabla u \cdot \nabla v d \Omega+\int_{\Omega} \sigma u v d \Omega=\int_{\Omega} f v d \Omega+\int_{\partial \Omega} \phi v d \gamma \quad \forall v \in \mathrm{H}^{1}(\Omega) .
\end{aligned}
$$

By taking $v=\varphi \in \mathscr{D}(\Omega)$ and counterintegrating by parts, we obtain

$$
\mathscr{D}^{\prime}(\Omega)\langle-\Delta u+\sigma u-f, \varphi\rangle_{\mathscr{D}(\Omega)}=0 \quad \forall \varphi \in \mathscr{D}(\Omega)
$$

Hence

$$
-\Delta u+\sigma u=f \quad \text { in } \mathscr{D}^{\prime}(\Omega)
$$

i.e.

$$
-\Delta u+\sigma u-f=0 \quad \text { a.e. in } \Omega
$$

In the case where $u \in C^{2}(\Omega)$ the application of Green's formula (3.17) in (3.29) leads to

$$
\int_{\Omega}(-\Delta u+\sigma u-f) v d \Omega+\int_{\partial \Omega}\left(\frac{\partial u}{\partial n}-\phi\right) v=0 \quad \forall v \in \mathrm{H}^{1}(\Omega)
$$

and therefore, by $(3.30)$,

$$
\frac{\partial u}{\partial n}=\phi \quad \text { on } \partial \Omega
$$

In the case where the solution $u$ of $(3.29)$ is only in $\mathrm{H}^{1}(\Omega)$ the generalized Green formula can be used, which states that there exists a unique linear and continuous functional $g \in\left(\mathrm{H}^{1 / 2}(\partial \Omega)\right)^{\prime}$ (called generalized normal derivative), which operates on the space $\mathrm{H}^{1 / 2}(\partial \Omega)$ and satisfies

$$
\int_{\Omega} \nabla u \cdot \nabla v d \Omega=\langle-\Delta u, v\rangle+\ll g, v \gg \quad \forall v \in \mathrm{H}^{1}(\Omega)
$$

We have denoted by $<\cdot, \cdot>$ the pairing between $\mathrm{H}^{1}(\Omega)$ and its dual, and by $\ll \cdot, \cdot \gg$ the pairing between $\mathrm{H}^{1 / 2}(\partial \Omega)$ and its dual. Clearly $g$ coincides with the classical normal derivative of $u$ if $u$ has sufficient regularity. For the sake of simplicity we use the notation $\partial u / \partial n$ for the generalized normal derivative in the remainder of this chapter. We therefore obtain that for $v \in \mathrm{H}^{1}(\Omega)$

$$
\langle-\Delta u+\sigma u-f, v\rangle+\ll \partial u / \partial n-\phi, v \gg=0
$$

using (3.30) we finally conclude that

$$
\ll \partial u / \partial n-\phi, v \gg=0 \quad \forall v \in \mathrm{H}^{1}(\Omega)
$$

and thus that $\partial u / \partial n=\phi$ a.e. on $\partial \Omega$. 

\subsection{More general elliptic problems}

Let us now consider the problem

$$
\left\{\begin{array}{lll}
-\operatorname{div}(\mu \nabla u)+\sigma u=f & \text { in } \Omega \\
u=g & & \text { on } \Gamma_{D} \\
\mu \frac{\partial u}{\partial n}=\phi & & \text { on } \Gamma_{N}
\end{array}\right.
$$

where $\Gamma_{D} \cup \Gamma_{N}=\partial \Omega$ with $\stackrel{\circ}{\Gamma}_{D} \cap \stackrel{\circ}{\Gamma_{N}}=\emptyset$. We will suppose that $f \in \mathrm{L}^{2}(\Omega), \mu, \sigma \in$ $\mathrm{L}^{\infty}(\Omega)$. Furthermore, we suppose that there is a $\mu_{0}>0$ such that $\mu(\mathbf{x}) \geq \mu_{0}$ and $\sigma(\mathbf{x}) \geq 0$ a.e. in $\Omega$. Only in the case where $\sigma=0$ we will require that $\Gamma_{D}$ is nonempty in order to prevent the solution from losing uniqueness. Finally, we will suppose that $g$ and $\phi$ are sufficiently regular functions on $\partial \Omega$, for instance $g \in \mathrm{H}^{1 / 2}\left(\Gamma_{D}\right)$ and $\phi \in \mathrm{L}^{2}\left(\Gamma_{N}\right)$

Also in this case, we proceed by multiplying the equation by a test function $v$ and by integrating (once again formally) on the domain $\Omega$ :

$$
\int_{\Omega}[-\operatorname{div}(\mu \nabla u)+\sigma u] v d \Omega=\int_{\Omega} f v d \Omega
$$

By applying Green's formula we obtain

$$
\int_{\Omega} \mu \nabla u \cdot \nabla v d \Omega+\int_{\Omega} \sigma u v d \Omega-\int_{\partial \Omega} \mu \frac{\partial u}{\partial n} v d \gamma=\int_{\Omega} f v d \Omega
$$

which can also be rewritten as

$$
\int_{\Omega} \mu \nabla u \cdot \nabla v d \Omega+\int_{\Omega} \sigma u v d \Omega-\int_{\Gamma_{D}} \mu \frac{\partial u}{\partial n} v d \gamma=\int_{\Omega} f v d \Omega+\int_{\Gamma_{N}} \mu \frac{\partial u}{\partial n} v d \gamma
$$

The function $\mu \partial u / \partial n$ is called conormal derivative of $u$ associated to the operator $-\operatorname{div}(\mu \nabla u)$. On $\Gamma_{D}$ we impose that the test function $v$ is null, while on $\Gamma_{N}$ we impose that the conormal derivative is equal to $\phi$. We obtain

$$
\int_{\Omega} \mu \nabla u \cdot \nabla v d \Omega+\int_{\Omega} \sigma u v d \Omega=\int_{\Omega} f v d \Omega+\int_{\Gamma_{N}} \phi v d \gamma
$$

Having denoted by $R_{g}$ a lifting of $g$, we set $\ddot{u}=u-R_{g}$. The weak formulation of problem $(3.31)$ is therefore

$$
\begin{aligned}
&\text { find } \dot{u} \in \mathrm{H}_{\Gamma_{D}}^{1}(\Omega): \\
&\qquad \int_{\Omega} \mu \nabla \dot{u}^{\circ} \cdot \nabla v d \Omega+\int_{\Omega} \sigma u v d \Omega=\int_{\Omega} f v d \Omega \\
&-\int \mu \nabla R_{g} \cdot \nabla v d \Omega-\int_{\Omega} \sigma R_{g} v d \Omega+\int_{\Gamma_{N}} \phi v d \gamma \quad \forall v \in \mathrm{H}_{\Gamma_{D}}^{1}(\Omega) .
\end{aligned}
$$

We define the bilinear form

$$
a: V \times V \rightarrow \mathbb{R}, \quad a(u, v)=\int_{\Omega} \mu \nabla u \cdot \nabla v d \Omega+\int_{\Omega} \sigma u v d \Omega
$$

and the linear and continuous functional

$$
F: V \rightarrow \mathbb{R}, F(v)=-a\left(R_{g}, v\right)+\int_{\Omega} f v d \Omega+\int_{\Gamma_{N}} \phi v d \gamma
$$

The previous problem can then be rewritten as

$$
\text { find } \stackrel{\circ}{u} \in \mathrm{H}_{\Gamma_{D}}^{1}(\Omega): \quad a(\dot{u}, v)=F(v) \quad \forall v \in \mathrm{H}_{\Gamma_{D}}^{1}(\Omega)
$$

A yet more general problem than (3.31) is the following

$$
\begin{cases}L u=f & \text { in } \Omega \\ u=g & \text { on } \Gamma_{D} \\ \frac{\partial u}{\partial n_{L}}=\phi & \text { on } \Gamma_{N}\end{cases}
$$

where, as usual, $\Gamma_{D} \cup \Gamma_{N}=\partial \Omega, \stackrel{\circ}{\Gamma}_{D} \cap \stackrel{\circ}{\Gamma_{N}}=\emptyset$, and having defined

$$
L u=-\sum_{i, j=1}^{2} \frac{\partial}{\partial x_{i}}\left(a_{i j} \frac{\partial u}{\partial x_{j}}\right)+\sigma u
$$

The coefficients $a_{i j}$ are functions defined on $\Omega$. The derivative

$$
\frac{\partial u}{\partial n_{L}}=\sum_{i, j=1}^{2} a_{i j} \frac{\partial u}{\partial x_{j}} n_{i}
$$

is called conormal derivative of $u$ associated to the operator $L$ (it coincides with the normal derivative when $L u=-\Delta u)$.

Let us suppose that $\sigma(\mathbf{x}) \in \mathrm{L}^{\infty}(\Omega)$ and that there exists an $\alpha_{0}>0$ such that $\sigma(\mathbf{x}) \geq \alpha_{0}$ a.e. in $\Omega$. Furthermore, let us suppose that the coefficients $a_{i j}: \bar{\Omega} \rightarrow \mathbb{R}$ are continuous functions $\forall i, j=1,2$, and that there exists a positive constant $\alpha$ such that

$$
\forall \boldsymbol{\xi}=\left(\xi_{1}, \xi_{2}\right)^{T} \in \mathbb{R}^{2} \quad \sum_{i, j=1}^{2} a_{i j}(\mathbf{x}) \xi_{i} \xi_{j} \geq \alpha \sum_{i=1}^{2} \xi_{i}^{2} \quad \text { a.e. in } \Omega
$$

In such case, the weak formulation is still the same as $(3.33)$, the functional $F$ is still the one introduced in (3.32), while

$$
a(u, v)=\int_{\Omega}\left(\sum_{i, j=1}^{2} a_{i j} \frac{\partial u}{\partial x_{j}} \frac{\partial v}{\partial x_{i}}+\sigma u v\right) d \Omega
$$

It can be shown (see Exercise 2 ) that under the ellipticity hypothesis on the coefficients (3.35), this bilinear form is continuous and coercive, in the sense of definitions (2.6) and (2.9). These properties will be exploited in the analysis of the well-posedness of problem (3.33) (see Sect. 3.4.1).

Elliptic problems for fourth-order operators are proposed in Exercises 4 and 6, while an elliptic problem deriving from the linear elasticity theory is analyzed in Exercise $7$.

Remark 3.5 (Robin conditions). The case where Robin boundary conditions are enforced on the whole boundary, say

$$
\mu \frac{\partial u}{\partial n}+\gamma u=0 \quad \text { on } \partial \Omega
$$

requires more care. In this case the weak form of the problem reads

$$
\text { find } u \in H^{1}(\Omega): a(u, v)=\int_{\Omega} f v d \Omega \quad \forall v \in H^{1}(\Omega)
$$

where $a(u, v)=\int_{\Omega} \mu \nabla u \cdot \nabla v d \Omega+\int_{\Omega} \gamma u v d \Omega$. This bilinear form is not coercive if $\gamma<0$. The analysis of this problem can be carried out e.g. by means of the PeetreTartar lemma, see [EG04].

\subsubsection{Existence and uniqueness theorem}

The following fundamental result holds (refer to Sect. $2.1$ for definitions):

Proof. This is based on two classical results of Functional Analysis: the Riesz representation theorem (see Theorem 2.1, Chap. 2), and the Banach closed range theorem. The interested reader can refer to, e.g., [QV94, Chap. 5].

The Lax-Milgram lemma thus ensures that the weak formulation of an elliptic problem is well posed, as long as the hypotheses on the form $a(\cdot, \cdot)$ and on the functional $F(\cdot)$ hold. Several consequences derive from this lemma. We report one of the most important in the following corollary. 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-054.jpg?height=204&width=720&top_left_y=116&top_left_x=97)

Proof. It is sufficient to choose $v=u$ in (3.37) and then to use the coercivity of the bilinear form $a(\cdot, \cdot)$. Indeed, we have

$$
\alpha\|u\|_{V}^{2} \leq a(u, u)=F(u)
$$

On the other hand, since $F$ is linear and continuous it is also bounded, and the upper bound

$$
|F(u)| \leq\|F\|_{V^{\prime}}\|u\|_{V}
$$

holds, hence the claim follows.

Remark 3.6. If the bilinear form $a(\cdot, \cdot)$ is additionally symmetric, that is

$$
a(u, v)=a(v, u) \quad \forall u, v \in V,
$$

then (3.37) is equivalent to the following variational problem (see Exercise 1 )

$$
\left\{\begin{array}{l}
\text { find } u \in V: \quad J(u)=\min _{v \in V} J(v) \\
\text { with } J(v)=\frac{1}{2} a(v, v)-F(v)
\end{array}\right.
$$

\subsection{Adjoint operator and adjoint problem}

In this section we will introduce the concept of adjoint of a given operator in Hilbert spaces, as well as the adjoint (or dual) problem of a given boundary-value problem. Then we will show how to obtain dual problems, with associated boundary conditions. The adjoint problem of a given differential problem plays a fundamental role, for instance, when establishing error estimates for Galerkin methods, both a priori and a posteriori (see Sects. 4.5.4 and 4.6.4-4.6.5, respectively), but also for the solution of optimal-control problems, as we will see in Chapter 18 .

Let $V$ be a Hilbert space with scalar product $(\cdot, \cdot)_{V}$ and norm $\|\cdot\|_{V}$, and let $V^{\prime}$ be its dual space. Let $a: V \times V \rightarrow \mathbb{R}$ be a continuous and coercive bilinear form and let $A: V \rightarrow V^{\prime}$ be its associated elliptic operator, that is $A \in \mathscr{L}\left(V, V^{\prime}\right)$

$$
{ }_{V^{\prime}}\langle A v, w\rangle_{V}=a(v, w) \quad \forall v, w \in V
$$

Let $a^{*}: V \times V \rightarrow \mathbb{R}$ be the bilinear form defined by

$$
a^{*}(w, v)=a(v, w) \quad \forall v, w \in V,
$$

and consider the operator $A^{*}: V \rightarrow V^{\prime}$ associated to the form $a^{*}(\cdot, \cdot)$, that is

$$
v^{\prime}\left\langle A^{*} w, v\right\rangle_{V}=a^{*}(w, v) \quad \forall v, w \in V
$$

Thanks to $(3.40)$ we have the following relation, known as the Lagrange identity

$$
{ }_{V^{\prime}}\left\langle A^{*} w, v\right\rangle_{V}={ }_{V^{\prime}}\langle A v, w\rangle_{V} \quad \forall v, w \in V
$$

Note that this is precisely the equation that stands at the base of the definition $(2.20)$ of the adjoint of a given operator $A$ acting between a Hilbert space and its dual. For coherence with (2.20), we should have noted this operator $A^{\prime}$. However, we prefer to denote it $A^{*}$ because the latter notation is more customarily used in the context of elliptic boundary value problems.

If $a(\cdot, \cdot)$ is a symmetric form, $a^{*}(\cdot, \cdot)$ coincides with $a(\cdot, \cdot)$ and $A^{*}$ with $A$. In such case $A$ is said to be self-adjoint $; A$ is said to be normal if $A A^{*}=A^{*} A$.

Naturally, the identity operator $I$ is self-adjoint $\left(I=I^{*}\right)$, while if an operator is selfadjoint, then it is also normal.

Some properties of the adjoint operators which are a consequence of the previous definition, are listed below:

- $A$ being linear and continuous, then also $A^{*}$ is, that is $A^{*} \in \mathscr{L}\left(V, V^{\prime}\right)$;

- $\left\|A^{*}\right\|_{\mathscr{L}\left(\mathscr{V}, \mathscr{V}^{\prime}\right)}=\|A\|_{\mathscr{L}\left(\mathscr{V}, \mathscr{V}^{\prime}\right)}$ (these norms are defined in $\left.(2.21)\right)$;

- $(A+B)^{*}=A^{*}+B^{*}$;

- $(A B)^{*}=B^{*} A^{*}$

- $\left(A^{*}\right)^{*}=A$

- $\quad\left(A^{-1}\right)^{*}=\left(A^{*}\right)^{-1}$ (if $A$ is invertible);

-

$A)^{*}=\alpha A^{*}$

$\forall \alpha \in \mathbb{R}$.

When we need to find the adjoint (or dual) problem of a given (primal) problem, we will use the Lagrange identity to characterize the differential equation of the dual problem, as well as its boundary conditions.

We provide an example of such a procedure, starting from a simple one-dimensional diffusion transport equation, completed by homogeneous Robin-Dirichlet boundary conditions

$$
\begin{cases}A v=-v^{\prime \prime}+v^{\prime}=f, & x \in I=(0,1), \\ v^{\prime}(0)+\beta v(0)=0, & v(1)=0\end{cases}
$$

assuming $\beta$ constant. Note that the weak form of this problem is

$$
\text { find } u \in V \text { such that } a(u, v)=\int_{0}^{1} f v d x \quad \forall v \in V,
$$

where $V=\left\{v \in H^{1}(0,1): v(1)=0\right\}$ and

$$
a: V \times V \rightarrow \mathbb{R}, a(u, v)=\int_{0}^{1}\left(u^{\prime}-u\right) v^{\prime} d x-(\beta+1) u(0) v(0)
$$

By (3.40) we obtain, $\forall v, w \in V$,

$$
\begin{aligned}
a^{*}(w, v)=a(v, w) &=\int_{0}^{1}\left(v^{\prime}-v\right) w^{\prime} d x-(\beta+1) v(0) w(0) \\
&=-\int_{0}^{1} v\left(w^{\prime \prime}+w^{\prime}\right) d x+\left[v w^{\prime}\right]_{0}^{1}-(\beta+1) v(0) w(0) \\
&=\int_{0}^{1}\left(-w^{\prime \prime}-w^{\prime}\right) v d x-\left[w^{\prime}(0)+(\beta+1) w(0)\right] v(0)
\end{aligned}
$$

Since definition (3.41) must hold, we will have

$$
A^{*} w=-w^{\prime \prime}-w^{\prime} \quad \text { in } \mathscr{D}^{\prime}(0,1)
$$

Moreover, as $v(0)$ is arbitrary, $w$ will need to satisfy the boundary conditions

$$
\left[w^{\prime}+(\beta+1) w\right](0)=0, \quad w(1)=0
$$

We observe that the transport field of the dual problem has an opposite direction with respect to that of the primal problem. Moreover, to homogeneous Robin-Dirichlet boundary conditions for the primal problem (3.43) correspond conditions of exactly the same nature for the dual problem.

The procedure illustrated for problem (3.43) can clearly be extended to the multidimensional case. In Table $3.1$ we provide a list of several differential operators with boundary conditions, and their corresponding adjoint operators with associated boundary conditions. (On the functions appearing in the table assume all the necessary regularity for the considered differential operators to be well-defined). We note, in particular, that to a given type of primal conditions do not necessarily correspond dual conditions of the same type, and that, for an operator that is not self-adjoint, to a conservative (resp. non-conservative) formulation of the primal problem corresponds a non-conservative (resp. conservative) formulation of the dual one.

\subsubsection{The nonlinear case}

The extension of the analysis in the previous section to the nonlinear case is not so immediate. For simplicity, we consider the one-dimensional problem

$$
\left\{\begin{array}{l}
A(v) v=-v^{\prime \prime}+v v^{\prime}=f, \quad x \in I=(0,1) \\
v(0)=v(1)=0
\end{array}\right.
$$



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-057.jpg?height=1129&width=655&top_left_y=113&top_left_x=129)

having denoted by $A(v)$ the operator

$$
A(v) \cdot=-\frac{d^{2}}{d x^{2}}+v \frac{d}{d x}
$$

The Lagrange identity (3.42) is now generalized as

$$
v^{\prime}\langle A(v) u, w\rangle_{V}={ }_{V}\left\langle u, A^{*}(v) w\right\rangle_{V^{\prime}}
$$

for each $u \in D(A)$ and $w \in D\left(A^{*}\right), D(A)$ being the domain of $A$, that is the set of functions of class $C^{2}$ that are null at $x=0$ and $x=1$, and $D\left(A^{*}\right)$ the domain of the adjoint (or dual) operator $A^{*}$ whose properties will be identified by imposing $(3.47)$. Starting from such identity, let us see which adjoint operator $A^{*}$ and which dual boundary conditions we get for problem (3.45). By integrating by parts the diffusion term twice and the transport term of order one once, we obtain

$$
\begin{aligned}
v^{\prime}\langle A(v) u, w\rangle_{V} &=-\int_{0}^{1} u^{\prime \prime} w d x+\int_{0}^{1} v u^{\prime} w d x \\
&=\int_{0}^{1} u^{\prime} w^{\prime} d x-\left.u^{\prime} w\right|_{0} ^{1}-\int_{0}^{1}(v w)^{\prime} u d x+\left.v u w\right|_{0} ^{1} \\
&=-\int_{0}^{1} u w^{\prime \prime} d x+\left.u w^{\prime}\right|_{0} ^{1}-\left.u^{\prime} w\right|_{0} ^{1}-\int_{0}^{1}(v w)^{\prime} u d x+\left.v u w\right|_{0} ^{1} .
\end{aligned}
$$

Let us analyze the boundary terms separately, by makin the contribution at both endpoints explicit. In order to guarantee (3.47), we must have

$u(1) w^{\prime}(1)-u(0) w^{\prime}(0)-u^{\prime}(1) w(1)+u^{\prime}(0) w(0)+v(1) u(1) w(1)-v(0) u(0) w(0)=0$

for each $u$ and $v \in D(A)$. We observe that the fact that $u$ belongs to $D(A)$ allows us to ignore, as vanishing, both the first two and the last two summands, so that we end up having

$$
-u^{\prime}(1) w(1)+u^{\prime}(0) w(0)=0
$$

Since such relation must hold for each $u \in D(A)$, we must choose homogeneous Dirichlet conditions for the dual operator, i.e.

$$
w(0)=w(1)=0
$$

Reverting to $(3.48)$, we then have

$$
\begin{aligned}
v^{\prime}\langle A(v) u, w\rangle_{V} &=-\int_{0}^{1} u^{\prime \prime} w d x+\int_{0}^{1} v u^{\prime} w d x \\
&=-\int_{0}^{1} u w^{\prime \prime} d x-\int_{0}^{1}(v w)^{\prime} u d x=v\left\langle u, A^{*}(v) w\right\rangle_{V^{\prime}}
\end{aligned}
$$

The adjoint operator $A^{*}$ of the primal operator $A$ defined in (3.46) therefore reads

$$
A^{*}(v) \cdot=-\frac{d^{2} \cdot}{d x^{2}}+\frac{d}{d x} v
$$

while the dual boundary conditions are provided by (3.49). To conclude, we note that the dual problem is always linear, even though we started from a nonlinear primal problem.

For more details on the differentiation and on the analysis of the adjoint problems, we refer the reader to, e.g., [Mar95].

\subsection{Exercises}

1. Prove that the weak problem (3.37) is equivalent to the variational problem (3.38) if the bilinear form is coercive and symmetric.

[Solution: let $u \in V$ be the solution of the weak problem and let $w$ be a generic element of $V$. Thanks to the bilinearity and to the symmetry of the form, we find

$$
\begin{aligned}
J(u+w) &=\frac{1}{2}[a(u, u)+2 a(u, w)+a(w, w)]-[F(u)+F(w)] \\
&=J(u)+[a(u, w)-F(w)]+\frac{1}{2} a(w, w)=J(u)+{ }_{2} a(w, w) .
\end{aligned}
$$

Thanks to the coercivity we then obtain that $J(u+w) \geq J(u)+(\alpha / 2)\|w\|_{V}^{2}$, that is $\forall v \in V$ with $v=u, J(v)>J(u)$. Conversely, if $u$ is a minimum for $J$, then by writing the extremality condition $\lim _{\delta \rightarrow 0}(J(u+\delta v)-J(u)) / \delta=0$ we find (3.37).]

2. Prove that the bilinear form (3.36) is continuous and coercive under the hypotheses listed in the text on the coefficients.

[Solution: the bilinear form is obviously continuous. Thanks to the hypothesis (3.35) and to the fact that $\sigma \in \mathrm{L}^{\infty}(\Omega)$ is positive a.e. in $\Omega$, it is also coercive as

$$
a(v, v) \geq \alpha|v|_{\mathrm{H}^{1}(\Omega)}^{2}+\alpha_{0}\|v\|_{\mathrm{L}^{2}(\Omega)}^{2} \geq \min \left(\alpha, \alpha_{0}\right)\|v\|_{V}^{2} \quad \forall v \in V
$$

We point out that if $V=\mathrm{H}^{1}(\Omega)$ then the condition $\alpha_{0}>0$ is necessary for the bilinear form to be coercive. In the case where $V=\mathrm{H}_{0}^{1}(\Omega)$, it is sufficient that $\alpha_{0}>-\alpha / C_{\Omega}^{2}, C_{\Omega}$ being the constant intervening in the Poincaré inequality $(\mathrm{see}$ (2.13)). In this case, the equivalence between $\|\cdot\|_{\mathrm{H}^{1}(\Omega)}$ and $|\cdot|_{\mathrm{H}^{1}(\Omega)}$ can indeed be exploited. See Property $2.5$ of Chapter 2.]

3. Let $V=\mathrm{H}_{0}^{1}(0,1)$, and take $a: V \times V \rightarrow \mathbb{R}$ and $F: V \rightarrow \mathbb{R}$ defined in the following way:

$$
F(v)=\int_{0}^{1}(-1-4 x) v(x) d x, \quad a(u, v)=\int_{0}^{1}(1+x) u^{\prime}(x) v^{\prime}(x) d x
$$

Prove that the problem: find $u \in V$ such that $a(u, v)=F(v) \forall v \in V$, admits a unique solution. Moreover, verify that this solution coincides with $u(x)=x^{2}-x$.

[Solution: it can be easily shown that the bilinear form is continuous and coercive in $V$. Then, since $F$ is a linear and continuous functional, by the Lax-Milgram lemma we can conclude that there exists a unique solution in $V$. We verify that the latter is indeed $u(x)=x^{2}-x$. The latter function belongs for sure to $V$ (since it is continuous and differentiable and such that $u(0)=u(1)=0$ ). Moreover, from the relation

$$
\int_{0}^{1}(1+x) u^{\prime}(x) v^{\prime}(x) d x=-\int_{0}^{1}\left((1+x) u^{\prime}(x)\right)^{\prime} v(x) d x=\int_{0}^{1}(-1-4 x) v(x) d x
$$

valid $\forall v \in V$, we deduce that in order for $u$ to be a solution we must have $((1+$ $\left.x) u^{\prime}(x)\right)^{\prime}=1+4 x$ almost everywhere in $(0,1)$. Such property holds for the proposed $u .]$

4. Find the weak formulation of the problem

$$
\begin{cases}\Delta^{2} u=f & \text { in } \Omega \\ u=0 & \text { on } \partial \Omega \\ \frac{\partial u}{\partial n}=0 & \text { on } \partial \Omega\end{cases}
$$

$\Omega \subset \mathbb{R}^{2}$ being a bounded open set with regular boundary $\partial \Omega, \Delta^{2} \cdot=\Delta \Delta \cdot$ the $b i-$ laplacian operator and $f \in \mathrm{L}^{2}(\Omega)$ an assigned function.

[Solution: the weak formulation, obtained by applying Green's formula twice to the bilaplacian operator, is

$$
\text { find } u \in \mathrm{H}_{0}^{2}(\Omega): \int_{\Omega} \Delta u \Delta v d \Omega=\int_{\Omega} f v d \Omega \quad \forall v \in \mathrm{H}_{0}^{2}(\Omega) \text {, }
$$

where $\mathrm{H}_{0}^{2}(\Omega)=\left\{v \in \mathrm{H}^{2}(\Omega): v=0, \partial v / \partial n=0\right.$ on $\left.\partial \Omega\right\}$..

5. For each function $v$ of the Hilbert space $\mathrm{H}_{0}^{2}(\Omega)$, defined in Exercise 4, it can be shown that the seminorm $|\cdot|_{\mathrm{H}^{2}(\Omega)}$ defined as $|v|_{\mathrm{H}^{2}(\Omega)}=\left(\int_{\Omega}|\Delta v|^{2} d \Omega\right)^{1 / 2}$ is in fact equivalent to the norm $\|\cdot\|_{\mathrm{H}^{2}(\Omega)}$. Using such property, prove that problem (3.50) admits a unique solution.

[Solution: let us set $V=\mathrm{H}_{0}^{2}(\Omega)$. Then

$$
a(u, v)=\int_{\Omega} \Delta u \Delta v d \Omega \quad \text { and } F(v)=\int_{\Omega} f v d \Omega
$$

are a bilinear form from $V \times V \rightarrow \mathbb{R}$ and a linear and continuous functional, respectively. To prove existence and uniqueness it is sufficient to invoke the Lax-Milgram lemma as the bilinear form is coercive and continuous. Indeed, thanks to the equivalence between norm and seminorm, there exist two positive constants $\alpha$ and $M$ such that

$$
\left.a(u, u)=|u|_{V}^{2} \geq \alpha\|u\|_{V}^{2}, \quad|a(u, v)| \leq M\|u\|_{V}\|v\|_{V} \cdot\right]
$$

6. Write the weak formulation of the fourth-order problem

$$
\begin{cases}-\operatorname{div}(\mu \nabla u)+\Delta^{2} u+\sigma u=0 & \text { in } \Omega \\ u=0 & \text { on } \partial \Omega \\ \partial u \\ \partial n & \text { on } \partial \Omega\end{cases}
$$

by introducing appropriate functional spaces, knowing that $\Omega \subset \mathbb{R}^{2}$ is a bounded open set with regular boundary $\partial \Omega$ and that $\mu(\mathbf{x})$ and $\sigma(\mathbf{x})$ are known functions defined on $\Omega$.

[Solution: proceed as in the two previous exercises by supposing that the coefficients $\mu$ and $\sigma$ lie in $\left.\mathrm{L}^{\infty}(\Omega) .\right]$

7. Let $\Omega \subset \mathbb{R}^{2}$ be a domain with a smooth boundary $\partial \Omega=\Gamma_{D} \cup \Gamma_{N}$ and $\stackrel{\circ}{\Gamma}_{D} \cap \stackrel{\circ}{\Gamma}_{N}=\emptyset$. By introducing appropriate functional spaces, find the weak formulation of the following linear elasticity problem

$$
\begin{cases}-\sum_{j=1}^{2} \frac{\partial}{\partial x_{j}} \sigma_{i j}(\mathbf{u})=f_{i} \quad \text { in } \Omega, \quad i=1,2 \\ u_{i}=0 & \text { on } \Gamma_{D}, \quad i=1,2 \\ \sum_{j=1}^{2} \sigma_{i j}(\mathbf{u}) n_{j}=g_{i} \quad & \text { on } \Gamma_{N}, \quad i=1,2\end{cases}
$$

having denoted as usual by $\mathbf{n}=\left(n_{1}, n_{2}\right)^{T}$ the outward unit normal vector to $\partial \Omega$, by $\mathbf{u}=\left(u_{1}, u_{2}\right)^{T}$ the unknown vector, and by $\mathbf{f}=\left(f_{1}, f_{2}\right)^{T}$ and $\mathbf{g}=\left(g_{1}, g_{2}\right)^{T}$ two assigned vector functions. Moreover, it has been set for $i, j=1,2$,

$$
\sigma_{i j}(\mathbf{u})=\lambda \operatorname{div}(\mathbf{u}) \delta_{i j}+2 \mu \varepsilon_{i j}(\mathbf{u}), \quad \varepsilon_{i j}(\mathbf{u})=\frac{1}{2}\left(\frac{\partial u_{i}}{\partial x_{j}}+\frac{\partial u_{j}}{\partial x_{i}}\right)
$$

$\lambda$ and $\mu$ being two positive constants and $\delta_{i j}$ the Kronecker symbol. The system (3.51) allows to describe the displacement $\mathbf{u}$ of an elastic body, homogeneous and isotropic, that occupies in its equilibrium position the region $\Omega$, under the action of an external body force whose density is $\mathbf{f}$ and of a surface charge distributed on $\Gamma_{N}$ with intensity $\mathbf{g}$ (see Fig. 3.4).

[Solution: the weak formulation of (3.51) can be found by observing that $\sigma_{i j}=\sigma_{j i}$ and by using the following Green formula

$$
\begin{aligned}
\sum_{i, j=1}^{2} \int_{\Omega} \sigma_{i j}(\mathbf{u}) \varepsilon_{i j}(\mathbf{v}) d \Omega &=\sum_{i, j=1}^{2} \int_{\partial \Omega} \sigma_{i j}(\mathbf{u}) n_{j} v_{i} d \gamma \\
&-\sum_{i, j=1}^{2} \int_{\Omega} \frac{\partial \sigma_{i j}(\mathbf{u})}{\partial x_{j}} v_{i} d \Omega
\end{aligned}
$$



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-062.jpg?height=204&width=310&top_left_y=164&top_left_x=300)

Fig. 3.4. A partially constrained body subject to the action of an external charge

By assuming $\mathbf{v} \in V=\left(\mathrm{H}_{\Gamma_{D}}^{1}(\Omega)\right)^{2}$ (the space of vectorial functions that have components $v_{i} \in \mathrm{H}_{\Gamma_{D}}^{1}(\Omega)$ for $i=1,2$ ), the weak formulation reads

$$
\text { find } \mathbf{u} \in V \text { such that } a(\mathbf{u}, \mathbf{v})=F(\mathbf{v}) \forall \mathbf{v} \in V,
$$

with

$$
\begin{gathered}
a(\mathbf{u}, \mathbf{v})=\int_{\Omega} \lambda \operatorname{div}(\mathbf{u}) \operatorname{div}(\mathbf{v}) d \Omega+2 \mu \sum_{i, j=1}^{2} \int \varepsilon_{i j}(\mathbf{u}) \varepsilon_{i j}(\mathbf{v}) d \Omega \\
F(\mathbf{v})=\int_{\Omega} \mathbf{f} \cdot \mathbf{v} d \Omega+\int_{\Gamma_{N}} \mathbf{g} \cdot \mathbf{v} d \gamma
\end{gathered}
$$

In order for the integrals to make sense, it will be sufficient to require $\mathbf{f} \in\left(\mathrm{L}^{2}(\Omega)\right)^{2}$ and $\left.\mathbf{g} \in\left(\mathrm{L}^{2}\left(\Gamma_{N}\right)\right)^{2} .\right]$

8. Prove, by applying the Lax-Milgram Lemma, that the solution of the weak formulation (3.52) exists and is unique under appropriate conditions on the regularity of the data and knowing that the following Korn inequality holds:

$$
\exists C_{0}>0: \sum_{i, j=1}^{2} \int_{\Omega} \varepsilon_{i j}(\mathbf{v}) \varepsilon_{i j}(\mathbf{v}) d \Omega \geq C_{0}\|\mathbf{v}\|_{V}^{2} \quad \forall \mathbf{v} \in V
$$

[Solution: consider the weak formulation introduced in the solution to the previous exercise. The bilinear form defined in (3.52) is continuous and also coercive because of the Korn inequality. $F$ is a linear and continuous functional; hence, by the Lax-Milgram lemma, the solution exists and is unique.] Chapter 4

\section{The Galerkin finite element method for elliptic problems}

In this chapter we describe the numerical solution of the elliptic boundary-value problems considered in Chapter 3 by introducing the Galerkin method. We then illustrate the finite element method as a particular case. The latter will be further developed in the following chapters.

\subsection{Approximation via the Galerkin method}

As seen in Chapter $3.2$, the weak formulation of a generic elliptic problem set on a domain $\Omega \subset \mathbb{R}^{d}, d=1,2,3$, can be written in the following way

$$
\text { find } u \in V: \quad a(u, v)=F(v) \quad \forall v \in V,
$$

$V$ being an appropriate Hilbert space, subspace of $\mathrm{H}^{1}(\Omega), a(\cdot, \cdot)$ being a continuous and coercive bilinear form from $V \times V$ in $\mathbb{R}, F(\cdot)$ being a continuous linear functional from $V$ in $\mathbb{R}$. Under such hypotheses, the Lax-Milgram Lemma of Sect. $3.4 .1$ ensures existence and uniqueness of the solution.

Let $V_{h}$ be a family of spaces that depends on a positive parameter $h$, such that

$$
V_{h} \subset V, \quad \operatorname{dim} V_{h}=N_{h}<\infty \quad \forall h>0
$$

The approximate problem takes the form

$$
\text { find } u_{h} \in V_{h}: \quad a\left(u_{h}, v_{h}\right)=F\left(v_{h}\right) \quad \forall v_{h} \in V_{h}
$$

and is called Galerkin problem. Denoting with $\left\{\varphi_{j}, j=1,2, \ldots, N_{h}\right\}$ a basis of $V_{h}$, it suffices that (4.2) be verified for each function of the basis, as all the functions in the space $V_{h}$ are a linear combination of the $\varphi_{j}$. We will then require that

$$
a\left(u_{h}, \varphi_{i}\right)=F\left(\varphi_{i}\right), \quad i=1,2, \ldots, N_{h}
$$

Obviously, since $u_{h} \in V_{h}$,

$$
u_{h}(\mathbf{x})=\sum_{j=1}^{N_{h}} u_{j} \varphi_{j}(\mathbf{x})
$$

where the $u_{j}, j=1, \ldots, N_{h}$, are unknown coefficients. Equations $(4.3)$ then become

$$
\sum_{j=1}^{N_{h}} u_{j} a\left(\varphi_{j}, \varphi_{i}\right)=F\left(\varphi_{i}\right), \quad i=1,2, \ldots, N_{h}
$$

We denote by A the matrix (called stiffness matrix) with elements

$$
a_{i j}=a\left(\varphi_{j}, \varphi_{i}\right)
$$

and by $\mathbf{f}$ the vector with components $f_{i}=F\left(\varphi_{i}\right)$. If we denote by $\mathbf{u}$ the vector having as components the unknown coefficients $u_{j},(4.4)$ is equivalent to the linear system

$$
\mathrm{A} \mathbf{u}=\mathbf{f}
$$

We point out some characteristics of the stiffness matrix that are independent of the basis chosen for $V_{h}$, but exclusively depend on the properties of the weak problem that is being approximated. Other properties, instead, such as the condition number or the sparsity structure, depend on the basis under exam and are therefore addressed in the sections dedicated to the specific numerical methods. For instance, bases formed by functions with small support are appealing, as all the elements $a_{i j}$ whose indices are related to basis functions having supports with empty intersections will be null. More in general, from a computational viewpoint, the most convenient choices of $V_{h}$ will be the ones requiring a modest computational effort for the computation of the matrix elements as well as the source term $\mathbf{f}$.

Theorem 4.1. The matrix A associated to the Galerkin discretization of an ellip-

tic problem whose bilinear form is coercive is positive definite.

Proof. We recall that a matrix $\mathrm{B} \in \mathbb{R}^{n \times n}$ is said to be positive definite if

$$
\mathbf{v}^{T} \mathrm{~B} \mathbf{v} \geq 0 \quad \forall \mathbf{v} \in \mathbb{R}^{n} \quad \text { and also } \mathbf{v}^{T} \mathrm{~B} \mathbf{v}=0 \Leftrightarrow \mathbf{v}=\mathbf{0}
$$

The correspondence

$$
\mathbf{v}=\left(v_{i}\right) \in \mathbb{R}^{N_{h}} \leftrightarrow \quad v_{h}(x)=\sum_{j=1}^{N_{h}} v_{j} \phi_{j} \in V_{h}
$$

defines a bijection between the spaces $\mathbb{R}^{N_{h}}$ and $V_{h}$. Given a generic vector $\mathbf{v}=\left(v_{i}\right)$ of $\mathbb{R}^{N_{h}}$, thanks to the bilinearity and coercivity of the form $a(\cdot, \cdot)$, we obtain

$$
\begin{aligned}
\mathbf{v}^{T} \mathrm{~A} \mathbf{v} &=\sum_{j=1}^{N_{h}} \sum_{i=1}^{N_{h}} v_{i} a_{i j} v_{j}=\sum_{j=1}^{N_{h}} \sum_{i=1}^{N_{h}} v_{i} a\left(\varphi_{j}, \varphi_{i}\right) v_{j} \\
&=\sum_{j=1}^{N_{h}} \sum_{i=1}^{N_{h}} a\left(v_{j} \varphi_{j}, v_{i} \varphi_{i}\right)=a\left(\sum_{j=1}^{N_{h}} v_{j} \varphi_{j}, \sum_{i=1}^{N_{h}} v_{i} \varphi_{i}\right) \\
&=a\left(v_{h}, v_{h}\right) \geq \alpha\left\|v_{h}\right\|_{V}^{2} \geq 0
\end{aligned}
$$

Moreover, if $\mathbf{v}^{T} \mathrm{~A} \mathbf{v}=0$, then, by what we have just obtained, $\left\|v_{h}\right\|_{V}^{2}=0$ too, i.e. $v_{h}=0$ and so $\mathbf{v}=\mathbf{0}$. Consequently the claim is proved, as the two conditions in (4.6) are fulfilled.

Furthermore, the following property can be proved (see Exercise 4):

For instance, in the case of the Poisson problem with either Dirichlet $(3.18)$ or mixed (3.27) boundary conditions, the matrix $\mathrm{A}$ is symmetric and positive definite. The numerical solution of such a system can be efficiently performed using both direct methods such as the Cholesky factorization, and iterative methods such as the conjugate gradient method (see Chap. 7 and, e.g., [QSS07, Chap. 4]).

\subsection{Analysis of the Galerkin method}

In this section, we aim at studying the Galerkin method, and in particular at verifying three of its fundamental properties:

- existence and uniqueness of the discrete solution $u_{h}$;

- stability of the discrete solution $u_{h}$;

- convergence of $u_{h}$ to the exact solution $u$ of problem (4.1), as $h \rightarrow 0$.

\subsubsection{Existence and uniqueness}

The Lax-Milgram Lemma stated in Sect. 3.4.1 holds for any Hilbert space, hence in particular for the space $V_{h}$, as the latter is a closed subspace of the Hilbert space $V$. Furthermore, the bilinear form $a(\cdot, \cdot)$ and the functional $F(\cdot)$ are the same as in the variational problem (4.1). The hypotheses required by the Lemma are therefore fulfilled. The following result then follows: 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-066.jpg?height=63&width=702&top_left_y=119&top_left_x=107)

It is nonetheless instructive to provide a constructive proof of this corollary without using the Lax-Milgram Lemma. As we have seen, in fact, the Galerkin problem (4.2) is equivalent to the linear system (4.5). Proving the existence and uniqueness for one means to prove automatically the existence and uniqueness of the other. We therefore focus our attention on the linear system (4.5).

The matrix $\mathrm{A}$ is invertible as the unique solution of system $\mathrm{A} \mathbf{u}=\mathbf{0}$ is the identically null solution. This immediately descends from the fact that $\mathrm{A}$ is positive definite. Consequently, the linear system (4.5) admits a unique solution, hence also its corresponding Galerkin problem admits a unique solution.

\subsubsection{Stability}

Corollary $3.1$ allows us to provide the following stability result.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-066.jpg?height=165&width=720&top_left_y=566&top_left_x=95)

The stability of the method guarantees that the $\operatorname{norm}\left\|u_{h}\right\|_{V}$ of the discrete solution remains bounded for $h$ tending to zero, uniformly with respect to $h$. Equivalently, it guarantees that $\left\|u_{h}-w_{h}\right\|_{V} \leq \frac{1}{\alpha}\|F-G\|_{V^{\prime}}, u_{h}$ and $w_{h}$ being numerical solutions corresponding to two different data $F$ and $G$.

\subsubsection{Convergence}

We now want to prove that the weak solution of the Galerkin problem converges to the solution of the weak problem (4.1) when $h$ tends to zero. Consequently, by taking a sufficiently small $h$, it will be possible to approximate the exact solution $u$ by the Galerkin solution $u_{h}$ as accurately as desired.

Let us first prove the following consistency property.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-066.jpg?height=129&width=722&top_left_y=1095&top_left_x=97)



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-067.jpg?height=251&width=366&top_left_y=112&top_left_x=272)

Fig. 4.1. Geometric interpretation of the Galerkin orthogonality

Proof. Since $V_{h} \subset V$, the exact solution $u$ satisfies the weak problem (4.1) for each element $v=v_{h} \in V_{h}$, hence we have

$$
a\left(u, v_{h}\right)=F\left(v_{h}\right) \quad \forall v_{h} \in V_{h} .
$$

By subtracting side by side (4.2) from (4.9), we obtain

$$
a\left(u, v_{h}\right)-a\left(u_{h}, v_{h}\right)=0 \quad \forall v_{h} \in V_{h},
$$

from which, thanks to the bilinearity of the form $a(\cdot, \cdot)$, the claim follows.

Let us point out that (4.9) coincides with the definition of strong consistency given in $(1.10)$.

If $a(\cdot, \cdot)$ is symmetric, (4.8) is interpreted as the orthogonality condition with respect to the scalar product $a(\cdot, \cdot)$, between the approximation error, $u-u_{h}$, and the subspace $V_{h}$. Borrowing terminology from the Euclidean case, the solution $u_{h}$ of the Galerkin method is said to be the orthogonal projection on $V_{h}$ of the exact solution $u$. Among all elements of $V_{h}, v_{h}$ is the one minimizing the distance to the exact solution $u$ in the energy norm, i.e. in the following norm induced by the scalar product $a(\cdot, \cdot)$ :

$$
\left\|u-u_{h}\right\|_{a}=\sqrt{a\left(u-u_{h}, u-u_{h}\right)}
$$

Remark 4.1. The geometric interpretation of the Galerkin method makes sense only in the case where the form $a(\cdot, \cdot)$ is symmetric. However, this does not impair the generality of the method or its consistency property in the case where the bilinear form is not symmetric.

Let us now consider the value taken by the bilinear form when both its arguments are equal to $u-u_{h}$. If $v_{h}$ is an arbitrary element of $V_{h}$ we obtain

$$
a\left(u-u_{h}, u-u_{h}\right)=a\left(u-u_{h}, u-v_{h}\right)+a\left(u-u_{h}, v_{h}-u_{h}\right)
$$

The last term is null by virtue of $(4.8)$, as $v_{h}-u_{h} \in V_{h}$. Moreover

$$
\left|a\left(u-u_{h}, u-v_{h}\right)\right| \leq M\left\|u-u_{h}\right\|_{V}\left\|u-v_{h}\right\|_{V}
$$

having exploited the continuity of the bilinear form. On the other hand, by the coercivity of $a(\cdot, \cdot)$ it follows

$$
a\left(u-u_{h}, u-u_{h}\right) \geq \alpha\left\|u-u_{h}\right\|_{V}^{2}
$$

hence we have

$$
\left\|u-u_{h}\right\|_{V} \leq \frac{M}{\alpha}\left\|u-v_{h}\right\|_{V} \quad \forall v_{h} \in V_{h} .
$$

Such inequality holds for all functions $v_{h} \in V_{h}$ and therefore we find

$$
\left\|u-u_{h}\right\|_{V} \leq \frac{M}{\alpha} \inf _{w_{h} \in V_{h}}\left\|u-w_{h}\right\|_{V}
$$

This fundamental property of the Galerkin method is known as Céa Lemma.

It is then evident that in order for the method to converge, it will be sufficient to require that, for $h$ tending to zero, the space $V_{h}$ tends to saturate the entire space $V$. Precisely, it must turn out that

$$
\lim _{h \rightarrow 0} \inf _{v_{h} \in V_{h}}\left\|v-v_{h}\right\|_{V}=0 \quad \forall v \in V
$$

In that case, the Galerkin method is convergent and it can be written that

$$
\lim _{h \rightarrow 0}\left\|u-u_{h}\right\|_{V}=0
$$

The space $V_{h}$ must therefore be carefully chosen in order to guarantee the saturation property (4.11). Once this requirement is satisfied, convergence will be verified in any case, independently of how $u$ looks like; conversely, the speed with which the discrete solution converges to the exact solution, i.e. the order of decay of the error with respect to $h$, will depend, in general, on both the choice of $V_{h}$ and the regularity of $u$ (see Theorem 4.3).

Remark 4.2. Obviously, $\inf _{v_{h} \in V_{h}}\left\|u-v_{h}\right\|_{V} \leq\left\|u-u_{h}\right\|_{V}$. Consequently, by (4.10), if $\frac{M}{\alpha}$ is has order 1, the error due to the Galerkin method can be identified with the best approximation error for $u$ in $V_{h}$. In any case, both errors have the same infinitesimal order with respect to $h$.

Remark 4.3. In the case where $a(\cdot, \cdot)$ is a symmetric bilinear form, and also continuous and coercive, then (4.10) can be improved as follows (see Exercise 5 )

$$
\left\|u-u_{h}\right\|_{V} \leq \sqrt{\frac{M}{\alpha}} \inf _{w_{h} \in V_{h}}\left\|u-w_{h}\right\|_{V}
$$



\section{$4.3$ The finite element method in the one-dimensional case}

Let us suppose that $\Omega$ is an interval $(a, b)$. The goal of this section is to create approximations of the space $\mathrm{H}^{\mathrm{l}}(a, b)$ that depend on a parameter $h$. To this end, we introduce a partition $\mathscr{T}_{h}$ of $(a, b)$ in $N+1$ subintervals $K_{j}=\left(x_{j-1}, x_{j}\right)$, also called elements, having width $h_{j}=x_{j}-x_{j-1}$ with

$$
a=x_{0}<x_{1}<\ldots<x_{N}<x_{N+1}=b
$$

and set $h=\max _{j} h_{j}$.

Since the functions of $\mathrm{H}^{1}(a, b)$ are continuous functions on $[a, b]$, we can construct the following family of spaces

$$
X_{h}^{r}=\left\{v_{h} \in C^{0}(\bar{\Omega}):\left.v_{h}\right|_{K_{j}} \in \mathbb{P}_{r} \forall K_{j} \in \mathscr{T}_{h}\right\}, \quad r=1,2, \ldots
$$

having denoted by $\mathbb{P}_{r}$ the space of polynomials with degree lower than or equal to $r$ in the variable $x$. The spaces $X_{h}^{r}$ are all subspaces of $\mathrm{H}^{1}(a, b)$, as they are constituted by differentiable functions except for at most a finite number of points (the vertices $x_{i}$ of the partition $\mathscr{T}_{h}$ ). They represent possible choices for the space $V_{h}$, provided that the boundary conditions are properly incorporated. The fact that the functions of $X_{h}^{r}$ are locally (element-wise) polynomials will make the stiffness matrix easy to compute.

We must now choose a basis $\left\{\varphi_{i}\right\}$ for the $X_{h}^{r}$ space. It is convenient, by what exposed in Sect. 4.1, that the support of the generic basis function $\varphi_{i}$ have non-empty intersection only with the support of a negligible number of other functions of the basis. In such way, many elements of the stiffness matrix will be null. It is also convenient that the basis be Lagrangian: in that case, the coefficients of the expansion of a generic function $v_{h} \in X_{h}^{r}$ in the basis itself will be the values taken by $v_{h}$ at carefully chosen points, which we call nodes and which, as we will see, generally form a superset of the vertices of $\mathscr{T}_{h}$. This does not prevent the use of non-Lagrangian bases, especially in their hierarchical version (as we will see later). We now provide some examples of bases for the spaces $X_{h}^{1}$ and $X_{h}^{2}$.

\subsubsection{The space $X_{h}^{1}$}

This space is constituted by the continuous and piecewise linear functions on a partition $\mathscr{T}_{h}$ of $(a, b)$ of the form (4.13). Since only one straight line can pass through two different points and the functions of $X_{h}^{1}$ are continuous, the degrees of freedom of the functions of this space, i.e. the values that must be assigned to define uniquely the functions themselves, will be equal to the number $N+2$ of vertices of the partition. In this case, therefore, nodes and vertices coincide. Consequently, having assigned $N+2$ basis functions $\varphi_{i}, i=0, \ldots, N+1$, the whole space $X_{h}^{1}$ will be completely defined. The characteristic Lagrangian basis functions are characterized by the following property

$$
\varphi_{i} \in X_{h}^{1} \quad \text { such that } \quad \varphi_{i}\left(x_{j}\right)=\delta_{i j}, \quad i, j=0,1, \ldots, N+1
$$

$\delta_{i j}$ being the Kronecker delta. The function $\varphi_{i}$ is therefore piecewise linear and equal to one at $x_{i}$ and zero at the remaining nodes of the partition (see Fig. 4.2). Its expression 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-070.jpg?height=198&width=386&top_left_y=111&top_left_x=279)

Fig. 4.2. The basis function of $X_{h}^{1}$ associated to node $x_{i}$

is given by:

$$
\varphi_{i}(x)= \begin{cases}\frac{x-x_{i-1}}{x_{i}-x_{i-1}} & \text { for } x_{i-1} \leq x \leq x_{i} \\ \frac{x_{i+1}-x}{x_{i+1}-x_{i}} & \text { for } x_{i} \leq x \leq x_{i+1} \\ 0 & \text { otherwise. }\end{cases}
$$

Obviously $\varphi_{i}$ has as support the union of the intervals $\left[x_{i-1}, x_{i}\right]$ and $\left[x_{i}, x_{i+1}\right]$ only, if $i \neq 0$ or $i \neq N+1$ (for $i=0$ or $i=N+1$ the support will be limited to the interval $\left[x_{0}, x_{1}\right]$ or $\left[x_{N}, x_{N+1}\right]$, respectively). Consequently, the only basis functions whose support overlaps with that of $\varphi_{i}$ are $\varphi_{i-1}$ and $\varphi_{i+1}$ (and, of course, $\left.\varphi_{i}\right)$. Hence the stiffness matrix is tridiagonal as $a_{i j}=0$ if $j \notin\{i-1, i, i+1\}$.

As visible in expression (4.15), the two basis functions $\varphi_{i}$ and $\varphi_{i+1}$ defined on each interval $\left[x_{i}, x_{i+1}\right]$ basically repeat themselves with no changes, up to a scaling factor linked to the length of the interval itself. In practice, the two basis functions $\varphi_{i}$ and $\varphi_{i+1}$ can be obtained by transforming two basis functions $\widehat{\varphi}_{0}$ and $\widehat{\varphi}_{1}$ built once and for all on a reference interval, typically the $[0,1]$ interval.

To this end, it is sufficient to exploit the fact that the generic interval $\left[x_{i}, x_{i+1}\right]$ of the partition of $(a, b)$ can be obtained starting from the interval $[0,1]$ via the linear transformation $\phi:[0,1] \rightarrow\left[x_{i}, x_{i+1}\right]$ defined as

$$
x=\phi(\xi)=x_{i}+\xi\left(x_{i+1}-x_{i}\right)
$$

If we define the two basis functions $\widehat{\varphi}_{0}$ and $\widehat{\varphi}_{1}$ on $[0,1]$ as

$$
\widehat{\varphi}_{0}(\xi)=1-\xi, \quad \widehat{\varphi}_{1}(\xi)=\xi
$$

the basis functions $\varphi_{i}$ and $\varphi_{i+1}$ on $\left[x_{i}, x_{i+1}\right]$ will simply be given by

$$
\varphi_{i}(x)=\widehat{\varphi}_{0}(\xi(x)), \quad \varphi_{i+1}(x)=\widehat{\varphi}_{1}(\xi(x))
$$

since $\xi(x)=\left(x-x_{i}\right) /\left(x_{i+1}-x_{i}\right)$ (see Figs. 4.3 and 4.4).

This way of proceeding (defining the basis on a reference element and then transforming it on a specific element) will be of fundamental importance when considering problems in several dimensions. 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-071.jpg?height=186&width=512&top_left_y=118&top_left_x=202)

Fig. 4.3. The basis function $\varphi_{i}$ in $\left[x_{i}, x_{i+1}\right]$ and the corresponding basis function $\widehat{\varphi}_{0}$ on the reference element
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-071.jpg?height=182&width=510&top_left_y=387&top_left_x=202)

Fig. 4.4. The basis function $\varphi_{i+1}$ in $\left[x_{i}, x_{i+1}\right]$ and the corresponding basis function $\widehat{\varphi}_{1}$ on the reference element

\subsubsection{The space $X_{h}^{2}$}

The functions of $X_{h}^{2}$ are piecewise polynomials of degree 2 on each interval of $\mathscr{T}_{h}$ and, consequently, they are determined once the values they take at three distinct points of each interval $K_{j}$ are assigned. To guarantee the continuity of the functions of $X_{h}^{2}$ two of these points will be the endpoints of the generic interval of $\mathscr{T}_{h}$, the third will be the midpoint of the latter. The degrees of freedom of the space $X_{h}^{2}$ are therefore the values of $v_{h}$ taken at the endpoints of the intervals composing the partition $\mathscr{T}_{h}$ and at their midpoints. We order the nodes starting from $x_{0}=a$ to $x_{2 N+2}=b ;$ in such way the midpoints correspond to the nodes with odd indices, and the endpoints to the nodes with even indices (refer to Exercise 6 for alternative numberings).

Exactly as in the previous case the Lagrangian basis for $X_{h}^{2}$ is the one formed by the functions

$$
\varphi_{i} \in X_{h}^{2} \quad \text { such that } \quad \varphi_{i}\left(x_{j}\right)=\delta_{i j}, \quad i, j=0,1, \ldots, 2 N+2
$$

These are therefore piecewise quadratic functions that are equal to 1 at the node to which they are associated and are null at the remaining nodes. Here is the explicit expression of the generic basis function associated to the endpoints of the intervals in the partition:

$$
\varphi_{i}(x)= \begin{cases}\frac{\left(x-x_{i-1}\right)\left(x-x_{i-2}\right)}{\left(x_{i}-x_{i-1}\right)\left(x_{i}-x_{i-2}\right)} & \text { if } x_{i-2} \leq x \leq x_{i} \\ \frac{\left(x_{i+1}-x\right)\left(x_{i+2}-x\right)}{\left(x_{i+1}-x_{i}\right)\left(x_{i+2}-x_{i}\right)} & \text { if } x_{i} \leq x \leq x_{i+2} \\ 0 & \text { otherwise. }\end{cases}
$$

For the midpoints of the intervals, we have

$$
(i \text { odd }) \quad \varphi_{i}(x)= \begin{cases}\frac{\left(x_{i+1}-x\right)\left(x-x_{i-1}\right)}{\left(x_{i+1}-x_{i}\right)\left(x_{i}-x_{i-1}\right)} & \text { if } x_{i-1} \leq x \leq x_{i+1} \\ 0 & \text { otherwise }\end{cases}
$$

See Fig. $4.5$ for an example.

As in the case of linear finite elements, in order to describe the basis it is sufficient to provide the expression of the basis functions on the reference interval $[0,1]$ and then to transform the latter via (4.16). We have

$$
\widehat{\varphi}_{0}(\xi)=(1-\xi)(1-2 \xi), \quad \widehat{\varphi}_{1}(\xi)=4(1-\xi) \xi, \quad \widehat{\varphi}_{2}(\xi)=\xi(2 \xi-1)
$$

We represent these functions in Fig. 4.5. Note that the generic basis function $\varphi_{2 i+1}$ relative to node $x_{2 i+1}$ has a support coinciding with the element to which the midpoint belongs. For its peculiar form, it is known as bubble function.

As previously anticipated, we can also introduce other non-Lagrangian bases. A particularly interesting one is the one constructed (locally) by the three functions

$$
\widehat{\psi}_{0}(\xi)=1-\xi, \quad \widehat{\psi}_{1}(\xi)=\xi, \quad \widehat{\psi}_{2}(\xi)=(1-\xi) \xi
$$

A basis of this kind is said to be hierarchical because, to construct the basis for $X_{h}^{2}$, it exploits the basis functions of the lower-dimension space, $X_{h}^{1}$. It is convenient from a computational viewpoint if one decides, during the approximation of a problem, to
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-072.jpg?height=210&width=712&top_left_y=966&top_left_x=99)

Fig. 4.5. The basis functions of $X_{h}^{2}$ (on the left) and the corresponding functions on the reference interval (on the right) increase only locally, i.e. only for such elements, the degree of interpolation (that is if one intends to perform the so-called adaptivity in the degree, or adaptivity of type $p$ ).

The Lagrange polynomials are linearly independent by construction. In general, however, such property must be verified to ensure that the set of chosen polynomials is effectively a basis. In the case of the functions $\widehat{\psi}_{0}, \widehat{\psi}_{1}$ and $\widehat{\psi}_{2}$ we must verify that

$$
\text { if } \alpha_{0} \widehat{\psi}_{0}(\xi)+\alpha_{1} \widehat{\psi}_{1}(\xi)+\alpha_{2} \widehat{\psi}_{2}(\xi)=0 \forall \xi, \quad \text { then } \quad \alpha_{0}=\alpha_{1}=\alpha_{2}=0
$$

Indeed, the equation

$$
\alpha_{0} \widehat{\psi}_{0}(\xi)+\alpha_{1} \widehat{\psi}_{1}(\xi)+\alpha_{2} \widehat{\psi}_{2}(\xi)=\alpha_{0}+\xi\left(\alpha_{1}-\alpha_{0}+\alpha_{2}\right)-\alpha_{2} \xi^{2}=0
$$

implies $\alpha_{0}=0, \alpha_{2}=0$ and therefore $\alpha_{1}=0$. We notice that the stiffness matrix in the case of finite elements of degree 2 will be pentadiagonal.

By proceeding in the same way it will be possible to generate bases for $X_{h}^{r}$ with an arbitrary positive integer $r:$ we point out, however, that as the polynomial degree increases, the number of degrees of freedom increases and so does the computational cost of solving the linear system (4.5). Moreover, a well known fact from polynomial interpolation theory, the use of high degrees combined with equispaced node distributions leads to increasingly less stable approximations, in spite of the theoretical increase in accuracy. A successful remedy is provided by the spectral element approximation that, using well-chosen nodes (the ones from the Gaussian quadrature), allows to generate approximations with arbitrarily high accuracy. To this purpose see Chap. 10.

\subsubsection{The approximation with linear finite elements}

We now examine how to approximate the following problem:

$$
\begin{cases}-u^{\prime \prime}+\sigma u=f, & a<x<b \\ u(a)=0, & u(b)=0\end{cases}
$$

whose weak formulation, as we have seen in the previous chapter, is

$$
\text { find } u \in \mathrm{H}_{0}^{1}(a, b): \int_{a}^{b} u^{\prime} v^{\prime} d x+\int_{a}^{b} \sigma u v d x=\int_{a}^{b} f v d x \quad \forall v \in \mathrm{H}_{0}^{1}(a, b)
$$

As we did in (4.13), we introduce a decomposition $\mathscr{T}_{h}$ of $(0,1)$ in $N+1$ subintervals $K_{j}$ and use linear finite elements. We therefore introduce the space

$$
V_{h}=\left\{v_{h} \in X_{h}^{1}: v_{h}(a)=v_{h}(b)=0\right\}
$$

that is the space of piecewise linear functions that vanish at the boundary (a function of such space has been introduced in Fig. 4.6). This is a subspace of $\mathrm{H}_{0}^{1}(a, b)$. 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-074.jpg?height=222&width=485&top_left_y=113&top_left_x=211)

Fig. 4.6. Example of a function of $V_{h}$

The corresponding finite element problem is therefore given by

$$
\text { find } u_{h} \in V_{h}: \int_{a}^{b} u_{h}^{\prime} v_{h}^{\prime} d x+\int_{a}^{b} \sigma u_{h} v_{h} d x=\int_{a}^{b} f v_{h} d x \quad \forall v_{h} \in V_{h}
$$

We use as a basis of $X_{h}^{1}$ the set of hat functions defined in (4.15) by caring to consider only the indices $1 \leq i \leq N$. By expressing $u_{h}$ as a linear combination of such functions $u_{h}(x)=\sum_{i=1}^{N} u_{i} \varphi_{i}(x)$, and imposing that (4.18) is satisfied for each element of the basis of $V_{h}$, we obtain a system of $N$ equations

$$
\mathrm{A} \mathbf{u}=\mathbf{f}
$$

where

$$
\begin{aligned}
&\mathrm{A}=\left[a_{i j}\right], \quad a_{i j}=\int_{a}^{b} \varphi_{j}^{\prime} \varphi_{i}^{\prime} d x+\int_{a}^{b} \sigma \varphi_{j} \varphi_{i} d x ; \\
&\mathbf{u}=\left[u_{i}\right] ; \quad \mathbf{f}=\left[f_{i}\right], \quad f_{i}=\int_{a}^{b} f \varphi_{i} d x
\end{aligned}
$$

Note that $u_{i}=u_{h}\left(x_{i}\right), 1 \leq i \leq N$, that is the finite element unknowns are the nodal values of the finite element solution $u_{h}$.

To find the numerical solution $u_{h}$ it is now sufficient to solve the linear system (4.19).

In the case of linear finite elements, the stiffness matrix $\mathrm{A}$ is not only sparse, but also results to be tridiagonal. To compute its elements, we proceed as follows. As we have seen it is not necessary to operate directly on the basis functions on the single intervals, but it is sufficient to refer to the ones defined on the reference interval: it will then be enough to transform appropriately the integrals that appear in the definition of the coefficients of $\mathrm{A}$.

A generic non-null element of the stiffness matrix is given by

$$
a_{i j}=\int_{a}^{b}\left(\varphi_{i}^{\prime} \varphi_{j}^{\prime}+\sigma \varphi_{i} \varphi_{j}\right) d x=\int_{x_{i-1}}^{x_{i}}\left(\varphi_{i}^{\prime} \varphi_{j}^{\prime}+\sigma \varphi_{i} \varphi_{j}\right) d x+\int_{x_{i}}^{x_{i+1}}\left(\varphi_{i}^{\prime} \varphi_{j}^{\prime}+\sigma \varphi_{i} \varphi_{j}\right) d x
$$

Let us consider the first summand by supposing $j=i-1$. Evidently, via the coordinate transformation (4.16), we can rewrite it as

$$
\begin{aligned}
&\int_{x_{i-1}}^{x_{i}}\left(\varphi_{i}^{\prime} \varphi_{i-1}^{\prime}+\sigma \varphi_{i} \varphi_{i-1}\right) d x= \\
&\int_{0}^{1}\left[\varphi_{i}^{\prime}(x(\xi)) \varphi_{i-1}^{\prime}(x(\xi))+\sigma(x(\xi)) \varphi_{i}(x(\xi)) \varphi_{i-1}(x(\xi))\right] h_{i} d \xi
\end{aligned}
$$

having noted that $d x=d\left(x_{i-1}+\xi h_{i}\right)=h_{i} d \xi$. On the other hand $\varphi_{i}(x(\xi))=\widehat{\varphi}_{1}(\xi)$ and $\varphi_{i-1}(x(\xi))=\widehat{\varphi}_{0}(\xi)$. We also note that

$$
\frac{d}{d x} \varphi_{i}(x(\xi))=\frac{d \xi}{d x} \widehat{\varphi}_{1}^{\prime}(\xi)=\frac{1}{h_{i}} \widehat{\varphi}_{1}^{\prime}(\xi)
$$

Similarly, we find that $\varphi_{i-1}^{\prime}(x(\xi))=\left(1 / h_{i}\right) \widehat{\varphi}_{0}^{\prime}(\xi)$. Hence, the element $a_{i, i-1}$ becomes

$$
a_{i, i-1}=\int_{0}^{1}\left(\frac{1}{h_{i}} \widehat{\varphi}_{1}^{\prime}(\xi) \widehat{\varphi}_{0}^{\prime}(\xi)+\sigma \widehat{\varphi}_{1}(\xi) \widehat{\varphi}_{0}(\xi) h_{i}\right) d \xi
$$

The advantage of this expression lies in the fact that in the case of constant coefficients, all the integrals appearing within the matrix A can be computed once and for all. We will see in the multi-dimensional case that this way of proceeding maintains its importance also in the case of variable coefficients.

\subsubsection{Interpolation operator and interpolation error}

Let us set $I=(a, b)$. For each $v \in C^{0}(\bar{I})$, we call interpolant of $v$ in the space of $X_{h}^{1}$, determined by the partition $\mathscr{T}_{h}$, the function $\Pi_{h}^{1} v$ such that

$$
\Pi_{h}^{1} v\left(x_{i}\right)=v\left(x_{i}\right) \quad \forall x_{i} \text { node of the partition, } i=0, \ldots, N+1
$$

By using the Lagrangian basis $\left\{\varphi_{i}\right\}$ of the space $X_{h}^{1}$, the interpolant can be expressed in the following way

$$
\Pi_{h}^{1} v(x)=\sum_{i=0}^{N+1} v\left(x_{i}\right) \varphi_{i}(x)
$$

Hence, when $v$ and a basis of $X_{h}^{1}$ are known, the interpolant of $v$ is easy to compute. The operator $\Pi_{h}^{1}: C^{0}(\bar{I}) \mapsto X_{h}^{1}$ mapping a function $v$ to its interpolant $\Pi_{h}^{1} v$ is called interpolation operator.

Analogously, we can define the operators $\Pi_{h}^{r}: C^{0}(\bar{I}) \mapsto X_{h}^{r}$, for all $r \geq 1$. Having denoted by $\Pi_{K_{j}}^{r}$ the local interpolation operator mapping a function $v$ to the polynomial $\Pi_{K_{j}}^{r} v \in \mathbb{P}_{r}\left(K_{j}\right)$, interpolating $v$ at the $r+1$ nodes of the element $K_{j} \in \mathscr{T}_{h}$, we define $\Pi_{h}^{r} v$ as

$$
\Pi_{h}^{r} v \in X_{h}^{r}:\left.\quad \Pi_{h}^{r} v\right|_{K_{j}}=\Pi_{K_{j}}^{r}\left(\left.v\right|_{K_{j}}\right) \quad \forall K_{j} \in \mathscr{T}_{h}
$$

Theorem 4.2. Let $v \in \mathrm{H}^{r+1}(I)$, for $r \geq 1$, and let $\Pi_{h}^{r} v \in X_{h}^{r}$ be its interpolating

$$
\begin{aligned}
&\left|v-\Pi_{h}^{r} v\right|_{\mathrm{H}^{k}(I)} \leq C_{k, r} h^{r+1-k}|v|_{\mathrm{H}^{r+1}(I)} \quad \text { for } k=0,1 \\
&\text { The constants } C_{k, r} \text { are independent of } v \text { and } h . \text { We recall that } \mathrm{H}^{0}(I)=\mathrm{L}^{2}(I) \text { and } \\
&\text { that }|\cdot|_{\mathrm{H}^{0}(I)}=\|\cdot\|_{\mathrm{L}^{2}(I)}
\end{aligned}
$$

Proof. We prove (4.21) for the case $r=1$, and refer to [QV94, Chap. 3] or [Cia78] for the more general case. We start by observing that if $v \in H^{r+1}(I)$ then $v \in C^{r}(I)$. In particular, for $r=1, v \in C^{1}(I)$. Let us set $e=v-\Pi_{h}^{1} v$. Since $e\left(x_{j}\right)=0$ for each node $x_{j}$, Rolle's theorem allows to conclude that there exist some $\xi_{j} \in K_{j}=\left(x_{j-1}, x_{j}\right)$, with $j=1, \ldots, N+1$, for which we have $e^{\prime}\left(\xi_{j}\right)=0$.

$\Pi_{h}^{1} v$ being a linear function in each interval $K_{j}$, we obtain that

$$
e^{\prime}(x)=\int_{\xi_{j}}^{x} e^{\prime \prime}(s) d s=\int_{\xi_{j}}^{x} v^{\prime \prime}(s) d s \quad \text { for } x \in K_{j}
$$

from which we deduce that

$$
\left|e^{\prime}(x)\right| \leq \int_{x_{j-1}}^{x_{j}}\left|v^{\prime \prime}(s)\right| d s \quad \text { for } x \in K_{j}
$$

Now, by using the Cauchy-Schwarz inequality we obtain

$$
\left|e^{\prime}(x)\right| \leq\left(\int_{x_{j-1}}^{x_{j}} 1^{2} d s\right)^{1 / 2}\left(\int_{x_{j-1}}^{x_{j}}\left|v^{\prime \prime}(s)\right|^{2} d s\right)^{1 / 2} \leq h^{1 / 2}\left(\int_{x_{j-1}}^{x_{j}}\left|v^{\prime \prime}(s)\right|^{2} d s\right)^{1 / 2}
$$

Hence,

$$
\int_{x_{j-1}}^{x_{j}}\left|e^{\prime}(x)\right|^{2} d x \leq h^{2} \int_{x_{j-1}}^{x_{j}}\left|v^{\prime \prime}(s)\right|^{2} d s
$$

An upper bound for $e(x)$ can be obtained by noting that, for each $x \in K_{j}$,

$$
e(x)=\int_{x_{j-1}}^{x} e^{\prime}(s) d s
$$

and therefore, by applying inequality (4.22),

$$
|e(x)| \leq \int_{x_{j-1}}^{x_{j}}\left|e^{\prime}(s)\right| d s \leq h^{3 / 2}\left(\int_{x_{j-1}}^{x_{j}}\left|v^{\prime \prime}(s)\right|^{2} d s\right)^{1 / 2}
$$

Hence,

$$
\int_{x_{j-1}}^{x_{j}}|e(x)|^{2} d x \leq h^{4} \int_{x_{j-1}}^{x_{j}}\left|v^{\prime \prime}(s)\right|^{2} d s
$$

By summing over the indices $j$ from 1 to $N+1$ in $(4.23)$ and $(4.24)$ we obtain the inequalities

$$
\left(\int_{a}^{b}\left|e^{\prime}(x)\right|^{2} d x\right)^{1 / 2} \leq h\left(\int_{a}^{b}\left|v^{\prime \prime}(x)\right|^{2} d x\right)^{1 / 2}
$$

and

$$
\left(\int_{a}^{b}|e(x)|^{2} d x\right)^{1 / 2} \leq h^{2}\left(\int_{a}^{b}\left|v^{\prime \prime}(x)\right|^{2} d x\right)^{1 / 2}
$$

respectively, that correspond to the desired estimates $(4.21)$ for $r=1$, with $C_{k, 1}=1$ and $k=0,1$.

\subsubsection{Estimate of the finite element error in the $\mathrm{H}^{1}$ norm}

Owing to result (4.21) we can obtain an estimate of the approximation error of the finite element method.

Proof. From (4.10), by setting $w_{h}=\Pi_{h}^{r} u$ we obtain

$$
\left\|u-u_{h}\right\|_{V} \leq \frac{M}{\alpha}\left\|u-\Pi_{h}^{r} u\right\|_{V}
$$

The right-hand side can now be bounded from above via the interpolation error estimate $(4.21)$ for $k=1$, from which the claim follows.

It follows from the latter theorem that, in order to increase the accuracy, two different strategies can be followed: reducing $h$, i.e. refining the grid, or increasing $r$, that is using finite elements of higher degree. However, the latter strategy makes sense only if the solution $u$ is regular enough: as a matter of fact, from $(4.25)$ we immediately infer that, if $u \in V \cap \mathrm{H}^{p+1}(I)$, the maximum value of $r$ that it makes sense to take is $r=p$. Values higher than $p$ do not ensure a better rate of convergence: therefore if the solution is not very regular it is not convenient to use finite elements of high degree, as the greater computational cost is not compensated by an improvement of the convergence. An interesting case is when the solution only has the minimum regularity $(p=0)$. From the relations $(4.10)$ and (4.11) we obtain that there is anyhow convergence, but estimate (4.25) is no longer valid. It is then impossible to say how the norm $V$ of the error tends to zero when $h$ decreases. We summarize these situations in Table $4.1$.

Table 4.1. Order of convergence with respect to $h$ for the finite element method for varying regularity of the solution and degree $r$ of the finite elements. We have highlighted on each column the result corresponding to the "optimal" choice of the polynomial degree

\begin{tabular}{llcccc}
\hline$r$ & $u \in \mathrm{H}^{1}(I)$ & $u \in \mathrm{H}^{2}(I)$ & $u \in \mathrm{H}^{3}(I)$ & $u \in \mathrm{H}^{4}(I)$ & $u \in \mathrm{H}^{5}(I)$ \\
\hline 1 & converges & $h^{1}$ & $h^{1}$ & $h^{1}$ & $h^{1}$ \\
2 & converges & $h^{1}$ & $h^{2}$ & $h^{2}$ & $h^{2}$ \\
3 & converges & $h^{1}$ & $h^{2}$ & $h^{3}$ & $h^{3}$ \\
4 & converges & $h^{1}$ & $h^{2}$ & $h^{3}$ & $h^{4}$ \\
\hline
\end{tabular}

In general, we can state that: if $u \in \mathrm{H}^{p+1}(I)$, for a given $p>0$, then there exists a constant $C$ independent of $u$ and $h$, such that

$$
\left\|u-u_{h}\right\|_{\mathrm{H}^{1}(I)} \leq C h^{s}|u|_{\mathrm{H}^{s+1}(I)}, \quad s=\min \{r, p\}
$$

\subsection{Finite elements, simplices and barycentric coordinates}

Before introducing finite element spaces in $2 \mathrm{D}$ and $3 \mathrm{D}$ domains we can attempt to provide a formal definition of finite element.

\subsubsection{An abstract definition of finite element in the Lagrangian case}

From the examples we considered we can deduce that there are three ingredients allowing to characterize a finite element in the general case, i.e. independently of the dimension:

- the domain of definition $K$ of the element. In the one-dimensional case it is an interval, in the two-dimensional case it is generally a triangle but it can also be a quadrilateral; in the three-dimensional case it can be a tetrahedron, a prism or a hexahedron;

- a space of polynomials $\Pi_{r}$ of dimension $N_{r}$ defined on $K$ and a basis $\left\{\varphi_{j}\right\}_{j=1}^{N_{r}}$ of $\Pi_{r}$. In the monodimensional case, $\Pi_{r}$ has been introduced in Sect. $4.3$ and $N_{r}=r+1$. For the multidimensional case, see Sect. 4.4.2;

- a set $\Sigma=\left\{\gamma_{i}: \Pi_{r} \rightarrow \mathbb{R}\right\}_{i=1}^{N_{r}}$ of functionals on $\Pi_{r}$, satisfying $\gamma_{i}\left(\varphi_{j}\right)=\delta_{i j}, \delta_{i j}$ being the Kronecker delta. These allow a unique identification of the coefficients $\left\{\alpha_{j}\right\}_{j=1}^{N_{r}}$ of the expansion of a polynomial $p \in \Pi_{r}$ with respect to the chosen basis, $p(x)=\sum_{j=1}^{N_{r}} \alpha_{j} \varphi_{j}(x)$. As a matter of fact, we have $\alpha_{i}=\gamma_{i}(p), i=1, \ldots, N_{r}$. These coefficients are called degrees of freedom of the finite element.

In the case of Lagrange finite elements the chosen basis is provided by the Lagrange polynomials and the degree of freedom $\alpha_{i}$ is equal to the value taken by the polynomial $p$ at a point $\mathbf{a}_{i}$ of $K$, called node, that is we have $\alpha_{i}=p\left(\mathbf{a}_{i}\right), i=1, \ldots, N_{r}$. We can then set, with a slight notation abuse, $\Sigma=\left\{\mathbf{a}_{j}\right\}_{j=1}^{N_{r}}$, since knowing the position of the nodes allows us to find the degrees of freedom (notice however that this is not true in general, think only of the case of the hierarchical basis introduced previously). In the remainder, we will exclusively refer to the case of Lagrange finite elements.

In the construction of a Lagrange finite element, the choice of nodes is not arbitrary. Indeed, the problem of interpolation on a given set $K$ may be ill posed. For this reason the following definition proves useful:

In such case, the triple $\left(K, \Sigma, \Pi_{r}\right)$ is called Lagrangian finite element. In the case of Lagrangian finite elements, the element is generally recalled by citing the sole polynomial space: hence the linear finite elements introduced previously are called $\mathbb{P}_{1}$, the quadratic ones $\mathbb{P}_{2}$, and so forth.

As we have seen in the $1 \mathrm{D}$ case, for the finite elements based on the use of local $\mathbb{P}_{1}$ and $\mathbb{P}_{2}$ polynomial spaces, it is convenient to define the finite element starting from a reference element $\widehat{K}$; typically this is the interval $(0,1)$. It will tipically be the right triangle with vertices $(0,0),(1,0)$ and $(0,1)$ in the two-dimensional case (when using triangular elements). (See Sect. 4.4.2 for the case in arbitrary dimensions.) Hence, via a transformation $\phi$, we move to the finite element defined on $K$. The transformation therefore concerns the finite element as a whole. More precisely, we observe that if $\left(\widehat{K}, \widehat{\Sigma}, \widehat{\Pi}_{r}\right)$ is a Lagrangian finite element and $\phi: \widehat{K} \rightarrow \mathbb{R}^{d}$ a continuous and injective map, and we define

$$
K=\phi(\widehat{K}), \quad P_{r}=\left\{p: K \rightarrow \mathbb{R}: p \circ \phi \in \widehat{\Pi}_{r}\right\}, \quad \Sigma=\phi(\widehat{\Sigma})
$$

then $\left(K, \Sigma, P_{r}\right)$ is still said to be a Lagrangian finite element. The space of polynomials defined on triangles and tetrahedra can be introduced as follows. 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-080.jpg?height=201&width=334&top_left_y=118&top_left_x=200)

Fig. 4.7. The unitary simplex in $\mathbb{R}^{d}, d=1,2,3$

\subsubsection{Simplexes}

If $\left\{\mathbf{N}_{0}, \ldots, \mathbf{N}_{d}\right\}$ are $d+1$ points in $\mathbb{R}^{d}, d \geq 1$, and the vectors $\left\{\mathbf{N}_{1}-\mathbf{N}_{0}, \ldots, \mathbf{N}_{d}-\mathbf{N}_{0}\right\}$ are linearly independent, then the convex hull of $\left\{\mathbf{N}_{0}, \ldots, \mathbf{N}_{d}\right\}$ is called a simplex, and $\left\{\mathbf{N}_{0}, \ldots, \mathbf{N}_{d}\right\}$ area called the vertices of the simplex. The standard simplex of $\mathbb{R}^{d}$ is the set

$$
\hat{K}_{d}=\left\{\mathbf{x} \in \mathbb{R}^{d}: x_{i} \geq 0,1 \leq i \leq d, \sum_{i=1}^{d} x_{i} \leq 1\right\}
$$

and it is a unit interval in $\mathbb{R}^{1}$, a unit triangle in $\mathbb{R}^{d}$, a unit tetrahedron in $\mathbb{R}^{d}$ (see Fig. 4.7). Its vertices are ordered in such a way that the Cartesian coordinates of $\mathbf{N}_{i}$ are all null, except the i-th one that is equal to 1 . On a $d$-dimensional simplex, the space of polynomials $\mathbb{P}_{r}$ is defined as follows

$$
\mathbb{P}_{r}=\left\{p(\mathbf{x})=\sum_{0 \leq i_{1}, \ldots, i_{d} \atop i_{1}+\cdots+i_{d} \leq d} a_{i_{1} \ldots i_{d}} x_{1}^{i_{1}} \ldots x_{d}^{i_{d}}, \quad a_{i_{1} \ldots i_{d}} \in \mathbb{R}\right\}
$$

Then

$$
N_{r}=\operatorname{dim} \mathbb{P}_{r}=\left(\begin{array}{c}
r+d \\
r
\end{array}\right)=\frac{1}{d !} \prod_{k=1}^{d}(r+k)
$$

\subsubsection{Barycentric coordinates}

For a given simplex $K$ in $\mathbb{R}^{d}$ (see Sect. 4.5.1) it is sometimes convenient to consider a coordinate frame alternative to the Cartesian one, that of the barycentric coordinates. The latter are $d+1$ functions, $\left\{\lambda_{0}, \ldots, \lambda_{d}\right\}$, defined as follows

$$
\lambda_{i}: \mathbb{R}^{d} \rightarrow \mathbb{R}, \lambda_{i}(\mathbf{x})=1-\frac{\left(\mathbf{x}-\mathbf{N}_{i}\right) \cdot \mathbf{n}_{i}}{\left(\mathbf{N}_{j}-\mathbf{N}_{i}\right) \cdot \mathbf{n}_{i}}, \quad 0 \leq i \leq d
$$

For every $i=0, \ldots, d$ let $F_{i}$ denote the face of $K$ opposite to $\mathbf{N}_{i} ; F_{i}$ is in fact a vertex if $d=1$, an edge if $d=2$, a triangle if $d=3$. In $(4.30), \mathbf{n}_{i}$ denotes the outward normal to $F_{i}$, while $\mathbf{N}_{j}$ is an arbitrary vertex belonging to $F_{i}$. The definition of $\lambda_{i}$ is however independent of which vertex of $F_{i}$ is chosen. 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-081.jpg?height=174&width=440&top_left_y=117&top_left_x=228)

Fig. 4.8. The barycentric coordinate $\lambda_{i}$ of the point $\mathbf{P}$ is the ratio $\frac{\left|K_{i}\right|}{K_{K}}$ between the measure of the simplex $K_{i}$ (whose vertices are $\mathbf{P}$ and $\left.\left\{\mathbf{N}_{j}, j \neq i\right\}\right)$ and that of the given simplex $K$ (a triangle on the left, a tetrahedron on the right). The shadowed simplex is $K_{0}$

Barycentric coordinates have a geometrical meaning. Indeed, for every point $\mathbf{P}$ belonging to $K$, its barycentric coordinate $\lambda_{i}, 0 \leq i \leq d$, represents the ratio between the measure of the simplex $K_{i}$ whose vertices are $\mathbf{P}$ and the vertices of $K$ sitting on the face $F_{i}$ opposite to the vertex $\mathbf{N}_{i}$, and the measure of $K$. See Fig. 4.8.

Remark 4.4. Let us consider the unitary simplex $\hat{K}_{d}$, whose vertices $\left\{\hat{\mathbf{N}}_{\mathbf{0}}, \ldots, \hat{\mathbf{N}}_{\mathbf{d}}\right\}$ are ordered in such a way that all the Cartesian coordinates of $\hat{\mathbf{N}}_{\mathbf{i}}$ are null, except $x_{i}$ which is equal to one. Then

$$
\lambda_{i}(\mathbf{x})=x_{i}, \quad 1 \leq i \leq d, \quad \lambda_{0}(\mathbf{x})=1-\sum_{i=1}^{d} \lambda_{i}(\mathbf{x})
$$

The barycentric coordinate $\lambda_{i}$ is therefore an affine function that is equal to 1 at $\mathbf{N}_{i}$ and vanishes on the face $F_{i}$ opposite to $\mathbf{N}_{i}$.

On a general simplex $K$ in $\mathbb{R}^{d}$, the following partition of unity property is satisfied

$$
0 \leq \lambda_{i}(\mathbf{x}) \leq 1, \quad \sum_{i=0}^{d} \lambda_{i}(\mathbf{x})=1 \quad \forall \mathbf{x} \in K
$$

A point $\mathbf{P}$ belonging to the interior of $K$ has therefore all its barycentric coordinates positive. This property is useful whenever one has to check which triangle in $2 \mathrm{D}$ or tetrahedron in $3 \mathrm{D}$ a given point belongs to, a situation that occurs when using Lagrangian derivatives (see Sect. 17.7.2) or computing suitable quantities (fluxes, streamlines, etc.) as a post-processing of finite element computations.

A remarkable property is that the center of gravity of $K$ has all its barycentric coordinates equal to $(d+1)^{-1}$. Another remarkable property is that

$$
\varphi_{i}=\lambda_{i}, \quad 0 \leq i \leq d
$$

where $\left\{\varphi_{i}, 0 \leq i \leq d\right\}$ are the characteristic Lagrangian functions on the simplex $K$ of degree $r=1$, that is

$$
\varphi_{i} \in \mathbb{P}_{1}\left(K_{d}\right), \quad \varphi_{i}\left(\mathbf{N}_{j}\right)=\delta_{i j}, \quad 0 \leq j \leq d
$$

(See Fig. 4.10, left, for the nodes.) For $r=2$ the above identity (4.33) does not hold anymore, however the characteristic Lagrangian functions $\left\{\varphi_{i}\right\}$ can still be expressed in terms of the barycentric coordinates $\left\{\lambda_{i}\right\}$ as follows:

$$
\begin{cases}\varphi_{i}=\lambda_{i}\left(2 \lambda_{i}-1\right), & 0 \leq i \leq d \\ \varphi_{d+i+j}=4 \lambda_{i} \lambda_{j}, & 0 \leq i<j \leq d\end{cases}
$$

For $0 \leq i \leq d, \varphi_{i}$ is the characteristic Lagrangian function associated to the vertex $\mathbf{N}_{i}$, while for $0 \leq i<j \leq d, \varphi_{d+i+j}$ it is the characteristic Lagrangian function associated to the midpoint of the edge whose endpoints are the vertices $\mathbf{N}_{i}$ and $\mathbf{N}_{j}$ (see Fig. 4.10, middle).

The previous identities justify the name "coordinates" that is used for the $\lambda_{i}$ 's. Indeed, if $\mathbf{P}$ is a generic point of the simplex $K$, its Cartesian coordinates $\left\{x_{j}^{(P)}, 1 \leq\right.$ $j \leq d\}$ can be expressed in terms of the barycentric coordinates $\left\{\lambda_{i}^{(P)}, 0 \leq i \leq d\right\}$ as follows

$$
x_{j}^{(P)}=\sum_{i=0}^{d} \lambda_{i}^{(P)} x_{j}^{(i)}, 1 \leq j \leq d
$$

where $\left\{x_{j}^{(i)}, 1 \leq j \leq d\right\}$ denote the Cartesian coordinates of the i-th vertex $\mathbf{N}_{i}$ of the simplex $K$

\subsection{The finite element method in the multi-dimensional case}

In this section we extend the finite element method introduced previously for onedimensional problems to the case of boundary-value problems in multi-dimensional regions. We will also specifically refer to the case of simplexes. Many of the results presented are in any case immediately generalizable to more general finite elements (see, for instance, [QV94]).

For the sake of simplicity, most often we will consider domains $\Omega \subset \mathbb{R}^{2}$ with polygonal shape and meshes (or grids) $\mathscr{T}_{h}$ which represent their cover with non-overlapping triangles. For this reason, $\mathscr{T}_{h}$ is also called a triangulation. We refer to Chapter 6 for a more detailed description of the essential features of a generic grid $\mathscr{T}_{h}$. In this way, the discretized domain

$$
\Omega_{h}=i n t\left(\bigcup_{K \in \mathscr{T}_{h}} K\right)
$$

represented by the internal part of the union of the triangles of $\mathscr{T}_{h}$ perfectly coincides with $\Omega$. We recall that we denote by $\operatorname{int}(A)$ the internal part of the set $A$, that is the region obtained by excluding the boundary from $A$. In fact, we will not analyze the error due to the approximation of a non-polygonal domain with a finite element grid (see Fig. 4.9). The interested reader may consult, for instance, [Cia78] or [SF73]. Hence, from now on we will adopt the symbol $\Omega$ to denote without distinction both the computational domain and its (optional) approximation. 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-083.jpg?height=214&width=722&top_left_y=112&top_left_x=95)

Fig. 4.9. Triangulation of a non-polygonal domain. The mesh induces an approximation $\Omega_{h}$ of the domain $\Omega$ such that $\lim _{h \rightarrow 0} \operatorname{meas}\left(\Omega-\Omega_{h}\right)=0$

Also in the multidimensional case, the parameter $h$ is related to the spacing of the grid. Having set $h_{K}=\operatorname{diam}(K)$, for each $K \in \mathscr{T}_{h}$, where $\operatorname{diam}(K)=\max _{x, y \in K}|x-y|$ is the diameter of element $K$, we define $h=\max _{K \in \mathscr{T}_{h}} h_{K}$. Moreover, we will impose that the grid satisfy the following regularity condition. Let $\rho_{K}$ be the diameter of the circle inscribed in the triangle $K$ (also called sphericity of $K$ ); a family of grids $\left\{\mathscr{T}_{h}, h>0\right\}$ is said to be regular if, for a suitable $\delta>0$, the condition

$$
\frac{h_{K}}{\rho_{K}} \leq \delta \quad \forall K \in \mathscr{T}_{h}
$$

is verified. We observe that condition (4.37) excludes immediately very deformed (i.e. stretched) triangles, and hence the option of using anisotropic computational grids.

On the other hand, anisotropic grids are often used in the context of fluid dynamics problems in the presence of boundary layers. See Remark 4.6, and especially references [AFG 00, DV02, FMP04]. Additional details on the generation of grids on two-dimensional domains are provided in Chapter $6$.

We denote by $\mathbb{P}_{r}$ the space of polynomials of global degree less than or equal to $r$, for $r=1,2, \ldots$ According to the general formula (4.28) we find

$$
\begin{aligned}
&\mathbb{P}_{1}=\left\{p\left(x_{1}, x_{2}\right)=a+b x_{1}+c x_{2}, \text { with } a, b, c \in \mathbb{R}\right\} \\
&\mathbb{P}_{2}=\left\{p\left(x_{1}, x_{2}\right)=a+b x_{1}+c x_{2}+d x_{1} x_{2}+e x_{1}^{2}+f x_{2}^{2}, \text { with } a, b, c, d, e, f \in \mathbb{R}\right\}, \\
&\vdots \\
&\mathbb{P}_{r}=\left\{p\left(x_{1}, x_{2}\right)=\sum_{i, j \geq 0, i+j \leq r} a_{i j} x_{1}^{i} x_{2}^{j}, \text { with } a_{i j} \in \mathbb{R}\right\}
\end{aligned}
$$

According to (4.29), the spaces $\mathbb{P}_{r}$ have dimension $(r+1)(r+2) / 2$. For instance, it results that $\operatorname{dim} \mathbb{P}_{1}=3, \operatorname{dim} \mathbb{P}_{2}=6$ and $\operatorname{dim} \mathbb{P}_{3}=10$, hence on every element of the grid $\mathscr{T}_{h}$ the generic function $v_{h}$ is well defined whenever its value at 3, resp. 6, resp. 10 suitably chosen nodes, is known (see Fig. 4.10). The nodes for linear $(r=1)$, quadratic $(r=2)$, and cubic $(r=3)$ polynomials on a three dimensional simplex are shown in Fig. $4.11$. 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-084.jpg?height=134&width=480&top_left_y=110&top_left_x=216)

Fig. 4.10. Nodes for linear $(r=1$, left $)$, quadratic $(r=2$, center) and cubic $(r=3$, right $)$ polynomials on a triangle. Such sets of nodes are unisolvent

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-084.jpg?height=137&width=170&top_left_y=330&top_left_x=166)

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-084.jpg?height=137&width=174&top_left_y=330&top_left_x=365)
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-084.jpg?height=138&width=580&top_left_y=328&top_left_x=166nomials on a tetrahedron (only those on visible faces are shown)

\subsubsection{Finite element solution of the Poisson problem}

We introduce the space of finite elements

$$
X_{h}^{r}=\left\{v_{h} \in C^{0}(\bar{\Omega}):\left.v_{h}\right|_{K} \in \mathbb{P}_{r} \forall K \in \mathscr{T}_{h}\right\}, \quad r=1,2, \ldots
$$

that is the space of globally continuous functions that are polynomials of degree $r$ on the single triangles (elements) of the triangulation $\mathscr{T}_{h}$.

Moreover, we define

$$
\dot{X}_{h}^{r}=\left\{v_{h} \in X_{h}^{r}:\left.v_{h}\right|_{\partial \Omega}=0\right\}
$$

The spaces $X_{h}^{r}$ and $X_{h}^{r}$ are suitable for the approximation of $\mathrm{H}^{1}(\Omega)$, resp. $\mathrm{H}_{0}^{1}(\Omega)$, thanks to the following property (for its proof see, e.g., [QV94]):

Property 4.2. A sufficient condition for a function $v$ to belong to $\mathrm{H}^{1}(\Omega)$ is that
$v \in C^{0}(\bar{\Omega})$ and $v \in \mathrm{H}^{1}(K) \forall K \in \mathscr{T}_{h}$.

Having set $V_{h}=X_{i}^{r}$, we can introduce the following finite element problem for the approximation of the Poisson problem (3.1) with Dirichlet boundary condition (3.2), in the homogeneous case (that is with $g=0$ )

$$
\text { find } u_{h} \in V_{h}: \int_{\Omega} \nabla u_{h} \cdot \nabla v_{h} d \Omega=\int_{\Omega} f v_{h} d \Omega \quad \forall v_{h} \in V_{h}
$$

As in the one-dimensional case, each function $v_{h} \in V_{h}$ is characterized, uniquely, by the values it takes at the nodes $\mathbf{N}_{i}$, with $i=1, \ldots, N_{h}$, of the grid $\mathscr{T}_{h}$ (excluding the 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-085.jpg?height=208&width=379&top_left_y=109&top_left_x=261)

Fig. 4.12. The basis function $\varphi_{j}$ of the space $X_{h}^{1}$ and its support

boundary nodes where $v_{h}=0$ ); consequently, a basis in the space $V_{h}$ can be the set of the characteristic Lagrangian functions $\varphi_{j} \in V_{h}, j=1, \ldots, N_{h}$, such that

$$
\varphi_{j}\left(\mathbf{N}_{i}\right)=\delta_{i j}=\left\{\begin{array}{ll}
0 & i \neq j \\
1 & i=j
\end{array}, \quad i, j=1, \ldots, N_{h}\right.
$$

In particular, if $r=1$, the nodes are vertices of the elements, with the exception of those vertices belonging to the boundary of $\Omega$, while the generic function $\varphi_{j}$ is linear on each triangle and is equal to 1 at the node $\mathbf{N}_{j}$ and 0 at all the other nodes of the triangulation (see Fig. 4.12).

A generic function $v_{h} \in V_{h}$ can be expressed through a linear combination of the basis functions of $V_{h}$ in the following way

$$
v_{h}(\mathbf{x})=\sum_{i=1}^{N_{h}} v_{i} \varphi_{i}(\mathbf{x}) \quad \forall \mathbf{x} \in \Omega, \text { with } v_{i}=v_{h}\left(\mathbf{N}_{i}\right)
$$

By expressing the discrete solution $u_{h}$ in terms of the basis $\left\{\varphi_{j}\right\}$ via $(4.42), u_{h}(\mathbf{x})=$ $\sum_{j=1}^{N_{h}} u_{j} \varphi_{j}(\mathbf{x})$, with $u_{j}=u_{h}\left(\mathbf{N}_{j}\right)$, and imposing that it verifies $(4.40)$ for each function of the basis itself, we find the following linear system of $N_{h}$ equations in the $N_{h}$ unknowns $u_{j}$, equivalent to problem (4.40),

$$
\sum_{j=1}^{N_{h}} u_{j} \int_{\Omega} \nabla \varphi_{j} \cdot \nabla \varphi_{i} d \Omega=\int_{\Omega} f \varphi_{i} d \Omega, \quad i=1, \ldots, N_{h}
$$

The stiffness matrix has dimensions $N_{h} \times N_{h}$ and is defined as

$$
\mathrm{A}=\left[a_{i j}\right] \quad \text { with } \quad a_{i j}=\int_{\Omega} \nabla \varphi_{j} \cdot \nabla \varphi_{i} d \Omega
$$

Moreover, we introduce the vectors

$$
\mathbf{u}=\left[u_{j}\right] \quad \text { with } \quad u_{j}=u_{h}\left(\mathbf{N}_{j}\right), \quad \mathbf{f}=\left[f_{i}\right] \quad \text { with } \quad f_{i}=\int_{\Omega} f \varphi_{i} d \Omega
$$

The linear system (4.43) can then be written as

$$
\mathrm{A} \mathbf{u}=\mathbf{f}
$$

As in the one-dimensional case, the unknowns are the nodal values of the finite element solution. It is evident, since the support of the generic function with basis $\varphi_{i}$ is only formed by the triangles having node $\mathbf{N}_{i}$ in common, that $\mathrm{A}$ is a sparse matrix. In particular, the number of non-null elements of $\mathrm{A}$ is of the order of $N_{h}$, as $a_{i j}$ is different from zero only if $\mathbf{N}_{j}$ and $\mathbf{N}_{i}$ are nodes of the same triangle. A has not necessarily a definite structure (e.g. banded), as that will depend on how the nodes are numbered.

Let us consider now the case of a non-homogeneous Dirichlet problem represented by equations $(3.1)-(3.2)$. We have seen in the previous chapter that we can in any case resort to the homogeneous case through a lifting (also called extension, or prolongation) of the boundary datum. In the corresponding discrete problem we build a lifting of a well-chosen approximation of the boundary datum, by proceeding in the following way.

We denote by $N_{h}$ the internal nodes of the grid $\mathscr{T}_{h}$ and by $N_{h}^{t}$ the total number, including the boundary nodes that for the sake of simplicity we will suppose to be numbered last. The set of boundary nodes will then be formed by $\left\{\mathbf{N}_{i}, i=N_{h}+1, \ldots, N_{h}^{t}\right\}$. A possible approximation $g_{h}$ of the boundary datum $g$ can be obtained by interpolating $g$ on the space formed by the trace functions on $\partial \Omega$ of functions of $X_{h}^{r}$. This can be written as a linear combination of the traces of the basis functions of $X_{h}^{r}$ associated to the boundary nodes

$$
g_{h}(\mathbf{x})=\sum_{i=N_{h}+1}^{N_{h}^{t}} g\left(\mathbf{N}_{i}\right) \varphi_{i}(\mathbf{x}) \quad \forall \mathbf{x} \in \partial \Omega
$$

Its lifting $R_{g_{h}} \in X_{h}^{r}$ is constructed as follows

$$
R_{g_{h}}(\mathbf{x})=\sum_{i=N_{h}+1}^{N_{h}^{t}} g\left(\mathbf{N}_{i}\right) \varphi_{i}(\mathbf{x}) \quad \forall \mathbf{x} \in \Omega
$$

In Fig. $4.13$ we provide an example of a possible lifting of a non-homogeneous Dirichlet boundary datum ( $3.2$ ), in the case where $g$ has a non-constant value. The finite element formulation of the Poisson problem then becomes:

find $\stackrel{\circ}{u}_{h} \in V_{h}$ :

$$
\int_{\Omega} \nabla \stackrel{u}_{h} \cdot \nabla v_{h} d \Omega=\int_{\Omega} f v_{h} d \Omega-\int_{\Omega} \nabla R_{g_{h}} \cdot \nabla v_{h} d \Omega \quad \forall v_{h} \in V_{h}
$$

The approximate solution will then be provided by $u_{h}=\stackrel{\circ}{u}_{h}+R_{g_{h}}$.

Notice how the particular lifting we adopted allows for the following algebraic interpretation of (4.49)

$$
\mathrm{A} \mathbf{u}=\mathbf{f}-\mathrm{B} \mathbf{g}
$$

where $\mathrm{A}$ and $\mathbf{f}$ are defined as in $(4.44)$ and $(4.45)$, now with $u_{j}=\stackrel{\circ}{u}_{h}\left(\mathbf{N}_{j}\right)$. Having set $N_{h}^{b}=N_{h}^{t}-N_{h}$ (this is the number of boundary nodes), the vector $\mathbf{g} \in \mathbb{R}^{N_{h}^{b}}$ and the 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-087.jpg?height=239&width=543&top_left_y=112&top_left_x=183)

Fig. 4.13. Example of lifting of a non-homogeneous Dirichlet boundary datum $u=g, g$ being variable

matrix $\mathrm{B} \in \mathbb{R}^{N_{h} \times N_{h}^{b}}$ have, respectively, the components

$$
\begin{aligned}
&g_{i}=g\left(\mathbf{N}_{i+N_{h}}\right), \quad i=1, \ldots, N_{h}^{b} \\
&b_{i j}=\int_{\Omega} \nabla \varphi_{j+N_{h}} \cdot \nabla \varphi_{i} d \Omega, \quad i=1, \ldots, N_{h}, j=1, \ldots, N_{h}^{b}
\end{aligned}
$$

Remark 4.5. The matrices $A$ and $B$ are both sparse. An efficient program will store exclusively their non-null elements. (See, e.g., [Saa96] for a description of possible storage formats for sparse matrices, and also Chapter 8). In particular, thanks to the special lifting we have adopted, in the matrix $\mathrm{B}$, all the lines corresponding to nonadjacent nodes to a boundary node will be null. (Two grid nodes are said to be adjacent if there exists an element $K \in \mathscr{T}_{h}$ to which they both belong.)

\subsubsection{Conditioning of the stiffness matrix}

We have seen that the stiffness matrix $\mathrm{A}=\left[a\left(\varphi_{j}, \varphi_{i}\right)\right]$ associated to the Galerkin problem and therefore, in particular, to the finite element method, is positive definite; moreover $\mathrm{A}$ is symmetric if the bilinear form $a(\cdot, \cdot)$ is symmetric.

For a symmetric and positive definite matrix, its condition number with respect to the norm $\|\cdot\|_{2}$ is given by

$$
K_{2}(\mathrm{~A})=\frac{\lambda_{\max }(\mathrm{A})}{\lambda_{\min }(\mathrm{A})}
$$

$\lambda_{\max }(\mathrm{A})$ and $\lambda_{\min }(\mathrm{A})$ being the maximum and minimum eigenvalues, respectively, of $\mathrm{A}$.

It can be proved that, both in the one-dimensional and the multi-dimensional case, the following relation holds for the stiffness matrix

$$
K_{2}(\mathrm{~A})=\mathrm{Ch}^{-2}
$$

where $C$ is a constant independent of the parameter $h$, but dependent on the degree of the finite elements being used. To prove (4.50), we recall that the eigenvalues of the matrix A verify the relation

$$
\mathrm{A} \mathbf{v}=\lambda_{h} \mathbf{v}
$$

$\mathbf{v}$ being an eigenvector corresponding to the eigenvalue $\lambda_{h}$. Let $v_{h}$ be the function of the space $V_{h}$ whose nodal values are the components $v_{i}$ of $\mathbf{v}$, see (4.7). We suppose $a(\cdot, \cdot)$ to be symmetric, so $\mathrm{A}$ is symmetric and its eigenvalues are real and positive. We then have

$$
\lambda_{h}=\frac{(\mathrm{Av}, \mathbf{v})}{|\mathbf{v}|^{2}}=\frac{a\left(v_{h}, v_{h}\right)}{|\mathbf{v}|^{2}}
$$

where $|\cdot|$ is the Euclidean vector norm. We suppose that the grid family $\left\{\mathscr{T}_{h}, h>0\right\}$ is regular (i.e. satisfies (4.37)) and moreover quasi-uniform, i.e. such that there exists a constant $\tau>0$ with

$$
\min _{K \in \mathscr{T}_{h}} h_{K} \geq \tau h \quad \forall h>0
$$

We now observe that, under the hypotheses made on $\mathscr{T}_{h}$, the following inverse inequality holds (for the proof, refer to [QV94])

$$
\exists C_{I}>0 \quad: \quad \forall v_{h} \in V_{h}, \quad\left\|\nabla v_{h}\right\|_{\mathrm{L}^{2}(\Omega)} \leq C_{I} h^{-1}\left\|v_{h}\right\|_{\mathrm{L}^{2}(\Omega)}
$$

the constant $C_{I}$ being independent of $h$. We can now prove that there exist two constants $C_{1}, C_{2}>0$ such that, for each $v_{h} \in V_{h}$ as in (4.7), we have

$$
C_{1} h^{d}|\mathbf{v}|^{2} \leq\left\|v_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2} \leq C_{2} h^{d}|\mathbf{v}|^{2}
$$

$d$ being the spatial dimension, with $d=1,2,3$. For the proof in the general case we refer to Proposition 6.3.1. [QV94]. We here limit ourselves to proving the second inequality in the one-dimensional case $(d=1)$ and for linear finite elements. Indeed, on each element $K_{i}=\left[x_{i-1}, x_{i}\right]$, we have

$$
\int_{K_{i}} v_{h}^{2}(x) d x=\int_{K_{i}}\left(v_{i-1} \varphi_{i-1}(x)+v_{i} \varphi_{i}(x)\right)^{2} d x
$$

with $\varphi_{i-1}$ and $\varphi_{i}$ defined according to (4.15). Then, a direct computation shows that

$$
\int_{K_{i}} v_{h}^{2}(x) d x \leq 2\left(v_{i-1}^{2} \int_{K_{i}} \varphi_{i-1}^{2}(x) d x+v_{i}^{2} \int_{K_{i}} \varphi_{i}^{2}(x) d x\right)=\frac{2}{3} h_{i}\left(v_{i-1}^{2}+v_{i}^{2}\right)
$$

with $h_{i}=x_{i}-x_{i-1}$. The inequality

$$
\left\|v_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2} \leq C h|\mathbf{v}|^{2}
$$

with $C=4 / 3$, can be found by simply summing the intervals $K$ and observing that each nodal contribution $v_{i}$ is counted twice.

On the other hand, from (4.51) we obtain, thanks to the continuity and coercivity of the bilinear form $a(\cdot, \cdot)$,

$$
\alpha \frac{\left\|v_{h}\right\|_{\mathrm{H}^{1}(\Omega)}^{2}}{|\mathbf{v}|^{2}} \leq \lambda_{h} \leq M \frac{\left\|v_{h}\right\|_{\mathrm{H}^{1}(\Omega)}^{2}}{|\mathbf{v}|^{2}}
$$

$M$ and $\alpha$ being the continuity and coercivity constants, respectively. Now, $\left\|v_{h}\right\|_{\mathrm{H}^{1}(\Omega)}^{2} \geq$ $\left\|v_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}$ by the definition of the norm in $\mathrm{H}^{1}(\Omega)$, while $\left\|v_{h}\right\|_{\mathrm{H}^{1}(\Omega)} \leq C_{3} h^{-1}\left\|v_{h}\right\|_{\mathrm{L}^{2}(\Omega)}$ (for a well-chosen constant $\left.C_{3}>0\right)$ thanks to (4.52). Thus, by using inequalities (4.53), we obtain

$$
\alpha C_{1} h^{d} \leq \lambda_{h} \leq M C_{3}^{2} C_{2} h^{-2} h^{d}
$$

We therefore have

$$
\frac{\lambda_{\max }(\mathrm{A})}{\lambda_{\min }(\mathrm{A})} \leq \frac{M C_{3}^{2} C_{2}}{\alpha C_{1}} h^{-2}
$$

that is $(4.50)$.

When the grid-size $h$ decreases, the condition number of the stiffness matrix increases, and therefore the associated system becomes more and more ill-conditioned. In particular, if the datum $\mathbf{f}$ of the linear system (4.46) is subject to a perturbation $\delta \mathbf{f}$ (i.e. it is affected by error), the latter in turn affects the solution with a perturbation $\delta \mathbf{u} ;$ then it can be proved that, if there are no perturbations on the matrix $\mathrm{A}$,

$$
\frac{|\delta \mathbf{u}|}{|\mathbf{u}|} \leq K_{2}(\mathrm{~A}) \frac{|\delta \mathbf{f}|}{|\mathbf{f}|}
$$

It is evident that the higher the conditioning number is, the more the solution is affected by the perturbation on the data. (On the other hand, notice that the latter is always affected by perturbations on the data caused by the inevitable roundoff errors introduced by the computer.)

As a further example we can study how conditioning affects the solution method. Consider, for instance, solving the linear system (4.46) using the conjugate gradient method (see Chap. 7). Then a sequence $\mathbf{u}^{(k)}$ of approximate solutions is iteratively constructed, converging to the exact solution $\mathbf{u}$. In particular, we have

$$
\left\|\mathbf{u}^{(k)}-\mathbf{u}\right\|_{\mathrm{A}} \leq 2\left(\frac{\sqrt{K_{2}(\mathrm{~A})}-1}{\sqrt{K_{2}(\mathrm{~A})}+1}\right)^{k}\left\|\mathbf{u}^{(0)}-\mathbf{u}\right\|_{\mathrm{A}}
$$

having denoted by $\|\mathbf{v}\|_{\mathrm{A}}=\sqrt{\mathbf{v}^{T} \mathrm{~A} \mathbf{v}}$ the so-called "A-norm" of a generic vector $\mathbf{v} \in$ $\mathbb{R}^{N_{h}}$. If we define

$$
\rho=\frac{\sqrt{K_{2}(\mathrm{~A})}-1}{\sqrt{K_{2}(\mathrm{~A})}+1}
$$

such quantity gives an idea of the convergence rate of the method: the closer $\rho$ is to 0 , the faster the method converges, wilst the closer $\rho$ is to 1 , the slower the convergence. Indeed, following (4.50), the more accurate one wants to be, by decreasing $h$, the more ill-conditioned the system will be, and therefore the more "problematic" its solution will turn out to be.

This calls for the system to be preconditioned, i.e. it is necessary to find an invertible matrix P, called (left) preconditioner, such that

$$
K_{2}\left(\mathrm{P}^{-1} \mathrm{~A}\right) \ll K_{2}(\mathrm{~A})
$$

and then apply the iterative method to the system preconditioned with $\mathrm{P}$, that is $\mathrm{P}^{-1} \mathrm{~A} \mathbf{x}=$ $P^{-1} \mathbf{b}$ (see Chapter 7). 

\subsubsection{Estimate of the approximation error in the energy norm}

Analogously to the one-dimensional case, for each $v \in C^{0}(\bar{\Omega})$ we define interpolant of $v$ in the space of $X_{h}^{1}$, determined by the grid $\mathscr{T}_{h}$, the function $\Pi_{h}^{1} v$ such that

$$
\Pi_{h}^{1} v\left(\mathbf{N}_{i}\right)=v\left(\mathbf{N}_{i}\right) \quad \text { for each node } \mathbf{N}_{i} \text { of } \mathscr{T}_{h}, i=1, \ldots, N_{h}
$$

If $\left\{\varphi_{i}\right\}$ is the Lagrangian basis of the space $X_{h}^{1}$, then

$$
\Pi_{h}^{1} v(\mathbf{x})=\sum_{i=1}^{N_{h}} v\left(\mathbf{N}_{i}\right) \varphi_{i}(\mathbf{x})
$$

The operator $\Pi_{h}^{1}: C^{0}(\bar{\Omega}) \rightarrow X_{h}^{1}$, associating to a continuous function $v$ its interpolant $\Pi_{h}^{1} v$, is called interpolation operator.

Similarly, we can define an operator $\Pi_{h}^{r}: C^{0}(\bar{\Omega}) \rightarrow X_{h}^{r}$, for each integer $r \geq 1$ Having denoted by $\Pi_{K}^{r}$ the local interpolation operator associated to a continuous function $v$ the polynomial $\Pi_{K}^{r} v \in \mathbb{P}_{r}(K)$, interpolating $v$ in the degrees of freedom of the element $K \in \mathscr{T}_{h}$, we define

$$
\Pi_{h}^{r} v \in X_{h}^{r}:\left.\quad \Pi_{h}^{r} v\right|_{K}=\Pi_{K}^{r}\left(\left.v\right|_{K}\right) \quad \forall K \in \mathscr{T}_{h}
$$

From now on we will suppose that $\mathscr{T}_{h}$ belongs to a family of regular grids of $\Omega$.

In order to obtain an estimate for the approximation error $\left\|u-u_{h}\right\|_{V}$ we follow a similar procedure to the one used in Theorem $4.3$ for the one-dimensional case. The first step is to derive a suitable estimate for the interpolation error. To this end, we will obtain useful information starting from the geometric parameters of each triangle $K$, i.e. its diameter $h_{K}$ and sphericity $\rho_{K}$. Moreover, we will exploit the affine and invertible transformation $F_{K}: \widehat{K} \rightarrow K$ between the reference triangle $\widehat{K}$ and the generic triangle $K$ (see Fig. 4.14). Such map is defined by $F_{K}(\hat{\mathbf{x}})=B_{K} \hat{\mathbf{x}}+\mathbf{b}_{K}$, with $B_{K} \in \mathbb{R}^{2 \times 2}$ and $\mathbf{b}_{K} \in \mathbb{R}^{2}$, and satisfies the relation $F_{K}(\widehat{K})=K$. We recall that the choice of the reference triangle $\widehat{K}$ is not unique.

We will need some preliminary results.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-090.jpg?height=149&width=364&top_left_y=1049&top_left_x=273)

Fig. 4.14. The map $F_{K}$ between the reference triangle $\widehat{K}$ and the generic triangle $K$ Proof. Since $C^{m}(K) \subset \mathrm{H}^{m}(K)$ densely, for each $m \geq 0$, we can limit ourselves to proving the previous two inequalities for the functions of $C^{m}(K)$, then extending by density the result to the functions of $\mathrm{H}^{m}(K)$. The derivatives in the remainders will therefore have to be intended in the classical sense. We recall that

$$
|\hat{v}|_{\mathrm{H}^{m}(\widehat{K})}=\left(\sum_{|\alpha|=m} \int_{\widehat{K}}\left|D^{\alpha} \hat{v}\right|^{2} d \hat{\mathbf{x}}\right)^{1 / 2}
$$

by referring to Chapter $2.3$ for the definition of the derivative $D^{\alpha}$. By using the chain rule for the differentiation of composite functions, we obtain

$$
\left\|D^{\alpha} \hat{v}\right\|_{L^{2}(\widehat{K})} \leq C\left\|B_{K}\right\|^{m} \sum_{|\boldsymbol{\beta}|=m}\left\|\left(D^{\beta} v\right) \circ F_{K}\right\|_{\mathrm{L}^{2}(\widehat{K})}
$$

Then

$$
\left\|D^{\alpha} \hat{v}\right\|_{L^{2}(\widehat{K})} \leq C\left\|B_{K}\right\|^{m}\left|\operatorname{det} B_{K}\right|^{-\frac{1}{2}}\left\|D^{\alpha} v\right\|_{L^{2}(K)}
$$

Inequality (4.55) follows after summing on the multi-index $\boldsymbol{\alpha}$, for $|\boldsymbol{\alpha}|=m$. The result (4.56) can be proved by proceeding in a similar way. Proof. Thanks to (4.57) we have

$$
\left\|B_{K}\right\|=\frac{1}{\hat{\rho}} \sup _{\boldsymbol{\xi} \in \mathbb{R}^{2},|\boldsymbol{\xi}|=\hat{\rho}}\left|B_{K} \boldsymbol{\xi}\right|
$$

For each $\boldsymbol{\xi}$, with $|\boldsymbol{\xi}|=\hat{\rho}$, we can find two points $\hat{\mathbf{x}}$ and $\hat{\mathbf{y}} \in \widehat{K}$ such that $\hat{\mathbf{x}}-\hat{\mathbf{y}}=\boldsymbol{\xi}$. Since $B_{K} \boldsymbol{\xi}=F_{K}(\hat{\mathbf{x}})-F_{K}(\hat{\mathbf{y}})$, we have $\left|B_{K} \boldsymbol{\xi}\right| \leq h_{K}$, that is (4.58). An analogous procedure leads to the result (4.59).

What we now need is an estimate in $\mathrm{H}^{m}(\widehat{K})$ of the seminorm of $\left(v-\Pi_{K}^{r} v\right) \circ F_{K}$, for each function $v$ of $\mathrm{H}^{m}(K)$. In the remainder, we denote the interpolant $\Pi_{K}^{r} v \circ F_{K}$ with $\left[\Pi_{K}^{r} v\right]^{-}$. The nodes of $K$ are $\mathbf{N}_{i}^{K}=F_{K}\left(\widehat{\mathbf{N}}_{i}\right), \widehat{\mathbf{N}}_{i}$ being the nodes of $\widehat{K}$, and, analogously, the basis functions $\hat{\varphi}_{i}$ defined on $\widehat{K}$ are determined by the relation $\hat{\varphi}_{i}=\varphi_{i}^{K} \circ F_{K}$, having denoted by $\varphi_{i}^{K}$ the basis functions associated to the element $K$. Thus,

$$
\left[\Pi_{K}^{r} v\right]^{-}=\Pi_{K}^{r} v \circ F_{K}=\sum_{i=1}^{M_{K}} v\left(\mathbf{N}_{i}^{K}\right) \varphi_{i}^{K} \circ F_{K}=\sum_{i=1}^{M_{K}} v\left(F_{K}\left(\widehat{\mathbf{N}}_{i}\right)\right) \hat{\varphi}_{i}=\Pi_{\widehat{K}}^{r} \hat{v}
$$

$M_{K}$ being the number of nodes on $K$ relating to the degree $r$. Then

$$
\left|\left(v-\Pi_{K}^{r} v\right) \circ F_{K}\right|_{\mathrm{H}^{m}(\hat{K})}=\left|\hat{v}-\Pi_{\hat{K}}^{r} \hat{v}\right|_{\mathrm{H}^{m}(\widehat{K})}
$$

In order to estimate the right side of the previous equality, we start by proving the following result:

Proof. Let $\hat{v} \in \mathrm{H}^{r+1}(\widehat{K})$. For each $\hat{p} \in \mathbb{P}_{r}(\widehat{K})$, thanks to $(4.61)$ and to definition (4.63) of the norm, we obtain

$$
|\widehat{L}(\hat{v})|_{\mathrm{H}^{m}(\widehat{K})}=|\widehat{L}(\hat{v}+\hat{p})|_{\mathrm{H}^{m}(\widehat{K})} \leq\|\widehat{L}\|_{\mathscr{L}\left(\mathrm{H}^{r+1}(\widehat{K}), \mathrm{H}^{m}(\widehat{K})\right)}\|\hat{v}+\hat{p}\|_{\mathrm{H}^{r+1}(\widehat{K})}
$$

Then (4.62) can be deduced thanks to the fact that $\hat{p}$ is arbitrary. The following result (whose proof is given, e.g., in [QV94, Chap. 3]) provides the last necessary tool to obtain the estimate for the interpolation error that we are seeking.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-093.jpg?height=160&width=723&top_left_y=196&top_left_x=96)

As a consequence of the two previous lemmas, we can provide the following

We are now able to prove the interpolation error estimate.

Proof. From Property $2.3$ we derive first of all that $\mathrm{H}^{r+1}(K) \subset C^{0}(K)$, for $r \geq 1$. The interpolation operator $\Pi_{K}^{r}$ thus results to be well defined in $\mathrm{H}^{r+1}(K)$. By using in succession $(4.56),(4.60),(4.59)$ and $(4.65)$ we have

$$
\begin{aligned}
\left|v-\Pi_{K}^{r} v\right|_{\mathrm{H}^{m}(K)} & \leq C_{1}\left\|B_{K}^{-1}\right\|^{m}\left|\operatorname{det} B_{K}\right|^{\frac{1}{2}}\left|\hat{v}-\Pi_{\widehat{K}}^{r} \hat{v}\right|_{\mathrm{H}^{m}(\widehat{K})} \\
& \leq C_{1} \frac{\hat{h}^{m}}{\rho_{K}^{m}}\left|\operatorname{det} B_{K}\right|^{\frac{1}{2}}\left|\hat{v}-\Pi_{\widehat{K}}^{r} \hat{v}\right|_{\mathrm{H}^{m}(\widehat{K})} \\
& \leq C_{2} \frac{\hat{h}^{m}}{\rho_{K}^{m}}\left|\operatorname{det} B_{K}\right|^{\frac{1}{2}}\|\widehat{L}\|_{\mathscr{L}\left(\mathrm{H}^{r+1}(\widehat{\mathcal{K}}), \mathrm{H}^{m}(\hat{K})\right)}|\hat{v}|_{\mathrm{H}^{r+1}(\widehat{K})} \\
&=C_{3} \frac{1}{\rho_{K}^{m}}\left|\operatorname{det} B_{K}\right|^{\frac{1}{2}}|\hat{v}|_{\mathrm{H}^{r+1}(\hat{K})}
\end{aligned}
$$

where $C_{1}=C_{1}(m), C_{2}=C_{2}(r, m, \widehat{K})$ and $C_{3}=C_{3}(r, m, \widehat{K})$ are suitably chosen constants, all independent of $h$. We note that the result (4.65) has been applied when identifying $\widehat{L}$ with the operator $I-\Pi_{\widehat{K}}^{r}$, with $\left(I-\Pi_{\widehat{K}}^{r}\right) \hat{p}=0$, for $\hat{p} \in \mathbb{P}_{r}(\widehat{K})$. Moreover the quantity $\hat{h}^{m}$ and the norm of the operator $\widehat{L}$ have been included in the constant $C_{3}$. At this point, by applying (4.55) and (4.58) we obtain (4.66), that is

$$
\left|v-\Pi_{K}^{r} v\right|_{\mathrm{H}^{m}(K)} \leq C_{4} \frac{1}{\rho_{K}^{m}}\left\|B_{K}\right\|^{r+1}|v|_{\mathrm{H}^{r+1}(K)} \leq C_{5} \frac{h_{K}^{r+1}}{\rho_{K}^{m}}|v|_{\mathrm{H}^{r+1}(K)}
$$

$C_{4}=C_{4}(r, m, \widehat{K})$ and $C_{5}=C_{5}(r, m, \widehat{K})$ being two well-chosen constants. The quantity $\hat{\rho}^{r+1}$ generated by (4.58) and relating to the sphericity of the reference element has been directly included in the constant $C_{5}$.

Finally, we can prove the global estimate for the interpolation error:

$$
\begin{aligned}
&\text { Theorem } 4.5 \text { (Global estimate for the interpolation error). Let } m=0,1 \text { and } \\
&r \geq 1 . \text { There exists a constant } C=C(r, m, \widehat{K})>0 \text { such that } \\
&\qquad\left|v-\Pi_{h}^{r} v\right|_{\mathrm{H}^{m}(\Omega)} \leq C\left(\sum_{K \in \mathscr{T} h} h_{K}^{2(r+1-m)}|v|_{\mathrm{H}^{r+1}(K)}^{2}\right)^{1 / 2} \quad \forall v \in \mathrm{H}^{r+1}(\Omega) . \\
&\text { In particular, we obtain } \\
&\qquad\left|v-\Pi_{h}^{r} v\right|_{\mathrm{H}^{m}(\Omega)} \leq C h^{r+1-m}|v|_{\mathrm{H}^{r+1}(\Omega)} \quad \forall v \in \mathrm{H}^{r+1}(\Omega)
\end{aligned}
$$

Proof. Thanks to (4.66) and to the regularity condition (4.37), we have

$$
\begin{aligned}
\left|v-\Pi_{h}^{r} v\right|_{\mathrm{H}^{m}(\Omega)}^{2} &=\sum_{K \in \mathscr{T}_{h}}\left|v-\Pi_{K}^{r} v\right|_{\mathrm{H}^{m}(K)}^{2} \\
& \leq C_{1} \sum_{K \in \mathscr{T}_{h}}\left(\frac{h_{K}^{r+1}}{\rho_{K}^{m}}\right)^{2}|v|_{\mathrm{H}^{r+1}(K)}^{2} \\
&=C_{1} \sum_{K \in \mathscr{T}_{h}}\left(\frac{h_{K}}{\rho_{K}}\right)^{2 m} h_{K}^{2(r+1-m)}|v|_{\mathrm{H}^{r+1}(K)}^{2} \\
& \leq C_{1} \delta^{2 m} \sum_{K \in \mathscr{T}_{h}} h_{K}^{2(r+1-m)}|v|_{\mathrm{H}^{r+1}(K)}^{2}
\end{aligned}
$$

i.e. (4.68), with $C_{1}=C_{1}(r, m, \widehat{K})$ and $C=C_{1} \delta^{2 m} .(4.69)$ follows thanks to the fact that $h_{K} \leq h$, for each $K \in \mathscr{T}_{h}$, and that for each integer $p \geq 0$

$$
|v|_{\mathrm{H}^{p}(\Omega)}=\left(\sum_{K \in \mathscr{T}_{h}}|v|_{\mathrm{H}^{p}(K)}^{2}\right)^{1 / 2}
$$

In the $m=0$ case, regularity of the grid is not necessary to obtain the estimate (4.69). This is no longer true for $m=1$. As a matter of fact, given a triangle $K$ and a function $v \in \mathrm{H}^{r+1}(K)$, with $r \geq 1$, the following inequality holds [QV94],

$$
\left|v-\Pi_{h}^{r} v\right|_{\mathrm{H}^{m}(K)} \leq \tilde{C} \frac{h_{K}^{r+1}}{\rho_{K}^{m}}|v|_{\mathrm{H}^{r+1}(K)}, \quad m=0,1
$$

with $\tilde{C}$ independent of $v$ and $\mathscr{T}_{h}$. Hence, in the case $m=1$ for a family of regular grids we obtain (4.69) by setting $C=\delta \tilde{C}, \delta$ being the constant appearing in (4.37). On the other hand, the need for a regularity condition can be proved by considering the particular case where, for each $C>0$, a (non-regular) grid can be constructed for which inequality (4.69) is not true, as we are about to prove in the following example which relates to the case $r=1$.

Example 4.1. Consider the triangle $K_{l}$ illustrated in Fig. 4.15, with vertices $(0,0)$, $(1,0),(0.5, l)$, with $l \leq \frac{\sqrt{3}}{2}$, and the function $v\left(x_{1}, x_{2}\right)=x_{1}^{2}$. Clearly $v \in \mathrm{H}^{2}\left(K_{l}\right)$, and its linear interpolant on $K_{l}$ is given by $\Pi_{h}^{1} v\left(x_{1}, x_{2}\right)=x_{1}-(4 l)^{-1} x_{2}$. Since in this case $h_{K_{l}}=1$, inequality (4.69), applied to the single triangle $K_{l}$, would yield

$$
\left|v-\Pi_{h}^{1} v\right|_{\mathrm{H}^{1}\left(K_{l}\right)} \leq C|v|_{\mathrm{H}^{2}\left(K_{l}\right)}
$$

Let us now consider the behaviour of the ratio $\eta_{l}=\frac{\left|v-\Pi_{h}^{1} v\right|_{\mathrm{H}^{1}\left(K_{l}\right)}}{|v|_{\mathrm{H}^{2}\left(K_{l}\right)}}$ when $l$ tends to zero, that is when the triangle is squeezed. We note that allowing $l$ to tend to zero is equivalent to violating the regularity condition (4.37), because for small enough values of $l, h_{K_{l}}=1$. At the same time, denoting by $p_{K_{l}}$ the perimeter of $K_{l}$ and by $\left|K_{l}\right|$ we have the surface of the element $K_{l}$, the sphericity of $K_{l}$

$$
\rho_{K_{l}}=\frac{4\left|K_{l}\right|}{p_{K_{l}}}=\frac{2 l}{1+\sqrt{1+4 l^{2}}}
$$
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-095.jpg?height=318&width=682&top_left_y=860&top_left_x=109)

Fig. 4.15. The triangle $K_{l}$ (left) and the behaviour of the relation $\left|v-\Pi_{h}^{1} v\right|_{\mathrm{H}^{1}\left(K_{l}\right)} /|v|_{\mathrm{H}^{2}\left(K_{l}\right)}$ as a function of $l$ (right) tends to zero. We have

$$
\eta_{l} \geq \frac{\left\|\partial_{x_{2}}\left(v-\Pi_{h}^{1} v\right)\right\|_{\mathrm{L}^{2}\left(K_{l}\right)}}{|v|_{\mathrm{H}^{2}\left(K_{l}\right)}}=\left(\frac{\int_{K_{l}}\left(\frac{1}{4 l}\right)^{2} d \mathbf{x}}{2 l}\right)^{\frac{1}{2}}=\frac{1}{8 l}
$$

Hence $\lim _{l \rightarrow 0} \eta_{l}=+\infty$ (see Fig. 4.15). Consequently, there cannot exist a constant $C$, independent of $\mathscr{T}_{h}$, for which (4.70) holds.

The theorem on the interpolation error estimate immediately provides us with an estimate for the approximation error of the Galerkin method. The proof is analogous to that of Theorem $4.3$ for the one-dimensional case. Indeed, it is sufficient to apply (4.10) and Theorem $4.5$ (for $m=1$ ) to obtain the following error estimate:

Theorem 4.6. Let $\left\{\mathscr{T}_{h}\right\}_{h>0}$ be a family of regular triangulations of the domain
$\Omega$. Let $u \in V$ be the exact solution of the variational problem $(4.1)$ and $u_{h}$ its
approximate solution using the finite element method of degree $r$. If $u \in \mathrm{H}^{r+1}(\Omega)$,
then the following a priori error estimates hold:

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-096.jpg?height=246&width=720&top_left_y=536&top_left_x=97)

Also in the multi-dimensional case, in order to increase the accuracy two different strategies can therefore be followed:

1. decreasing $h$, i.e. refining the grid;

2. increasing $r$, i.e. using finite elements of higher degree.

However, the latter approach can only be pursued if the solution $u$ is regular enough. In general, we can say that if $u \in C^{0}(\bar{\Omega}) \cap \mathrm{H}^{p+1}(\Omega)$ for some $p>0$, then

$$
\left\|u-u_{h}\right\|_{\mathrm{H}^{1}(\Omega)} \leq C h^{s}|u|_{\mathrm{H}^{s+1}(\Omega)}, \quad s=\min \{r, p\},
$$

as already observed in the one-dimensional case (see (4.26)). Note that a sufficient condition for $u$ to be continuous is $p>\frac{d}{2}-1$ ( $d$ being the spatial dimension of the problem, $d=1,2,3$ ). Moreover, it is possible to prove an error estimate in the maximum norm. For instance, if $r=1$, one has

$$
\left\|u-u_{h}\right\|_{L^{\infty}(\Omega)} \leq C h^{2}|\log h||u|_{W^{2, \infty}(\Omega)}
$$

where $C$ is a positive constant independent of $h$ and the last term on the right-hand side is the seminorm of $u$ in the Sobolev space $W^{2, \infty}(\Omega)$ (see Sect. 2.5). For the proof of this and other error estimates in $W^{k, \infty}(\Omega)$-norms see, e.g., [Cia78] and [BS94].

Remark 4.6 (Case of anisotropic grids). The interpolation error estimate (4.66) (and the consequent discretization error estimate) can be generalized in the case of anisotropic grids. In such case however, the left term of (4.66) takes a more complex expression: these estimates, in fact, because of their directional nature, must take into account the information coming from the characteristic directions associated to the single triangles which replace the "global" information concentrated in the seminorm $|v|_{\mathrm{H}^{r+1}(K)}$. The interested reader can consult [Ape99, FP01]. Moreover, we refer to Fig. $4.18$ and $13.21$ for examples of anisotropic grids.

\subsubsection{Estimate of the approximation error in the $\mathrm{L}^{2}$ norm}

The inequality (4.72) provides an estimate of the approximation error in the energy norm. Analogously, it is possible to obtain an error estimate in the $\mathrm{L}^{2}$ norm. Since the latter norm is weaker than the former one, one must expect a higher convergence rate with respect to $h$.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-097.jpg?height=289&width=722&top_left_y=591&top_left_x=97)

For the proof see, e.g., [Bre86, Gri11].

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-097.jpg?height=234&width=720&top_left_y=938&top_left_x=98)

Proof. We will limit ourselves to proving this result for the Poisson problem (3.13), the weak formulation of which is given in (3.18). Let $e_{h}=u-u_{h}$ be the approximation error, and consider the following auxiliary Poisson problem (called adjoint problem, see Sect. 3.5) with source term given by the error function $e_{h}$

$$
\begin{cases}-\Delta \phi=e_{h} & \text { in } \Omega \\ \phi=0 & \text { on } \partial \Omega\end{cases}
$$

whose weak formulation is

$$
\text { find } \phi \in V: \quad a(\phi, v)=\int_{\Omega} e_{h} v d \Omega \quad \forall v \in V
$$

with $V=\mathrm{H}_{0}^{1}(\Omega)$. Taking $v=e_{h}(\in V)$, we have

$$
\left\|e_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}=a\left(\phi, e_{h}\right)
$$

Since the bilinear form is symmetric, by the Galerkin orthogonality (4.8) we have

$$
a\left(e_{h}, \phi_{h}\right)=a\left(\phi_{h}, e_{h}\right)=0 \quad \forall \phi_{h} \in V_{h}
$$

It follows that

$$
\left\|e_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}=a\left(\phi, e_{h}\right)=a\left(\phi-\phi_{h}, e_{h}\right)
$$

Now, taking $\phi_{h}=\Pi_{h}^{1} \phi$, applying the Cauchy-Schwarz inequality to the bilinear form $a(\cdot, \cdot)$ and using the interpolation error estimate (4.69) we obtain

$$
\left\|e_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2} \leq\left|e_{h}\right|_{\mathrm{H}^{1}(\Omega)}\left|\phi-\phi_{h}\right|_{\mathrm{H}^{1}(\Omega)} \leq\left|e_{h}\right|_{\mathrm{H}^{1}(\Omega)} C h|\phi|_{\mathrm{H}^{2}(\Omega)}
$$

Notice that the interpolation operator $\Pi_{h}^{1}$ can be applied to $\phi$ since, $\phi \in \mathrm{H}^{2}(\Omega)$ thanks to Lemma $4.6$ and thus, in particular, $\phi \in C^{0}(\bar{\Omega})$, thanks to property $2.3$ in Chap. 2 . By applying Lemma $4.6$ to the adjoint problem (4.76) we obtain the inequality

$$
|\phi|_{\mathrm{H}^{2}(\Omega)} \leq C\left\|e_{h}\right\|_{\mathrm{L}^{2}(\Omega)}
$$

which, applied to (4.79), eventually provides

$$
\left\|e_{h}\right\|_{\mathrm{L}^{2}(\Omega)} \leq C h\left|e_{h}\right|_{\mathrm{H}^{1}(\Omega)}
$$

where $C$ accounts for all the constants that have appeared so far. By now exploiting the error estimate in the energy norm (4.72), we obtain (4.75).

Let us generalize the result we have just proved for the Poisson problem to the case of a generic elliptic boundary-value problem approximated with finite elements and for which an estimate of the approximation error in the energy norm such as (4.72) holds, and an elliptic regularity property analogous to the one of Lemma $4.6$ holds. In particular, let us consider the case where the bilinear form $a(\cdot, \cdot)$ is not necessarily symmetric. Let $u$ be the exact solution of the problem

$$
\text { find } u \in V: \quad a(u, v)=(f, v) \quad \forall v \in V,
$$

and $u_{h}$ the solution of the Galerkin problem

$$
\text { find } u_{h} \in V_{h}: \quad a\left(u_{h}, v_{h}\right)=\left(f, v_{h}\right) \quad \forall v_{h} \in V_{h} .
$$

Finally, suppose that the error estimate (4.72) holds and let us consider the following problem, which we will call adjoint problem of $(4.81)$ : for each $g \in \mathrm{L}^{2}(\Omega)$,

$$
\text { find } \phi=\phi(g) \in V: \quad a^{*}(\phi, v)=(g, v) \quad \forall v \in V,
$$

where we have defined $(\operatorname{see}(3.40)) a^{*}: V \times V \rightarrow \mathbb{R}$.

Obviously if $a$ is symmetric the two problems coincide, as seen for instance in the case of problem (4.77).

Let us suppose that for the solution $u$ of the primal problem (4.81) an elliptic regularity result holds; it can then be verified that the same result is valid for the adjoint problem (4.82), that is

$$
\exists C>0: \quad\|\phi(g)\|_{\mathrm{H}^{2}(\Omega)} \leq C\|g\|_{\mathrm{L}^{2}(\Omega)} \quad \forall g \in \mathrm{L}^{2}(\Omega)
$$

In particular, this is true for a generic elliptic problem with Dirichlet or Neumann (but not mixed) data on a polygonal and convex domain $\Omega$ [Gri11]. We now choose $g=e_{h}$ and denote, for simplicity, $\phi=\phi\left(e_{h}\right)$. Furthermore, having chosen $v=e_{h}$, we have

$$
\left\|e_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}=a\left(e_{h}, \phi\right)
$$

Since by the elliptic regularity of the adjoint problem $\phi \in \mathrm{H}^{2}(\Omega)$, and $\|\phi\|_{\mathrm{H}^{2}(\Omega)} \leq$ $C\left\|e_{h}\right\|_{\mathrm{L}^{2}(\Omega)}$ thanks to the Galerkin orthogonality, we have that

$$
\begin{aligned}
\left\|e_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2} &=a\left(e_{h}, \phi\right)=a\left(e_{h}, \phi-\Pi_{h}^{1} \phi\right) \\
& \leq C_{1}\left\|e_{h}\right\|_{\mathrm{H}^{1}(\Omega)}\left\|\phi-\Pi_{h}^{1} \phi\right\|_{\mathrm{H}^{1}(\Omega)} \\
& \leq C_{2}\left\|e_{h}\right\|_{\mathrm{H}^{1}(\Omega)} h\|\phi\|_{\mathrm{H}^{2}(\Omega)} \\
& \leq C_{3}\left\|e_{h}\right\|_{\mathrm{H}^{1}(\Omega)} h\left\|e_{h}\right\|_{\mathrm{L}^{2}(\Omega)}
\end{aligned}
$$

where we have exploited the continuity of the form $a(\cdot, \cdot)$ and the estimate (4.72). Thus

$$
\left\|e_{h}\right\|_{\mathrm{L}^{2}(\Omega)} \leq C_{3} h\left\|e_{h}\right\|_{\mathrm{H}^{1}(\Omega)}
$$

from which (4.75) follows, using the estimate (4.73) of the error in $\mathrm{H}^{1}(\Omega)$. 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-100.jpg?height=379&width=531&top_left_y=124&top_left_x=185)

Fig. 4.16. Behaviour with respect to $h$ of the error in $\mathrm{H}^{1}(\Omega)$ norm (lines without crosses) and in $\mathrm{L}^{2}(\Omega)$ norm (lines with crosses) for linear (solid lines) and quadratic (etched lines) finite elements for the solution of the problem reported in Example $4.2$

Remark 4.7. The technique illustrated above, depending upon the use of the adjoint problem for the estimate of the $\mathrm{L}^{2}$-norm of the discretization error, is known in the literature as Aubin-Nitsche trick [Aub67, Nit68]. Several examples of how to determine the adjoint of a given problem will be presented in Sect. $3.5$.

Example 4.2. We consider the model problem $-\Delta u+u=f$ in $\Omega=(0,1)^{2}$ with $u=g$ on $\partial \Omega$. Suppose to choose the source term $f$ and the function $g$ so that the exact solution of the problem is $u(x, y)=\sin (2 \pi x) \cos (2 \pi y)$. We solve such a problem with the Galerkin method with finite elements of degree 1 and 2 on a uniform grid with step-size $h$. The graph of Fig. $4.16$ shows the behaviour of the error when the grid-size $h$ decreases, both in the norm $\mathrm{L}^{2}(\Omega)$ and in that of $\mathrm{H}^{1}(\Omega)$. As shown by inspecting the slope of the lines in the figure, the error's decrease when using $\mathrm{L}^{2}$ norm (crossed lines) is quadratic if linear finite elements are used (solid line), and cubic when quadratic finite elements are used (etched line).

With respect to the $\mathrm{H}^{1}$ norm (lines without crosses) instead, there is a linear reduction of the erorr with respect to the linear finite elements (solid line), and quadratic when quadratic finite elements are used (etched line). Fig. $4.17$ shows the solution on the grid with grid-size $1 / 8$ obtained with linear (left) and quadratic (right) finite elements.

\subsection{Grid adaptivity}

In Theorems $4.6$ and $4.7$ we have derived some a priori estimates for the finite element approximation error. 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-101.jpg?height=226&width=704&top_left_y=158&top_left_x=106)

Fig. 4.17. Solutions computed using piecewise linear (left) and piecewise quadratic (right) finite elements on a uniform grid with grid-size $1 / 8$

Since the parameter $h$ is the maximal length of the finite element edges, if we referred to (4.72) we could be tempted to refine the grid everywhere in the hope of reducing the error $\left\|u-u_{h}\right\|_{\mathrm{H}^{1}(\Omega)}$. However, it is more convenient to refer to $(4.71)$ where the upper bound is the sum of elemental contributions involving the solution seminorm $|u|_{\mathrm{H}^{r+1}(K)}$ on each element $K$ and the local grid-size $h_{K}$.

Indeed, in order to have an efficient grid that minimizes the number of elements necessary to obtain the desired accuracy, we can equidistribute the error on each element $K \in \mathscr{T}_{h}$. In particular, we would like to obtain

$$
h_{K}^{r}|u|_{\mathrm{H}^{r+1}(K)} \simeq \eta \quad \forall K \in \mathscr{T}_{h}
$$

where $\eta$ is a well-chosen constant that only depends on the desired accuracy and on the number of elements of the grid.

A larger contribution from $|u|_{\mathrm{H}^{r+1}(K)}$ (due to a more pronounced variability of $\left.u\right|_{K}$ ) will need to be balanced either by a smaller local grid-size $h_{K}$ or by a higher polynomial degree $r$. In the first case, we will talk about $h$-adaptivity of the grid, in the second case of $p$-adaptivity (where $p$ stands for "polynomial"). In the remainder of this chapter we will only focus on the first technique. However, we refer to Chap. 10 for the analysis of error estimates which are better suited for polynomial adaptivity.

The remarks made up to now, although correct, turn out to be of little use as the solution $u$ is not known. We can therefore proceed according to different strategies. The first one is to use the a priori error estimate (4.71) by replacing the exact solution $u$ with a well-chosen approximation, easily computable on each single element. In such case, we talk about a priori adaptivity.

A second approach is instead based on the use of an a posteriori error estimate able to link the approximation error to the behaviour of the approximate numerical solution $u_{h}$, known after solving the problem numerically. In such case, the optimal computational grid will be constructed through an iterative process where solution, error estimate and modification of the computational grid are recomputed until reaching the requested accuracy. In this case, we talk about a posteriori adaptivity. The a priori and a posteriori adaptivity strategies are not mutually exclusive, actually they can coexist. For instance, having generated an appropriate starting grid through an a priori adaptivity, the latter can be further refined through a posteriori analysis.

\subsubsection{A priori adaptivity based on derivatives reconstruction}

An a priori adaptivity technique is based on estimate (4.71) where the derivatives of $u$ are carefully approximated on each element, with the purpose of estimating the local seminorms of $u$. To do this, an approximate solution $u_{h^{*}}$ is used, computed on a tentative grid with step-size $h^{*}$, with $h^{*}$ large enough so that the computation is cheap, but not too large to generate an excessive error in the approximation of the derivatives, which could affect the effectiveness of the whole procedure.

We exemplify the algorithm for linear finite elements, in which case (4.71) takes the form

$$
\left\|u-u_{h}\right\|_{\mathrm{H}^{1}(\Omega)} \leq C\left(\sum_{K \in \mathscr{T}_{h}} h_{K}^{2}|u|_{\mathrm{H}^{2}(K)}^{2}\right)^{\frac{1}{2}}
$$

( $C$ accounts for the continuity and coercivity constants of the bilinear form). Our aim is eventually to solve our problem on a grid $\mathscr{T}_{h}$ guaranteeing that the right-hand side of $(4.83)$ stands below a predefined tolerance $\varepsilon>0$. Let us suppose that we have computed a solution, say $u_{h^{*}}$, on a preliminary grid $\mathscr{T}_{h^{*}}$ with $N^{*}$ triangles. We use $u_{h^{*}}$ to approximate the second derivatives of $u$ that intervene in the definition of the seminorm $|u|_{\mathrm{H}^{2}(K)}$. Since $u_{h^{*}}$ does not have any continuous second derivatives in $\Omega$, it is necessary to proceed with an adequate reconstruction technique. For each node $\mathbf{N}_{i}$ of the grid we consider the set (patch) $K_{\mathbf{N}_{i}}$ of the elements sharing $\mathbf{N}_{i}$ as a node (that is the set of the elements forming the support of $\varphi_{i}$, see Fig. 4.12). We then find the planes $\pi_{i}^{j}(\mathbf{x})=\mathbf{a}_{i}^{j} \cdot \mathbf{x}+b_{i}^{j}$ by minimizing

$$
\int_{K_{\mathbf{N}}_{i}}\left|\pi_{i}^{j}(\mathbf{x})-\frac{\partial u_{h^{*}}}{\partial x_{j}}(\mathbf{x})\right|^{2} d \mathbf{x}, \quad j=1,2
$$

solving a two-equation system for the coefficients $\mathbf{a}_{i}^{j}$ and $b_{i}^{j}$. This can be regarded as the local projection phase. We thus build a piecewise linear approximation $\mathbf{g}_{h^{*}} \in\left(X_{h^{*}}^{1}\right)^{2}$ of the gradient $\nabla u_{h^{*}}$ defined as

$$
\left[\mathbf{g}_{h^{*}}(\mathbf{x})\right]^{j}=\sum_{i} \pi_{i}^{j}\left(\mathbf{x}_{i}\right) \varphi_{i}(\mathbf{x}), \quad j=1,2,
$$

where the sum spans over all the nodes $\mathbf{N}_{i}$ of the grid. Once the gradient is reconstructed we can proceed in two different ways, depending on the type of reconstruction that we want to obtain for the second derivatives. We recall first of all that the Hessian matrix associated to a function $u$ is defined by $\mathbf{D}^{2}(u)=\nabla(\nabla u)$, that is

$$
\left[\mathbf{D}^{2}(u)\right]_{i, j}=\frac{\partial^{2} u}{\partial x_{i} \partial x_{j}}, \quad i, j=1,2
$$

A piecewise constant approximation of the latter is obtained by setting, for each $K^{*} \in \mathscr{T}_{h^{*}}$

$$
\left.\mathbf{D}_{h}^{2}\right|_{K^{*}}=\left.\frac{1}{2}\left(\nabla \mathbf{g}_{h^{*}}+\left(\nabla \mathbf{g}_{h^{*}}\right)^{T}\right)\right|_{K^{*}}
$$

Notice the use of the symmetric form of the gradient, which is necessary for Hessian symmetry.

Should one be interested in a piecewise linear reconstruction of the Hessian, the same projection technique defined by (4.84) and (4.85) could be directly applied to the reconstructed $\mathbf{g}_{h^{*}}$, by then symmetrizing the matrix obtained in this way via (4.86). In any case, we are now able to compute an approximation of $|u|_{\mathrm{H}^{2}\left(K^{*}\right)}$ on a generic triangle $K^{*}$ of $\mathscr{T}_{h^{*}}$, an approximation that will obviously be linked to the reconstructed $\mathbf{D}_{h}^{2}$.

From (4.83) we deduce that, to obtain the approximate solution $u_{h}$ with an error smaller than or equal to a predefined tolerance $\varepsilon$, we must construct a new grid $\mathscr{T}_{h}^{n e w}$ such that

$$
\sum_{K \in \mathscr{T}_{h}^{n e w}} h_{K}^{2}|u|_{\mathrm{H}^{2}(K)}^{2} \simeq \sum_{K \in \mathscr{T}_{h}^{n e w}} h_{K}^{2} \sum_{i, j=1}^{2}\left\|\left[\mathbf{D}_{h}^{2}\right]_{i j}\right\|_{\mathrm{L}^{2}(K)}^{2} \leq\left(\frac{\varepsilon}{C}\right)^{2}
$$

Ideally one would wish the error to be equidistributed on each element $K$ of the new grid.

A possible adaptation procedure then consists in generating the new grid by appropriately partitioning all of the $N^{*}$ triangles $K^{*}$ of $\mathscr{T}_{h^{*}}$ for which we have

$$
\eta_{K^{*}}^{2}=h_{K^{*}}^{2} \sum_{i, j=1}^{2}\left\|\left[\mathbf{D}_{h}^{2}\right]_{i j}\right\|_{\mathrm{L}^{2}\left(K^{*}\right)}^{2}>\frac{1}{N^{*}}\left(\frac{\varepsilon}{C}\right)^{2}
$$

This method is said to be a refinement as it only aims at creating a finer grid than the initial one, but it clearly does not allow to fully satisfy the equidistribution condition.

More sophisticated algorithms also allow to derefine the grid in presence of the triangles for which the inequality (4.87) is verified with the sign $\ll$ (i.e. much smaller than) instead of $>.$ However, derefinement procedures are of more difficult implementation than refinement ones. Hence, one often prefers to construct the new grid from scratch (a procedure called remeshing). For this purpose, on the basis of the error estimate, the following spacing function $H$ (constant on each element) is introduced

$$
\left.H\right|_{K^{*}}=\frac{\varepsilon}{C \sqrt{N^{*}}\left(\sum_{i, j=1}^{2}\left\|\left[\mathbf{D}_{h}^{2}\right]_{i j}\right\|_{\mathrm{L}^{2}(K)}^{2}\right)^{1 / 2}\left|u_{h^{*}}\right|_{\mathrm{H}^{2}\left(K^{*}\right)}} \quad \forall K^{*} \in \mathscr{T}_{h^{*}}
$$

and is used to construct the adapted grid by applying one of the grid generation algorithms illustrated in Chap. $6$. The adaptation algorithm often requires the function $H$ to be continuous and linear on each triangle. In this case we can again resort to a local projection, like that in (4.84).

The adaptation can then be repeated for the solution computed on the new grid, until inequality (4.87) is inverted on all of the elements. 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-104.jpg?height=294&width=720&top_left_y=112&top_left_x=96)

Fig. 4.18. The function $u$ (left) and the third adapted grid (right) for Example $4.3$

Remark 4.8. The $C$ constant appearing in inequality (4.83) can be estimated by applying the same inequality to known functions (which makes therefore possible to compute the exact error). An alternative that does not require explicitly knowing $C$ consists in realizing the grid that equally distributes the error for a number $N^{*}$ of a priori fixed elements. In this case the value of $H$ computed by setting $\varepsilon$ and $C$ to one in (4.88) is rescaled, by multiplying it by a constant, so that the new grid has a number $N^{*}$ of elements fixed a priori.

Example 4.3. We consider the function $u\left(x_{1}, x_{2}\right)=10 x_{1}^{3}+x_{2}^{3}+\tan ^{-1}\left(10^{-4} /\left(\sin \left(5 x_{2}\right)-\right.\right.$ $\left.2 x_{1}\right)$ ) on the domain $\Omega=(-1,1)^{2}$, which features a strong gradient across the curve $x_{1}=0.5 \sin \left(x_{2}\right)$, as can be observed from Fig. $4.18$ on the left. Starting from an initial structured grid constituted by 50 triangles and using an adaptive procedure guided by the Hessian of $u$, we obtain, after 3 iterations, the grid in Fig. $4.18$ (right), made of 3843 elements. Most of the triangles are located in the proximity of the functions' jump: indeed, while few medium-large surface triangles are necessary to describe $u$ in a satisfactory way in the regions located far enough from the jump, the abrupt variation of $u$ in presence of discontinuities requires the use of small triangles, i.e. a reduced discretization grid-size. Furthermore, we note the anisotropic nature of the grid in Fig. 4.18, visible by the presence of elements whose shape is very stretched with respect to that of an equilateral triangle (typical of an isotropic grid). Such grid has been obtained by generalizing the estimator (4.87) to the anisotropic case. The idea is essentially to exploit the information provided by the components $\left[\mathbf{D}_{h}^{2}\right]_{i j}$ separately instead of "mixing" them through the $\mathrm{L}^{2}\left(K^{*}\right)$ norm. By using the same adaptive procedure in the isotropic case (i.e. the estimator in (4.87)), we would have obtained, after 3 iterations, an adapted grid made of 10535 elements.

\subsubsection{A posteriori adaptivity}

The procedures described in the previous section can be unsatisfactory because the recostruction of $u$ 's derivatives starting from $u_{h^{*}}$ is often subject to errors that are not easy to quantify. A radical alternative consists in adopting a posteriori estimates of the error. The latter do not make use of the a priori estimate (4.71) (and consequently of any approximate derivatives of the unknown solution $u$ ). Rather, they are obtained as a function of computable quantities, normally based on the so-called residue of the approximate solution.

Let us consider problem (4.1) together with its Galerkin approximation (4.2). We define the residue $R \in V^{\prime}$ by

$$
\langle R, v\rangle=F(v)-a\left(u_{h}, v\right) \quad \forall v \in V,
$$

that is

$$
\langle R, v\rangle=a\left(u-u_{h}, v\right) \quad \forall v \in V
$$

Then

$$
\alpha\left\|u-u_{h}\right\|_{V} \leq\|R\|_{V^{\prime}} \leq M\left\|u-u_{h}\right\|_{V}
$$

Indeed, using (4.90) and the continuity of $a(\cdot, \cdot)$,

$$
\|R\|_{V^{\prime}}=\sup _{v \in V} \frac{\langle R, v\rangle}{\|v\|_{V}} \leq M\left\|u-u_{h}\right\|_{V}
$$

On the other hand, taking $v=u-u_{h}$ in (4.90) and using the coercitivity of $a(\cdot, \cdot)$,

$$
\begin{aligned}
\alpha\left\|u-u_{h}\right\|_{V}^{2} & \leq a\left(u-u_{h}, u-u_{h}\right)=\left\langle R, u-u_{h}\right\rangle \\
& \leq\|R\|_{V^{\prime}}\left\|u-u_{h}\right\|_{V}
\end{aligned}
$$

whence the first inequality of $(4.91)$.

Now our goal is to express $R$ in terms of computable quantities on every element $K$ of the finite element triangulation. For the sake of exposition let us consider, as an example, the Poisson problem (3.13). Its weak formulation is given by (3.18), while its approximation using finite elements is described by $(4.40)$, where $V_{h}$ is the space $X_{h}^{r}$ defined in (4.39). In this specific case, $V=H_{0}^{1}(\Omega), V^{\prime}=H^{-1}(\Omega), \alpha=M=1$. Using the Galerkin orthogonality, together with (4.90) and (4.89), for every $v \in H_{0}^{1}(\Omega)$ and every $v_{h} \in V_{h}$, we have

$$
\begin{aligned}
\langle R, v\rangle &=\int_{\Omega} \nabla\left(u-u_{h}\right) \cdot \nabla v d \Omega=\int_{\Omega} \nabla\left(u-u_{h}\right) \cdot \nabla\left(v-v_{h}\right) d \Omega \\
&=\int_{\Omega} f\left(v-v_{h}\right) d \Omega-\int_{\Omega} \nabla u_{h} \cdot \nabla\left(v-v_{h}\right) d \Omega \\
&=\int_{\Omega} f\left(v-v_{h}\right) d \Omega+\sum_{K \in \mathscr{T}} \int_{K} \Delta u_{h}\left(v-v_{h}\right) d \Omega-\sum_{K \in \mathscr{T}_{h}} \int_{\partial K} \frac{\partial u_{h}}{\partial n}\left(v-v_{h}\right) d \gamma \\
&=\sum_{K \in \mathscr{T}_{h}} \int_{K}\left(f+\Delta u_{h}\right)\left(v-v_{h}\right) d \Omega-\sum_{K \in \mathscr{T}_{h}} \int_{\partial K} \frac{\partial u_{h}}{\partial n}\left(v-v_{h}\right) d \gamma
\end{aligned}
$$

We observe that all the local integrals make sense.

Having denoted by $e$ a side of the generic triangle $K$, we define the jump of the normal derivative of $u_{h}$ through the internal side $e$ the quantity

$$
\left[\frac{\partial u_{h}}{\partial n}\right]_{e}=\left.\nabla u_{h}\right|_{K_{1}} \cdot \mathbf{n}_{1}+\left.\nabla u_{h}\right|_{K_{2}} \cdot \mathbf{n}_{2}=\left(\left.\nabla u_{h}\right|_{K_{1}}-\left.\nabla u_{h}\right|_{K_{2}}\right) \cdot \mathbf{n}_{1}
$$

where $K_{1}$ and $K_{2}$ are the two triangles sharing the side $e$, whose normal outgoing unit vectors are given by $\mathbf{n}_{1}$ and $\mathbf{n}_{2}$ respectively, with $\mathbf{n}_{1}=-\mathbf{n}_{2}$ (see Fig. 4.19). In order to extend such definition also to the boundary sides, we introduce the so-called generalized jump, given by

$$
\left[\frac{\partial u_{h}}{\partial n}\right]= \begin{cases}{\left[\frac{\partial u_{h}}{\partial n}\right]_{e}} & \text { for } e \in \mathscr{E}_{h} \\ 0 & \text { for } e \in \partial \Omega\end{cases}
$$

where $\mathscr{E}_{h}$ indicates the set of inner sides in the grid. We note that, in the case of linear finite elements, (4.94) identifies a piecewise constant function defined on all the sides of the grid $\mathscr{T}_{h}$. Moreover, the definition (4.94) can be suitably modified in the case where problem (3.13) is completed with boundary conditions that are not necessarily of Dirichlet type.

Thanks to $(4.94)$ we can therefore write that

$$
\begin{aligned}
&-\sum_{K \in \mathscr{T}_{h}} \int_{\partial K} \frac{\partial u_{h}}{\partial n}\left(v-v_{h}\right) d \gamma=-\sum_{K \in \mathscr{T}} \sum_{e \in \partial K} \int_{e} \frac{\partial u_{h}}{\partial n}\left(v-v_{h}\right) d \gamma \\
&=-\sum_{K \in \mathscr{T}_{h}} \sum_{e \in \partial K} \frac{1}{2} \int_{e}\left[\frac{\partial u_{h}}{\partial n}\right]\left(v-v_{h}\right) d \gamma=-\frac{1}{2} \sum_{K \in \mathscr{T}_{h}} \int_{\partial K}\left[\frac{\partial u_{h}}{\partial n}\right]\left(v-v_{h}\right) d \gamma
\end{aligned}
$$

where the factor $1 / 2$ takes into account the fact that each internal side $e$ of the grid is shared by two elements. Moreover, since $v-v_{h}=0$ on the boundary, in (4.94) we could assign any value different from zero in presence of $e \in \partial \Omega$, as the terms of (4.95) associated to the boundary sides would be null in any case.

By now inserting $(4.95)$ in (4.92) and applying the Cauchy-Schwarz inequality, we

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-106.jpg?height=163&width=223&top_left_y=1014&top_left_x=345)

Fig. 4.19. Triangles involved in the definition of the jump of the normal derivative of $u_{h}$ through an internal side $e$ obtain

$$
\begin{aligned}
\langle R, v\rangle \leq \sum_{K \in \mathscr{T}_{h}}\left\{\left\|f+\Delta u_{h}\right\|_{\mathrm{L}^{2}(K)}\left\|v-v_{h}\right\|_{\mathrm{L}^{2}(K)}\right.\\
&\left.+\frac{1}{2}\left\|\left[\frac{\partial u_{h}}{\partial n}\right]\right\|_{\mathrm{L}^{2}(\partial K)}\left\|v-v_{h}\right\|_{\mathrm{L}^{2}(\partial K)}\right\}
\end{aligned}
$$

Now we look for $v_{h} \in V_{h}$ that allows to express the norms of $v-v_{h}$ as a function of a well-chosen norm of $v$. Moreover, we want this norm to be "local", i.e. computed over a region $\widetilde{K}$ containing $K$, but as little as possible. If $v$ were continuous, we could take as $v_{h}$ the Lagrangian interpolant of $v$ and use the previously cited interpolation error estimates on $K$. Unfortunately, in our case $v \in \mathrm{H}^{1}(\Omega)$ is not necessarily continuous. However, if $\mathscr{T}_{h}$ is a regular grid, we can introduce the so-called Clément interpolation operator $\mathscr{R}_{h}: \mathrm{H}^{1}(\Omega) \rightarrow V_{h}$ defined, in the case of linear finite elements, as

$$
\mathscr{R}_{h} v(\mathbf{x})=\sum_{\mathbf{N}_{j}}\left(P_{j} v\right)\left(\mathbf{N}_{j}\right) \varphi_{j}(\mathbf{x}) \quad \forall v \in \mathrm{H}^{1}(\Omega)
$$

where $P_{j} v$ denotes a local $L^{2}$ projection of $v$. More precisely it is a linear function defined on the patch $K_{\mathbf{N}_{j}}$ of the grid elements that share the node $\mathbf{N}_{j}$ (see Fig. 4.20), which is determined by the relations

$$
\int_{K_{\mathrm{N}}_{j}}\left(P_{j} v-v\right) \psi d \mathbf{x}=0 \quad \text { for } \psi=1, x_{1}, x_{2}
$$

As usual, the $\varphi_{j}$ are the characteristic Lagrangian basis functions of the finite element space under exam.

For each $v \in \mathrm{H}^{1}(\Omega)$ and each $K \in \mathscr{T}_{h}$, the following inequalities hold (see, e.g., [BG98, BS94, Clé75]):

$$
\begin{aligned}
&\left\|v-\mathscr{R}_{h} v\right\|_{\mathrm{L}^{2}(K)} \leq C_{1} h_{K}|v|_{\mathrm{H}^{1}(\widetilde{K})}, \\
&\left\|v-\mathscr{R}_{h} v\right\|_{\mathrm{L}^{2}(\partial K)} \leq C_{2} h_{K}^{\frac{1}{2}}\|v\|_{\mathrm{H}^{1}(\widetilde{K})},
\end{aligned}
$$

where $C_{1}$ and $C_{2}$ are two positive constants that depend on the minimal angle of the elements of the triangulation, while $\widetilde{K}=\left\{K_{j} \in \mathscr{T}_{h}: K_{j} \cap K \neq \emptyset\right\}$ represents the union of $K$ with all the triangles that share an edge or a vertex with it (see Fig. 4.20).
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-107.jpg?height=142&width=510&top_left_y=1014&top_left_x=201)

Fig. 4.20. The set $\widetilde{K}$ of elements that have in common with $K$ at least a node of the grid (left), and the set $K_{\mathbf{N}_{j}}$ of the elements that share node $\mathbf{N}_{j}$ (middle and right) Alternatively to $\mathscr{R}_{h} v$ we could use the local Scott-Zhang interpolation operator, see [BS94, Sect. 4.8]. The rest of the proof would proceed similarly.

By choosing in (4.96) $v_{h}=\mathscr{R}_{h} v$, setting $C=\max \left(C_{1}, C_{2}\right)$ and using the discrete Cauchy-Schwarz inequality, we obtain

$$
\begin{aligned}
\langle R, v\rangle & \leq C \sum_{K \in \mathscr{T}_{h}} \rho_{K}\left(u_{h}\right)\|v\|_{\mathrm{H}^{1}(\widetilde{K})} \\
& \leq C\left(\sum_{K \in \mathscr{T}_{h}}\left[\rho_{K}\left(u_{h}\right)\right]^{2}\right)^{\frac{1}{2}}\left(\sum_{K \in \mathscr{T}_{h}}\|v\|_{\mathrm{H}^{1}(\widetilde{K})}^{2}\right)^{\frac{1}{2}}
\end{aligned}
$$

We have denoted by

$$
\rho_{K}\left(u_{h}\right)=h_{K}\left\|f+\Delta u_{h}\right\|_{\mathrm{L}^{2}(K)}+\frac{1}{2} h_{K}^{\frac{1}{2}}\left\|\left[\frac{\partial u_{h}}{\partial n}\right]\right\|_{\mathrm{L}^{2}(\partial K)}
$$

the so-called local residue, constituted by the internal residue $\left\|f+\Delta u_{h}\right\|_{\mathrm{L}^{2}(K)}$ and by the boundary residue $\left\|\left[\frac{\partial u_{h}}{\partial n}\right]\right\|_{\mathrm{L}^{2}(\partial K)}$.

We now observe that, since $\mathscr{T}_{h}$ is regular, the number of elements in $\widetilde{K}$ is necessarily bounded by a positive integer independent of $h$, which we denote by $n$. Thus,

$$
\|v\|_{H^{1}(\Omega)} \leq\left(\sum_{K \in \mathscr{T}_{h}}\|v\|_{\mathrm{H}^{1}(\widetilde{K})}^{2}\right)^{\frac{1}{2}} \leq \sqrt{n}\|v\|_{\mathrm{H}^{1}(\Omega)}
$$

Because of the Poincaré inequality (2.13),

$$
\|v\|_{H^{1}(\Omega)} \leq C\|v\|_{H_{0}^{1}(\Omega)}, \quad C=\sqrt{1+C_{\Omega}^{2}}
$$

(see the proof of Property $2.5$ ), whence

$$
\|R\|_{H^{-1}(\Omega)}=\sup _{v \in H_{0}^{1}(\Omega)} \frac{\langle R, v\rangle}{\|v\|_{H_{0}^{1}(\Omega)}} \leq C \sqrt{n}\left(\sum_{k \in \mathscr{T}_{h}}\left[\rho_{k}\left(u_{h}\right)\right]^{2}\right)^{\frac{1}{2}}
$$

Thanks to the first inequality of $(4.91)$ (and the fact that $\alpha=1$ in the current case), we conclude with the following residual-based a posteriori error estimate

$$
\left\|u-u_{h}\right\|_{\mathrm{H}^{1}(\Omega)} \leq C \sqrt{n}\left(\sum_{K \in \mathscr{T}_{h}}\left[\rho_{K}\left(u_{h}\right)\right]^{2}\right)^{\frac{1}{2}}
$$

Table 4.2. Cardinality, relative error and normalized estimator associated with the initial grid and with the first six adaptive grids

\begin{tabular}{cccc}
\hline iteration & $\mathscr{N}_{h}$ & $\left\|u-u_{h}\right\|_{\mathrm{H}^{1}(\Omega)} /\left\|u_{h}\right\|_{\mathrm{H}^{1}(\Omega)}$ & $\eta /\left\|u_{h}\right\|_{\mathrm{H}^{1}(\Omega)}$ \\
\hline 0 & 324 & $0.7395$ & $5.8333$ \\
1 & 645 & $0.3229$ & $3.2467$ \\
2 & 1540 & $0.1538$ & $1.8093$ \\
3 & 3228 & $0.0771$ & $0.9782$ \\
4 & 7711 & $0.0400$ & $0.5188$ \\
5 & 17753 & $0.0232$ & $0.2888$ \\
6 & 35850 & $0.0163$ & $0.1955$ \\
\hline
\end{tabular}

Notice that $\rho_{K}\left(u_{h}\right)$ is an effectively computable quantity, being a function of the datum $f$, of the geometric parameter $h_{K}$ and of the computed solution $u_{h}$. The most delicate point of this analysis is the not-always-immediate estimate of the constants $C$ and $n$.

The a posteriori estimate (4.99) can, for instance, be used in order to guarantee that

$$
\frac{1}{2} \varepsilon \leq \frac{\left\|u-u_{h}\right\|_{\mathrm{H}^{1}(\Omega)}}{\left\|u_{h}\right\|_{\mathrm{H}^{1}(\Omega)}} \leq \frac{3}{2} \varepsilon
$$

$\varepsilon>0$ being a pre-established tolerance. To this end, via an iterative procedure illustrated in Fig. 4.21, we can locally make finer and coarser the grid $\mathscr{T}_{h}$ until when, for each $K$, the following local inequalities are satisfied

$$
\frac{1}{4} \frac{\varepsilon^{2}}{N}\left\|u_{h}\right\|_{\mathrm{H}^{1}(\Omega)}^{2} \leq\left[\rho_{K}\left(u_{h}\right)\right]^{2} \leq \frac{9}{4} \frac{\varepsilon^{2}}{N}\left\|u_{h}\right\|_{\mathrm{H}^{1}(\Omega)}^{2}
$$

having denoted by $N$ the number of elements of the grid $\mathscr{T}_{h}$. This ensures that the global inequalities (4.100) are satisfied, up to the contribution of the constant $C \sqrt{n}$. Alternatively, we can construct a well-chosen grid spacing function $H$, analogously to what was done in Sect. 4.6.1.

Naturally, the flow diagram reported in Fig. $4.21$ can also be used for boundaryvalue problems differing from (4.40).

\subsubsection{Numerical examples of adaptivity}

We illustrate the concept of grid adaptivity on two simple differential problems. For this purpose, we adopt the iterative procedure reported in Fig. 4.21, although we will limit ourselves to the sole refinement phase. The coarsening process turns out to be of more difficult implementation: as a matter of fact, the most commonly used software only allows to refine the initial grid, hence it will be necessary to choose the latter to be suitably coarse.

Finally, for both reported examples, the reference estimator for the discretization error is represented by the right term of $(4.99)$. 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-110.jpg?height=726&width=577&top_left_y=113&top_left_x=165)

Fig. 4.21. Example of iterative grid adaptation procedure

\section{First example}

Let us consider the problem $-\Delta u=f$ in $\Omega=(-1,1)^{2}$, with homogeneous Dirichlet conditions on the whole boundary $\partial \Omega$. Moreover, we choose a forcing term $f$ such that the exact solution is $u\left(x_{1}, x_{2}\right)=\sin \left(\pi x_{1}\right) \sin \left(\pi x_{2}\right) \exp \left(10 x_{1}\right)$. We begin the adaptive procedure by starting from a uniform initial grid, made of 324 elements, and with a tolerance $\varepsilon=0.2$. The iterative procedure converges after 7 iterations. We report in Fig. $4.22$ the initial grid together with three of the adapted grids obtained in this way, while Table $4.2$ summarizes the number $\mathscr{A}_{h}$ of elements of the grid $\mathscr{T}_{h}$, the relative error $\left\|u-u_{h}\right\|_{\mathrm{H}^{1}(\Omega)} /\left\|u_{h}\right\|_{\mathrm{H}^{1}(\Omega)}$ and the normalized estimator $\eta /\left\|u_{h}\right\|_{\mathrm{H}^{1}(\Omega)}$ on the initial grid and on the first six adapted grids.

The grids in Fig. $4.22$ provide a qualitative feedback on the reliability of the chosen adaptivity procedure: as expected, triangles tend to concentrate in those regions 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-111.jpg?height=674&width=694&top_left_y=118&top_left_x=108)

Fig. 4.22. Initial grid (top left) and three grids adapted by choosing the adaptive procedure of Fig. 4.21, at the second (top right), third (bottom left) and fifth (bottom right) iteration

where $u$ attains its extrema. On the other hand, the values in Table $4.2$ also allow to perform a quantitative analysis: both the relative error and the normalized estimator progressively decrease, when the iterations increase. However, we can notice an average overestimate of about $10-11$ times with respect to the fixed tolerance $\varepsilon$. This is not unusual and can basically be explained by the fact that the constant $C \sqrt{n}$ in the inequalities (4.100) and (4.101) has been neglected (i.e. set to 1 ). It is clear that such choice actually leads to requiring a tolerance $\widetilde{\varepsilon}=\varepsilon /(C \sqrt{n})$, that will therefore coincide with the original $\varepsilon$ only in the case where we have $C \sqrt{n} \sim 1$. More precise procedures, taking the constant $C \sqrt{n}$ into account, are in any case possible by starting, e.g., from the (theoretical and numerical) analysis provided in [BDR92, EJ88].

\section{Second example}

Let us consider the problem $-\Delta u=0$ in $\Omega=\left\{\mathbf{x}=r(\cos \theta, \sin \theta)^{T}, r \in(0,1), \quad \theta \in\right.$ $\left.\left(0, \frac{3}{4} \pi\right)\right\}$, with $u$ assigned on the boundary of $\Omega$ so that $u(r, \theta)=r^{4 / 3} \sin \left(\frac{4}{3} \theta\right)$ is the 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-112.jpg?height=278&width=710&top_left_y=115&top_left_x=100)

Fig. 4.23. Initial grid (left) and twentieth adapted grid (right)

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-112.jpg?height=277&width=350&top_left_y=495&top_left_x=100)
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-112.jpg?height=278&width=708&top_left_y=494&top_left_x=100tieth adapted grid (right)

exact solution. This function features low regularity in a neighborhood of the origin. Suppose we approximate such problem via the Galerkin method using linear finite elements on the quasi-uniform grid drawn on in the left of Fig. 4.23, and made of 138 triangles. The distortion in the isolines of $u_{h}$ in the left of Fig. $4.24$ shows that the solution obtained in this way is quite inaccurate near the origin. We now use the estimator (4.99) to generate an adapted grid which better suits the approximation of $u$. By following an adaptive procedure such as the one illustrated in Fig. $4.21$ we obtain after 20 steps the grid made of 859 triangles of in Fig. $4.23$ on the right. As in Fig. $4.24$ on the right, the isolines associated to the corresponding discrete solution denote a higher regularity, an evidence of the improved quality of the solution. As a comparison, in order to obtain a solution characterized by the same accuracy $\varepsilon$ with respect to the norm $H^{1}$ of the error (required to be equal to $\left.0.01\right)$ on a uniform grid, 2208 triangles are necessary. 

\subsubsection{A posteriori error estimates in the $\mathbf{L}^{2}$ norm}

Besides (4.99) it is possible to derive an a posteriori estimate of the error in $\mathrm{L}^{2}$ norm. To this end, we will again resort to the duality technique of Aubin-Nitsche used in Sect. 4.5.4, and in particular we will consider the adjoint problem (4.76) associated to the Poisson problem (3.13). Moreover, we will suppose that the domain $\Omega$ is sufficiently regular (for instance, a convex polygon) in order to guarantee that the elliptic regularity result (4.74) stated in Lemma $4.6$ is true.

Moreover, we will exploit the following local estimates for the interpolation error associated with the operator $\Pi_{h}^{r}$ applied to functions $v \in \mathrm{H}^{2}(\Omega)$

$$
\left\|v-\Pi_{h}^{r} v\right\|_{\mathrm{L}^{2}(\partial K)} \leq \widetilde{C}_{1} h_{K}^{\frac{3}{2}}|v|_{\mathrm{H}^{2}(K)}
$$

(see [BS94] or [Cia78]), and

$$
\left\|v-\Pi_{h}^{r} v\right\|_{\mathrm{L}^{2}(K)} \leq \widetilde{C}_{2} h_{K}^{2}|v|_{\mathrm{H}^{2}(K)}
$$

The latter inequality is obtained from (4.67).

Starting from the adjoint problem (4.76) and exploiting the Galerkin orthogonality (4.8), we have, for each $\phi_{h} \in V_{h}$,

$$
\left\|e_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}=\sum_{K \in \mathscr{T}_{h}} \int_{K} f\left(\phi-\phi_{h}\right) d \Omega-\sum_{K \in \mathscr{T}_{h}} \int_{K} \nabla u_{h} \cdot \nabla\left(\phi-\phi_{h}\right) d \Omega
$$

Counterintegrating by parts, we obtain

$$
\left\|e_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}=\sum_{K \in \mathscr{T}_{h}} \int_{K}\left(f+\Delta u_{h}\right)\left(\phi-\phi_{h}\right) d \Omega-\sum_{K \in \mathscr{T}_{h}} \int_{\partial K} \frac{\partial u_{h}}{\partial n}\left(\phi-\phi_{h}\right) d \gamma
$$

Using the definition (4.94) of generalized jump of the normal derivative of $u_{h}$ across the triangle edges and setting $\phi_{h}=\Pi_{h}^{r} \phi$, we have

$$
\begin{aligned}
\left\|e_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}=& \sum_{K \in \mathscr{T}_{h}}\left[\int_{K}\left(f+\Delta u_{h}\right)\left(\phi-\Pi_{h}^{r} \phi\right) d \Omega\right.\\
&\left.-\frac{1}{2} \int_{\partial K}\left[\frac{\partial u_{h}}{\partial n}\right]\left(\phi-\Pi_{h}^{r} \phi\right) d \gamma\right]
\end{aligned}
$$

We estimate the two terms in the right-hand side separately. By using the CauchySchwarz inequality and (4.103), it follows that

$$
\begin{aligned}
\left|\int_{K}\left(f+\Delta u_{h}\right)\left(\phi-\Pi_{h}^{r} \phi\right) d \Omega\right| & \leq\left\|f+\Delta u_{h}\right\|_{\mathrm{L}^{2}(K)}\left\|\phi-\Pi_{h}^{r} \phi\right\|_{\mathrm{L}^{2}(K)} \\
& \leq \widetilde{C}_{2} h_{K}^{2}\left\|f+\Delta u_{h}\right\|_{\mathrm{L}^{2}(K)}|\phi|_{\mathrm{H}^{2}(K)}
\end{aligned}
$$

Moreover, thanks to $(4.102)$ we obtain

$$
\begin{aligned}
\left|\int_{\partial K}\left[\frac{\partial u_{h}}{\partial n}\right]\left(\phi-\Pi_{h}^{r} \phi\right) d \gamma\right| & \leq\left\|\left[\frac{\partial u_{h}}{\partial n}\right]\right\|_{L^{2}(\partial K)}\left\|\phi-\Pi_{h}^{r} \phi\right\|_{L^{2}(\partial K)} \\
& \leq \widetilde{C}_{1} h_{K}^{\frac{3}{2}}\left\|\left[\frac{\partial u_{h}}{\partial n}\right]\right\|_{L^{2}(\partial K)}|\phi|_{\mathrm{H}^{2}(K)}
\end{aligned}
$$

By now inserting (4.105) and (4.106) in (4.104) and applying the discrete CauchySchwarz inequality we have

$$
\begin{aligned}
\left\|e_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2} \leq C \sum_{K \in \mathscr{T}_{h}} h_{K} \rho_{K}\left(u_{h}\right)|\phi|_{\mathrm{H}^{2}(K)} & \leq C \sqrt{\sum_{K \in \mathscr{T}_{h}}\left[h_{K} \rho_{K}\left(u_{h}\right)\right]^{2}}|\phi|_{\mathrm{H}^{2}(\Omega)} \\
& \leq C \sqrt{\sum_{K \in \mathscr{T}_{h}}\left[h_{K} \rho_{K}\left(u_{h}\right)\right]^{2}}\left\|e_{h}\right\|_{\mathrm{L}^{2}(\Omega)}
\end{aligned}
$$

with $C=\max \left(\widetilde{C}_{1}, \widetilde{C}_{2}\right)$, having introduced the notation (4.98) and having exploited the elliptic regularity property $(4.80)$ in the last inequality. We can then conclude that

$$
\left\|u-u_{h}\right\|_{\mathrm{L}^{2}(\Omega)} \leq C\left(\sum_{K \in \mathscr{T}_{h}} h_{K}^{2}\left[\rho_{K}\left(u_{h}\right)\right]^{2}\right)^{\frac{1}{2}}
$$

$C>0$ being a constant independent of $h$.

Remark 4.9. Among the most widespread a posteriori estimates in engineering, we cite for its simplicity and computational effectiveness the estimator proposed by Zienkiewicz and Zhu in the context of a finite element approximation of linear elasticity problems [ZZ87]. The basic idea of this estimator is very simple. Suppose we want to control the energy $\operatorname{norm}\left(\int_{\Omega}\left|\nabla u-\nabla u_{h}\right|^{2} d \Omega\right)^{1 / 2}$ of the discretization error associated to a finite element approximation of the model problem (3.13). This estimator replaces the exact gradient $\nabla u$ in the latter norm with a corresponding reconstruction obtained through a suitable post-processing of the discrete solution $u_{h}$. Over the years, several "recipes" have been proposed in the literature for the construction of the gradient $\nabla u$ (see, e.g., [ZZ92, Rod94, PWY90, LW94, NZ04, BMMP06]). The same procedure illustrated in Sect. 4.6.1 that leads to the reconstructed $\mathbf{g}_{h^{*}}$ defined in (4.85) can be used here for this purpose. Thus, having chosen a reconstruction, say $G_{R}\left(u_{h}\right)$, of $\nabla u$, the Zienkiewicz and Zhu-type estimator is represented by the quantity $\eta=\left(\int_{\Omega}\left|G_{R}\left(u_{h}\right)-\nabla u_{h}\right|^{2} d \Omega\right)^{1 / 2}$. Clearly, to each new definition of $G_{R}\left(u_{h}\right)$ corresponds a new error estimator. For this reason, a posteriori error estimators with such structure are commonly called recovery-based.

\subsubsection{A posteriori estimates of a functional of the error}

In the previous section, the adjoint problem (4.76) was used in a purely formal way, because the error $e_{h}$, that represents its forcing term, is unknown. There exists another family of a posteriori estimators of the error, again based on the adjoint problem, which, instead, explicitly use the information provided by the latter (see, e.g., [Ran99]). In such case, an estimate is provided for a suitable functional $J$ of the error $e_{h}$, instead of for a suitable norm of $e_{h}$. This prerogative turns out to be particularly useful whene one wants to provide significant estimates of the error for quantities of physical relevance, such as, for instance, resistance or drag in the case of bodies immersed in fluids, average values of concentration, strains, deformations, fluxes, etc. For this purpose, it will be sufficient to operate a suitable choice for the functional $J$. This type of adaptivity is called goal-oriented. To illustrate this new paradigm, let us still refer to the Poisson problem (3.13) and assume that we want to control the error of a given functional $J: H_{0}^{1}(\Omega) \rightarrow \mathbb{R}$ of the solution $u$. Let us consider the following weak formulation of the corresponding adjoint problem

$$
\text { find } \phi \in V: \quad \int_{\Omega} \nabla \phi \cdot \nabla w d \Omega=J(w) \quad \forall w \in V
$$

with $V=\mathrm{H}_{0}^{1}(\Omega)$. By using the Galerkin orthogonality and proceeding as done in the previous section, we find

$$
\begin{aligned}
J\left(e_{h}\right)=& \int_{\Omega} \nabla e_{h} \cdot \nabla \phi d \Omega=\sum_{K \in \mathscr{T}_{h}}\left[\int_{K}\left(f+\Delta u_{h}\right)\left(\phi-\phi_{h}\right) d \Omega\right.\\
&\left.-\frac{1}{2} \int_{\partial K}\left[\frac{\partial u_{h}}{\partial n}\right]\left(\phi-\phi_{h}\right) d \gamma\right],
\end{aligned}
$$

where $\phi_{h} \in V_{h}$ is typically a convenient interpolant of $\phi$. By using the Cauchy-Schwarz inequality on each element $K$, we obtain

$$
\begin{aligned}
\left|J\left(e_{h}\right)\right| &=\left|\int_{\Omega} \nabla e_{h} \cdot \nabla \phi d \Omega\right| \\
& \leq \sum_{K \in \mathscr{T}_{h}}\left(\left\|f+\Delta u_{h}\right\|_{\mathrm{L}^{2}(K)}\left\|\phi-\phi_{h}\right\|_{\mathrm{L}^{2}(K)}+\frac{1}{2}\left\|\left[\frac{\partial u_{h}}{\partial n}\right]\right\|_{\mathrm{L}^{2}(\partial K)}\left\|\phi-\phi_{h}\right\|_{\mathrm{L}^{2}(\partial K)}\right) \\
& \leq \sum_{K \in \mathscr{T}_{h}}\left[\rho_{K}\left(u_{h}\right) \max \left(\frac{1}{h_{K}}\left\|\phi-\phi_{h}\right\|_{\mathrm{L}^{2}(K)}, \frac{1}{h_{K}^{1 / 2}}\left\|\phi-\phi_{h}\right\|_{\mathrm{L}^{2}(\partial K)}\right)\right]
\end{aligned}
$$

$\rho_{K}\left(u_{h}\right)$ being defined according to (4.98). We now introduce the so-called local weights

$$
\omega_{K}(\phi)=\max \left(\frac{1}{h_{K}}\left\|\phi-\phi_{h}\right\|_{L^{2}(K)}, \frac{1}{h_{K}^{1 / 2}}\left\|\phi-\phi_{h}\right\|_{L^{2}(\partial K)}\right)
$$

Thus,

$$
\left|J\left(e_{h}\right)\right| \leq \sum_{K \in \mathscr{T}_{h}} \rho_{K}\left(u_{h}\right) \omega_{K}(\phi)
$$

We can observe that, in contrast to the residue-type estimates introduced in Sects. $4.6 .2$ and $4.6 .4$, the estimate (4.111) depends not only on the discrete solution $u_{h}$ but also on the solution $\phi$ of the dual problem. In particular, having considered the local estimator $\rho_{K}\left(u_{h}\right) \omega_{K}(\phi)$, we can say that, while the residue $\rho_{K}\left(u_{h}\right)$ measures how the discrete solution approximates the differential problem under exam, the weight $\omega_{K}(\phi)$ takes into account how this information is propagated in the domain as an effect of the chosen functional. Hence, the grids obtained for different choices of the functional $J$, i.e. of the forcing term of the adjoint problem (4.108), will be different even if we start from the same differential problem (for more details, we refer to Example 13.12).

Moreover, to make the estimate (4.111) efficient, we proceed by replacing the norms $\left\|\phi-\phi_{h}\right\|_{\mathrm{L}^{2}(K)}$ and $\left\|\phi-\phi_{h}\right\|_{\mathrm{L}^{2}(\partial K)}$ in (4.110) with suitable estimates of the interpolation error, having chosen $\phi_{h}$ as a suitable interpolant of the dual solution $\phi$.

We point out two particular cases. Choosing $J(w)=\int_{\Omega} w e_{h} d \Omega$ in (4.108) we would find again the estimate (4.107) for the $\mathrm{L}^{2}$-norm of the discretization error, provided of course that we can guarantee that the elliptic regularity result (4.74), stated in Lemma 4.6, is true. Instead, if we are interested in controlling $e_{h}$ at a point $\mathbf{x}$ of $\Omega$, it will be indeed sufficient to define $J$ as $J(w)=w^{\prime}\left\langle\delta_{\mathbf{x}}, w\right\rangle_{W}$, with $W=\mathrm{H}_{0}^{1}(\Omega) \cap C^{0}(\bar{\Omega})$ and $\delta_{\mathbf{x}}$ being Dirac's delta function at $\mathbf{x}$ (see Chapter 2 ).

Remark 4.10. The a posteriori analysis of this section, as well as that of the previous Sects. 4.6.2 and 4.6.4, can be extended to the case of more complex differential problems, like for instance transport and diffusion problems, and more general boundary conditions (see Example 13.12). The procedure remains basically the same. What changes is the definition of the local residue (4.98) and of the generalized jump (4.94). Indeed, while $\rho_{K}\left(u_{h}\right)$ directly depends on the differential formulation of the problem under exam, $\left[\partial u_{h} / \partial n\right]$ will need to take into account the conditions assigned on the boundary.

For a thorough description of the adaptivity techniques provided up to now and for a presentation of other possible adaptive techniques, we refer the reader to [Ver96, Ran99, AO00].

\section{$4.7$ Exercises}

\section{Heat transfer in a thin rod.}

Let us consider a thin rod of length $L$, having temperature $t_{0}$ at the endpoint $x=0$ and insulated at the other endpoint $x=L$. Let us suppose that the cross-section of the rod has constant area equal to $A$ and that the perimeter of $A$ is $p$. The temperature $t$ of the rod at a generic point $x \in(0, L)$ then satisfies the following mixed boundary-value problem:

$$
\begin{cases}-k A t^{\prime \prime}+\sigma p t=0, & x \in(0, L) \\ t(0)=t_{0}, & t^{\prime}(L)=0\end{cases}
$$

having denoted by $k$ the thermal conductivity coefficient and by $\sigma$ the convective transfer coefficient.

Verify that the exact solution of this problem is

$$
t(x)=t_{0} \frac{\cosh [m(L-x)]}{\cosh (m L)}
$$

with $m=\sqrt{\sigma p / k A}$. Write the weak formulation of (4.112), then its Galerkin-finite element approximation. Show how the approximation error in the $\mathrm{H}_{0}^{1}(0, L)$-norm depends on the parameters $k, \sigma, p$ and $t_{0}$.

Finally, solve this problem using linear and quadratic finite elements on uniform grids, then evaluate the approximation error.

2. Temperature of a fluid between two parallel plates.

We consider a viscous fluid located between two horizontal parallel plates, at a distance of $2 H$. Suppose that the upper plate, which has temperature $t_{\text {sup }}$, moves at a relative speed of $U$ with respect to the lower one, having temperature $t_{i n f}$. In such case the temperature $t:(0,2 H) \rightarrow \mathbb{R}$ of the fluid satisfies the following Dirichlet problem:

$$
\begin{cases}-\frac{d^{2} t}{d y^{2}}=\alpha(H-y)^{2}, & y \in(0,2 H) \\ t(0)=t_{i n f}, & t(2 H)=t_{s u p}\end{cases}
$$

where $\alpha=\frac{4 U^{2} \mu}{H^{4} k}, k$ being the thermal conductivity coefficient and $\mu$ the viscosity of the fluid. Find the exact solution $t(y)$, then write the weak formulation and the Galerkin finite element formulation.

[Solution: the exact solution is

$$
\left.t(y)=-\frac{\alpha}{12}(H-y)^{4}+\frac{t_{i n f}-t_{s u p}}{2 H}(H-y)+\frac{t_{i n f}+t_{s u p}}{2}+\frac{\alpha H^{4}}{12} .\right]
$$

3. Deformation of a rope.

Let us consider a rope with tension $T$ and unit length, fixed at the endpoints. The function $u(x)$, measuring the vertical displacement of the rope when subject to a transversal charge of intensity $w$, satisfies the following Dirichlet problem:

$$
\begin{cases}-u^{\prime \prime}+\frac{k}{T} u=\frac{w}{T} & \text { in }(0,1) \\ u(0)=0, & u(1)=0\end{cases}
$$

having indicated with $k$ the elasticity coefficient of the rope. Write the weak formulation and the Galerkin-finite element formulation.

4. Prove Property $4.1$.

[Solution: it suffices to observe that $\left.a_{i j}=a\left(\varphi_{j}, \varphi_{i}\right) \forall i, j .\right]$

5. Prove (4.12). 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-118.jpg?height=334&width=688&top_left_y=118&top_left_x=113)

Fig. 4.25. Left: the sparsity pattern of the Galerkin finite element matrix associated to a discretization using 10 elements of the one-dimensional Poisson problem with quadratic finite elements. Unknowns are numbered as explained in Exercise 6. Right: the pattern of the $\mathrm{L}$ and U factors of $A$. Note that, because of the fill-in, the number of non-null finite elements has increased from 81 in the matrix to 141 in the factors

[Solution: since the form is symmetric, the procedure contained in Remark $3.2$ can be repeated, noting that the solution $u_{h}$ satisfies $a\left(u_{h}, v_{h}\right)=a\left(u, v_{h}\right)$ for each $v_{h} \in V_{h}$. We deduce therefore that $u_{h}$ minimizes $J\left(v_{h}\right)=a\left(v_{h}, v_{h}\right)-2 a\left(u, v_{h}\right)$ and therefore also $J^{*}\left(v_{h}\right)=J\left(v_{h}\right)+a(u, u)=a\left(u-v_{h}, u-v_{h}\right)$ (the last equality is made possible thanks to the symmetry of the bi-linear form). On the other hand,

$$
\sqrt{\alpha}\left\|u-v_{h}\right\|_{V} \leq \sqrt{a\left(u-v_{h}, u-v_{h}\right)} \leq \sqrt{M}\left\|u-v_{h}\right\|_{V}
$$

hence the desired result.]

6. Given a partition of an interval $(a, b)$ into $N+1$ sub-intervals, suppose to number first the endpoints of the single sub-intervals and then their midpoints. Is this labelling more or less convenient than the one introduced in Sect. $4.3$ for the discretization of the Poisson problem with finite elements in $X_{h}^{2}$ ? Suppose to solve the linear system by a factorization method.

[Solution: the obtained matrix still has only five diagonals different from zero, as the one obtained using the numbering proposed in Sect. 4.3. However, it features a higher bandwidth. Consequently, in case it is factorized, it is subject to a larger fill-in, as shown in Fig. 4.25.]

7. Consider the following one-dimensional boundary-value problem:

$$
\begin{cases}-\left(\alpha u^{\prime}\right)^{\prime}+\gamma u=f, & 0<x<1 \\ u=0 & \text { at } x=0 \\ \alpha u^{\prime}+\delta u=0 & \text { at } x=1\end{cases}
$$

where $\alpha=\alpha(x), \gamma=\gamma(x), f=f(x)$ are assigned functions with $0 \leq \gamma(x) \leq \gamma_{1}$ and $0<\alpha_{0} \leq \alpha(x) \leq \alpha_{1} \forall x \in[0,1]$, while $\delta \in \mathbb{R}$. Moreover, suppose that $f \in \mathrm{L}^{2}(0,1)$ Write the problem's weak formulation specifying the appropriate functional spaces and hypotheses on the data to guarantee existence and uniqueness of the solution. Suppose to find an approximate solution $u_{h}$ using the linear finite element method. What can be said about the existence, stability and accuracy of $u_{h} ?$ [Solution: we seek $u \in V=\left\{v \in \mathrm{H}^{1}(0,1): v(0)=0\right\}$ such that $a(u, v)=F(v)$ $\forall v \in V$ where

$$
a(u, v)=\int_{0}^{1} \alpha u^{\prime} v^{\prime} d x+\int_{0}^{1} \gamma u v d x+\delta u(1) v(1), \quad F(v)=\int_{0}^{1} f v d x
$$

The existence and uniqueness of the solution of the weak problem are guaranteed if the hypotheses of the Lax-Milgram lemma hold. The form $a(\cdot, \cdot)$ is continuous as we have

$$
|a(u, v)| \leq 2 \max \left(\alpha_{1}, \gamma_{1}\right)\|u\|_{V}\|v\|_{V}+|\delta||v(1)||u(1)|,
$$

from which, considering that $u(1)=\int_{0}^{1} u^{\prime} d x$, we obtain

$$
|a(u, v)| \leq M\|u\|_{V}\|v\|_{V} \quad \text { with } M=3 \max \left(\alpha_{1}, \gamma_{1},|\delta|\right) .
$$

We have coercivity if $\delta \geq 0$, for in such case we find

$$
a(u, u) \geq \alpha_{0}\left\|u^{\prime}\right\|_{\mathrm{L}^{2}(0,1)}^{2}+u^{2}(1) \delta \geq \alpha_{0}\left\|u^{\prime}\right\|_{\mathrm{L}^{2}(0,1)}^{2}
$$

To find the inequality in $\|\cdot\|_{V}$ invoking the Poincaré inequality $(2.13)$, it suffices to prove that

$$
\frac{1}{1+C_{\Omega}^{2}}\|u\|_{V}^{2} \leq\left\|u^{\prime}\right\|_{\mathrm{L}^{2}(0,1)}^{2}
$$

and then to conclude that

$$
a(u, u) \geq \alpha^{*}\|u\|_{V}^{2} \quad \text { with } \alpha^{*}=\frac{\alpha_{0}}{1+C_{\Omega}^{2}}
$$

The fact that $F$ is a linear and continuous functional can be verified immediately. The finite element method is a Galerkin method with $V_{h}=\left\{v_{h} \in X_{h}^{1}: v_{h}(0)=0\right\}$. Consequently, thanks to Corollaries $4.1,4.2$ we deduce that the solution $u_{h}$ exists and is unique. From the estimate (4.72) we furthermore deduce that, since $r=1$, the error measured in the norm of $V$ will tend to zero linearly with respect to $h .]$

8. Consider the following two-dimensional boundary-value problem:

$$
\left\{\begin{array}{lll}
-\operatorname{div}(\alpha \nabla u)+\gamma u=f & \text { in } \Omega \subset \mathbb{R}^{2} \\
u=0 & & \text { on } \Gamma_{D} \\
\alpha \nabla u \cdot \mathbf{n}=0 & & \text { on } \Gamma_{N}
\end{array}\right.
$$

$\Omega$ being a bounded open domain having regular boundary $\partial \Omega=\Gamma_{D} \cup \Gamma_{N}$, with $\stackrel{\circ}{\Gamma_{D}} \cap \stackrel{\circ}{\Gamma_{N}}=\emptyset$ and unit outgoing normal $\mathbf{n} ; \alpha \in \mathrm{L}^{\infty}(\Omega), \gamma \in \mathrm{L}^{\infty}(\Omega)$, and $f \in \mathrm{L}^{2}(\Omega)$ are three assigned functions with $\gamma(\mathbf{x}) \geq 0$ and $0<\alpha_{0} \leq \alpha(\mathbf{x})$ a.e. in $\Omega$.

Analyze the existence and uniqueness of the weak solution and the stability of the solution obtained using the Galerkin method. Suppose that $u \in \mathrm{H}^{4}(\Omega)$. Which polynomial degree would it be convenient to use?

[Solution: the weak problem consists in finding $u \in V=\mathrm{H}_{\Gamma_{D}}^{1}$ such that $a(u, v)=$ $F(v) \forall v \in V$, where

$$
a(u, v)=\int_{\Omega} \alpha \nabla u \nabla v d \Omega+\int_{\Omega} \gamma u v d \Omega, \quad F(v)=\int_{\Omega} f v d \Omega
$$

The bilinear form is continuous; indeed

$$
\begin{aligned}
|a(u, v)| & \leq \int_{\Omega} \alpha|\nabla u||\nabla v| d \Omega+\int_{\Omega}|\gamma||u||v| d \Omega \\
& \leq\|\alpha\|_{L^{\infty}(\Omega)}\|\nabla u\|_{L^{2}(\Omega)}\|\nabla v\|_{L^{2}(\Omega)}+\|\gamma\|_{L^{\infty}(\Omega)}\|u\|_{L^{2}(\Omega)}\|v\|_{L^{2}(\Omega)} \\
& \leq M\|u\|_{V}\|v\|_{V}
\end{aligned}
$$

having taken $M=2 \max \left\{\|\alpha\|_{L^{\infty}(\Omega)},\|\gamma\|_{L^{\infty}(\Omega)}\right\}$. Moreover, it is coercive (see the solution to Exercise 7) with coercivity constant given by $\alpha^{*}=\frac{\alpha_{0}}{1+C_{\Omega}^{2}}$. Since $F$ is a linear and bounded functional, owing to the Lax-Milgram lemma the weak solution exists and is unique. As far as the Galerkin method is concerned, we introduce a subspace $V_{h}$ of $V$ with finite dimension. Then there exists a unique solution $u_{h}$ of the Galerkin problem: find $u_{h} \in V_{h}$ such that $a\left(u_{h}, v_{h}\right)=F\left(v_{h}\right) \forall v_{h} \in V_{h}$. Moreover, by Corollary $4.2$ we have stability. As far as the choice of the optimal polynomial degree $r$ is concerned, it is sufficient to note that the exponent $s$ appearing in $(4.26)$ is the minimum between $r$ and $p=3$. Hence, it will be convenient to use elements of degree $3 .]$

The fundamental steps of a finite element code can be summarized as follows:

(a) input the data;

(b) build the grid $\mathscr{T}_{h}=\{K\}$;

(c) build the local matrices $A_{K}$ and the right-hand side elements $f_{K}$;

(d) assemble the global matrix $A$ and the one of the source term $\mathbf{f} ;$

(e) solve the linear system $A \mathbf{u}=\mathbf{f}$;

(f) post-process the results.

Suppose we use linear finite elements and consider the patch of elements in Fig. 4.26.

a) Referring to steps (c) and (d), explicitly write the matrix $T_{K}$ allowing to pass from the local matrix $A_{K}$ to the global matrix $A$ via a transformation of the kind $T_{K}^{T} A_{K} T_{K}$. What is the dimension of $T_{K}$ ?

b) What sparsity pattern characterizes the matrix $A$ associated to the patch of elements in Fig. 4.26? 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-121.jpg?height=219&width=261&top_left_y=116&top_left_x=326)

Fig. 4.26. Patch of elements for the assembly of the global matrix $A$

c) Write the elements of the matrix A explicitly as a function of the elements of the local matrices $A_{K}$.

d) In the case of a general grid $\mathscr{T}_{h}$ with $\mathrm{N}_{\mathrm{V}}$ vertices and $\mathrm{N}_{\mathrm{T}}$ triangles, what dimension does the global matrix $A$ have in the case of linear and quadratic finite elements, respectively?

For a more exaustive treatment of this subject, we refer to Chapter $12$.

9. Prove the results summarized in Table $3.1$ by using the Lagrange identity (3.42). 

\section{Parabolic equations}

In this chapter we consider parabolic equations of the form

$$
\frac{\partial u}{\partial t}+L u=f, \quad \mathbf{x} \in \Omega, t>0
$$

where $\Omega$ is a domain of $\mathbb{R}^{d}, d=1,2,3, f=f(\mathbf{x}, t)$ is a given function, $L=L(\mathbf{x})$ is a generic elliptic operator acting on the unknown $u=u(\mathbf{x}, t)$. When solved only for a bounded temporal interval, say for $0<t<T$, the region $Q_{T}=\Omega \times(0, T)$ is called cylinder in the space $\mathbb{R}^{d} \times \mathbb{R}^{+}$(see Fig. 5.1). In the case where $T=+\infty, Q=\{(\mathbf{x}, t):$ $\mathbf{x} \in \Omega, t>0\}$ will be an infinite cylinder.

Equation (5.1) must be completed by assigning an initial condition

$$
u(\mathbf{x}, 0)=u_{0}(\mathbf{x}), \quad \mathbf{x} \in \Omega
$$

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-122.jpg?height=274&width=334&top_left_y=842&top_left_x=294)

Fig. 5.1. The cylinder $Q_{T}=\Omega \times(0, T), \Omega \subset \mathbb{R}^{2}$

together with boundary conditions, which can take the following form:

$$
\begin{array}{lrl}
u(\mathbf{x}, t)=\varphi(\mathbf{x}, t), & \mathbf{x} & \in \Gamma_{D} \text { and } t>0 \\
\frac{\partial u(\mathbf{x}, t)}{\partial n}=\psi(\mathbf{x}, t), & & \mathbf{x} \in \Gamma_{N} \text { and } t>0
\end{array}
$$

where $u_{0}, \varphi$ and $\psi$ are given functions and $\left\{\Gamma_{D}, \Gamma_{N}\right\}$ provides a boundary partition, that is $\Gamma_{D} \cup \Gamma_{N}=\partial \Omega, \stackrel{\circ}{\Gamma_{D}} \cap \stackrel{\circ}{\Gamma_{N}}=\emptyset$. For obvious reasons, $\Gamma_{D}$ is called Dirichlet boundary and $\Gamma_{N}$ Neumann boundary.

In the one-dimensional case, the problem:

$$
\begin{aligned}
&\frac{\partial u}{\partial t}-v \frac{\partial^{2} u}{\partial x^{2}}=f, \quad 0<x<d, \quad t>0 \\
&u(x, 0)=u_{0}(x), \quad 0<x<d \\
&u(0, t)=u(d, t)=0, \quad t>0
\end{aligned}
$$

describes the evolution of the temperature $u(x, t)$ at point $x$ and time $t$ of a metal bar of length $d$ occupying the interval $[0, d]$, whose thermal conductivity is $v$ and whose endpoints are kept at a constant temperature of zero degrees. The function $u_{0}$ describes the initial temperature, while $f$ represents the heat generated (per unit length) by the bar. For this reason, (5.4) is called heat equation. For a particular case, see Example $1.5$ of Chapter 1 .

\subsection{Weak formulation and its approximation}

In order to solve problem (5.1)-(5.3) numerically, we will introduce a weak formulation, as we did to handle elliptic problems.

We proceed formally, by multiplying for each $t>0$ the differential equation by a test function $v=v(\mathbf{x})$ and integrating on $\Omega$. We set $V=\mathrm{H}_{\Gamma_{D}}^{1}(\Omega)$ (see $\left.(3.26)\right)$ and for each $t>0$ we seek $u(t) \in V$ such that

$$
\int_{\Omega} \frac{\partial u(t)}{\partial t} v d \Omega+a(u(t), v)=\int_{\Omega} f(t) v d \Omega \quad \forall v \in V
$$

where $u(0)=u_{0}, a(\cdot, \cdot)$ is the bilinear form associated to the elliptic operator $L$, and where we have supposed for simplicity $\varphi=0$ and $\psi=0$. The modification of $(5.5)$ in the case where $\varphi \neq 0$ and $\psi \neq 0$ is left to the reader .

A sufficient condition for the existence and uniqueness of the solution to problem $(5.5)$ is that the following hypotheses hold:

the bilinear form $a(\cdot, \cdot)$ is continuous and weakly coercive, that is

$$
\exists \lambda \geq 0, \exists \alpha>0: \quad a(v, v)+\lambda\|v\|_{\mathrm{L}^{2}(\Omega)}^{2} \geq \alpha\|v\|_{V}^{2} \quad \forall v \in V
$$

yielding for $\lambda=0$ the standard definition of coercivity. Moreover, we require $u_{0} \in \mathrm{L}^{2}(\Omega)$ and $f \in \mathrm{L}^{2}(Q)$. Then, problem (5.5) admits a unique solution $u \in \mathrm{L}^{2}\left(\mathbb{R}^{+} ; V\right) \cap C^{0}\left(\mathbb{R}^{+} ; \mathrm{L}^{2}(\Omega)\right)$, with $V=\mathrm{H}_{\Gamma_{D}}^{1}(\Omega)$

For the definition of these functional spaces, see Sect. 2.7. For the proof, see [QV94, Sect. 11.1.1].

Some a priori estimates of the solution $u$ will be provided in the following section.

We now consider the Galerkin approximation of problem (5.5): for each $t>0$, find $u_{h}(t) \in V_{h}$ such that

$$
\int_{\Omega} \frac{\partial u_{h}(t)}{\partial t} v_{h} d \Omega+a\left(u_{h}(t), v_{h}\right)=\int_{\Omega} f(t) v_{h} d \Omega \quad \forall v_{h} \in V_{h}
$$

with $u_{h}(0)=u_{0 h}$, where $V_{h} \subset V$ is a suitable space of finite dimension and $u_{0 h}$ is a convenient approximation of $u_{0}$ in the space $V_{h}$. Such problem is called semi-discretization of (5.5), as the temporal variable has not yet been discretized.

To provide an algebraic interpretation of $(5.6)$ we introduce a basis $\left\{\varphi_{j}\right\}$ for $V_{h}$ (as we did in the previous chapters), and we observe that it suffices that $(5.6)$ is verified for the basis functions in order to be satisfied by all the functions of the subspace. Moreover, since for each $t>0$ the solution to the Galerkin problem belongs to the subspace as well, we will have

$$
u_{h}(\mathbf{x}, t)=\sum_{j=1}^{N_{h}} u_{j}(t) \varphi_{j}(\mathbf{x})
$$

where the coefficients $\left\{u_{j}(t)\right\}$ represent the unknowns of problem $(5.6)$

Denoting by $\dot{u}_{j}(t)$ the derivatives of the function $u_{j}(t)$ with respect to time, $(5.6)$ becomes

$$
\int_{\Omega} \sum_{j=1}^{N_{h}} \dot{u}_{j}(t) \varphi_{j} \varphi_{i} d \Omega+a\left(\sum_{j=1}^{N_{h}} u_{j}(t) \varphi_{j}, \varphi_{i}\right)=\int_{\Omega} f(t) \phi_{i} d \Omega, \quad i=1,2, \ldots, N_{h}
$$

that is

$$
\sum_{j=1}^{N_{h}} \dot{u}_{j}(t) \underbrace{\int_{\Omega} \varphi_{j} \varphi_{i} d \Omega}_{m_{i j}}+\sum_{j=1}^{N_{h}} u_{j}(t) \underbrace{a\left(\varphi_{j}, \varphi_{i}\right)}_{a_{i j}}=\underbrace{\int_{\Omega} f(t) \phi_{i} d \Omega}_{f_{i}(t)}, \quad i=1,2, \ldots, N_{h}
$$

If we define the vector of unknowns $\mathbf{u}=\left(u_{1}(t), u_{2}(t), \ldots, u_{N_{h}}(t)\right)^{T}$, the mass $m a-$ trix $\mathrm{M}=\left[m_{i j}\right]$, the stiffness matrix $\mathrm{A}=\left[a_{i j}\right]$ and the right-hand side vector $\mathbf{f}=$ $\left(f_{1}(t), f_{2}(t), \ldots, f_{N_{h}}(t)\right)^{T}$, the system (5.7) can be rewritten in matrix form as

$$
\mathbf{M \dot { u }}(t)+\mathbf{A u}(t)=\mathbf{f}(t)
$$

For the numerical solution of this ODE system, many finite difference methods are available. See, e.g., [QSS07, Chap. 11]. Here we limit ourselves to considering the socalled $\theta$-method. The latter discretizes the temporal derivative by a simple difference quotient and replaces the other terms with a linear combination of the value at time $t^{k}$ and of the value at time $t^{k+1}$, depending on the real parameter $\theta(0 \leq \theta \leq 1)$,

$$
\mathrm{M} \frac{\mathbf{u}^{k+1}-\mathbf{u}^{k}}{\Delta t}+\mathrm{A}\left[\theta \mathbf{u}^{k+1}+(1-\theta) \mathbf{u}^{k}\right]=\theta \mathbf{f}^{k+1}+(1-\theta) \mathbf{f}^{k}
$$

As usual, the real positive parameter $\Delta t=t^{k+1}-t^{k}, k=0,1, \ldots$, denotes the discretization step (here assumed to be constant), while the superscript $k$ indicates that the quantity under consideration refers to the time $t^{k}$. Let us see some particular cases of $(5.8):$

- for $\theta=0$ we obtain the forward Euler (or explicit Euler) method

$$
\mathrm{M} \frac{\mathbf{u}^{k+1}-\mathbf{u}^{k}}{\Delta t}+\mathrm{A} \mathbf{u}^{k}=\mathbf{f}^{k}
$$

which is accurate to order one with respect to $\Delta t$;

- for $\theta=1$ we have the backward Euler (or implicit Euler) method

$$
\mathrm{M} \frac{\mathbf{u}^{k+1}-\mathbf{u}^{k}}{\Delta t}+\mathrm{A} \mathbf{u}^{k+1}=\mathbf{f}^{k+1}
$$

also of first order with respect to $\Delta t$;

- for $\theta=1 / 2$ we have the Crank-Nicolson (or trapezoidal) method

$$
\mathrm{M} \frac{\mathbf{u}^{k+1}-\mathbf{u}^{k}}{\Delta t}+\frac{1}{2} \mathrm{~A}\left(\mathbf{u}^{k+1}+\mathbf{u}^{k}\right)=\frac{1}{2}\left(\mathbf{f}^{k+1}+\mathbf{f}^{k}\right)
$$

which is of second order in $\Delta t$. (More precisely, $\theta=1 / 2$ is the only value for which we obtain a second-order method.)

Let us consider the two extremal cases, $\theta=0$ and $\theta=1$. For both, we obtain a system of linear equations: if $\theta=0$, the system to solve has matrix $\frac{\mathrm{M}}{\Delta t}$, in the second case it has matrix $\frac{M}{\Delta t}+A$. We observe that the M matrix is invertible, being positive definite (see Exercise 1).

In the $\theta=0$ case, if we make $\mathrm{M}$ diagonal, we actually decouple the system. This operation is performed by the so-called lumping of the mass matrix (see Sect. 13.5). However, this scheme is not unconditionally stable (see Sect. 5.4) and in the case where $V_{h}$ is a subspace of finite elements we have the following stability condition (see Sect. $5.4$ )

$$
\exists c>0: \Delta t \leq c h^{2} \quad \forall h>0
$$

so $\Delta t$ icannot be chosen irrespective of $h$.

In case $\theta>0$, the system will have the form $K \mathbf{u}^{k+1}=\mathbf{g}$, where $\mathbf{g}$ is the source term and $\mathrm{K}=\frac{\mathrm{M}}{\Delta t}+\theta \mathrm{A}$. Such matrix is however invariant in time (the operator $L$, and therefore the matrix A, being independent of time); if the space mesh does not change, it can then be factorized once and for all at the beginning of the process. Since $\mathrm{M}$ is symmetric, if $\mathrm{A}$ is symmetric too, the $\mathrm{K}$ matrix associated to the system 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-126.jpg?height=246&width=712&top_left_y=113&top_left_x=102)

Fig. 5.2. Solution of the heat equation for the problem of Example $5.1$

will also be symmetric. Hence, we can use, for instance, the Cholesky factorization, $\mathrm{K}=\mathrm{H} \mathrm{H}^{T}, \mathrm{H}$ being lower triangular. At each time step, we will therefore have to solve two triangular systems in $N_{h}$ unknowns:

$$
\begin{aligned}
&\mathrm{Hy}=\mathbf{g} \\
&\mathrm{H}^{T} \mathbf{u}^{k+1}=\mathbf{y}
\end{aligned}
$$

(see Chap. 7 and also [QSS07, Chap. 3]).

Example 5.1. Let us suppose to solve the heat equation $\frac{\partial u}{\partial t}-0.1 \Delta u=0$ on the domain $\Omega \subset \mathbb{R}^{2}$ of Fig. $5.2$ (left), which is the union of two circles of radius $0.5$ and center $(-0.5,0)$ resp. $(0.5,0)$ ). We assign Dirichlet conditions on the whole boundary taking $u(\mathbf{x}, t)=1$ for the points on $\partial \Omega$ for which $x_{1} \geq 0$ and $u(\mathbf{x}, t)=0$ if $x_{1}<0$ The initial condition is $u(\mathbf{x}, 0)=1$ for $x_{1} \geq 0$ and null elsewhere. In Fig. $5.2$, we report the solution obtained at time $t=1$. We have used linear finite elements in space and the implicit Euler method in time with $\Delta t=0.01$. As it can be seen, the initial discontinuity has been regularized, in accordance with the boundary conditions.

\subsection{A priori estimates}

Let us consider problem (5.5); since the corresponding equations must hold for each $v \in V$, it will be legitimate to set $v=u(t)$ ( $t$ being given), solution of the problem itself, yielding

$$
\int_{\Omega} \frac{\partial u(t)}{\partial t} u(t) d \Omega+a(u(t), u(t))=\int_{\Omega} f(t) u(t) d \Omega \quad \forall t>0
$$

Considering the individual terms, we have

$$
\int_{\Omega} \frac{\partial u(t)}{\partial t} u(t) d \Omega=\frac{1}{2} \frac{\partial}{\partial t} \int_{\Omega}|u(t)|^{2} d \Omega=\frac{1}{2} \frac{\partial}{\partial t}\|u(t)\|_{L^{2}(\Omega)}^{2}
$$

If we assume for simplicity that the bilinear form is coercive (with coercivity constant equal to $\alpha$ ), we obtain

$$
a(u(t), u(t)) \geq \alpha\|u(t)\|_{V}^{2}
$$

while thanks to the Cauchy-Schwarz inequality, we find

$$
(f(t), u(t)) \leq\|f(t)\|_{\mathrm{L}^{2}(\Omega)}\|u(t)\|_{\mathrm{L}^{2}(\Omega)}
$$

In the remainder, we will often use Young's inequality

$$
\forall a, b \in \mathbb{R}, \quad a b \leq \varepsilon a^{2}+\frac{1}{4 \varepsilon} b^{2} \quad \forall \varepsilon>0
$$

which descends from the elementary inequality

$$
\left(\sqrt{\varepsilon} a-\frac{1}{2 \sqrt{\varepsilon}} b\right)^{2} \geq 0
$$

Using first Poincaré' inequality (2.13) and Young's inequality, we obtain

$$
\begin{aligned}
\frac{1}{2} \frac{d}{d t}\|u(t)\|_{L^{2}(\Omega)}^{2}+\alpha\|\nabla u(t)\|_{L^{2}(\Omega)}^{2} & \leq\|f(t)\|_{L^{2}(\Omega)}\|u(t)\|_{L^{2}(\Omega)} \\
& \leq \frac{C_{\Omega}^{2}}{2 \alpha}\|f(t)\|_{L^{2}(\Omega)}^{2}+\frac{\alpha}{2}\|\nabla u(t)\|_{L^{2}(\Omega)}^{2}
\end{aligned}
$$

Then, by integrating in time we obtain, for all $t>0$,

$$
\|u(t)\|_{L^{2}(\Omega)}^{2}+\alpha \int_{0}^{t}\|\nabla u(s)\|_{L^{2}(\Omega)}^{2} d s \leq\left\|u_{0}\right\|_{L^{2}(\Omega)}^{2}+\frac{C_{\Omega}^{2}}{\alpha} \int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)}^{2} d s
$$

This is an a priori energy estimate. Different kinds of a priori estimates can be obtained as follows. Note that

$$
\frac{1}{2} \frac{d}{d t}\|u(t)\|_{L^{2}(\Omega)}^{2}=\|u(t)\|_{L^{2}(\Omega)} \frac{d}{d t}\|u(t)\|_{L^{2}(\Omega)}
$$

Then from (5.9), using (5.10) and (5.11) we obtain (still using the Poincaré inequality)

$$
\begin{aligned}
&\|u(t)\|_{L^{2}(\Omega)} \frac{d}{d t}\|u(t)\|_{L^{2}(\Omega)}+\frac{\alpha}{C_{\Omega}}\|u(t)\|_{L^{2}(\Omega)}\|\nabla u(t)\|_{L^{2}(\Omega)} \\
&\leq\|f(t)\|_{L^{2}(\Omega)}\|u(t)\|_{L^{2}(\Omega)}, \quad t>0
\end{aligned}
$$

If $\|u(t)\|_{L^{2}(\Omega)} \neq 0$ (otherwise we should proceed differently, even though the final result is still true) we can divide by $\|u(t)\|_{L^{2}(\Omega)}$ and integrate in time to obtain

$$
\|u(t)\|_{L^{2}(\Omega)} \leq\left\|u_{0}\right\|_{L^{2}(\Omega)}+\int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)} d s, \quad t>0
$$

This is a further a priori estimate.

Let us now use the first inequality in (5.13) and integrate in time to yield

$$
\begin{aligned}
&\|u(t)\|_{L^{2}(\Omega)}^{2}+2 \alpha \int_{0}^{t}\|\nabla u(s)\|^{2} d s \\
&\quad \leq\left\|u_{0}\right\|_{L^{2}(\Omega)}^{2}+2 \int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)}\|u(s)\|_{L^{2}(\Omega)} d s \\
&\quad \leq\left\|u_{0}\right\|_{L^{2}(\Omega)}^{2}+2 \int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)} \cdot\left(\left\|u_{0}\right\|_{L^{2}(\Omega)}^{2}+\int_{0}^{s}\|f(\tau)\|_{L^{2}(\Omega)} d \tau\right) d s
\end{aligned}
$$

(using $(5.15))$

$$
\begin{aligned}
&=\left\|u_{0}\right\|_{L^{2}(\Omega)}^{2}+2 \int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)}\left\|u_{0}\right\|_{L^{2}(\Omega)}+2 \int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)} \int_{0}^{s}\|f(\tau)\|_{L^{2}(\Omega)} d \tau \\
&=\left(\left\|u_{0}\right\|_{L^{2}(\Omega)}+\int_{0}^{t}\|f(s)\| d s\right)^{2}
\end{aligned}
$$

The latter equality follows upon noticing that

$$
\|f(s)\|_{L^{2}(\Omega)} \int_{0}^{s}\|f(\tau)\|_{L^{2}(\Omega)} d \tau=\frac{d}{d s}\left(\int_{0}^{s}\|f(\tau)\|_{L^{2}(\Omega)} d \tau\right)^{2}
$$

We therefore conclude with the additional a priori estimate

$$
\begin{aligned}
&\left(\|u(t)\|_{L^{2}(\Omega)}^{2}+2 \alpha \int_{0}^{t}\|\nabla u(s)\|_{L^{2}(\Omega)}^{2} d s\right)^{\frac{1}{2}} \\
&\leq\left\|u_{0}\right\|_{L^{2}(\Omega)}+\int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)} d s, \quad t>0
\end{aligned}
$$

We have seen that we can formulate the Galerkin problem (5.6) for problem $(5.5)$ and that the latter, under suitable hypotheses, admits a unique solution. Similarly to what we did for problem (5.5) we can prove the following a priori (stability) estimates for the solution to problem (5.6):

$$
\begin{aligned}
\left\|u_{h}(t)\right\|_{L^{2}(\Omega)}^{2}+\alpha \int_{0}^{t} \| & \nabla u_{h}(s) \|_{L^{2}(\Omega)}^{2} d s \\
& \leq\left\|u_{0 h}\right\|_{L^{2}(\Omega)}^{2}+\frac{C_{\Omega}^{2}}{\alpha} \int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)}^{2} d s, \quad t>0
\end{aligned}
$$

For its proof we can take, for every $t>0, v_{h}=u_{h}(t)$ and proceed as we did to obtain (5.13). Then, by recalling that the initial data is $u_{h}(0)=u_{0 h}$, we can deduce the following discrete counterparts of $(5.15)$ and $(5.17)$ :

$$
\left\|u_{h}(t)\right\|_{L^{2}(\Omega)} \leq\left\|u_{0 h}(t)\right\|_{L^{2}(\Omega)}+\int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)} d s, \quad t>0
$$

and

$$
\begin{aligned}
&\left(\left\|u_{h}(t)\right\|_{L^{2}(\Omega)}^{2}+2 \alpha \int_{0}^{t}\left\|\nabla u_{h}(s)\right\|_{L^{2}(\Omega)}^{2} d s\right)^{\frac{1}{2}} \\
&\leq\left\|u_{0 h}\right\|_{L^{2}(\Omega)}+\int_{0}^{t}\|f(s)\|_{L^{2}(\Omega)} d s, \quad t>0
\end{aligned}
$$



\subsection{Convergence analysis of the semi-discrete problem}

Let us consider problem (5.5) and its approximation (5.6). We want to prove the convergence of $u_{h}$ to $u$ in suitable norms.

By the coercivity hypotheses we can write

$$
\begin{aligned}
\alpha\left\|\left(u-u_{h}\right)(t)\right\|_{\mathrm{H}^{1}(\Omega)}^{2} \leq & a\left(\left(u-u_{h}\right)(t),\left(u-u_{h}\right)(t)\right) \\
=& a\left(\left(u-u_{h}\right)(t),\left(u-v_{h}\right)(t)\right) \\
&+a\left(\left(u-u_{h}\right)(t),\left(v_{h}-u_{h}\right)(t)\right) \quad \forall v_{h}: v_{h}(t) \in V_{h}, \forall t>0 .
\end{aligned}
$$

For the sake of clarity, we suppress the dependence from $t$. By subtracting equation (5.6) from equation $(5.5)$ and setting $w_{h}=v_{h}-u_{h}$ we have

$$
\left(\frac{\partial\left(u-u_{h}\right)}{\partial t}, w_{h}\right)+a\left(u-u_{h}, w_{h}\right)=0
$$

where $(v, w)=\int_{\Omega} v w$ is the scalar product of $L^{2}(\Omega)$. Then

$$
\alpha\left\|u-u_{h}\right\|_{\mathrm{H}^{1}(\Omega)}^{2} \leq a\left(u-u_{h}, u-v_{h}\right)-\left(\frac{\partial\left(u-u_{h}\right)}{\partial t}, w_{h}\right)
$$

We analyze the two right-hand side terms separately:

- using the continuity of the form $a(\cdot, \cdot)$ and Young's inequality, we obtain

$$
\begin{array}{r}
a\left(u-u_{h}, u-v_{h}\right) \leq M\left\|u-u_{h}\right\|_{\mathrm{H}^{1}(\Omega)}\left\|u-v_{h}\right\|_{\mathrm{H}^{1}(\Omega)} \\
\leq \frac{\alpha}{2}\left\|u-u_{h}\right\|_{\mathrm{H}^{1}(\Omega)}^{2}+\frac{M^{2}}{2 \alpha}\left\|u-v_{h}\right\|_{\mathrm{H}^{1}(\Omega)}^{2}
\end{array}
$$

- writing $w_{h}$ in the form $w_{h}=\left(v_{h}-u\right)+\left(u-u_{h}\right)$ we obtain

$$
-\left(\frac{\partial\left(u-u_{h}\right)}{\partial t}, w_{h}\right)=\left(\frac{\partial\left(u-u_{h}\right)}{\partial t}, u-v_{h}\right)-\frac{1}{2} \frac{d}{d t}\left\|u-u_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}
$$

Replacing these two results in $(5.21)$, we obtain

$$
\frac{1}{2} \frac{d}{d t}\left\|u-u_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\frac{\alpha}{2}\left\|u-u_{h}\right\|_{\mathrm{H}^{1}(\Omega)}^{2} \leq \frac{M^{2}}{2 \alpha}\left\|u-v_{h}\right\|_{\mathrm{H}^{1}(\Omega)}^{2}+\left(\frac{\partial\left(u-u_{h}\right)}{\partial t}, u-v_{h}\right)
$$

Multiplying both sides by 2 and integrating in time between 0 and $t$ we find

$$
\begin{aligned}
&\left\|\left(u-u_{h}\right)(t)\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\alpha \int_{0}^{t}\left\|\left(u-u_{h}\right)(s)\right\|_{\mathrm{H}^{1}(\Omega)}^{2} d s \leq\left\|\left(u-u_{h}\right)(0)\right\|_{\mathrm{L}^{2}(\Omega)}^{2} \\
&+\frac{M^{2}}{\alpha} \int_{0}^{t}\left\|\left(u-v_{h}\right)(s)\right\|_{\mathrm{H}^{1}(\Omega)}^{2} d s+2 \int_{0}^{t}\left(\frac{\partial}{\partial s}\left(u-u_{h}\right)(s),\left(u-v_{h}\right)(s)\right) d s
\end{aligned}
$$

Integrating by parts and using Young's inequality, we obtain

$$
\begin{aligned}
&\int_{0}^{t}\left(\frac{\partial}{\partial s}\left(u-u_{h}\right)(s),\left(u-v_{h}\right)(s)\right) d s=-\int_{0}^{t}\left(\left(u-u_{h}\right)(s), \frac{\partial}{\partial s}\left(\left(u-v_{h}\right)(s)\right)\right) d s \\
&\quad+\left(\left(u-u_{h}\right)(t),\left(u-v_{h}\right)(t)\right)-\left(\left(u-u_{h}\right)(0),\left(u-v_{h}\right)(0)\right) \\
&\leq \int_{0}^{t}\left\|\left(u-u_{h}\right)(s)\right\|_{\mathrm{L}^{2}(\Omega)}\left\|\frac{\partial\left(\left(u-v_{h}\right)(s)\right)}{\partial s}\right\|_{\mathrm{L}^{2}(\Omega)} d s+\frac{1}{4}\left\|\left(u-u_{h}\right)(t)\right\|_{\mathrm{L}^{2}(\Omega)}^{2} \\
&\quad+\left\|\left(u-v_{h}\right)(t)\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\frac{1}{2}\left\|\left(u-u_{h}\right)(0)\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\frac{1}{2}\left\|u(0)-v_{h}(0)\right\|_{\mathrm{L}^{2}(\Omega)}^{2}
\end{aligned}
$$

From (5.23) we thus obtain

$$
\begin{aligned}
&\frac{1}{2}\left\|\left(u-u_{h}\right)(t)\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\alpha \int_{0}^{t}\left\|\left(u-u_{h}\right)(s)\right\|_{\mathrm{H}^{1}(\Omega)}^{2} d s \\
&\quad \leq 2 \int_{0}^{t}\left\|\left(u-u_{h}\right)(s)\right\|_{\mathrm{L}^{2}(\Omega)}\left\|\frac{\partial\left(\left(u-v_{h}\right)(s)\right)}{\partial s}\right\|_{\mathrm{L}^{2}(\Omega)} d s \\
&\quad+2\left\|\left(u-v_{h}\right)(t)\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\left\|u(0)-v_{h}(0)\right\|_{\mathrm{L}^{2}(\Omega)}^{2}
\end{aligned}
$$

Let us now suppose that $V_{h}$ is the space of finite elements of degree $r$, more precisely $V_{h}=\left\{v_{h} \in X_{h}^{r}:\left.v_{h}\right|_{\Gamma_{D}}=0\right\}$, and let us choose, at each $t, v_{h}(t)=\Pi_{h}^{r} u(t)$, the interpolant of $u(t)$ in $V_{h}$ (see (4.20)). Thanks to (4.69) we have, assuming that $u$ is sufficiently regular,

$$
h\left\|u(t)-\Pi_{h}^{r} u(t)\right\|_{\mathrm{H}^{1}(\Omega)}+\left\|u(t)-\Pi_{h}^{r} u(t)\right\|_{\mathrm{L}^{2}(\Omega)} \leq C_{2} h^{r+1}|u(t)|_{\mathrm{H}^{r+1}(\Omega)}
$$

Let us consider and bound from above some of the summands of the right-hand side of inequality $(5.24)$ :

$$
\begin{aligned}
&E_{1}=2\left\|\left(u-u_{h}\right)(0)\right\|_{\mathrm{L}^{2}(\Omega)}^{2} \leq C_{1} h^{2 r}\left|u_{0}\right|_{\mathrm{H}^{r}(\Omega)}^{2} . \\
&E_{2}=\frac{M^{2}}{\alpha} \int_{0}^{t}\left\|u(s)-v_{h}(s)\right\|_{\mathrm{H}^{1}(\Omega)}^{2} d s \leq C_{2} h^{2 r} \int_{0}^{t}|u(s)|_{\mathrm{H}^{r+1}(\Omega)}^{2} d s \\
&E_{3}=2\left\|u(t)-v_{h}(t)\right\|_{\mathrm{L}^{2}(\Omega)}^{2} \leq C_{3} h^{2 r}|u(t)|_{\mathrm{H}^{r}(\Omega)}^{2} \\
&E_{4}=\left\|u(0)-v_{h}(0)\right\|_{L^{2}(\Omega)}^{2} \leq C_{4} h^{2 r}|u(0)|_{H^{r}(\Omega)}^{2}
\end{aligned}
$$

Consequently,

$$
E_{1}+E_{2}+E_{3}+E_{4} \leq C h^{2 r} N(u)
$$

where $N(u)$ is a suitable function depending on $u$ and on $\frac{\partial u}{\partial t}$, and $C$ is a suitable positive constant. Finally

$$
E_{5}(s)=\left\|\frac{\partial\left(u(s)-v_{h}(s)\right)}{\partial s}\right\|_{\mathrm{L}^{2}(\Omega)} \leq C_{5} h^{r}\left|\frac{\partial u(s)}{\partial s}\right|_{H^{r}(\Omega)}
$$

In this way, from $(5.24)$ we obtain the inequality

$$
\begin{aligned}
\left\|\left(u-u_{h}\right)(t)\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+2 \alpha & \int_{0}^{t}\left\|\left(u-u_{h}\right)(s)\right\|_{\mathrm{H}^{1}(\Omega)}^{2} d s \\
\leq & C h^{2 r} N(u)+4 C_{5} h^{r} \int_{0}^{t}\left|\frac{\partial u(s)}{\partial s}\right|_{H^{r}(\Omega)}\left\|\left(u-u_{h}\right)(s)\right\|_{\mathrm{L}^{2}(\Omega)} d s .
\end{aligned}
$$

Applying the Gronwall lemma (Lemma $2.2 i i$ ) ), we obtain the a priori error estimate

$$
\begin{aligned}
&\left\{\left\|\left(u-u_{h}\right)(t)\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+2 \alpha \int_{0}^{t}\left\|u-u_{h}\right\|_{\mathrm{H}^{1}(\Omega)}^{2}\right\}^{1 / 2} \\
&\leq \bar{C} h^{r}\left(\sqrt{N(u)}+\int_{0}^{t}\left|\frac{\partial u(s)}{\partial s}\right|_{H^{r}(\Omega)} d s\right)
\end{aligned}
$$

for a suitable positive constant $\bar{C}$ and for all $t>0$.

An alternative proof that does not make use of Gronwall' lemma can be carried out as follows. If we subtract $(5.6)$ from $(5.5)$ and set $E_{h}=u-u_{h}$, we obtain that (the dependence of $E_{h}$ on $t$ is understood)

$$
\left(\frac{\partial E_{h}}{\partial t}, v_{h}\right)+a\left(E_{h}, v_{h}\right)=0 \quad \forall v_{h} \in V_{h}, \quad \forall t>0
$$

If, for the sake of simplicity, we suppose that $a(\cdot, \cdot)$ is symmetric, we can define the orthogonal projection operator

$$
\Pi_{1, h}^{r}: V \rightarrow V_{h}: \forall w \in V, a\left(\Pi_{1, h}^{r} w-w, v_{h}\right)=0 \quad \forall v_{h} \in V_{h}
$$

Using the results seen in Chap. 3, we can prove (see [QV94, Sect. 3.5]) that there exists a constant $C>0$ such that, $\forall w \in V \cap H^{r+1}(\Omega)$,

$$
\left\|\Pi_{1, h}^{r} w-w\right\|_{\mathrm{H}^{1}(\Omega)}+h^{-1}\left\|\Pi_{1, h}^{r} w-w\right\|_{\mathrm{L}^{2}(\Omega)} \leq C h^{p}|w|_{\mathrm{H}^{p+1}(\Omega)}, 0 \leq p \leq r
$$

Then we set

$$
E_{h}=\sigma_{h}+e_{h}=\left(u-\Pi_{1, h}^{r} u\right)+\left(\Pi_{1, h}^{r} u-u_{h}\right)
$$

Note that the orthogonal projection error $\sigma_{h}$ can be bounded by inequality $(5.27)$ and that $e_{h}$ is an element of the subspace $V_{h}$. Then

$$
\left(\frac{\partial e_{h}}{\partial t}, v_{h}\right)+a\left(e_{h}, v_{h}\right)=-\left(\frac{\partial \sigma_{h}}{\partial t}, v_{h}\right)-a\left(\sigma_{h}, v_{h}\right) \quad \forall v_{h} \in V_{h}, \quad \forall t>0
$$

If we take at every $t>0, v_{h}=e_{h}(t)$, and proceed as done in Sect. $5.2$ to deduce the a priori estimates on the semi-discrete solution $u_{h}$, we obtain

$$
\begin{aligned}
\frac{1}{2} \frac{d}{d t}\left\|e_{h}(t)\right\|_{L^{2}(\Omega)}^{2} &+\alpha\left\|\nabla e_{h}(t)\right\|_{L^{2}(\Omega)}^{2} \\
& \leq\left|a\left(\sigma_{h}(t), e_{h}(t)\right)\right|+\left|\left(\frac{\partial}{\partial t} \sigma_{h}(t), e_{h}(t)\right)\right|
\end{aligned}
$$

Using the continuity of the bilinear form $a(\cdot, \cdot)$ ( $M$ being the continuity constant) and Young's inequality $(5.12)$, we obtain

$$
\left|a\left(\sigma_{h}(t), e_{h}(t)\right)\right| \leq \frac{\alpha}{4}\left\|\nabla e_{h}(t)\right\|_{L^{2}(\Omega)}^{2}+\frac{M^{2}}{\alpha}\left\|\nabla \sigma_{h}(t)\right\|_{L^{2}(\Omega)}^{2}
$$

Moreover, using the Poincaré inequality and once more the Young's inequality it follows that

$$
\begin{aligned}
\left|\left(\frac{\partial}{\partial t} \sigma_{h}(t), e_{h}(t)\right)\right| & \leq\left\|\frac{\partial}{\partial t} \sigma_{h}(t)\right\|_{L^{2}(\Omega)} C_{\Omega}\left\|\nabla e_{h}(t)\right\|_{L^{2}(\Omega)} \\
& \leq \frac{\alpha}{4}\left\|\nabla e_{h}(t)\right\|_{L^{2}(\Omega)}^{2}+\frac{C_{\Omega}^{2}}{\alpha}\left\|\frac{\partial}{\partial t} \sigma_{h}(t)\right\|_{L^{2}(\Omega)}^{2}
\end{aligned}
$$

Using these bounds in $(5.29)$ we obtain, after integrating with respect to $t$ :

$$
\begin{aligned}
&\left\|e_{h}(t)\right\|_{L^{2}(\Omega)}^{2}+\alpha \int_{0}^{t}\left\|\nabla e_{h}(t)\right\|_{L^{2}(\Omega)}^{2} d s \\
&\leq\left\|e_{h}(0)\right\|_{L^{2}(\Omega)}^{2}+\frac{2 M^{2}}{\alpha} \int_{0}^{t}\left\|\nabla \sigma_{h}(s)\right\|_{L^{2}(\Omega)}^{2} d s+\frac{2 C_{\Omega}^{2}}{\alpha} \int_{0}^{t}\left\|\frac{\partial}{\partial t} \sigma_{h}(s)\right\|_{L^{2}(\Omega)}^{2} d s, \quad t>0
\end{aligned}
$$

At this point we can use $(5.27)$ to bound the errors on the right-hand side:

$$
\begin{aligned}
&\left\|\nabla \sigma_{h}(t)\right\|_{L^{2}(\Omega)} \leq C h^{r}|u(t)|_{H^{r+1}(\Omega)} \\
&\left\|\frac{\partial}{\partial t} \sigma_{h}(t)\right\|_{L^{2}(\Omega)}=\left\|\left(\frac{\partial u}{\partial t}-\Pi_{1, h}^{r} \frac{\partial u}{\partial t}\right)(t)\right\|_{L^{2}(\Omega)} \leq C h^{r}\left|\frac{\partial u(t)}{\partial t}\right|_{H^{r}(\Omega)}
\end{aligned}
$$

Finally, note that $\left\|e_{h}(0)\right\|_{L^{2}(\Omega)} \leq C h^{r}\left|u_{0}\right|_{H^{r}(\Omega)}$, still using (5.27). Since, for any norm $\|\cdot\|$,

$$
\left\|u-u_{h}\right\| \leq\left\|\sigma_{h}\right\|+\left\|e_{h}\right\|
$$

(owing to $5.28$ ), using the previous estimates we can conclude that there exists a constant $C>0$ independent of both $t$ and $h$ such that

$$
\begin{aligned}
\left\{\left\|u(t)-u_{h}(t)\right\|_{L^{2}(\Omega)}^{2}+\alpha \int_{0}^{t}\left\|\nabla u(s)-\nabla u_{h}(s)\right\|_{L^{2}(\Omega)}^{2} d s\right\}^{1 / 2} \\
& \leq C h^{r}\left\{\left|u_{0}\right|_{H^{r}(\Omega)}^{2}+\int_{0}^{t}|u(s)|_{H^{r+1}(\Omega)}^{2} d s+\int_{0}^{t}\left|\frac{\partial u(s)}{\partial s}\right|_{H^{r+1}(\Omega)}^{2} d s\right\}^{1 / 2}
\end{aligned}
$$

Further error estimates are proven, e.g. in [QV94, Chap. 11]. 

\subsection{Stability analysis of the $\theta$-method}

We now analyze the stability of the fully discretized problem.

Applying the $\theta$-method to the Galerkin problem (5.6) we obtain

$$
\begin{aligned}
\left(\frac{u_{h}^{k+1}-u_{h}^{k}}{\Delta t}, v_{h}\right)+a\left(\theta u_{h}^{k+1}+(1-\theta) u_{h}^{k}, v_{h}\right) \\
=\theta F^{k+1}\left(v_{h}\right)+(1-\theta) F^{k}\left(v_{h}\right) \quad \forall v_{h} \in V_{h},
\end{aligned}
$$

for each $k \geq 0$, with $u_{h}^{0}=u_{0 h} ; F^{k}$ indicates that the functional is evaluated at time $t^{k}$. We will limit ourselves to the case where $F=0$ and start to consider the case of the implicit Euler method $(\theta=1)$ that is

$$
\left(\frac{u_{h}^{k+1}-u_{h}^{k}}{\Delta t}, v_{h}\right)+a\left(u_{h}^{k+1}, v_{h}\right)=0 \quad \forall v_{h} \in V_{h}
$$

By choosing $v_{h}=u_{h}^{k+1}$, we obtain

$$
\left(u_{h}^{k+1}, u_{h}^{k+1}\right)+\Delta t a\left(u_{h}^{k+1}, u_{h}^{k+1}\right)=\left(u_{h}^{k}, u_{h}^{k+1}\right)
$$

By exploiting the following inequalities

$$
a\left(u_{h}^{k+1}, u_{h}^{k+1}\right) \geq \alpha\left\|u_{h}^{k+1}\right\|_{V}^{2}, \quad\left(u_{h}^{k}, u_{h}^{k+1}\right) \leq \frac{1}{2}\left\|u_{h}^{k}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\frac{1}{2}\left\|u_{h}^{k+1}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}
$$

the former deriving from the coercivity of the bilinear form $a(\cdot, \cdot)$, and the latter from the Cauchy-Schwarz and Young inequalities, we obtain

$$
\left\|u_{h}^{k+1}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+2 \alpha \Delta t\left\|u_{h}^{k+1}\right\|_{V}^{2} \leq\left\|u_{h}^{k}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}
$$

By summing over $k$ from 0 to $n-1$ we deduce that

$$
\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+2 \alpha \Delta t \sum_{k=0}^{n-1}\left\|u_{h}^{k+1}\right\|_{V}^{2} \leq\left\|u_{0 h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}
$$

Observing that $\left\|u_{h}^{k+1}\right\|_{V} \geq\left\|u_{h}^{k+1}\right\|_{\mathrm{L}^{2}(\Omega)}$, we deduce from (5.31) that for each given $\Delta t>0$

$$
\lim _{k \rightarrow \infty}\left\|u_{h}^{k}\right\|_{\mathrm{L}^{2}(\Omega)}=0
$$

that is the backward Euler method is absolutely stable without any restriction on the time step $\Delta t$.

When $f \neq 0$, using the discrete Gronwall lemma (see Sect. $2.7$ ) it can be proved in a similar way that

$$
\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+2 \alpha \Delta t \sum_{k=1}^{n}\left\|u_{h}^{k}\right\|_{V}^{2} \leq C\left(t^{n}\right)\left(\left\|u_{0 h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\sum_{k=1}^{n} \Delta t\left\|f^{k}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}\right)
$$

Such relation is similar to $(5.20)$, provided that the integrals $\int_{0}^{t} \cdot d s$ are approximated by a composite numerical integration formula with time step $\Delta t$ [QSS07].

Before analyzing the general case where $\theta$ is an arbitrary parameter ranging between 0 and 1 , we introduce the following definition.

We say that the scalar $\lambda$ is an eigenvalue of the bilinear form $a(\cdot, \cdot): V \times V \mapsto \mathbb{R}$ and that $w \in V$ is its corresponding eigenfunction if it turns out that

$$
a(w, v)=\lambda(w, v) \quad \forall v \in V
$$

If the bilinear form $a(\cdot, \cdot)$ is symmetric and coercive, it has positive, real eigenvalues forming an infinite sequence; moreover, its eigenfunctions form a basis of the space $V$.

The eigenvalues and eigenfunctions of $a(\cdot, \cdot)$ can be approximated by finding the pairs $\lambda_{h} \in \mathbb{R}$ and $w_{h} \in V_{h}$ which satisfy

$$
a\left(w_{h}, v_{h}\right)=\lambda_{h}\left(w_{h}, v_{h}\right) \quad \forall v_{h} \in V_{h} .
$$

From an algebraic viewpoint, problem (5.33) can be formulated as follows

$$
\mathrm{Aw}=\lambda_{h} \mathrm{M} \mathbf{w}
$$

where $A$ is the stiffness matrix and $M$ the mass matrix. We are therefore dealing with a generalized eigenvalue problem.

Such eigenvalues are all positive and $N_{h}$ in number $\left(N_{h}\right.$ being as usual the dimension of the subspace $V_{h}$ ); after ordering them in ascending order, $\lambda_{h}^{1} \leq \lambda_{h}^{2} \leq \ldots \leq \lambda_{h}^{N_{h}}$ we have

$$
\lambda_{h}^{N_{h}} \rightarrow \infty \quad \text { for } N_{h} \rightarrow \infty
$$

Moreover, the corresponding eigenfunctions form a basis for the subspace $V_{h}$ and can be chosen to be orthonormal with respect to the scalar product of $\mathrm{L}^{2}(\Omega)$. This means that, denoting by $w_{h}^{i}$ the eigenfunction corresponding to the eigenvalue $\lambda_{h}^{i}$, we have $\left(w_{h}^{i}, w_{h}^{j}\right)=\delta_{i j} \quad \forall i, j=1, \ldots, N_{h}$. Thus, each function $v_{h} \in V_{h}$ can be represented as follows

$$
v_{h}(\mathbf{x})=\sum_{j=1}^{N_{h}} v_{j} w_{h}^{j}(\mathbf{x})
$$

and, thanks to the eigenfunction orthonormality,

$$
\left\|v_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}=\sum_{j=1}^{N_{h}} v_{j}^{2}
$$

Let us consider an arbitrary $\theta \in[0,1]$ and let us limit ourselves to the case where the bilinear form $a(\cdot, \cdot)$ is symmetric (otherwise, although the final stability result holds in general, the following proof would not work, as the eigenfunctions would not necessarily form a basis). Since $u_{h}^{k} \in V_{h}$, we can write

$$
u_{h}^{k}(\mathbf{x})=\sum_{j=1}^{N_{h}} u_{j}^{k} w_{h}^{j}(\mathbf{x})
$$

We observe that in this modal expansion, the $u_{j}^{k}$ no longer represent the nodal values of $u_{h}^{k}$. If we now set $F=0$ in $(5.30)$ and take $v_{h}=w_{h}^{i}$, we find

$$
\frac{1}{\Delta t} \sum_{j=1}^{N_{h}}\left[u_{j}^{k+1}-u_{j}^{k}\right]\left(w_{h}^{j}, w_{h}^{i}\right)+\sum_{j=1}^{N_{h}}\left[\theta u_{j}^{k+1}+(1-\theta) u_{j}^{k}\right] a\left(w_{h}^{j}, w_{h}^{i}\right)=0
$$

for each $i=1, \ldots, N_{h}$. For each pair $i, j=1, \ldots, N_{h}$ we have

$$
a\left(w_{h}^{j}, w_{h}^{i}\right)=\lambda_{h}^{j}\left(w_{h}^{j}, w_{h}^{i}\right)=\lambda_{h}^{j} \delta_{i j}=\lambda_{h}^{i}
$$

and thus, for each $i=1, \ldots, N_{h}$,

$$
\frac{u_{i}^{k+1}-u_{i}^{k}}{\Delta t}+\left[\theta u_{i}^{k+1}+(1-\theta) u_{i}^{k}\right] \lambda_{h}^{i}=0
$$

Solving now for $u_{i}^{k+1}$, we find

$$
u_{i}^{k+1}=u_{i}^{k} \frac{1-(1-\theta) \lambda_{h}^{i} \Delta t}{1+\theta \lambda_{h}^{i} \Delta t}
$$

Recalling (5.34), we can conclude that for the method to be absolutely stable, we must impose the inequality

$$
\left|\frac{1-(1-\theta) \lambda_{h}^{i} \Delta t}{1+\theta \lambda_{h}^{i} \Delta t}\right|<1
$$

that is

$$
-1-\theta \lambda_{h}^{i} \Delta t<1-(1-\theta) \lambda_{h}^{i} \Delta t<1+\theta \lambda_{h}^{i} \Delta t
$$

Hence,

$$
-\frac{2}{\lambda_{h}^{i} \Delta t}-\theta<\theta-1<\theta
$$

The second inequality is always verified, while the first one can be rewritten as

$$
2 \theta-1>-\frac{2}{\lambda_{h}^{i} \Delta t}
$$

If $\theta \geq 1 / 2$, the left-hand side is non-negative, while the right-hand side is negative, so the inequality holds for each $\Delta t$. Instead, if $\theta<1 / 2$, the inequality is satisfied (hence the method is stable) only if

$$
\Delta t<\frac{2}{(1-2 \theta) \lambda_{h}^{i}}
$$

As such relation must hold for all the eigenvalues $\lambda_{h}^{i}$ of the bilinear form, it will suffice to require that it holds for the largest among them, which we have supposed to be $\lambda_{h}^{N_{h}}$. To summarize, we have:

- if $\theta \geq 1 / 2$, the $\theta$-method is unconditionally absolutely stable, i.e. it is absolutely stable for each $\Delta t$;

- if $\theta<1 / 2$, the $\theta$-method is absolutely stable only for $\Delta t \leq \frac{2}{(1-2 \theta) \lambda_{h}^{N_{h}}}$.

Thanks to the definition of eigenvalue (5.33) and to the continuity property of $a(\cdot, \cdot)$, we deduce

$$
\lambda_{h}^{N_{h}}=\frac{a\left(w_{N_{h}}, w_{N_{h}}\right)}{\left\|w_{N_{h}}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}} \leq \frac{M\left\|w_{N_{h}}\right\|_{V}^{2}}{\left\|w_{N_{h}}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}} \leq M\left(1+C^{2} h^{-2}\right)
$$

The constant $C>0$ which appears in the latter step derives from the following inverse inequality

$$
\exists C>0:\left\|\nabla v_{h}\right\|_{\mathrm{L}^{2}(\Omega)} \leq C h^{-1}\left\|v_{h}\right\|_{\mathrm{L}^{2}(\Omega)} \quad \forall v_{h} \in V_{h},
$$

for whose proof we refer to [QV94, Chap. 3].

Hence, for $h$ small enough, $\lambda_{h}^{N_{h}} \leq C h^{-2}$. In fact, we can prove that $\lambda_{h}^{N_{h}}$ is indeed of the order of $h^{-2}$, that is

$$
\lambda_{h}^{N_{h}}=\max _{i} \lambda_{h}^{i} \simeq c h^{-2}
$$

Keeping this into account, we obtain that for $\theta<1 / 2$ the method is absolutely stable only if

$$
\Delta t \leq C(\theta) h^{2}
$$

where $C(\theta)$ denotes a positive constant depending on $\theta$. The latter relation implies that for $\theta<1 / 2, \Delta t$ cannot be chosen arbitrarily but is bound to the choice of $h$.

\subsection{Convergence analysis of the $\theta$-method}

We can prove the following convergence theorem

Proof. The proof is carried out by comparing the solution of the fully discretized problem (5.30) with that of the semi-discrete problem (5.6), using the stability result (5.32) as well as the decay rate of the truncation error of the time discretization. For simplicity, we will limit ourselves to considering the backward Euler method (corresponding to $\theta=1$ )

$$
\frac{1}{\Delta t}\left(u_{h}^{k+1}-u_{h}^{k}, v_{h}\right)+a\left(u_{h}^{k+1}, v_{h}\right)=\left(f^{k+1}, v_{h}\right) \quad \forall v_{h} \in V_{h}
$$

We refer the reader to [QV94], Sect. 11.3.1, for the proof in the general case.

Let $\Pi_{1, h}^{r}$ be the orthogonal projector operator introduced in $(5.26)$. Then

$$
\left\|u\left(t^{k}\right)-u_{h}^{k}\right\|_{\mathrm{L}^{2}(\Omega)} \leq\left\|u\left(t^{k}\right)-\Pi_{1, h}^{r} u\left(t^{k}\right)\right\|_{\mathrm{L}^{2}(\Omega)}+\left\|\Pi_{1, h}^{r} u\left(t^{k}\right)-u_{h}^{k}\right\|_{\mathrm{L}^{2}(\Omega)}
$$

The first term can be estimated by referring to $(5.27)$. To analyze the second term, where $\varepsilon_{h}^{k}=u_{h}^{k}-\Pi_{1, h}^{r} u\left(t^{k}\right)$, we obtain

$$
\frac{1}{\Delta t}\left(\varepsilon_{h}^{k+1}-\varepsilon_{h}^{k}, v_{h}\right)+a\left(\varepsilon_{h}^{k+1}, v_{h}\right)=\left(\delta^{k+1}, v_{h}\right) \quad \forall v_{h} \in V_{h}
$$

having set,

$$
\left(\delta^{k+1}, v_{h}\right)=\left(f^{k+1}, v_{h}\right)-\frac{1}{\Delta t}\left(\Pi_{1, h}^{r}\left(u\left(t^{k+1}\right)-u\left(t^{k}\right)\right), v_{h}\right)-a\left(u\left(t^{k+1}\right), v_{h}\right)
$$

and having exploited on the last summand the orthogonality (5.26) of the operator $\Pi_{1, h}^{r}$. The sequence $\left\{\varepsilon_{h}^{k}, k=0,1 \ldots\right\}$ satisfies problem (5.39), which is similar to (5.37) (provided that we take $\delta^{k+1}$ instead of $f^{k+1}$ ). By adapting the stability estimate (5.32), we obtain, for each $n \geq 1$,

$$
\left\|\varepsilon_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+2 \alpha \Delta t \sum_{k=1}^{n}\left\|\varepsilon_{h}^{k}\right\|_{V}^{2} \leq C\left(t^{n}\right)\left(\left\|\varepsilon_{h}^{0}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\sum_{k=1}^{n} \Delta t\left\|\delta^{k}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}\right)
$$

The norm associated to the initial time-level can easily be estimated: for instance, if $u_{0 h}=\Pi_{h}^{r} u_{0}$ is the finite element interpolant of $u_{0}$, by suitably using the estimates (4.69) and (5.27) we obtain

$$
\begin{aligned}
\left\|\varepsilon_{h}^{0}\right\|_{\mathrm{L}^{2}(\Omega)} &=\left\|u_{0 h}-\Pi_{1, h}^{r} u_{0}\right\|_{\mathrm{L}^{2}(\Omega)} \\
& \leq\left\|\Pi_{h}^{r} u_{0}-u_{0}\right\|_{\mathrm{L}^{2}(\Omega)}+\left\|u_{0}-\Pi_{1, h}^{r} u_{0}\right\|_{\mathrm{L}^{2}(\Omega)} \leq C h^{r}\left|u_{0}\right|_{\mathrm{H}^{r}(\Omega)}
\end{aligned}
$$

Let us now focus on estimating the norm $\left\|\delta^{k}\right\|_{\mathrm{L}^{2}(\Omega)}$. Thanks to $(5.5)$,

$$
\left(f^{k+1}, v_{h}\right)-a\left(u\left(t^{k+1}\right), v_{h}\right)=\left(\frac{\partial u\left(t^{k+1}\right)}{\partial t}, v_{h}\right)
$$

This allows us to rewrite $(5.40)$ as

$$
\begin{gathered}
\left(\delta^{k+1}, v_{h}\right)=\left(\frac{\partial u\left(t^{k+1}\right)}{\partial t}, v_{h}\right)-\frac{1}{\Delta t}\left(\Pi_{1, h}^{r}\left(u\left(t^{k+1}\right)-u\left(t^{k}\right)\right), v_{h}\right) \\
=\left(\frac{\partial u\left(t^{k+1}\right)}{\partial t}-\frac{u\left(t^{k+1}\right)-u\left(t^{k}\right)}{\Delta t}, v_{h}\right)+\left(\left(I-\Pi_{1, h}^{r}\right)\left(\frac{u\left(t^{k+1}\right)-u\left(t^{k}\right)}{\Delta t}\right), v_{h}\right)
\end{gathered}
$$

Using the Taylor formula with the remainder in integral form, we have

$$
\frac{\partial u\left(t^{k+1}\right)}{\partial t}-\frac{u\left(t^{k+1}\right)-u\left(t^{k}\right)}{\Delta t}=\frac{1}{\Delta t} \int_{t^{k}}^{t^{k+1}}\left(s-t^{k}\right) \frac{\partial^{2} u}{\partial s^{2}}(s) d s
$$

having made suitable regularity requirements on the function $u$ with respect to the temporal variable. By now using the fundamental theorem of calculus and exploiting the commutativity between the projection operator $\Pi_{1, h}^{r}$ and the temporal derivative, we obtain

$$
\left(I-\Pi_{1, h}^{r}\right)\left(u\left(t^{k+1}\right)-u\left(t^{k}\right)\right)=\int_{t^{k}}^{t^{k+1}}\left(I-\Pi_{1, h}^{r}\right)\left(\frac{\partial u}{\partial s}\right)(s) d s
$$

By choosing $v_{h}=\delta^{k+1}$ in (5.43), thanks to $(5.44)$ and $(5.45)$, we can deduce the following upper bound

$$
\begin{aligned}
&\left\|\delta^{k+1}\right\|_{L^{2}(\Omega)} \\
&\leq\left\|\frac{1}{\Delta t} \int_{t^{k}}^{t^{k+1}}\left(s-t^{k}\right) \frac{\partial^{2} u}{\partial s^{2}}(s) d s\right\|_{L^{2}(\Omega)}+\left\|\frac{1}{\Delta t} \int_{t^{k}}^{t^{k+1}}\left(I-\Pi_{1, h}^{r}\right)\left(\frac{\partial u}{\partial s}\right)(s) d s\right\|_{L^{2}(\Omega)} \\
&\leq \int_{t^{k}}^{t^{k+1}}\left\|\frac{\partial^{2} u}{\partial s^{2}}(s)\right\|_{L^{2}(\Omega)} d s+\frac{1}{\Delta s} \int_{t^{k}}^{t^{k+1}}\left\|\left(I-\Pi_{1, h}^{r}\right)\left(\frac{\partial u}{\partial s}\right)(s)\right\|_{L^{2}(\Omega)} d s
\end{aligned}
$$

By reverting to the stability estimate $(5.41)$ and exploiting $(5.42)$ and the estimate (5.46) with suitably scaled indices, we have

$$
\begin{aligned}
\left\|\varepsilon_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2} & \leq C\left(t^{n}\right)\left(h^{2 r}\left|u_{0}\right|_{\mathrm{H}^{\prime}(\Omega)}^{2}+\sum_{k=1}^{n} \Delta t\left[\left(\int_{t^{k-1}}^{t^{k}}\left\|\frac{\partial^{2} u}{\partial s^{2}}(s)\right\|_{\mathrm{L}^{2}(\Omega)} d s\right)^{2}\right.\right.\\
&\left.\left.+\frac{1}{\Delta t^{2}}\left(\int_{t^{k-1}}^{t^{k}}\left\|\left(I-\Pi_{1, h}^{r}\right)\left(\frac{\partial u}{\partial s}\right)(s)\right\|_{\mathrm{L}^{2}(\Omega)} d s\right)^{2}\right]\right)
\end{aligned}
$$

Then, using the Cauchy-Schwarz inequality and estimate $(5.27)$ for the projection operator $\Pi_{1, h}^{r}$, we obtain

$$
\begin{gathered}
\left\|\varepsilon_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2} \leq C\left(t^{n}\right)\left(h^{2 r}\left|u_{0}\right|_{\mathrm{H}^{r}(\Omega)}^{2}+\sum_{k=1}^{n} \Delta t\left[\Delta t \int_{t^{k-1}}^{t^{k}}\left\|\frac{\partial^{2} u}{\partial s^{2}}(s)\right\|_{\mathrm{L}^{2}(\Omega)}^{2} d s\right.\right. \\
\left.\left.+\frac{1}{\Delta t^{2}}\left(\int_{t^{k-1}}^{t^{k}} h^{r}\left|\frac{\partial u}{\partial s}(s)\right|_{\mathrm{H}^{r}(\Omega)} d s\right)^{2}\right]\right)
\end{gathered}
$$



$$
\begin{gathered}
\leq C\left(t^{n}\right)\left(h^{2 r}\left|u_{0}\right|_{\mathrm{H}^{r}(\Omega)}^{2}+\Delta t^{2} \sum_{k=1}^{n} \int_{t^{k-1}}^{t^{k}}\left\|\frac{\partial^{2} u}{\partial s^{2}}(s)\right\|_{\mathrm{L}^{2}(\Omega)}^{2} d s\right. \\
\left.+\frac{1}{\Delta t} h^{2 r} \sum_{k=1}^{n} \Delta t \int_{t^{k-1}}^{t^{k}}\left|\frac{\partial u}{\partial s}(s)\right|_{\mathrm{H}^{r}(\Omega)}^{2} d s\right)
\end{gathered}
$$

The result now follows using $(5.38)$ and estimate $(5.27)$.

More stability and convergence estimates can be found in [Tho84].

\subsection{Exercises}

1. Verify that the mass matrix M introduced in (5.7) is positive definite.

2. Prove the stability condition (10.42) for the pseudo-spectral approximation of equation (5.4) (after replacing the interval $(0,1)$ with $(-1,1)$ ).

[Solution: proceed as done in Sect. $5.4$ for the finite element case and use the properties given in Lemma $10.2$ and 10.3.]

3. Consider the problem:

$$
\begin{cases}\frac{\partial u}{\partial t}-\frac{\partial}{\partial x}\left(\alpha \frac{\partial u}{\partial x}\right)-\beta u=0 & \text { in } Q_{T}=(0,1) \times(0, \infty) \\ u=u_{0} & \text { for } x \in(0,1), t=0 \\ u=\eta & \text { for } x=0, t>0 \\ \alpha \frac{\partial u}{\partial x}+\gamma u=0 & \text { for } x=1, t>0\end{cases}
$$

where $\alpha=\alpha(x), u_{0}=u_{0}(x)$ are given functions and $\beta, \gamma, \eta \in \mathbb{R}$ (with positive $\beta$ ).

a) Prove existence and uniqueness of the weak solution for varying $\gamma$, providing suitable limitations on the coefficients and suitable regularity hypotheses on the functions $\alpha$ and $u_{0}$.

b) Introduce the spatial semi-discretization of the problem using the Galerkinfinite element method, and carry out its stability and convergence analysis.

c) In the case where $\gamma=0$, approximate the same problem with the explicit Euler method in time and carry out its stability analysis. 4. Consider the following problem: find $u(x, t), 0 \leq x \leq 1, t \geq 0$, such that

$$
\begin{cases}\frac{\partial u}{\partial t}+\frac{\partial v}{\partial x}=0, & 0<x<1, t>0 \\ v+\alpha(x) \frac{\partial u}{\partial x}-\gamma(x) u=0, & 0<x<1, t>0 \\ v(1, t)=\beta(t), u(0, t)=0, & t>0 \\ u(x, 0)=u_{0}(x), & 0<x<1\end{cases}
$$

where $\alpha, \gamma, \beta, u_{0}$ are given functions.

a) Introduce an approximation based on finite elements of degree two in $x$ and the implicit Euler method in time and prove its stability.

b) How will the error behave as a function of the parameters $h$ and $\Delta t$ ?

c) Suggest a way to provide an approximation for $v$ starting from the one for $u$ as well as its approximation error.

5. Consider the following (diffusion-transport-reaction) initial-boundary value problem: find $u:(0,1) \times(0, T) \rightarrow \mathbb{R}$ such that

$$
\begin{cases}\frac{\partial u}{\partial t}-\frac{\partial}{\partial x}\left(\alpha \frac{\partial u}{\partial x}\right)+\frac{\partial}{\partial x}(\beta u)+\gamma u=0, & 0<x<1,0<t<T \\ u=0 & \text { for } x=0,0<t<T \\ \alpha \frac{\partial u}{\partial x}+\delta u=0 & \text { for } x=1,0<t<T \\ u(x, 0)=u_{0}(x), & 0<x<1, t=0\end{cases}
$$

where $\alpha=\alpha(x), \beta=\beta(x), \gamma=\gamma(x), \delta=\delta(x), u_{0}=u_{0}(x), x \in[0,1]$ are given functions.

a) Write its weak formulation.

b) In addition to the hypotheses:

$$
\begin{aligned}
&\text { a. } \quad \exists \beta_{0}, \alpha_{0}, \alpha_{1}>0: \forall x \in(0,1) \alpha_{1} \geq \alpha(x) \geq \alpha_{0}, \beta(x) \leq \beta_{0}, \\
&\text { b. } \quad \frac{1}{2} \beta^{\prime}(x)+\gamma(x) \geq 0 \quad \forall x \in(0,1)
\end{aligned}
$$

provide further possible hypotheses on the data so that the problem is wellposed. Moreover, give an a priori estimate of the solution. Treat the same problem with non-homogeneous Dirichlet data $u=g$ for $x=0$ and $0<t<T$.

c) Consider a semi-discretization based on the linear finite elements method and prove its stability.

d) Finally, provide a full discretization where the temporal derivative is approximated using the implicit Euler scheme and prove its stability. 6. Consider the heat equation

$$
\left\{\begin{array}{lc}
\frac{\partial u}{\partial t}-\frac{\partial^{2} u}{\partial x^{2}}=0, & -1<x<1, t>0 \\
u(x, 0)=u_{0}(x), & -1<x<1 \\
u(-1, t)=u(1, t)=0, & t>0
\end{array}\right.
$$

and approximate it by the G-NI method in space and the backward Euler finite difference method in time. Then carry out the stability analysis.

7. Consider the following fourth-order initial-boundary value problem: find $u: \Omega \times(0, T) \rightarrow \mathbb{R}$ such that

$$
\begin{cases}\frac{\partial u}{\partial t}-\operatorname{div}(\mu \nabla u)+\Delta^{2} u+\sigma u=0 & \text { in } \Omega \times(0, T) \\ u(\mathbf{x}, 0)=u_{0} & \text { in } \Omega, \\ \frac{\partial u}{\partial n}=u=0 & \text { on } \Sigma_{T}=\partial \Omega \times(0, T)\end{cases}
$$

where $\Omega \subset \mathbb{R}^{2}$ is a bounded open domain with "regular" boundary $\partial \Omega, \Delta^{2}=\Delta \Delta$ is the bi-harmonic operator, $\mu(\mathbf{x}), \sigma(\mathbf{x})$ and $u_{0}(\mathbf{x})$ are known functions defined in $\Omega$. It is known that

$$
\sqrt{\int_{\Omega}|\Delta u|^{2} \mathrm{~d} \Omega} \simeq\|u\|_{\mathrm{H}^{2}(\Omega)} \quad \forall u \in \mathrm{H}_{0}^{2}(\Omega)
$$

that is the two norms $\|u\|_{\mathrm{H}^{2}(\Omega)}$ and $\|\Delta u\|_{\mathrm{L}^{2}(\Omega)}$ are equivalent on the space

$$
\mathrm{H}_{0}^{2}(\Omega)=\left\{u \in \mathrm{H}^{2}(\Omega): u=\partial u / \partial n=0 \text { on } \partial \Omega\right\}
$$

a) Write its weak formulation and verify that the solution exists and is unique, formulating suitable regularity hypotheses on the data.

b) Consider a semi-discretization based on triangular finite elements and provide the minimum degree that such elements must have in order to solve the given problem adequately. (Use the following property (see, e.g., [QV94]): if $\mathscr{T}_{h}$ is a regular triangulation of $\Omega$ and $v_{h \mid K}$ is a polynomial for each $K \in \mathscr{T}_{h}$, then $v_{h} \in \mathrm{H}^{2}(\Omega)$ if and only if $v_{h} \in C^{1}(\bar{\Omega})$, that is $v_{h}$ and its first derivatives are continuous across the interfaces of the elements of $\mathscr{T}_{h}$. ) Chapter 6

Generation of $1 \mathrm{D}$ and $2 \mathrm{D}$ grids

As we have seen, the finite element method for the solution of partial differential equations requires a "triangulation" of the computational domain, i.e. a partition of the domain in simpler geometric entities (for instance, triangles or quadrangles in two dimensions, tetrahedra, prisms or hexahedra in three dimensions), called the elements, which verify a number of conditions. Similar partitions stand at the base of other approximation methods, such as the finite volume method (see Chapter 9) and the spectral element method (see Chapter 10 ). The set of all elements is the so-called computational grid (or, simply, grid, or mesh).

In this chapter, for simplicity, we focus on the main partitioning techniques for one- and two-dimensional domains, with no ambition of completeness. If necessary, we will refer the reader to the relevant specialized literature. We will deal only with the case of polygonal domains; for computational domains with curved boundaries, the interested reader can consult [Cia78], [BS94], [GB98]. The techniques exposed for the $2 \mathrm{D}$ case can be extended to three-dimensional domains.

\subsection{Grid generation in 1D}

Suppose that the computational domain $\Omega$ be an interval $(a, b)$. The most elementary partition in sub-intervals is the one where the step $h$ is constant. Having chosen the number of elements, say $N$, we set $h=\frac{b-a}{N}$ and introduce the points $x_{i}=x_{0}+i h$, with $x_{0}=a$ and $i=0, \ldots, N$. Such points $\left\{x_{i}\right\}$ are called "vertices" in analogy to the twodimensional case, where they will actually be the vertices of the triangles whose union covers the domain $\Omega$. The partition thus obtained is called grid. The latter is uniform as it is composed by elements of the same length.

In the more general case, we will use non-uniform grids, possibly generated according to a given law. Among the possible different procedures, we illustrate a fairly general one. Let a strictly positive function $\mathscr{H}:[a, b] \rightarrow \mathbb{R}^{+}$, called spacing function, be assigned and let us consider the problem of generating a partition of the interval $[a, b]$ having $N+1$ vertices $x_{i}$. The value $\mathscr{H}(x)$ represents the desired spacing in correspondence of the point $x$. For instance, if $\mathscr{H}=h$ (constant), with $h=(b-a) / M$ for a given integer $M$, we fall exactly in the preceding case of the uniform grid, with $N=M$. More generally, we compute $\mathscr{N}=\int_{a}^{b} \mathscr{H}^{-1}(x) d x$ and we set $N=\max (1,[\mathscr{N}])$, where $[\mathscr{N}]$ denotes the integer part of $\mathscr{N}$, i.e. the largest positive integer smaller than or equal to $\mathscr{N}$. Note that the resulting grid will have at least one element. Then we set $\kappa=\frac{N}{\mathscr{N}}$ and look for the points $x_{i}$ such that

$$
\kappa \int_{a}^{x_{i}} \mathscr{H}^{-1}(x) d x=i
$$

for $i=0, \ldots, N$. The constant $\kappa$ is a positive correction factor, with a value as close as possible to 1 , whose purpose is to guarantee that $N$ is indeed an integer. In fact, for a given $\mathscr{H}$, the number $N$ of elements is itself an unknown of the problem. Instead, the $\mathscr{H}^{-1}$ function defines a density function: to higher values of $\mathscr{H}^{-1}$ correspond denser nodes, and conversely, to smaller values of $\mathscr{H}^{-1}$ correspond sparser nodes.

Obviously, if we wish to construct a grid with a given number $N$ of elements, as well as a given variation on $[a, b]$, is sufficient to renormalize the spacing function so that the integral on $(a, b)$ of the corresponding density is exactly equal to $N$. In any case, to compute the points $x_{i}$, it is useful to introduce the following Cauchy problem

$$
y^{\prime}(x)=\kappa \mathscr{H}^{-1}(x), x \in(a, b), \text { with } y(a)=0
$$

The points $x_{i}$ will then be defined by the relation $y\left(x_{i}\right)=i$, for $i=1, \ldots, N-1$. Then, it will automatically follow that $x_{0}=a$ and $x_{N}=b$. We will then be able to use a numerical solution method to find the roots of the functions $f_{j}(x)=y(x)-j$, for each value of $j \in\{1, \ldots, N-1\}$ (see e.g. [QSS07]).

Besides being quite general, this procedure can be easily extended to the generation of vertices on the curved boundary of a two-dimensional domain, as we will see in Sect. $6.4 .2$.

In the case where $\mathscr{H}$ does not exhibit excessive variations in the interval $(a, b)$, we can also use a simplified procedure which consists in computing a set of preliminary points $\tilde{x}_{i}$, for $i=0, \ldots, N$, defined as follows:

1. Set $\tilde{x}_{0}=a$ and define $\tilde{x_{i}}=\tilde{x}_{i-1}+\mathscr{H}\left(\tilde{x}_{i-1}\right), i=1,2, \ldots$, until finding the value $M$ such that $\tilde{x}_{M} \geq b$ and $\tilde{x}_{M-1}<b$

2. if $\tilde{x}_{M}-b \leq b-\tilde{x}_{M-1}$ set $N=M$, otherwise define $N=M-1$.

Then the final set of vertices are obtained by setting

$$
x_{i}=x_{i-1}+k \mathscr{H}\left(\tilde{x}_{i-1}\right), \quad i=1, \ldots, N,
$$

with $x_{0}=a$ and $k=\left(b-x_{N-1}\right) /\left(x_{N}-x_{N-1}\right)$.

The MATLAB program mesh_1d allows to construct a grid on an interval with endpoints a and $b$ with step specified in the macro H, using the previous simplified algorithm. For instance, with the following MATLAB commands:

$a=0 ; b=1 ; H=0.1 '$

coord $=$ mesh_1d $(a, b, H)$;

we create a uniform grid on $[0,1]$ with 10 sub-intervals with step $h=0.1$. 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-144.jpg?height=266&width=700&top_left_y=116&top_left_x=105)

Fig. 6.1. On the left-hand side, the behaviour of the grid step (on the $\mathrm{x}$-axis) associated to the function $\mathrm{H}=1 /(\exp (4 * \mathrm{x})+2)$ ', on the right-hand side the one relating to the function $\mathrm{H}=$ ' $1 *(x<3)+.05 *(x>5)+.05^{\prime}$. The graph also reports the corresponding vertex distributions

Setting $H=^{\prime} 1 /(\exp (4 * x)+2)^{\prime}$ we obtain a grid that becomes finer when approaching the second endpoint of the interval, while for $\mathrm{H}=^{\prime} .1 *(\mathrm{x}<.3)+.05 *(\mathrm{x}>.5)+.05^{\prime}$ we obtain a grid with a discontinuously varying step (see Fig. 6.1).

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-144.jpg?height=513&width=726&top_left_y=587&top_left_x=92)

We point out that in case $\mathscr{H}$ is determined by an error estimate, Program 1 will allow to perform grid adaptivity.

We now tackle the problem of constructing the grid for two-dimensional domains. 

\subsection{Grid of a polygonal domain}

Given a bounded polygonal domain $\Omega$ in $\mathbb{R}^{2}$, we can associate it with a grid (or partition) $\mathscr{T}_{h}$ of $\Omega$ in polygons $K$ such that

$$
\bar{\Omega}=\bigcup_{K \in \mathscr{T}_{h}} K
$$

where $\bar{\Omega}$ is the closure of $\Omega$, and

- $\stackrel{\circ}{K} \neq \emptyset \forall K \in \mathscr{T}_{h}$

- $K_{1} \cap K_{2}=\emptyset$ for each $K_{1}, K_{2} \in \mathscr{T}_{h}$ such that $K_{1} \neq K_{2}$

- if $F=K_{1} \cap K_{2} \neq \emptyset$ with $K_{1}, K_{2} \in \mathscr{T}_{h}$ and $K_{1} \neq K_{2}$, then $F$ is either a whole edge or a vertex of the grid;

- having denoted by $h_{K}$ the diameter of $K$ for each $K \in \mathscr{T}_{h}$, we define $h=\max _{K \in \mathscr{T}_{h}} h_{K}$.

We have denoted by $\stackrel{\circ}{K}=K \backslash \partial K$ the interior of $K$. The grid $\mathscr{T}_{h}$ is also called mesh, or sometimes triangulation (in a broad sense) of $\bar{\Omega}$.

The constraints imposed on the grid by the first two conditions are obvious: in particular, the second one requires that given two distinct elements, their interiors do not overlap. The third condition limits the admissible triangulations to the so-called conforming ones. To illustrate the concept, we represent in Fig. $6.2$ a conforming (left) and nonconforming (right) triangulation. In the remainder, we will only consider conforming triangulations. However, there exist very specific finite element approximations, not considered in the present book, which use nonconforming grids, i.e. grids that do not satisfy the third condition. These methods are therefore more flexible, at least as far as the choice of the computational grid is concerned. They allow, among other things, the coupling of grids constructed from elements of different nature, for instance triangles, quadrilaterals and, more generally, polygons in $2 \mathrm{D}$ and polyhedra in 3D. This is for instance the case of the so-called virtual elements [Bre15], or mimetic finite differences [BLM14].

The fourth condition links the parameter $h$ to the maximum diameter of the elements of $\mathscr{T}_{h}$.

For reasons linked to the interpolation error theory recalled in Chapter 4, we will only consider regular triangulations $\mathscr{T}_{h}$, i.e. the ones for which, for each element $K \in$
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-145.jpg?height=206&width=548&top_left_y=990&top_left_x=183)

Fig. 6.2. Example of conforming (left) and nonconforming (right) grid 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-146.jpg?height=240&width=514&top_left_y=112&top_left_x=201)

Fig. 6.3. Diameter and sphericity for a triangular (left) and quadrilateral element (right)

$\mathscr{T}_{h}$, the ratio between the diameter $h_{K}$ and the sphericity $\rho_{K}$ (i.e. the diameter of the inscribed circle) is less that a given constant. More precisely, the grids satisfy Property (4.37). Fig. $6.3$ illustrates the meaning of diameter and sphericity for a triangular or quadrilateral element.

In actual applications, it is customary to distinguish between structured and $u n-$ structured grids. Structured grids basically use quadrangular elements and are characterized by the fact that access to the vertices adjacent to a given node (or to the elements adjacent to a given element) is immediate. Indeed, it is possible to establish a bijective relationship between the vertices of the grid and the pairs of integer numbers $(i, j), i=1, \ldots, N_{i}, \quad j=1, \ldots, N_{j}$ such that, given the node of indices $(i, j)$ the four adjacent vertices are in correspondence with the indices $(i-1, j),(i+1, j)$, $(i, j-1)$ and $(i, j+1)$. The total number of vertices is therefore $N_{i} N_{j}$. An analogous association can be established between the elements of the grid and the pairs $(I, J), I=$ $1, \ldots, N_{i}-1, \quad J=1, \ldots, N_{j}-1$. Moreover, it is possible to identify directly the vertices corresponding to each element, without having to memorize the connectivity matrix explicitly (the latter is the matrix which, for each element, provides its vertex numbering). Fig. $6.4$ (left) illustrates such situation.

In a computer code, pairs of indices are typically replaced by a numbering formed by a single integer that is biunivocally associated to the indices described above. For instance, for the numbering of vertices, we can choose to associate the integer number $k=i+(j-1) N_{i}$ to each pair $(i, j)$, and, conversely, we uniquely associate to the vertex $k$ the indices $i=\left((k-1) \bmod N_{i}\right)+1$ and $j=\left((k-1) \operatorname{div} N_{i}\right)+1$, where mod and div denote the remainder and the quotient of the integer division.

In unstructured grids, the association between an element of the grid and its vertices must instead be stored in the connectivity matrix explicitly.

Code developed for structured grids can benefit from the "structure" of the grid, and, for an equal number of elements, it will normally produce a more efficient algorithm, both in terms of memory and in terms of computational time, with respect to a similar scheme on a non-structured grid. In contrast, non-structured grids offer a greater flexibility both from the viewpoint of a triangulation of domains of complex shape and for the possibility to locally refine/derefine the grid. Fig. $6.4$ (right) shows an example of a non-structured grid whose spacing has been adapted to the specific problem under exam. Such localized refinements would be more difficult to obtain using a structured type of grid.

Non-structured two-dimensional grids are generally formed by triangles, although it is possible to have quadrangular non-structured grids.

\subsection{Generation of structured grids}

The most elementary idea to generate a structured grid on a domain $\Omega$ of arbitrary shape consists in finding a regular and invertible map $\mathscr{M}$ between the square $\widehat{\Omega}=$ $[0,1] \times[0,1]$ (which we will call reference square) and $\bar{\Omega}$. Note that the map must be regular up to the boundary (a requirement that can in some cases be relaxed). We proceed by generating a uniform - say - reticulation in the reference square, then we use the mapping $\mathscr{M}$ to transform the coordinates of the vertices in $\widehat{\Omega}$ into the corresponding ones in $\bar{\Omega}$.

There are different aspects of this procedure to be considered with due care.

1. Finding the map $\mathscr{M}$ is often difficult. Moreover, such map is not unique. In general it is preferable that the latter is as regular as possible.

2. A uniform mesh of the reference square does not generally provide an optimal grid in $\Omega$. Indeed, we usually want to control the distribution of vertices in $\Omega$, and generally this can only be done by generating non-uniform grids on the reference square, whose spacing will depend both on the desired spacing in $\Omega$ and on the chosen map $\mathscr{M}$.

3. Even if the mapping is regular (for instance of class $C^{1}$ ), the elements of the grid produced in $\Omega$ are not necessarily admissible, as the latter are not the image under $\mathscr{M}$ of the corresponding elements in $\widehat{\Omega}$. For instance, if we desire piecewise bilinear $\left(\mathbb{Q}_{1}\right)$ finite elements in $\Omega$, the edges of the latter will need to be parallel to the Cartesian axes, while the image of a mesh $\mathbb{Q}_{1}$ on the reference square produces
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-147.jpg?height=282&width=648&top_left_y=870&top_left_x=130)

Fig. 6.4. (Left) $(I, J)$-Numbering of the vertices of an element in a structured grid. (Right) A non-structured triangular grid in an external region of an airfoil, adapted to improve the accuracy of the numerical solution for a given flow condition 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-148.jpg?height=234&width=678&top_left_y=114&top_left_x=118)

Fig. 6.5. Construction of a structured grid: on the left-hand side, identification of the map on the boundary; on the right-hand side, grid corresponding to a uniform partitioning of the reference square into $24 \times 24$ elements

curved edges in $\Omega$ if the mapping is nonlinear. In other words, the map is made effective only on the vertices, not on the edges, of the grid of $\widehat{\Omega}$.

An option to construct the map $\mathscr{M}$ consists in using the transfinite interpolation (10.3) that will be illustrated in Chap. 10. Such methodology is however not always easily applicable. We will therefore illustrate in the remainder a more general methodology, which we will apply to a specific example, and refer to the specific literature [TWM85, TSW99] for further examples and details.

Suppose we have a domain $\Omega$ whose boundary can be divided in four consecutive parts $\Gamma_{1}, \ldots, \Gamma_{4}$, as illustrated in Fig. $6.5$ for a particularly simple domain. Moreover, suppose we can describe such portions of $\partial \Omega$ via four parametric curves $\mathbf{g}_{1}, \ldots, \mathbf{g}_{4}$ oriented as in the figure, where the parameter $s$ varies between 0 and 1 on each curve. This construction allows us to create a bijective map between the sides of the reference square and the domain boundary. Indeed, we will associate each curve to the corresponding side of the square, as exemplified in Fig. $6.5$. We now need to understand how to extend the mapping to the whole $\widehat{\Omega}$.

Remark 6.1. Note that the curves $\mathbf{g}_{i}, i=1, \ldots, 4$, are generally not differentiable on all of $(0,1)$, but can exhibit a finite number of "corners" where $\frac{d \mathrm{~g}_{i}}{d s}$ is undefined. In Fig. $6.5$, for instance, the curve $\mathbf{g}_{2}$ is not differentiable at the "corner" marked by a small black square.

An option to construct the map $\mathscr{M}: \hat{\mathbf{x}}=(\widehat{x}, \hat{y}) \mapsto \mathbf{x}(x, y)$ consists in solving the following elliptic system in $\widehat{\Omega}$ :

$$
-\frac{\partial^{2} \mathbf{x}}{\partial \widehat{x}^{2}}-\frac{\partial^{2} \mathbf{x}}{\partial \widehat{y}^{2}}=0 \quad \text { in } \widehat{\Omega}=(0,1)^{2}
$$

with boundary conditions

$$
\begin{array}{ll}
\mathbf{x}(\widehat{x}, 0)=\mathbf{g}_{1}(\widehat{x}), \mathbf{x}(\widehat{x}, 1)=\mathbf{g}_{3}(\widehat{x}), & \widehat{x} \in(0,1), \\
\mathbf{x}(1, \hat{y})=\mathbf{g}_{2}(\widehat{y}), \mathbf{x}(0, \hat{y})=\mathbf{g}_{4}(\widehat{y}), & \widehat{y} \in(0,1) .
\end{array}
$$



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-149.jpg?height=260&width=287&top_left_y=123&top_left_x=319)

Fig. 6.6. Triangulation of a non-convex domain. Identification of the boundary map and mesh obtained by solving the elliptic problem (6.1)

The vertices of a grid in the reference square can then be transformed into the vertices of a grid in $\Omega$. Note that the solution of problem (6.1) will generally be found by using a numerical method, for instance via a finite difference (or finite element) scheme. Moreover, to abide by the geometry of the boundary of $\Omega$ suitably, it is necessary to ensure that a vertex is generated at each "edge". In Fig. $6.5$ (right) we illustrate the result of this methodology to the domain in Fig. $6.5$ (left). It can be noted that the grid corresponding to a regular partition of the reference square is not particularly satisfactory if, for instance, we want to have a higher distribution of vertices at the edge.

Moreover, the methodology described above is not applicable to non-convex domains. Indeed, let us consider Fig. $6.6$ where we show an $L$-shaped domain, with the corresponding boundary partition, and the grid obtained by solving problem (6.1) starting from a regular partition of the reference domain. It is evident that such grid is unacceptable.

To solve such problems, we can proceed in several (not mutually exclusive) ways:

- we use in $\widehat{\Omega}$ a non-uniform grid, that accounts for the geometric features of $\Omega$;

- we use a different map $\mathscr{M}$, obtained, for instance, by solving the following new differential problem instead of $(6.1)$

$$
-\alpha \frac{\partial^{2} \mathbf{x}}{\partial \widehat{x}^{2}}-\beta \frac{\partial^{2} \mathbf{x}}{\partial \widehat{y}^{2}}+\gamma \mathbf{x}=\mathbf{f} \quad \text { in } \widehat{\Omega}
$$

where $\alpha>0, \beta>0, \gamma \geq 0$ and $\mathbf{f}$ are suitable functions of $\widehat{x}$ and $\widehat{y}$. They are chosen depending on the geometry of $\Omega$ and in order to control the vertex distribution;

- we partition $\Omega$ in sub-domains that are triangulated separately. This technique is normally known as blockwise structured grid generation. If we wish the global grid to be conforming, we need to be very careful on how to distribute the number of vertices on the boundaries of the interfaces between the different sub-domains. The problem can become extremely complex when the number of sub-domains is very large. Methods of the type illustrated above are called elliptic schemes of grid generation, as they are based on the solution of elliptic equations, such as $(6.1)$ and $(6.2)$.

The interested reader is referred to the above-cited specialized literature.

\subsection{Generation of non-structured grids}

We will here consider the generation of non-structured grids with triangular elements. The two main algorithms used for this purpose are the Delaunay triangulation and the advancing front technique.

\subsubsection{Delaunay triangulation}

A triangulation of a set of $n$ points of $\mathbb{R}^{2}$ is a Delaunay triangulation if the disc circumscribed to each triangle contains no vertex (see Fig. 6.7).

A Delaunay triangulation features the following properties:

1. given a set of points, the Delaunay triangulation is unique, except for specific situations where $M$ points (with $M>3$ ) lie on a circle;

2. among all possible triangulations, the Delaunay triangulation is the one maximizing the minimum angle of the grid triangles (this is called the max-min regularity property);

3. the set composed by the union of triangles is the convex figure of minimum surface that encloses the given set of points (and is called convex hull).

The third property makes the Delaunay algorithm inapplicable to non-convex domains, at least in its original form.

However, there exists a variant, called Constrained Delaunay Triangulation ( $C D T)$, that allows to fix a priori a set of the grid edges to generate: the resulting grid necessarily associates such edges to some triangle. In particular, we can a priori impose those edges which define the boundary of the grid.
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-150.jpg?height=204&width=686&top_left_y=925&top_left_x=115)

Fig. 6.7. On the left-hand side, an example of Delaunay grid on a triangular shaped convex domain. It can be easily verified that the circle circumscribed to each triangle does not include any vertex of the grid. On the right-hand side, a detail of a grid which does not satisfy the Delaunay condition: indeed, the vertex $P$ falls inside the circle circumscribed to the triangle $K$ In order to better specify the concept of CDT, we state beforehand the following definition: given two points $P_{1}$ and $P_{2}$, we will say that these are reciprocally visible if the segment $P_{1} P_{2}$ passes through none of the boundary sides (or, more generally, the edges we want to fix a priori). A constrained Delaunay triangulation satisfies the following property: the interior of the circle circumscribed to each triangle $K$ contains no vertex visible from an internal point to $K$.

Once again, it can be proved that such triangulation is unique and satisfies the maxmin regularity property. The CDT is therefore not a proper Delaunay triangulation, as some of its triangles could contain vertices belonging to the initial set. In any case, the vertices are only the original ones specified in the set, and no further vertices are added. However, two variants are possible: the Conforming Delaunay Triangulation and the Conforming Constrained Delaunay Triangulation (or CCDT). The former is a triangulation where each triangle is a Delaunay triangulation, but each edge to be fixed can be further subdivided in sub-segments; in this case, new vertices can be added to obtain shorter segments. The additional vertices are often necessary to guarantee the max-min Delaunay property and at the same time to ensure that each prescribed side is correctly represented. The second variant represents a triangulation where the triangles are of the constrained Delaunay type. Also in this case, we can add additional vertices, and the edges to be fixed cannot be divided in smaller segments. In the latter case, however, the aim is not to guarantee that the edges are preserved, but to improve the triangles' quality.

Among the available software for the generation of Delaunay grids, or their variants, Triangle [She] allows to generate Delaunay triangulations, with the option to modulate the regularity of the resulting grids in terms of maximal and minimal angles of the triangles. The geometry is given as input to Triangle in the form of a graph, called Planar Straight Line Graph (PSLG). Such codification is written in an input file with extension . poly: the latter basically contains a list of vertices and edges, but can also include information on cavities or concavities present in the geometry.

A sample . poly file is reported below.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-151.jpg?height=369&width=573&top_left_y=848&top_left_x=91)



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-152.jpg?height=158&width=362&top_left_y=116&top_left_x=96)

The example above illustrates a geometry representing a square with a square hole. The first part of the file lists the vertices, while the second one defines the sides to fix. The first line declares that eight vertices are going to follow, that the spatial dimension of the grid is two (we are in $\mathbb{R}^{2}$ ), that no other attribute is associated to the vertices and that a boundary marker is defined on each point. The attributes represent possible physical properties relating to the mesh nodes, such as conductibility and viscosity values, etc. The boundary markers are integer-valued flags which can be used within a computational code to assign suitable boundary conditions at different vertices. The following lines display the eight vertices, with their abscissae and ordinates, followed by the boundary marker value, zero in this case. The first line of the second part declares that there are five sides ensuing, and that on each of them a value will be specified for the boundary marker. Then, five boundary sides follow one another, specified by their respective endpoints, and by the value of the boundary marker. In the final section of the file, a hole is defined by specifying the center coordinates, in the last line, preceded by the progressive numbering (in this case, limited to 1 ) of the holes.

The constrained Delaunay grid associated to this geometry, say box.poly, is obtained via the command

The parameter - p declares that the input file is a .poly, while the option - c prevents that the concavities are removed, as would normally happen without it. De facto, this option forces the triangulation of the convex hull of the PSLG graph. The result will be the creation of three files, box. 1. poly, box. 1. node and box.1. ele. The first file contains the description of the sides of the produced triangulation, the second one contains the node description, and the latter defines the connectivity of the generated elements. For the sake of conciseness, we will not describe the format of these three files in detail. Finally, we point out that the numerical value, 1 in this example, that separates the name of these three files from their respective extensions, plays the role of an iteration counter: Triangle can indeed successively refine or modify the triangulations produced time after time. The resulting triangulation is depicted in Fig. $6.8$.

A software attached to Triangle, called Show Me, allows to visualize the outputs of Triangle. For instance, Fig $6.8$ (left) is obtained via the command

\section{showme box}

To obtain a constrained conforming triangulation we must specify the command triangle with other parameters, such as $-\mathrm{q},-\mathrm{a}$ or $-\mathrm{u}$. The first one imposes a constraint on the minimum angle, the second one fixes a maximum value for the surface 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-153.jpg?height=242&width=516&top_left_y=113&top_left_x=199)

Fig. 6.8. Delaunay triangulation of a square with a square hole: CDT on the left-hand side, CCDT on the right-hand side

of the triangles, while the third one forces the dimension of the triangles, typically through an external function which the user must provide. For example, via the command

triangle - pcq20 box

we obtain the constrained conforming Delaunay triangulation reported in Fig. $6.8$ (right), characterized by a minimum angle of $20^{\circ}$. Finally, the conforming Delaunay triangulation is obtained by further specifying the option -D. A more complex example is represented in Fig. 6.9. The command used

triangle -pca0.01q30 naca

fixes the minimum angle to $30^{\circ}$ and the maximum surface of the generated triangles to $0.01$. The initial PSLG file naca.poly describes the geometry via 65 vertices, as many sides and one cavity. The final mesh consists of 711 vertices, 1283 elements and 137 edges on the boundary.

We refer to the wide on-line documentation and to the detailed help of Triangle for several further usage options of the software.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-153.jpg?height=290&width=435&top_left_y=905&top_left_x=239)

Fig. 6.9. Delaunay triangulation of a naca 4415 airfoil Returning to the properties of Delaunay grids, the Delaunay triangulation does not allow to control the aspect ratio (maximum over minimum edge) of the generated elements, exactly because of the above-mentioned max-min property. On the other hand, in some situations it can be useful to generate "stretched" triangles in a given direction, for instance if we want to represent properly a boundary layer. To this end, the algorithm called generalized Delaunay triangulation has been developed, where the condition on the circumscribed triangle is replaced by an analogous condition on the ellipse circumscribed to the triangle under exam. In this way, by suitably ruling the length and orientation of the axes of each ellipse, we can generate elements stretched in the desired direction.

The most currently used algorithms for the generation of Delaunay grids are incremental, i.e. they generate a sequence of Delaunay grids by adding a vertex at a time. Hence, it is necessary to find procedures providing the new vertices in accordance with the desired grid spacing, and stopping such procedure as soon as the grid generated this way results to be unsatisfactory. For further details, [GB98] and [TSW99, Chap. 16 ] can be consulted, among others. A detailed description of the geometric properties of the constrained Delaunay triangulation, both for domains of $\mathbb{R}^{2}$ and of $\mathbb{R}^{3}$, can be found in [BE92].

\subsubsection{Advancing front technique}

We roughly described another widely used technique used for the generation of nonstructured grids, the advancing front technique. A necessary ingredient is the knowledge of the desired spacing to be generated for the grid elements. Let us then suppose that a spacing function $\mathscr{H}$, defined on $\bar{\Omega}$, provides for each point $P$ of $\bar{\Omega}$ the dimensions of the grid desired in that point, for instance, through the diameter $h_{K}$ of the elements that must be generated in a neighborhood of $P$. If we want to control the shape aspect of the generated elements, $\mathscr{H}$ will have a more complex shape. In fact, it will be a positive definite symmetric tensor, i.e. $\mathscr{H}: \Omega \rightarrow \mathbb{R}^{2 \times 2}$ such that, for each point $P$ of the domain, the (perpendicular) eigenvectors of $\mathscr{H}$ denote the direction of maximum and minimum stretching of the triangles that will need to be generated in the neighborhood of $P$, while the eigenvalues (more precisely, the square roots of the eigenvalue inverses), characterize the two corresponding spacings (see [GB98]). In the remainder, we will only consider the case where $\mathscr{H}$ is a scalar function.

The first operation to perform is to generate the vertices along the domain boundary. Let us suppose that $\partial \Omega$ is described as the union of parametric curves $\mathbf{g}_{i}(s)$, $i=1, \ldots N$, for instance splines or polygonal splits. For simplicity, we assume that, for each curve, the parameter $s$ varies between 0 and 1 . If we wish to generate $N_{i}+1$ vertices along the curve $\mathbf{g}_{i}$ it is sufficient to create a vertex for all the values of $s$ for which the function

$$
f_{i}(s)=\int_{0}^{s} \mathscr{H}^{-1}\left(\mathbf{g}_{i}(\tau)\right)\left|\frac{d \mathbf{g}_{i}}{d s}(\tau)\right| d \tau
$$


![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-155.jpg?height=280&width=616&top_left_y=110&top_left_x=146)

Fig. 6.10. Advancement of the front. The previously triangulated part of the domain has been shaded

takes integer values. More precisely, the curvilinear coordinates $s_{i}^{(j)}$ of the nodes to generate along the curve $\mathbf{g}_{i}$ satisfy the relations

$$
f_{i}\left(s_{i}^{(j)}\right)=j, \quad j=0, \cdots, N_{i} \text { with the constraints } s_{i}^{(0)}=0, s_{i}^{\left(N_{i}\right)}=1
$$

The procedure is similar to the one described in Sect. 6.1. Note that the term $\left|\frac{d \mathrm{~g}_{i}}{d s}\right|$ accounts for the intrinsic metric of the curve.

This being done, the advancing front process can start. The latter is described by a data structure that contains the list of the sides defining the boundary between the already triangulated portion of $\Omega$ and the one yet to be. At the beginning of the process, the front contains the boundary sides.

During the process of grid generation, each side of the front is available to create a new element, which is constructed by connecting the chosen side with a new or previously existing vertex of the grid. The choice whether to use an existing vertex or to create a new one depends on several factors, among which the compatibility between the dimension and the shape of the element that would be generated and the ones provided by the spacing function $\mathscr{H}$. Moreover, the new element must not intersect any side of the front.

Once the new element has been generated, its new sides will be "added" to the front so that the latter describes the new boundary between the triangulated and nontriangulated part, while the initial side is removed from the data list. In this way, during the generation process the front will progress from the already triangulated zones toward the zone yet to be triangulated (see Fig. 6.10).

The general advancing front algorithm hence consists of the following steps:

1. define the boundary of the domain to be triangulated;

2. initialize the front by a piecewise linear curve conforming to the boundary;

3. choose the side to be removed from the front using some criterion (typically the choice of the shortest side provides good quality meshes);

4. for the side, say $A B$, chosen this way: a) select the "candidate" vertex $C$, i.e. the point inside the domain whose distance from $A B$ is prescribed by the desired spacing function $\mathscr{H}$;

b) seek an already existing point $C^{\prime}$ on the front in a suitable neighbourhood of $C$. If the search is successful, $C^{\prime}$ becomes the new candidate point $C$. Continue the search;

c) establish whether the triangle $A B C$ intersects some other side of the front. If so, select a new candidate point from the front and start back from step 4.b);

5. add the new point $C$, the new edges and the new triangle $A B C$ to the corresponding lists;

6. erase the edge $A B$ from the front and add the new edges;

7. if the front is non-empty, continue from point $3$.

It is obvious that if we wish the computational cost to be a linear function of the number of generated elements, it will be necessary to make the above-described operations as independent as possible from the number of dimensions of the grid we are generating and, in particular, from the dimensions of the advancing front. Such an objective is not trivial, especially because operations such as the control of the intersection of a new triangle, or the search for the vertices of the front close to a generic point, span the whole front. We refer for this to the specialized literature, and in particular to Chaps. 14 and 17 of [TSW99].

As previously pointed out in the algorithm description, the quality of the generated grid depends on the procedure of choice of the front edge on which to generate the new triangle. In particular, a frequently adopted technique consists in choosing the side with the smallest length: intuitively, this also allows to satisfy non-uniform spacing requirements, without risking that the zones where a more dense node distributions is required are overwritten by triangles associated to a coarser spacing. An example of mesh obtained through such technique, in correspondence of the choice $\mathscr{H}\left(x_{1}, x_{2}\right)=$ $e^{4 \sin \left(8 \pi x_{1}\right)} e^{-2 x_{2}}$, is represented in Fig. 6.11.

By implementing the suitable tricks and data structures, the advancing algorithm provides a grid whose spacing is coherent with the requested one, with computational times almost proportional to the number of generated elements.

The advancing front technique can also be used for the generation of quadrangular grids.

\subsection{Regularization techniques}

Once the grid has been generated, a post-processing can be necessary in order to improve its regularity. Some methods allow to transform the grid via operations that improve the triangles' shape. In particular, we will examine regularization techniques that modify either the topological features (by diagonal exchange) or the geometrical features (by node displacement). 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-157.jpg?height=358&width=365&top_left_y=119&top_left_x=271)

Fig. 6.11. Advancing front technique. Example of non-uniform spacing

a)

b)

Fig. 6.12. The two configurations obtained via diagonal exchange in the convex quadrilateral formed by two adjacent elements. The two configurations are compared based on an optimality criterion

\subsubsection{Diagonal swap}

The exchange of diagonals is a technique allowing to modify the topology of the grid without changing the position and number of its vertices. Such technique is based on the fact that a quadrilateral can be subdivided into a couple of triangles sharing a common side in two different ways (see Fig. 6.12).

In general, diagonal exchange is used to improve the quality of non-structured grids by following a given optimality criterion. Suppose, for example, the goal is to avoid angles that are too big, as when the sum of two opposite angles is bigger than $\pi$. Exchanging the diagonals would in this case solve the problem.

A general scheme for a possible diagonal exchange algorithm is obtained by defining the optimality criterion at the element level, under the form of an appropriate non- negative function $S: K \rightarrow \mathbb{R}^{+} \cup\{0\}$ that takes value 0 when $K$ has the "optimal" shape and dimension. For instance, we can use

$$
S(K)=\left|\frac{|K|}{\sum_{i=1}^{3}\left|e_{i}^{K}\right|^{2}}-\frac{\sqrt{3}}{12}\right|
$$

where $|K|$ denotes the size of $K, e_{i}^{K}$ represents a generic side of $K$ and $\left|e_{i}^{K}\right|$ is its length. Using this function, we privilege triangles that are close to being equilateral, for which $S(K)=0$. Thus, we will generally obtain a grid as regular as possible, which does not take the spacing into account. With reference to Fig. $6.12$, the algorithm will proceed as follows:

1. Cycle 0 : set the exchanged side counter to zero: swap $=0$;

2. span all internal sides $e$ of the current mesh;

3. if the two triangles adjacent to $e$ form a convex quadrilateral:

a) compute $G=S^{2}\left(K_{1}\right)+S^{2}\left(K_{2}\right)-\left[S^{2}\left(K_{1}^{*}\right)+S^{2}\left(K_{2}^{*}\right)\right]$

b) if $G \geq \tau$, with $\tau>0$ a predetermined, then execute the diagonal exchange (hence modify the current grid) and set $\operatorname{swap}=$ swap $+1$;

4. if $s w a p>0$ start back from Cycle 0 . Otherwise, the procedure terminates.

It can be easily verified that this algorithm necessarily terminates in a finite number of steps because, for each diagonal exchange, the positive quantity $\sum_{K} S^{2}(K)$, where the sum is extended to all the triangles of the current grid, is reduced by the finite quantity $G$ (note that, although the grid is modified, at each diagonal exchange the number of elements and sides remains unchanged).

Remark 6.2. It is not always a good option to construct the optimality function $S$ at the element level. For instance, based on the available data structures, $S$ can also be associated to the nodes or to the sides of the grid.

The diagonal exchange technique is also the basis for a widely used algorithm (the Lawson algorithm) for the Delaunay triangulation. It can indeed be proved that starting from any triangulation of a convex domain, the corresponding Delaunay triangulation (which, we recall, is unique) can be obtained through a finite number of diagonal exchanges. Moreover, the maximum number of necessary swaps for this purpose can be determined a priori and is a function of the number of grid vertices. The technique (and convergence results) can be extended to constrained Delaunay triangulations, through a suitable modification of the algorithm. We refer to the specialized literature, for instance [GB98], for the details.

\subsubsection{Node displacement}

Another method to improve the quality of the grid consists in moving its points without modifying its topology. Let us consider an internal vertex $P$ and the polygon $\mathscr{K}_{P}$ constituted by the union of the grid elements containing it. The set $\mathscr{K}_{P}$ is often called "patch" associated to $P$ and has been considered in Sect. 4.6. For an example, see Fig. 4.20, right. A regularization technique, called Laplacian regularization, or barycentrization, consists in moving $P$ to the center of gravity of $\mathscr{K}_{P}$, that is in computing its new position $\mathbf{x}_{P}$ as follows:

$$
\mathbf{x}_{P}=\left|\mathscr{K}_{P}\right|^{-1} \int_{\mathscr{K}_{P}} \mathbf{x} d \mathbf{x}
$$

(see Fig. 6.13). This procedure will obviously be iterated on all the internal vertices of the mesh and repeated several times. In case of convergence, the final grid is the one minimizing the quantity

$$
\sum_{P} \int_{\mathscr{K}_{P}}\left(\mathbf{x}_{P}-\mathbf{x}\right)^{2} d \mathbf{x}
$$

where the sum is extended to all the internal vertices of the grid. The name of such procedure derives from the known property of harmonic functions (those in the kernel of the Laplacian) which take in a point of the domain a value equal to that of the average on a closed curve containing the point.

The final grid will generally depend on the order with which the vertices are displaced, one after the other. Moreover, note that this procedure can provide an unac-

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-159.jpg?height=216&width=509&top_left_y=580&top_left_x=202)

Fig. 6.13. Displacement of a point to the center of gravity of the convex polygon $\mathscr{K}_{P}$ formed by the union of the elements containing $P$

ceptable grid if $\mathscr{K}_{P}$ is a concave polygon, as $\mathbf{x}_{P}$ can fall out of the polygon. We present an extension of the procedure that is suitable for generic patches of elements. Consider Fig. 6.14, which shows a concave patch $\mathscr{K}_{P}$. We define $\mathscr{C}_{P}$ as the locus of points of $\mathscr{K}_{P}$ "visible" to all boundary points of $\mathscr{K}_{P}$, that is $\mathscr{C}_{P}=\left\{A \in \mathscr{K}_{P}: A B \subset \mathscr{K}_{P}, \forall B \in \partial \mathscr{K}_{P}\right\}$; note that $\mathscr{C}_{P}$ is always convex. The modification of the regularization algorithm consists in placing $P$ not in the center of gravity of $\mathscr{K}_{P}$, but in that of $\mathscr{C}_{P}$, as illustrated in Fig. 6.14. Clearly, in the case of convex patches, we have $\mathscr{C}_{P}=\mathscr{K}_{P}$. The set $\mathscr{C}_{P}$ can be constructed in a computationally efficient manner by using suitable algorithms, whose description is beyond the scope of this book.

Another option consists in displacing the vertex to the center of gravity of the boundary of $\mathscr{K}_{P}$ (or $\mathscr{C}_{P}$ in the case of concave patches), i.e. in setting

$$
\mathbf{x}_{P}=\left|\partial \mathscr{K}_{P}\right|^{-1} \int_{\partial \mathscr{K}_{P}} \mathbf{x} d \mathbf{x}
$$


![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-160.jpg?height=218&width=498&top_left_y=110&top_left_x=210)

Fig. 6.14. Modification of the Laplacian regularization algorithm for concave patches. On the left-hand side, the initial patch; on the right-hand side, the modification due to regularization. We have shaded the concave polygon $\mathscr{C}_{P}$

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-160.jpg?height=277&width=280&top_left_y=431&top_left_x=149)
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-160.jpg?height=278&width=614&top_left_y=430&top_left_x=148)

This is equivalent to minimizing the square of the distance between the vertex $P$ and the sides forming the patch boundary.

A further technique, often found in the literature, consists in displacing each internal vertex to the center of gravity of the vertices belonging to the associated patch, i.e. in computing the new position of each internal vertex $P$ via

$$
\mathbf{x}_{P}=\left(\sum_{N \in \mathscr{K}_{P} \atop N \neq P} \mathbf{x}_{N}\right) /\left(\sum_{N \in \mathscr{\mathscr { X }}_{P} \atop N \neq P} 1\right)
$$

where the sum is extended to all the vertices $N$ belonging to the patch. Despite being the simplest methodology, the latter often yields bad results, in particular if the distribution of vertices inside the patch is very irregular. Moreover, it is more difficult to extend it to concave patches. Thus the two previous procedures are preferable. In Fig. $6.15$ we present an example of successive application of both of the abovedescribed regularization techniques. Note that the regularization algorithms presented here tend to uniform the grid, and therefore to prevent its thickenings or coarsenings due for instance to grid adaptivity procedures such as the ones described in Chap. 4. However, it is possible to modify them to account for a non-uniform spacing. For instance, a weighted barycentrization can be used, i.e. by setting

$$
\mathbf{x}_{P}=\left(\int_{\mathscr{K}_{P}} \mu(\mathbf{x}) d \mathbf{x}\right)^{-1} \int_{\mathscr{K}_{P}} \mu(\mathbf{x}) \mathbf{x} d \mathbf{x}
$$

where the strictly positive weight function $\mu$ depends on the grid spacing function. In the case of non-uniform spacing, $\mu$ will take larger values in the zones where the grid must be finer. When choosing for instance $\mu=\mathscr{H}^{-1}$ the resulting grid (approximately) minimizes

$$
\sum_{P} \int_{\mathscr{K}_{P}}\left[\mathscr{H}^{-1}(\mathbf{x})\left(\mathbf{x}_{P}-\mathbf{x}\right)\right]^{2} d \mathbf{x}
$$

where the sum is extended to the internal vertices.

Also concerning the diagonal exchange procedure we can take the spacing into account when evaluating the "optimal" configuration, for instance by suitably changing the definition of function $S(K)$ in $(6.3)$. Chapter 7

Algorithms for the solution of linear systems

This chapter serves as a quick and elementary introduction of some of the basic algorithms that are used to solve a system of linear algebraic equations. For a more thorough presentation we advise the reader to refer to, e.g., [QSS07, Chaps. 3 and 4$]$, [Saa96] and [vdV03].

A system of $m$ linear equations in $n$ unknowns is a set of algebraic relations of the form

$$
\sum_{j=1}^{n} a_{i j} x_{j}=b_{i}, \quad i=1, \ldots, m
$$

$x_{j}$ being the unknowns, $a_{i j}$ the system's coefficients and $b_{i}$ given numbers. System (7.1) will more commonly be written in matrix form

$$
\mathrm{Ax}=\mathbf{b}
$$

having denoted by $\mathrm{A}=\left(a_{i j}\right) \in \mathbb{R}^{m \times n}$ the coefficient matrix, $\mathbf{b}=\left(b_{i}\right) \in \mathbb{R}^{m}$ being the right hand side vector and $\mathbf{x}=\left(x_{i}\right) \in \mathbb{R}^{n}$ the unknown vector. We call solution of $(7.2)$ any $n$-tuple of values $x_{i}$ verifying $(7.1)$.

In the following sections we recall some numerical techniques for the solution of (7.2) in the case where $m=n$; we will obviously suppose that $\mathrm{A}$ is non-singular, i.e. that $\operatorname{det}(\mathrm{A}) \neq 0$. Numerical methods are called direct if they lead to the solution of the system in a finite number of operations, or iterative if they require a (theoretically) infinite number.

\subsection{Direct methods}

The solution of a linear system can be found through the Gauss elimination method (GEM), where the initial system $A \mathbf{x}=\mathbf{b}$ is reduced in $n$ steps to an equivalent system (i.e. having the same solution) of the form $\mathrm{A}^{(n)} \mathbf{x}=\mathbf{b}^{(n)}$, where $\mathrm{A}^{(n)}=\mathrm{U}$ is a nonsingular upper triangular matrix and $\mathbf{b}^{(n)}$ is a new source term. It will be possible to solve the latter system with a computational cost of the order of $n^{2}$ operations, through the following backward substitution algorithm:

$$
\begin{aligned}
x_{n} &=\frac{b_{n}^{(n)}}{u_{n n}} \\
x_{i} &=\frac{1}{u_{i i}}\left(b_{i}^{(n)}-\sum_{j=i+1}^{n} u_{i j} x_{j}\right), \quad i=n-1, \ldots, 1
\end{aligned}
$$

Denoting by $\mathrm{A}^{(1)} \mathbf{x}=\mathbf{b}^{(1)}$ the original system, the kth step of GEM is achieved via the following formulae:

$$
\begin{aligned}
&m_{i k}=\frac{a_{i k}^{(k)}}{a_{k k}^{(k)}}, \quad i=k+1, \ldots, n \\
&a_{i j}^{(k+1)}=a_{i j}^{(k)}-m_{i k} a_{k j}^{(k)}, \quad i, j=k+1, \ldots, n \\
&b_{i}^{(k+1)}=b_{i}^{(k)}-m_{i k} b_{k}^{(k)}, \quad i=k+1, \ldots, n .
\end{aligned}
$$

We note that in this way, the elements $a_{i j}^{(k+1)}$ with $j=k$ and $i=k+1, \ldots, n$ are null. The elements $m_{i k}$ are called multipliers, while the denominators $a_{k k}^{(k)}$ are named pivotal elements. The GEM can obviously be achieved only if all the pivotal elements are non null. This happens, for instance, for symmetric positive definite matrices and for strict diagonal dominant ones. In general, it will be necessary to resort to the pivoting method, i.e. to the swapping of rows (and/or columns) of $\mathrm{A}^{(k)}$, in order to ensure that the element $a_{k k}^{(k)}$ be non-null.

To complete the Gauss eliminations, we need $2(n-1) n(n+1) / 3+n(n-1)$ flops, to which we must add $n^{2}$ flops to solve the upper triangular system $U \mathbf{x}=\mathbf{b}^{(n)}$ via the backward substitution method. Hence, about $\left(2 n^{3} / 3+2 n^{2}\right)$ flops are needed to solve the linear system via the GEM. More simply, by neglecting lower order terms in $n$, it can be said that the Gaussian elimination process requires $2 n^{3} / 3$ flops.

The GEM is equivalent to factorizing the matrix $\mathrm{A}$, i.e. to rewriting $\mathrm{A}$ as the product $\mathrm{LU}$ of two matrices. The matrix $\mathrm{U}$, upper triangular, coincides with the matrix $\mathrm{A}^{(n)}$ obtained at the end of the elimination process. The matrix $\mathrm{L}$ is lower triangular, its diagonal elements are equal to 1 while the ones located in the remaining lower triangular portion are equal to the multipliers.

Once the matrices $L$ and $U$ are known, the solution of the initial linear system simply involves the (successive) solution of the two triangular systems

$$
\mathbf{L} \mathbf{y}=\mathbf{b}, \quad \mathbf{U} \mathbf{x}=\mathbf{y}
$$

Obviously, the computational cost of the factorization process is the same as the one required by the GEM. The advantages of such a reinterpretation are evident: as $\mathrm{L}$ and $\mathrm{U}$ depend on A only, and not on the known term, the same factorization can be used to solve different linear systems having the same matrix A, but a variable known term b (think for instance of the discretization of a linear parabolic problem by an implicit method where at each time step it is necessary to solve a system with the same matrix all the time, but with a different constant term). Consequently, as the computational cost is concentrated in the elimination procedure, we have in this way a considerable reduction in the number of operations when we want to solve several linear systems having the same matrix.

If $\mathrm{A}$ is a positive-definite, symmetric matrix, the LU factorization can be conveniently specialized. Indeed, there exists only one upper triangular matrix $\mathrm{H}$ with positive elements on the diagonal such that

$$
\mathrm{A}=\mathrm{H}^{\mathrm{T}} \mathrm{H}
$$

Equation (7.5) is the so-called Cholesky factorization. The elements $h_{i j}$ of $\mathrm{H}^{T}$ are given by the following formulae: $h_{11}=\sqrt{a_{11}}$ and, for $i=2, \ldots, n$ :

$$
\begin{aligned}
&h_{i j}=\left(a_{i j}-\sum_{k=1}^{j-1} h_{i k} h_{j k}\right) / h_{j j}, \quad j=1, \ldots, i-1 \\
&h_{i i}=\left(a_{i i}-\sum_{k=1}^{i-1} h_{i k}^{2}\right)^{1 / 2}
\end{aligned}
$$

This algorithm only requires about $n^{3} / 3$ flops, i.e. it saves about twice the computing time of the LU factorization and about half the memory.

Let us now consider the particular case of a linear system with non-singular tridiagonal matrix A of the form

$$
\mathrm{A}=\left[\begin{array}{cccc}
a_{1} & c_{1} & & 0 \\
b_{2} & a_{2} & \ddots & \\
& \ddots & & c_{n-1} \\
0 & & b_{n} & a_{n}
\end{array}\right]
$$

In this case, the matrices $\mathrm{L}$ and $\mathrm{U}$ of the LU factorization of $\mathrm{A}$ are bidiagonal matrices of the type

$$
\mathrm{L}=\left[\begin{array}{cccc}
1 & & & 0 \\
\beta_{2} & 1 & & \\
& \ddots & \ddots & \\
0 & & \beta_{n} & 1
\end{array}\right], \quad \mathrm{U}=\left[\begin{array}{cccc}
\alpha_{1} & c_{1} & & 0 \\
& \alpha_{2} & \ddots & \\
& & \ddots & c_{n-1} \\
0 & & & \alpha_{n}
\end{array}\right]
$$

The unknown coefficients $\alpha_{i}$ and $\beta_{i}$ can be easily computed by the following equations:

$$
\alpha_{1}=a_{1}, \quad \beta_{i}=\frac{b_{i}}{\alpha_{i-1}}, \quad \alpha_{i}=a_{i}-\beta_{i} c_{i-1}, i=2, \ldots, n
$$

This algorithm is named Thomas' algorithm and can be seen as a particular kind of LU factorization without pivoting.

\subsection{Iterative methods}

Iterative methods aim at constructing the solution $\mathbf{x}$ of a linear system as the limit of a sequence $\left\{\mathbf{x}^{(n)}\right\}$ of vectors. To obtain the single elements of the sequence, computing the residue $\mathbf{r}^{(n)}=\mathbf{b}-A \mathbf{x}^{(n)}$ of the system is required. In the case where the matrix is full and of order $n$, the computational cost of an iterative method is therefore of the order of $n^{2}$ operations per iteration. Such cost must be compared with the approximately $2 n^{3} / 3$ operations required by a direct method. Consequently, iterative methods are competitive with direct methods only if the number of necessary iterations to reach convergence (within a given tolerance) is independent of $n$ or depends on $n$ in a sub-linear way.

Other considerations in the choice between an iterative method and a direct one intervene as soon as the matrix is sparse.

\subsubsection{Classical iterative methods}

A general strategy to construct iterative methods is based on an additive decomposition, called splitting, starting from a matrix $\mathrm{A}$ of the form $\mathrm{A}=\mathrm{P}-\mathrm{N}$, where $\mathrm{P}$ and $\mathrm{N}$ are two suitable matrices and $\mathrm{P}$ is non-singular. For reasons which will become evident in the remainder, $\mathrm{P}$ is also called preconditioning matrix or preconditioner.

Precisely, given $\mathbf{x}^{(0)}$, we obtain $\mathbf{x}^{(k)}$ for $k \geq 1$ by solving the new systems

$$
\mathbf{P x}^{(k+1)}=\mathbf{N} \mathbf{x}^{(k)}+\mathbf{b}, \quad k \geq 0
$$

or, equivalently,

$$
\mathbf{x}^{(k+1)}=\mathbf{B} \mathbf{x}^{(k)}+\mathbf{P}^{-1} \mathbf{b}, \quad k \geq 0
$$

having denoted by $\mathrm{B}=\mathrm{P}^{-1} \mathrm{~N}$ the iteration matrix.

We are interested in convergent iterative methods, i.e. such that $\lim _{k \rightarrow \infty} \mathbf{e}^{(k)}=\mathbf{0}$ for each choice of the initial vector $\mathbf{x}^{(0)}$, having denoted by $\mathbf{e}^{(k)}=\mathbf{x}^{(k)}-\mathbf{x}$ the error. With a recursive argument we find

$$
\mathbf{e}^{(k)}=\mathrm{B}^{k} \mathbf{e}^{(0)}, \quad \forall k=0,1, \ldots
$$

so can conclude that an iterative method of the form (7.6) is convergent if and only if $\rho(\mathrm{B})<1, \rho(\mathrm{B})$ being the spectral radius of the iteration matrix $\mathrm{B}$, i.e. the maximum modulus of the eigenvalues of $\mathrm{B}$. Equation (7.6) can also be formulated in the form

$$
\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}+\mathbf{P}^{-1} \mathbf{r}^{(k)}
$$

having denoted by

$$
\mathbf{r}^{(k)}=\mathbf{b}-\mathrm{A} \mathbf{x}^{(k)}
$$

the residue at step $k$. Equation (7.9) thus expresses the fact that to update the solution at step $k+1$, it is necessary to solve a linear system with matrix $\mathrm{P}$. Hence, beside being non-singular, P must be invertible at a low computational cost if we want to prevent the overall cost of the scheme from increasing excessively (obviously, in the limit case where $\mathrm{P}$ is equal to $\mathrm{A}$ and $\mathrm{N}=0$, method $(7.9)$ converges in only one iteration, but at the cost of a direct method).

Let us now see how to accelerate the convergence of the iterative methods $(7.6)$ by exploiting the latter form. We denote by

$$
\mathrm{R}_{\mathrm{P}}=\mathrm{I}-\mathrm{P}^{-1} \mathrm{~A}
$$

the iteration matrix associated to method (7.9). Equation (7.9) can be generalized by introducing a suitable relaxation (or acceleration) parameter $\alpha$. In this way, we obtain the stationary Richardson methods (or, simply, Richardson methods), of the form

$$
\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}+\alpha \mathrm{P}^{-1} \mathbf{r}^{(k)}, \quad k \geq 0
$$

More generally, supposing $\alpha$ to be dependent on the iteration index, we obtain the non-stationary Richardson methods given by

$$
\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}+\alpha_{k} \mathrm{P}^{-1} \mathbf{r}^{(k)}, \quad k \geq 0
$$

If we set $\alpha=1$, we can recover two classical iterative methods: the Jacobi method if $\mathrm{P}=\mathrm{D}(\mathrm{A})$ (the diagonal part of $A$ ), the Gauss-Seidel method if $\mathrm{P}=\mathrm{L}(\mathrm{A})$ (the lower triangular part of $A$ ).

The iteration matrix at step $\mathrm{k}$ for such methods is given by

$$
\mathrm{R}\left(\alpha_{k}\right)=\mathrm{I}-\alpha_{k} \mathrm{P}^{-1} \mathrm{~A}
$$

(note that the latter depends on $k$ ). In the case where $\mathrm{P}=\mathrm{I}$, the methods under exam will be called non preconditioned.

We can rewrite (7.12) (and therefore also (7.11)) in a form of greater computational interest. Indeed, having set $\mathbf{z}^{(k)}=\mathrm{P}^{-1} \mathbf{r}^{(k)}$ (the so-called preconditioned residue), we have that $\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}+\alpha_{k} \mathbf{z}^{(k)}$ and $\mathbf{r}^{(k+1)}=\mathbf{b}-\mathrm{A} \mathbf{x}^{(k+1)}=\mathbf{r}^{(k)}-\alpha_{k} \mathrm{~A} \mathbf{z}^{(k)}$. To summarize, a non-stationary Richardson method at step $k+1$ requires the following operations:

$$
\text { solving the linear system } \mathrm{P} \mathbf{z}^{(k)}=\mathbf{r}^{(k)} \text {, }
$$

computing the acceleration parameter $\alpha_{k}$,

$$
\text { updating the solution } \mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}+\alpha_{k} \mathbf{z}^{(k)} \text {, }
$$

$$
\text { updating the residue } \mathbf{r}^{(k+1)}=\mathbf{r}^{(k)}-\alpha_{k} \mathbf{A} \mathbf{z}^{(k)} \text {. }
$$

As far as the convergence of the stationary Richardson method (for which $\alpha_{k}=\alpha$, for each $k \geq 0$ ) is concerned, the following result holds:

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-167.jpg?height=473&width=722&top_left_y=401&top_left_x=96)

If $\mathrm{P}$ and $\mathrm{A}$ are both symmetric and positive definite, it can be proved that the Richardson method converges monotonically with respect to the vector norms $\|\cdot\|_{2}$ and $\|\cdot\|_{\mathrm{A}}$. We recall that $\|\mathbf{v}\|_{2}=\left(\sum_{i=1}^{n} v_{i}^{2}\right)^{1 / 2}$ and $\|\mathbf{v}\|_{\mathrm{A}}=\left(\sum_{i, j=1}^{n} v_{i} a_{i j} v_{j}\right)^{1 / 2}$

In this case, thanks to $(7.16)$, we can relate $\rho_{o p t}$ with the condition number introduced in Sect. $4.5 .2$ in the following way:

$$
\rho_{o p t}=\frac{K_{2}\left(\mathrm{P}^{-1} \mathrm{~A}\right)-1}{K_{2}\left(\mathrm{P}^{-1} \mathrm{~A}\right)+1}, \quad \alpha_{o p t}=\frac{2\left\|\mathrm{~A}^{-1} \mathrm{P}\right\|_{2}}{K_{2}\left(\mathrm{P}^{-1} \mathrm{~A}\right)+1}
$$

The importance of the choice of the preconditioner $\mathrm{P}$ in a Richardson method is therefore clear. We refer to Chap. 4 of [QSS07] for some examples of preconditioners. 

\subsubsection{Gradient and conjugate gradient methods}

The optimal expression of the acceleration parameter $\alpha$, indicated in (7.15), turns out to be of little practical utility, as it requires knowing the maximum and minimum eigenvalues of the matrix $\mathrm{P}^{-1} \mathrm{~A}$. In the particular case of positive definite symmetric matrices, it is however possible to evaluate the optimal acceleration parameter in a dynamic way, that is as a function of quantities computed by the method itself at step $k$, as we show below.

First of all, we observe that in the case where $\mathrm{A}$ is a symmetric positive definite matrix, solving system (7.2) is equivalent to finding the minimum $\mathbf{x} \in \mathbb{R}^{n}$ of the quadratic form

$$
\Phi(\mathbf{y})=\frac{1}{2} \mathbf{y}^{T} \mathrm{~A} \mathbf{y}-\mathbf{y}^{T} \mathbf{b}
$$

called energy of system (7.2).

The problem is thus reduced to determining the minimum point $\mathbf{x}$ of $\Phi$ starting from a point $\mathbf{x}^{(0)} \in \mathbb{R}^{n}$ and, consequently, choosing suitable directions along which to move to approach the solution $\mathbf{x}$ as quickly as possible. The optimal direction, joining $\mathbf{x}^{(0)}$ and $\mathbf{x}$, is obviously unknown a priori: we will therefore have to move from $\mathbf{x}^{(0)}$ along another direction $\mathbf{d}^{(0)}$ and fix a new point $\mathbf{x}^{(1)}$ on the latter, then repeat the procedure until convergence.

At the generic step $k$ we will then determine $\mathbf{x}^{(k+1)}$ as

$$
\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}+\alpha_{k} \mathbf{d}^{(k)}
$$

$\alpha_{k}$ being the value fixing the length of the step along $\mathbf{d}^{(k)}$. The most natural idea, consisting in taking as downhill direction that of the greatest increase of $\Phi$, given by $\mathbf{r}^{(k)}=-\nabla \Phi\left(\mathbf{x}^{(k)}\right)$, leads to the gradient or steepest descent method.

The latter leads to the following algorithm: given $\mathbf{x}^{(0)} \in \mathbb{R}^{n}$, and having set $\mathbf{r}^{(0)}=$ $\mathbf{b}-\mathbf{A} \mathbf{x}^{(0)}$, for $k=0,1, \ldots$ until convergence, we compute

$$
\begin{aligned}
&\alpha_{k}=\frac{\mathbf{r}^{(k)^{T}} \mathbf{r}^{(k)}}{\mathbf{r}^{(k)^{T}} \mathrm{Ar}^{(k)}} \\
&\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}+\alpha_{k} \mathbf{r}^{(k)} \\
&\mathbf{r}^{(k+1)}=\mathbf{r}^{(k)}-\alpha_{k} \mathrm{~A} \mathbf{r}^{(k)}
\end{aligned}
$$

Its preconditioned version takes the following form: given $\mathbf{x}^{(0)} \in \mathbb{R}^{n}$, and having set $\mathbf{r}^{(0)}=b-\mathbf{A} \mathbf{x}^{(0)}, \mathbf{z}^{(0)}=P^{-1} \mathbf{r}^{(0)}$, for $k=0,1, \ldots$ until convergence, we compute

$$
\begin{aligned}
&\alpha_{k}=\frac{\mathbf{z}^{(k)^{T}} \mathbf{r}^{(k)}}{\mathbf{z}^{(k)^{T}} \mathbf{A} \mathbf{z}^{(k)}} \\
&\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}+\alpha_{k} \mathbf{z}^{(k)} \\
&\mathbf{r}^{(k+1)}=\mathbf{r}^{(k)}-\alpha_{k} \mathbf{A} \mathbf{z}^{(k)} \\
&P \mathbf{z}^{(k+1)}=\mathbf{r}^{(k+1)}
\end{aligned}
$$

As far as the convergence properties of the descent method are concerned, the following result holds

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-169.jpg?height=212&width=723&top_left_y=183&top_left_x=97)

A similar result, with $K_{2}$ (A) replaced by $K_{2}\left(\mathrm{P}^{-1} \mathrm{~A}\right)$, holds also in the case of the preconditioned gradient method, as long as we assume that $P$ is also symmetric and positive definite.

An even more effective alternative consists in using the conjugate gradient method, where the descent directions no longer coincide with that of the residue. In particular, having set $\mathbf{p}^{(0)}=\mathbf{r}^{(0)}$, we seek directions of the form

$$
\mathbf{p}^{(k+1)}=\mathbf{r}^{(k+1)}-\beta_{k} \mathbf{p}^{(k)}, \quad k=0,1, \ldots
$$

where the parameters $\beta_{k} \in \mathbb{R}$ are to be determined so that

$$
\left(\mathrm{Ap}^{(j)}\right)^{T} \mathbf{p}^{(k+1)}=0, \quad j=0,1, \ldots, k .
$$

Directions of this type are called A-orthogonal (or A-conjugated). The method in the preconditioned case then takes the form: given $\mathbf{x}^{(0)} \in \mathbb{R}^{n}$, having set $\mathbf{r}^{(0)}=\mathbf{b}-\mathbf{A} \mathbf{x}^{(0)}$, $\mathbf{z}^{(0)}=\mathrm{P}^{-1} \mathbf{r}^{(0)}$ and $\mathbf{p}^{(0)}=\mathbf{z}^{(0)}$, the $k$-th iteration, with $k=0,1 \ldots$, is

$$
\begin{aligned}
&\alpha_{k}=\frac{\mathbf{p}^{(k)^{T}} \mathbf{r}^{(k)}}{\left(\mathrm{A} \mathbf{p}^{(k)}\right)^{T} \mathbf{p}^{(k)}} \\
&\mathbf{x}^{(k+1)}=\mathbf{x}^{(k)}+\alpha_{k} \mathbf{p}^{(k)} \\
&\mathbf{r}^{(k+1)}=\mathbf{r}^{(k)}-\alpha_{k} \mathrm{~A} \mathbf{p}^{(k)}, \\
&\mathrm{P} \mathbf{z}^{(k+1)}=\mathbf{r}^{(k+1)} \\
&\beta_{k}=\frac{\left(\mathrm{A} \mathbf{p}^{(k)}\right)^{T} \mathbf{z}^{(k+1)}}{\mathbf{p}^{(k)^{T}} \mathrm{Ap}^{(k)}}, \\
&\mathbf{p}^{(k+1)}=\mathbf{z}^{(k+1)}-\beta_{k} \mathbf{p}^{(k)} .
\end{aligned}
$$

The parameter $\alpha_{k}$ is chosen in order to guarantee that the error $\left\|\mathbf{e}^{(k+1)}\right\|_{\mathrm{A}}$ be minimized along the descent direction $\mathbf{p}^{(k)}$. The parameter $\beta_{k}$, instead, is chosen so that the new direction $\mathbf{p}^{(k+1)}$ is A-conjugate to $\mathbf{p}^{(k)}$, that is $\left(\mathrm{Ap}^{(k)}\right)^{T} \mathbf{p}^{(k+1)}=0$. Indeed, it can be proved (thanks to the induction principle) that if the latter relation is verified, then so are all the ones in $(7.21)$ relative to $j=0, \ldots, k-1$. For a complete justification of the method, see e.g. [QSS07, Chap. 4] or [Saa96].

It can be proved that the conjugate gradient method converges in exact arithmetics in at most $n$ steps, and that

$$
\left\|\mathbf{e}^{(k)}\right\|_{\mathrm{A}} \leq \frac{2 c^{k}}{1+c^{2 k}}\left\|\mathbf{e}^{(0)}\right\|_{\mathrm{A}}
$$

with

$$
c=\frac{\sqrt{K_{2}\left(\mathrm{P}^{-1} \mathrm{~A}\right)}-1}{\sqrt{K_{2}\left(\mathrm{P}^{-1} \mathrm{~A}\right)}+1}
$$

In the absence of roundoff errors, the CG method can therefore be seen as a direct method as it terminates after a finite number of operations.

On the other hand, for matrices of large dimension, it is usually applied as an iterative method and is arrested as soon as an error estimator (as for instance the relative residue) is less than a given tolerance.

Thanks to (7.23), the dependence on the reduction factor of the error on the matrix condition number is more favourable than the one of the gradient method (due to the presence of the square root of $\left.K_{2}\left(P^{-1} \mathrm{~A}\right)\right)$.

It can be noted that the number of iterations required for convergence (up to a prescribed tolerance) is proportional to $\frac{1}{2} \sqrt{K_{2}\left(P^{-1} A\right)}$ for the preconditioned conjugate gradient method, a clear improvement with respect to $\frac{1}{2} K_{2}\left(P^{-1} A\right)$ for the preconditioned gradient method. Of course, the PCG method is costlier per iteration, both in CPU time and storage.

\subsubsection{Krylov subspace methods}

Generalizations of the gradient method in the case where the matrix $\mathrm{A}$ is not symmetric lead to the so-called Krylov methods. Notable examples are the GMRES method and the conjugate bigradient method BiCG, as well as its stabilized version, the BiCGSTAB method. The interested reader can consult [QSS07, Chap. 4], [Saa96] and [vdV03].

Here we briefly review the GMRES (generalized minimal residual) method. We start by a revisitation of the Richardson method $(7.13)$ with $\mathrm{P}=\mathrm{I}$; the residual at the $k$-th step can be related to the initial residual by

$$
\mathbf{r}^{(k)}=\prod_{j=0}^{k-1}\left(\mathrm{I}-\alpha_{j} \mathrm{~A}\right) \mathbf{r}^{(0)}=p_{k}(\mathrm{~A}) \mathbf{r}^{(0)}
$$

where $p_{k}$ (A) is a polynomial in A of degree $k$. If we introduce the space

$$
K_{m}(\mathrm{~A} ; \mathbf{v})=\operatorname{span}\left\{\mathbf{v}, \mathrm{Av}, \ldots, \mathrm{A}^{m-1} \mathbf{v}\right\}
$$

it follows from (7.24) that $\mathbf{r}^{(k)} \in K_{k+1}\left(\mathrm{~A} ; \mathbf{r}^{(0)}\right)$. The space defined in $(7.25)$ is called the Krylov subspace of order $m$ associated with the matrix $A$ and the vector $\mathbf{v}$. It is a subspace of $\mathbb{R}^{n}$ that can be written as $\mathbf{u}=p_{m-1}(\mathrm{~A}) \mathbf{v}$, where $p_{m-1}$ is a polynomial in A of degree $\leq m-1$.

Similarly, the $k$-th iterate of the Richardson method can be represented as follows

$$
\mathbf{x}^{(k)}=\mathbf{x}^{(0)}+\sum_{j=0}^{k-1} \alpha_{j} \mathbf{r}^{(j)}
$$

whence $\mathbf{x}^{(k)}$ belongs to the space

$$
W_{k}=\left\{\mathbf{v}=\mathbf{x}^{(0)}+\mathbf{y}, \mathbf{y} \in K_{k}\left(\mathrm{~A} ; \mathbf{r}^{(0)}\right)\right\}
$$

Notice also that $\sum_{j=0}^{k-1} \alpha_{j} \mathbf{r}^{(j)}$ is a polynomial in A of degree less than $k-1$. In the nonpreconditioned Richardson method we are thus looking for an approximate solution to $\mathbf{x}$ in the space $W_{k}$. More generally, one can devise methods that search for approximate solutions of the form

$$
\mathbf{x}^{(k)}=\mathbf{x}^{(0)}+q_{k-1}(\mathrm{~A}) \mathbf{r}^{(0)}
$$

where $q_{k-1}$ is a polynomial selected in such a way that $\mathbf{x}^{(k)}$ is, in a sense that must be made precise, the best approximation of $\mathbf{x}$ in $W_{k}$. A method that looks for a solution of the form (7.27) is called a Krylov method.

A first question concerning Krylov subspace iterations is whether the dimension of $K_{m}(\mathrm{~A} ; \mathbf{v})$ increases as the order $m$ grows. A partial answer is provided below.

Property 7.2. Let $\mathrm{A} \in \mathbb{R}^{n \times n}$ and $\mathbf{v} \in \mathbb{R}^{n}$. The Krylov subspace $K_{m}(\mathrm{~A} ; \mathbf{v})$ has di-

mension equal to $m$ iff the degree of $\mathbf{v}$ with respect to A, denoted by deg $_{\mathrm{A}}(\mathbf{v})$,

is not less than $m ;$ the degree of $\mathbf{v}$ is defined as the minimum degree of a monic

nonnull polynomial $p$ in A for which $p(\mathrm{~A}) \mathbf{v}=\mathbf{0}$

The dimension of $K_{m}(\mathrm{~A} ; \mathbf{v})$ is thus equal to the minimum between $m$ and the degree of $\mathbf{v}$ with respect to $\mathrm{A}$ and, as a consequence, the dimension of the Krylov subspaces is a nondecreasing function of $m$. The degree of $\mathbf{v}$ cannot be greater than $n$ due to the Cayley-Hamilton theorem (see [QSS07, Sect. 1.7]).

Example 7.1. Consider the $4 \times 4$ matrix $\mathrm{A}=\operatorname{tridiag}_{4}(-1,2,-1)$. The vector $\mathbf{v}=$ $[1,1,1,1]^{T}$ has degree 2 with respect to A since $p_{2}(\mathrm{~A}) \mathbf{v}=\mathbf{0}$ with $p_{2}(\mathrm{~A})=\mathrm{I}_{4}-3 \mathrm{~A}+\mathrm{A}^{2}$ ( $\mathrm{I}_{4}$ is the $4 \times 4$ identity matrix $)$, while there is no monic polynomial $p_{1}$ of degree 1 for which $p_{1}(\mathrm{~A}) \mathbf{v}=\mathbf{0}$. All Krylov subspaces from $K_{2}(\mathrm{~A} ; \mathbf{v})$ on have therefore dimension equal to $2$. The vector $\mathbf{w}=[1,1,-1,1]^{T}$ has, instead, degree 4 with respect to $\mathrm{A}$.

For a fixed $m$, it is possible to compute an orthonormal basis for $K_{m}(\mathrm{~A} ; \mathbf{v})$ using the so-called Arnoldi algorithm.

Setting $\mathbf{v}_{1}=\mathbf{v} /\|\mathbf{v}\|_{2}$, this method generates an orthonormal basis $\left\{\mathbf{v}_{i}\right\}$ for $K_{m}\left(\mathrm{~A} ; \mathbf{v}_{1}\right)$ using the Gram-Schmidt procedure (see [QSS07, Sect. 3.4.3]). For $k=1, \ldots, m$, the Arnoldi algorithm computes

$$
\begin{array}{ll}
h_{i k} & =\mathbf{v}_{i}^{T} \mathrm{~A} \mathbf{v}_{k}, & i=1,2, \ldots, k \\
\mathbf{w}_{k} & =\mathrm{A} \mathbf{v}_{k}-\sum_{i=1}^{k} h_{i k} \mathbf{v}_{i}, & h_{k+1, k}=\left\|\mathbf{w}_{k}\right\|_{2} .
\end{array}
$$

If $\mathbf{w}_{k}=\mathbf{0}$ the process terminates and in such a case we say that a breakdown of the algorithm has occurred; otherwise, we set $\mathbf{v}_{k+1}=\mathbf{w}_{k} /\left\|\mathbf{w}_{k}\right\|_{2}$ and the algorithm restarts, incrementing $k$ by 1 .

It can be shown that if the method terminates at step $m$ then the vectors $\mathbf{v}_{1}, \ldots, \mathbf{v}_{m}$ form a basis for $K_{m}(\mathrm{~A} ; \mathbf{v})$. In such a case, if we denote by $\mathrm{V}_{m} \in \mathbb{R}^{n \times m}$ the matrix whose columns are the vectors $\mathbf{v}_{i}$, we have

$$
\mathrm{V}_{m}^{T} \mathrm{AV}_{m}=\mathrm{H}_{m}, \quad \mathrm{~V}_{m+1}^{T} \mathrm{AV}_{m}=\widehat{\mathrm{H}}_{m}
$$

where $\widehat{\mathrm{H}}_{m} \in \mathbb{R}^{(m+1) \times m}$ is the upper Hessenberg matrix whose entries $h_{i j}$ are given by (7.28), and $\mathrm{H}_{m} \in \mathbb{R}^{m \times m}$ is the restriction of $\widehat{\mathrm{H}}_{m}$ to the first $m$ rows and $m$ columns.

The algorithm terminates at an intermediate step $k<m$ iff $\operatorname{deg}_{\mathrm{A}}\left(\mathbf{v}_{1}\right)=k$. As for the stability of the procedure, all the considerations valid for the Gram-Schmidt method hold. For more efficient and stable computational variants of $(7.28)$, we refer to [Saa96].

We are now ready to solve the linear system (7.2) by a Krylov method. We look for the iterate $\mathbf{x}^{(k)}$ under the form $(7.27)$; for a given $\mathbf{r}^{(0)}, \mathbf{x}^{(k)}$ is in the unique element in $W_{k}$ which satisfies a criterion of minimal distance from $\mathbf{x}$. The criterion for selecting $\mathbf{x}^{(k)}$ is precisely the distinguishing feature of a Krylov method.

The most natural idea consists in searching for $\mathbf{x}^{(k)} \in W_{k}$ as the vector which minimizes the Euclidean norm of the error. This approach, however, does not work in practice since $\mathbf{x}^{(k)}$ would depend on the (unknown) solution $\mathbf{x}$. Two alternative strategies can be pursued:

1. compute $\mathbf{x}^{(k)} \in W_{k}$ by enforcing that the residual $\mathbf{r}^{(k)}$ is orthogonal to any vector in $K_{k}\left(\mathrm{~A} ; \mathbf{r}^{(0)}\right)$, i.e., we look for $\mathbf{x}^{(k)} \in W_{k}$ such that

$$
\mathbf{v}^{T}\left(\mathbf{b}-\mathbf{A} \mathbf{x}^{(k)}\right)=0 \quad \forall \mathbf{v} \in K_{k}\left(\mathrm{~A} ; \mathbf{r}^{(0)}\right)
$$

2. compute $\mathbf{x}^{(k)} \in W_{k}$ by minimizing the Euclidean norm of the residual $\left\|\mathbf{r}^{(k)}\right\|_{2}$, i.e.

$$
\left\|\mathbf{b}-\mathrm{A} \mathbf{x}^{(k)}\right\|_{2}=\min _{\mathbf{v} \in W_{k}}\|\mathbf{b}-\mathrm{A} \mathbf{v}\|_{2}
$$

Alternative 1 leads to the Arnoldi method (more commonly known as FOM, full orthogonalization method), while Alternative 2 yields the GMRES (generalized minimal residual) method.

We shall assume that $k$ steps of the Arnoldi algorithm have been carried out, so that an orthonormal basis for $K_{k}\left(\mathrm{~A} ; \mathbf{r}^{(0)}\right)$ has been generated and stored into the column vectors of the matrix $V_{k}$ with $\mathbf{v}_{1}=\mathbf{r}^{(0)} /\left\|\mathbf{r}^{(0)}\right\|_{2}$. In such a case the new iterate $\mathbf{x}^{(k)}$ can always be written as

$$
\mathbf{x}^{(k)}=\mathbf{x}^{(0)}+\mathrm{V}_{k} \mathbf{z}^{(k)},
$$

where $\mathbf{z}^{(k)}$ must be selected according to a suitable criterion that we are going to specify. Consequently we have

$$
\mathbf{r}^{(k)}=\mathbf{r}^{(0)}-\mathrm{A} \mathrm{V}_{k} \mathbf{z}^{(k)}
$$

Since $\mathbf{r}^{(0)}=\mathbf{v}_{1}\left\|\mathbf{r}^{(0)}\right\|_{2}$, and using $(7.29)$, relation $(7.33)$ becomes

$$
\mathbf{r}^{(k)}=\mathrm{V}_{k+1}\left(\left\|\mathbf{r}^{(0)}\right\|_{2} \mathbf{e}_{1}-\widehat{\mathrm{H}}_{k} \mathbf{z}^{(k)}\right)
$$

where $\mathbf{e}_{1}$ is the first vector of the canonical basis of $\mathbb{R}^{k+1}$. Therefore, in the GMRES method the solution at step $k$ can be computed through (7.32), provided

$$
\mathbf{z}^{(k)} \text { minimizes }\|\| \mathbf{r}^{(0)}\left\|_{2} \mathbf{e}_{1}-\widehat{\mathbf{H}}_{k} \mathbf{z}^{(k)}\right\|_{2}
$$

(we note that the matrix $\mathrm{V}_{k+1}$ appearing in $(7.34)$ does not alter the value of $\|\cdot\|_{2}$, since it is orthogonal).

Similarly to the CG method, the GMRES method enjoys a finite termination property, that is it terminates at most after $n$ iterations, yielding the exact solution (in exact arithmetic). Indeed, the kth iterate minimizes the residual in the Krylov subspace $K_{k}$. Since every subspace is contained in the next one, the residual decreases monotonically. After $n$ iterations, where $n$ is the size of the matrix $A$, the Krylov space $K_{n}$ is the whole of $\mathbb{R}^{n}$ and hence the GMRES method arrives at the exact solution. Premature stops are due to a breakdown in Arnoldi's orthonormalization algorithm. More precisely, we have the following result.

Property 7.3. A breakdown occurs for the GMRES method at a step $\mathrm{m}$ (with

However, the idea is that after a small number of iterations (relatively to $n$ ), the vector $\mathbf{x}^{(k)}$ is already a good approximation of the exact solution. This is confirmed by the convergence results that we describe later in this section.

To improve the efficiency of the GMRES algorithm it is necessary to devise a stopping criterion which does not require the explicit evaluation of the residual at each step. This is possible, provided that the linear system with upper Hessenberg matrix $\mathrm{H}_{k}$ is appropriately solved.

In practice, the matrix $\widehat{\mathrm{H}}_{k}$ in (7.29) is transformed into an upper triangular matrix $\mathrm{R}_{k} \in \mathbb{R}^{(k+1) \times k}$ with $r_{k+1, k}=0$ and such that $\mathrm{Q}_{k}^{T} \mathrm{R}_{k}=\widehat{\mathrm{H}}_{k}$, where $\mathrm{Q}_{k}$ is a matrix obtained as the product of $k$ Givens rotations. Then, since $\mathrm{Q}_{k}$ is orthogonal, minimizing \|\| $\mathbf{r}^{(0)}\left\|_{2} \mathbf{e}_{1}-\widehat{\mathrm{H}}_{k} \mathbf{z}^{(k)}\right\|_{2}$ is equivalent to minimizing $\left\|\mathbf{f}_{k}-\mathrm{R}_{k} \mathbf{z}^{(k)}\right\|_{2}$, with $\mathbf{f}_{k}=$ $\mathrm{Q}_{k}\left\|\mathbf{r}^{(0)}\right\|_{2} \mathbf{e}_{1}$. It can also be shown that the $k+1$-th component of $\mathbf{f}_{k}$ is, in absolute value, the Euclidean norm of the residual at the $k$-th step.

As FOM, the GMRES method entails a high computational effort and a large amount of memory, unless convergence occurs after few iterations. For this reason, two variants of the algorithm are available, one named GMRES $(m)$ and based on the restart after $m$ steps, with $\mathbf{x}(m)$ as initial guess, the other named Quasi-GMRES or QGMRES and based on stopping the Arnoldi orthogonalization process. It is worth noting that these two methods do not enjoy Property $7.3$.

The convergence analysis of GMRES is not trivial, and we report just some elementary results here. If $A$ is positive definite, i.e., its symmetric part $A_{S}$ has positive eigenvalues, then the $k$-th residual decreases according to the following bound

$$
\left\|\mathbf{r}^{(k)}\right\|_{2} \leq \sin ^{k}(\beta)\left\|\mathbf{r}^{(0)}\right\|_{2},
$$

where $\cos (\beta)=\lambda_{\min }\left(L_{S}\right) /\|L\|$ with $\beta \in[0, \pi / 2)$. Moreover, GMRES $(m)$ converges for all $m \geq 1$. In order to obtain a bound on the residual at a step $k \geq 1$, let us assume that the matrix $A$ is diagonalizable

$$
A=T \Lambda T^{-1}
$$

where $\Lambda$ is the diagonal matrix of the eigenvalues $\left\{\lambda_{j}\right\}_{j=1, \ldots, n}$, and $T=\left[\boldsymbol{\omega}^{1}, \ldots, \boldsymbol{\omega}^{n}\right]$ is the matrix whose columns are the right eigenvectors of $A$. Under these assumptions, the residual norm after $k$ steps of GMRES satisfies

$$
\left\|\mathbf{r}^{(k)}\right\| \leq K_{2}(T) \delta\left\|\mathbf{r}^{(0)}\right\|
$$

where $K_{2}(T)=\|T\|_{2}\left\|T^{-1}\right\|_{2}$ is the condition number of $T$ and

$$
\delta=\min _{p \in \mathbb{P}_{k}, p(0)=1} \max _{1 \leq i \leq k}\left|p\left(\lambda_{i}\right)\right|
$$

Moreover, suppose that the initial residual is well represented by the first $m$ eigenvectors, i.e., $\mathbf{r}^{0}=\sum_{j=1}^{m} \alpha_{j} \boldsymbol{\omega}^{j}+\mathbf{e}$, with $\|\mathbf{e}\|$ small in comparison to $\left\|\sum_{j=1}^{m} \alpha_{j} \boldsymbol{\omega}^{j}\right\|$, and assume that if some complex $\boldsymbol{\omega}^{j}$ appears in the previous sum, then its conjugate $\bar{\omega}^{j}$ appears as well. Then

$$
\begin{gathered}
\left\|\mathbf{r}^{(k)}\right\| \leq K_{2}(T) c_{k}\|\mathbf{e}\| \\
c_{k}=\max _{p>k} \prod_{j=1}^{k}\left|\frac{\lambda_{p}-\lambda_{j}}{\lambda_{j}}\right|
\end{gathered}
$$

Very often, $c_{k}$ is of order one; hence, $k$ steps of GMRES reduce the residual norm to the order of $\|\mathbf{e}\|$ provided that $\kappa_{2}(T)$ is not too large.

In general, as highlighted from the previous estimate, the eigenvalue information alone is not enough, and information on the eigensystem is also needed. If the eigensystem is orthogonal, as for normal matrices, then $K_{2}(T)=1$, and the eigenvalues retain all the information about convergence. Otherwise, upper bounds for $\left\|\mathbf{r}^{(k)}\right\|$ can be provided in terms of both spectral and pseudospectral information, as well as the so-called field of values of $A$

$$
\mathscr{F}(A)=\left\{\mathbf{v}^{*} A \mathbf{v} \mid\|\mathbf{v}\|=1\right\}
$$

If $0 \notin \mathscr{F}(A)$, then estimate (7.36) can be improved by replacing $\lambda_{\min }\left(A_{S}\right)$ with $\operatorname{dist}(0, \mathscr{F}(A))$

An extensive discussion on the convergence of GMRES and GMRES $(m)$ can be found in [Saa96], [Emb99], [Emb03], [TE05], and [vdV03].

The GMRES method can of course be implemented for a preconditioned system. We provide here an implementation of the preconditioned GMRES method with a left preconditioner $P$.

\section{Preconditioned GMRES (PGMRES) Method}

Initialize

$$
\mathbf{x}^{(0)}, P \mathbf{r}^{(0)}=\mathbf{f}-A \mathbf{x}^{(0)}, \beta=\left\|\mathbf{r}^{(0)}\right\|_{2}, \mathbf{x}^{(1)}=\mathbf{r}^{(0)} / \beta
$$

Iterate

$$
\begin{aligned}
&\text { For } j=1, \ldots, k \text { Do } \\
&\text { Compute } P \mathbf{w}^{(j)}=A \mathbf{x}^{(j)} \\
&\text { For } i=1, \ldots, j \text { Do } \\
&\qquad \begin{array}{l}
g_{i j}=\left(\mathbf{x}^{(i)}\right)^{T} \mathbf{w}^{(j)} \\
\mathbf{w}^{(j)}=\mathbf{w}^{(j)}-g_{i j} \mathbf{x}_{i} \\
\text { End Do } \\
g_{j+1, j}=\left\|\mathbf{w}^{(j)}\right\|_{2} \\
\text { (if } \left.g_{j+1, j}=0 \operatorname{set} k=j \text { and Goto }(1)\right) \\
\mathbf{x}^{(j+1)}=\mathbf{w}^{(j)} / g_{j+1, j} \\
\text { End Do } \\
V_{k}=\left[\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(k)}\right], \hat{H}_{k}=\left\{g_{i j}\right\}, 1 \leq j \leq k, 1 \leq i \leq j+1 ; \\
\text { (1) Compute } \mathbf{z}^{(k)}, \text { the minimizer of }\left\|\beta \mathbf{e}_{1}-\hat{H}_{k} \mathbf{z}\right\| \\
\operatorname{Set} \mathbf{x}^{(k)}=\mathbf{x}^{(0)}+V_{k} \mathbf{z}^{(k)}
\end{array}
\end{aligned}
$$

More generally, as proposed by Saad (1996), a variable preconditioner $P_{k}$ can be used at the $k$-th iteration, yielding the so-called $f$ lexible GMRES method. This is especially interesting in those situations where the preconditioner is not explicitly given, but implicitly defined, for instance, as an approximate Jacobian in a Newton iteration or by a few steps of an inner iteration process (see Chapter 17). Another meaningful case is the one of domain decomposition preconditioners (of either Schwarz or Schur type) where the preconditioning step involves one or several substeps of local solves in the subdomains (see Chapter 19).

Several considerations for the practical implementation of GMRES, its relation with FOM, how to restart GMRES, and the Householder version of GMRES can be found in [Saa96]. Remark 7.1 (Projection methods). Denoting by $Y_{k}$ and $L_{k}$ two generic $m$-dimensional subspaces of $\mathbb{R}^{n}$, we call projection method a process which generates an approximate solution $\mathbf{x}^{(k)}$ at step $k$, enforcing that $\mathbf{x}^{(k)} \in Y_{k}$ and that the residual $\mathbf{r}^{(k)}=\mathbf{b}-\mathbf{A} \mathbf{x}^{(k)}$ be orthogonal to $L_{k}$. If $Y_{k}=L_{k}$, the projection process is said to be orthogonal, and oblique otherwise (see [Saa96]).

Krylov methods are projection methods. For instance, the Arnoldi method is an orthogonal projection method where $L_{k}=Y_{k}=K_{k}\left(\mathrm{~A} ; \mathbf{r}^{(0)}\right)$, while the GMRES method is an oblique projection method with $Y_{k}=K_{k}\left(\mathrm{~A} ; \mathbf{r}^{(0)}\right)$ and $L_{k}=\mathrm{A} Y_{k}$. It is worth noticing that some classical methods introduced in previous sections fall into this category. For example, the Gauss-Seidel method is an orthogonal projection method where at the $k$-th step $K_{k}\left(\mathrm{~A} ; \mathbf{r}^{(0)}\right)=\operatorname{span}\left\{\mathbf{e}_{k}\right\}$, with $k=1, \ldots, n$. The projection steps are carried out cyclically from 1 to $n$ until convergence.

\subsubsection{The Multigrid method}

The geometric Multigrid (MG) method is an iterative algorithm to solve the algebraic system associated to a certain grid by making use of one or several additional coarser grids. For the sake of simplicity, we describe only the case of a two-grid algorithm. Thus we suppose that $(7.2)$ represents the algebraic system arising from, say, a finite element approximation of a boundary-value problem on a (fine) grid $\mathscr{T}_{h}$. For the sake of clarity we can rewrite $(7.2)$ as

$$
A_{h} \mathbf{u}_{h}=\mathbf{b}_{h}
$$

where, as usual, $h=\max _{k \in \mathscr{T}_{h}} \operatorname{diam}(K), A_{h}$ is the stiffness $\mathrm{FE}$ matrix, $\mathbf{b}_{h}$ is the right hand side, $\mathbf{u}_{h}$ the vector of nodal values. Let $\mathscr{T}_{H}$ represent a coarse grid such that $\mathscr{T}_{h}$ can be regarded as a refinement of $\mathscr{T}_{H}$, for instance so that the vertices of $\mathscr{T}_{h}$ are obtained as the midpoints of edges from $\mathscr{T}_{H}$. In that case, $h=H / 2$.

The generic iteration of the MG algorithm on these two grids consists of:

1. Pre-smoothing step:

perform $m_{1}(\geq 1)$ iterations on the fine grid using an iterative algorithm (e.g. Jacobi, or Gauss-Seidel, or Richardson),

$$
\mathbf{u}_{h}^{(l)}=S_{h}\left(\mathbf{u}_{h}^{(l-1)}, \mathbf{b}_{h}\right) \quad l=1, \ldots, m_{1}
$$

for a suitable $\mathbf{u}_{h}^{(0)}$;

2. Residual computation:

$$
\mathbf{r}_{h}=\mathbf{b}_{h}-A_{h} \mathbf{u}_{h}^{\left(m_{1}\right)}
$$

3. Restriction to the coarse grid:

$$
\mathbf{r}_{H}=I_{h}^{H} \mathbf{r}_{h}
$$

where $I_{h}^{H}: \mathbb{R}^{N_{h}} \rightarrow \mathbb{R}^{N_{H}}$ is a fine-to-coarse operator, $N_{h}$ is the number of unknowns of the fine-grid problem, $N_{H}$ that of the coarse-grid one; 4. Solution of the coarse-grid problem:

$$
A_{H} \mathbf{e}_{H}=\mathbf{r}_{H}
$$

where $A_{H}$ is the stiffness matrix associated with the $\mathrm{FE}$ discretization on the coarse grid $\mathscr{T}_{H}$

5. Coarse-grid correction:

$$
\mathbf{u}_{h}^{\left(m_{1}+1\right)}=\mathbf{u}_{h}^{\left(m_{1}\right)}+I_{H}^{h} \mathbf{e}_{H}
$$

where $I_{H}^{h}: \mathbb{R}^{N_{H}} \rightarrow \mathbb{R}^{N_{h}}$ is a coarse-to-fine operator;

6. Post-smoothing step:

Perform $m_{2}(\geq 1)$ iterations on the fine grid:

$$
\mathbf{u}_{h}^{(l)}=S_{h}\left(\mathbf{u}_{h}^{(l-1)}, \mathbf{b}_{h}\right) \quad l=m_{1}+1, \ldots, m_{1}+m_{2}+1
$$

The two inter-grid operators (i.e. matrices) are typically adjoint to each other.

Let $V_{h}$ be the finite element space associated with $\mathscr{T}_{h}, V_{H}$ that associated with $\mathscr{T}_{H}$. For every $\mathbf{w} \in \mathbb{R}^{N_{H}}$, let $w_{H}=\sum_{i=1}^{N_{H}} w_{i} \varphi_{i}^{H}$ be the corresponding FE function in $V_{H}$. Similarly, for every $\mathbf{v} \in \mathbb{R}^{N_{h}}$, let $v_{h}=\sum_{j=1}^{N_{h}} v_{j} \varphi_{j}^{h}$ be the corresponding $\mathrm{FE}$ function in $V_{h}($ see $(4.7))$. Here $\varphi_{i}^{h}$ (resp. $\left.\varphi_{j}^{H}\right)$ indicate the Lagrangian basis functions in $V_{h}$ (resp. $\left.V_{H}\right)$. Let $\mathscr{I}_{H}^{h}: V_{H} \rightarrow V_{h}$ be the operator corresponding to $I_{H}^{h}$, that is

$$
\mathscr{I}_{H}^{h} w_{H}=v_{h} \quad \text { iff } \quad I_{H}^{h} \mathbf{w}=\mathbf{v}
$$

Typically $\mathscr{I}_{H}^{h}$ is the natural injection, in the sense that

$$
\mathscr{I}_{H}^{h} w_{H}=w_{H} \quad \forall w_{H} \in V_{H} .
$$

This means that their nodal values $\left(\mathscr{I}_{H}^{h} w_{H}\right)\left(\mathbf{N}_{j}^{h}\right)=w_{H}\left(\mathbf{N}_{j}^{h}\right)$ are the same at all nodes $\mathbf{N}_{j}^{h}$ of $\mathscr{T}_{h}$. The entries of $I_{H}^{h}$ are therefore $\left(I_{H}^{h}\right)_{i j}=\varphi_{i}^{H}\left(\mathbf{N}_{j}^{h}\right), j=1, \ldots, N_{h}, i=\cdots, N_{H}$. The operator $I_{h}^{H}$ is the weighted transpose of $I_{H}^{h}$, that is

$$
\left(I_{h}^{H} \mathbf{v}, \mathbf{w}\right)_{H}=\left(\mathbf{v}, I_{H}^{h} \mathbf{w}\right)_{h} \quad \forall \mathbf{v} \in \mathbb{R}^{N_{h}}, \forall \mathbf{w} \in \mathbb{R}^{N_{H}}
$$

where we have introduced the weigthed (mesh-dependent) inner products:

$$
\begin{aligned}
&(\mathbf{v}, \mathbf{w})_{h}=h^{2} \sum_{j=1}^{N_{h}} v_{j} w_{j} \quad \forall \mathbf{v}, \mathbf{w} \in \mathbb{R}^{N_{h}}, \\
&(\mathbf{x}, \mathbf{y})_{H}=H^{2} \sum_{i=1}^{N_{H}} x_{i} y_{i} \quad \forall \mathbf{x}, \mathbf{y} \in \mathbb{R}^{N_{H}} .
\end{aligned}
$$

A simple algebraic calculation shows that

$$
\left(I_{h}^{H}\right)_{i j}=\frac{h^{2}}{H^{2}}\left(I_{H}^{h}\right)_{j i}, \quad i=1, \ldots, N_{H}, j=\ldots, N_{h}
$$

that is

$$
I_{h}^{H}=\frac{h^{2}}{H^{2}}\left(I_{H}^{h}\right)^{T}
$$

What we have described is a two-grid V-cycle algorithm. As a matter of fact, a pictorial representation of one step of this algorithm, where the fine is a high level whereas the coarse is a low level, looks like a "V"' workflow.

This iterative algorithm can be associated with the following iteration matrix

$$
M G=S_{h}^{m_{2}}\left(I_{h}-I_{H}^{h}\left(A_{H}\right)^{-1} I_{h}^{H} A_{h}\right) S_{h}^{m_{1}}
$$

where $I_{h} \in \mathbb{R}^{N_{h} \times N_{h}}$ is the identity matrix and $S_{h}$ is the smoothing iteration matrix (e.g., the Richardson matrix $R_{P}$ or $R\left(\alpha_{k}\right)$ on the fine grid).

The convergence analysis of the two-grid V-cycle algorithm, as well as that of the more general multi-grid case with either V- or W-cycle, is carried out, e.g., in [Hac] and, for finite element discretization, in [BS94]. Chapter 8

Elements of finite element programming

In this chapter we focus more deeply on a number of aspects relating to the translation of the finite-element method into computer code. This implementation process can hide some pitfalls. Beyond the syntactic requirements of a given programming language, the need for a high computational efficiency leads to an implementation that is generally not the immediate translation of what has been seen during the theoretical presentation. Efficiency depends on many factors, including the language used and the architecture on which one works $^{1}$. Personal experience can play a role as fundamental as learning from a textbook. Moreover, although spending time searching for a bug in the code or for a more efficient data structure can sometimes appear to be a waste of time, it (almost) never is. For this reason, we wish to propose the present chapter as a sort of "guideline" for trials that the reader can perform on his own, rather than a chapter to be studied in the traditional sense.

A final note about the chapter style. The approach followed here is to provide general guidelines: obviously, each problem has specific features that can be exploited in a careful way for a yet more efficient implementation.

\subsection{Working steps of a finite element code}

The execution of a Finite-Element computation can be logically split into four working steps (Fig. 8.1).

1. Pre-processing. This step consists in setting up the problem and coding its computational domain, which, as seen in Chapter 4 , requires the construction of the mesh (or grid). In general, setting aside the trivial cases (for instance in one dimension), the construction of an adequate mesh is a numerical problem of considerable inter-

${ }^{1}$ Currently, engineering applications involving scientific computing are running on parallel architectures with hundreds or thousands of Central Processor Units (CPUs) or Graphical Processor Units (GPUs), and this requires specific coding techniques. This topic is beyond the scope of the present book.


![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-180.jpg?height=432&width=547&top_left_y=113&top_left_x=183)

Fig. 8.1. Working steps of a finite element code

est, for which ad hoc techniques have been developed. Generally, this operation is performed by dedicated programs or modules within a solver, where great effort has been recently devolved to the aspect of interface and interfacing with CAD (Computer Aided Design) software. Chapter 6 is dedicated to the fundamental techniques for grid generation.

2. Assembly. In this phase, we construct the "functional" data structures, starting from the "geometric" ones obtained by the mesh and by the user's choices concerning the desired type of finite elements to be used. Moreover, based on the problem we want to solve and on its boundary conditions, we compute the stiffness matrix associated to the discretization (see Chapters 4 and 13). In an unsteady problem, this operation may need to be included in the time advancing loop, when the matrix depends on time (like for instance for the linearization of nonlinear problems, see Chapters 5 and 17). Strictly speaking, the term "assembly" refers to the construction of the matrix of the linear system, moving from the local computation performed on the reference element to the global one that concurs to determine the matrix associated to the discretized problem. Fig. $8.2$ summarizes the different operations during the assembly phase for the preparation of the algebraic system.

3 . Solution of the algebraic system. The core of the solution of any finite-element computation is represented by the solution of a linear system. As previously said, this will eventually be part of a temporal cycle (based on an implicit discretization method) or of an iterative cycle arising from the linearization of a nonlinear problem. The choice of the solution method is generally left to the user. For this reason, it is very important that the user understands the problem under exam, which, as 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-181.jpg?height=563&width=671&top_left_y=113&top_left_x=124)

Fig. 8.2. Scheme of the assembly. The geometric and topological information (top table), suitably stored, describes the grid. Through the mapping on the reference element, we compute the discretization matrix $A$ and of the term $\mathbf{b}$, first by proceeding element by element (local computation) and then, by exploiting the additivity of integration operation, we update the global matrix. The symbols representing each element of the matrix are obtained through the overlap of the symbols used to define each element of the mesh. Finally, we implement the boundary conditions, which ideally remove the degrees of freedom with associated Dirichlet conditions, getting to the final structures $A$ and $\mathbf{b}$. As we will see, the operation is often implemented in a different way

we have seen in Chapter 4, has implications on the structure of the matrix (for instance, symmetry and positivity); on the other hand she/he should be aware of the available methods to perform an optimal choice (which rarely is the default one). This is why in Chapter 7 we recalled the main properties of numerical methods for the solution of linear systems.

Nowadays, a host of very efficient computational libraries exist for the solution of various types of linear systems, hence the trend in the coding phase is generally to include such libraries rather than implementing from scratch. Among others, we remind PetSC (see [Pet]), UMFPACK [UMF], and TriLinos [Tri].

4. Post-processing. Since the amount of numerical data generated by a finite-element code might be huge, a post-processing is often necessary in order to present results that are concise and in a usable format. However, this may not be a trivial task. In particular, a reckless post-processing for the a posteriori computation of differential quantities (e.g. stresses from displacements, fluxes or vorticity from velocities, etc ...) can introduce unacceptable overhead errors.

Since grid generation techniques were addressed in Chap. 6 , and we saw the algorithms for the solution of linear systems in Chap. 7, the main focus of this chapter will be on the Assembly phase (Sect. 8.4).

Before dealing with this subject, though, in Sect. $8.2$ we will deal with quadrature formula for the numerical computation of integrals, while sparse matrix storing will be discussed in Sect. 8.3.

As far as the Post-processing step is concerned, we refer to the specific literature, and recall that the techniques used above have been introduced in Chap. 4 for the computation of a posteriori estimates.

Eventually, Sect. $8.6$ will discuss a complete example.

\subsubsection{The code in a nutshell}

There are many programming languages and environments available today, characterized by different philosophies and objectives. When facing the implementation of a numerical method, it is necessary to make a pondered choice in this respect. Amongst the most useful programming environments for the construction of prototypes, Matlab is certainly an excellent tool under many viewpoints, although, as with all interpreted languages, it is weaker under the computational efficiency profile. Another environment targeted to the solution of differential problems in $2 \mathrm{D}$ through the finite-element method is FreeFem++ (see www.freefem.org). This environment comprises all four phases indicated above in a single package (free and usable under different operating systems). Its particularly captivating syntax reduces the gap between coding and theoretical formulation by bringing the former significantly closer to the latter. This operation has a clear "educational" merit, which is to quickly produce simulations also for non trivial problems. However, the computational costs and the difficulty of implementing new strategies that require an extension of the syntax can be penalizing in actual cases of interest. In [FSV12] several solved examples and problems are solved with FreeFem++.

Among compiled programming languages, Fortran (Fortran 77 in particular) is traditionally the one that has had the biggest success in the numerical domain, because it generates very efficient executable codes. Recently, the abstraction feature that is intrinsic to the object-oriented programming philosophy has proven to be very suitable for finite element programming. The level of abstraction made possible by far-reaching mathematical tools seems to find an excellent counterpart in the abstraction of object-oriented programming, based on the design of data types made by the user (more than on operations to perform, as in procedural programming) and on their polymorphism (see e.g. [LL00, Str00]). However, the computational cost of such an abstraction has sometimes reduced the interest for a theoretically attractive programming style. The latter is often operationally weak for science problems, where computational efficiency is (almost) always crucial. This has required the development of more sophisticated programming techniques (for instance, Expression Templates), that avoid the cost associated to the interpretation of abstract objects to become too heavy during code execution (see e.g. [Vel95, Fur97, Pru06, DV09]). Hence, besides Fortran, languages like $\mathrm{C}++$ (born as an object-oriented improvement of the language C) are nowadays more and more frequent in the scientific domain; amongst others, we recall FEniCS OpenFOAM and LifeV..

In the code excerpts presented below we will refer to $\mathrm{C}++$. An accurate examination of the code (which we will henceforth call "Programs" for simplicity) requires some basic knowledge of $\mathrm{C}++$, for which we refer to [LL00]. However, as we want to use this chapter as a basis for autonomous experiments, it is not essential to master the $\mathrm{C}++$ syntax to understand the text; a mild familiarity with its basic syntax will be enough for the reader who might prefer a different language.

\subsection{Numerical computation of integrals}

The effective numerical computation of the integrals in the finite element formulation is typically performed via quadrature formulae. For an introduction to the subject of numerical quadrature, we refer to basic numerical analysis textbooks (e.g. [QSS07]). Here, it will suffice to recall that a generic quadrature formula has the form

$$
\int_{K} f(\mathbf{x}) d \mathbf{x} \approx \sum_{i q=1}^{n q n} f\left(\mathbf{x}_{i q}\right) w_{i q}
$$

where $K$ denotes the region over which we integrate (typically an element of the finite element grid), $n q n$ is the number of quadrature nodes for the selected formula, $\mathbf{x}_{i q}$ are the coordinates of the quadrature nodes and $w_{i q}$ are the weights. Typically, the accuracy of the formula and its computational cost grow with the number of quadrature nodes. As we will see in Chapter 10, Sects. $10.2 .2$ and $10.2 .3$, the formulae which guarantee the best accuracy for the same number of nodes are the Gaussian ones.

The computation of integrals is generally performed on the reference element (where the expression of basis functions is known) through a suitable change of variable (Sect. 4.3).

Let us denote with $\hat{x}_{i}$ and $x_{i}$ (for $\left.i=1, \ldots, d\right)$ the coordinates on the reference element $\hat{K}$ and those on the generic element $K$, respectively. Integration in the reference space will then require the knowledge of the Jacobian matrices $J_{K}(\hat{\mathbf{x}})$ of the geometric transformation $F_{K}$ that maps the reference element $\hat{K}$ on the element $K$ (see Fig. 4.14),

$$
J_{K}(\hat{\mathbf{x}})=\left[\frac{\partial x_{i}}{\partial \hat{x}_{j}}(\hat{\mathbf{x}})\right]_{i, j=1}^{d}
$$

We then have

$$
\int_{K} f(\mathbf{x}) d \mathbf{x}=\int_{\hat{K}} \hat{f}(\hat{\mathbf{x}})\left|\operatorname{det} J_{K}(\hat{\mathbf{x}})\right| d \hat{\mathbf{x}} \approx \sum_{q} \hat{f}\left(\hat{\mathbf{x}}_{q}\right)\left|\operatorname{det} J_{K}\left(\hat{\mathbf{x}}_{q}\right)\right| \hat{w}_{q}
$$

where $\hat{f}=f \circ F_{K}$, and $\hat{w}_{q}$ are the weights on the reference element. In case of integrals involving derivatives, denoting with $\widehat{J}_{K}(\mathbf{x})$ the Jacobian matrix associated to $F_{K}^{-1}$, i.e.

$$
\widehat{J}_{K}(\mathbf{x})=\left[\frac{\partial \hat{x}_{i}}{\partial x_{j}}(\mathbf{x})\right]_{i, j=1}^{d}
$$

we have, for $j=1, \ldots, d$,

$$
\frac{\partial f}{\partial x_{i}}(\mathbf{x})=\sum_{j=1}^{d} \frac{\partial \hat{f}}{\partial \widehat{x}_{j}}(\hat{\mathbf{x}}) \frac{\partial \widehat{x}_{j}}{\partial x_{i}}(\mathbf{x}), \quad \nabla_{x} f(\mathbf{x})=\left[\widehat{J}_{K}(\mathbf{x})\right]^{T} \nabla_{\widehat{x}} \widehat{f}(\hat{\mathbf{x}})
$$

We can prove that

$$
\left[\widehat{J_{K}}(\mathbf{x})\right]^{T}=\frac{1}{\operatorname{det} J_{K}(\hat{\mathbf{x}})} J_{K}^{\operatorname{cof}}(\hat{\mathbf{x}})
$$

$J_{K}^{c o f}(\hat{\mathbf{x}})$ being the matrix of the cofactors of the elements of $J_{K}(\hat{\mathbf{x}})$, i.e. (in the twodimensional case)

$$
J_{K}^{c o f}(\hat{\mathbf{x}})=\left[\begin{array}{cc}
\frac{\partial x_{2}}{\partial \hat{x}_{2}}(\hat{\mathbf{x}}) & -\frac{\partial x_{2}}{\partial \hat{x}_{1}}(\hat{\mathbf{x}}) \\
-\frac{\partial x_{1}}{\partial \hat{x}_{2}}(\hat{\mathbf{x}}) & \frac{\partial x_{1}}{\partial \hat{x}_{1}}(\hat{\mathbf{x}})
\end{array}\right]
$$

The gradient of the function $f$ can thus be expressed in terms of the variables in the reference space as following

$$
\nabla_{x} f(\mathbf{x})=\frac{1}{\operatorname{det} J_{K}(\hat{\mathbf{x}})} J_{K}^{c o f}(\hat{\mathbf{x}}) \nabla_{\hat{x}} \hat{f}(\hat{\mathbf{x}})
$$

Denoting with $\alpha$ and $\beta$ the indices of two generic basis functions, the typical element of the stiffness matrix can therefore be computed as follows:

$$
\begin{aligned}
&\int_{K} \nabla_{x} \varphi_{\alpha}(\mathbf{x}) \nabla_{x} \varphi_{\beta}(\mathbf{x}) d \mathbf{x}= \\
&\int\left(J_{K}^{c o f}(\hat{\mathbf{x}}) \nabla_{\hat{x}} \hat{\varphi}_{\alpha}(\hat{\mathbf{x}})\right)\left(J_{K}^{\operatorname{cof}}(\hat{\mathbf{x}}) \nabla_{\hat{x}} \hat{\varphi}_{\beta}(\hat{\mathbf{x}})\right) \frac{1}{\left|\operatorname{det} J_{K}(\hat{\mathbf{x}})\right|} d \hat{\mathbf{x}} \simeq \\
&\hat{K}_{q}\left[\frac{\hat{w}_{q}}{\left|\operatorname{det} J_{K}\left(\hat{\mathbf{x}}_{q}\right)\right|} \sum_{j=1}^{d}\left(\sum_{l=1}^{d}\left[J_{K}^{c o f}\left(\hat{\mathbf{x}}_{q}\right)\right]_{j l} \frac{\partial \hat{\varphi}_{\alpha}}{\partial \hat{x}_{l}}\left(\hat{\mathbf{x}}_{q}\right)\right)\left(\sum_{m=1}^{d}\left[J_{K}^{\operatorname{cof}}\left(\hat{\mathbf{x}}_{q}\right)\right]_{j m} \frac{\partial \hat{\varphi}_{\beta}}{\partial \hat{x}_{m}}\left(\hat{\mathbf{x}}_{q}\right)\right)\right]
\end{aligned}
$$

Note that the matrices $J_{K}$, and consequently the matrices $J_{K}^{c o f}$, are constant on the element $K$ if $K$ is a triangle or a rectangle in $2 \mathrm{D}$ (a tetrahedron or a parallelepiped in 3D) with no curved boundaries.

The class coding a quadrature formula stores quadrature nodes and their associated weights. In the effective integral computation we will then obtain the necessary mapping information for the actual computation, which depends on the geometry of $K$. The choice of a quadrature formula responds to two (conflicting) needs:

1. On one hand, the higher the accuracy is, the smaller is the integration error eventually affecting the overall quality of the numerical computation; a proper choice of the quadrature rule, based on the concept of degree of exactness, may make the numerical integration error vanish. We control for problems whose differential operator has constant (or polynomial) coefficients.

2. On the other hand, a larger number of $n q n$ nodes and in increase of the computational cost of assembly is necessary to obtain an increase of accuracy.

The appropriate synthesis of these two needs evidently depends on the requirements of the problem we want to solve, as well as on the accuracy and speed specifications to execute the computation.

\subsubsection{Numerical integration using barycentric coordinates}

The numerical evaluation of integrals on simplexes (intervals in 1D, triangles in $2 \mathrm{D}$, tetrahedra in $3 \mathrm{D}$ ) can profit from the use of the barycentric coordinates that were introduced in Sect. 4.4.3. To start with, we observe that the following exact integration formulas hold (see, e.g., [Aki94, Chap.9] or [Hug00, Chap. 3]):

in $1 \mathrm{D}$

$$
\int_{\widehat{K}_{1}} \lambda_{0}^{a} \lambda_{1}^{b} d \omega=\frac{a ! b !}{(a+b+1) !} \operatorname{lenght}\left(\widehat{K}_{1}\right)
$$

in $2 \mathrm{D}$

$$
\int_{\widehat{K}_{2}} \lambda_{0}^{a} \lambda_{1}^{b} \lambda_{2}^{c} d \omega=\frac{a ! b ! c !}{(a+b+c+2) !} 2 \operatorname{Area}\left(\widehat{K}_{2}\right)
$$

in $3 \mathrm{D}$

$$
\int_{\widehat{K}_{3}} \lambda_{0}^{a} \lambda_{1}^{b} \lambda_{2}^{c} \lambda_{3}^{d} d \omega=\frac{a ! b ! c ! d !}{(a+b+c+d+3) !} 6 \operatorname{Vol}\left(\widehat{K}_{3}\right)
$$

More in general,

$$
\int_{\hat{K}_{d}}^{d} \prod_{i=0}^{d_{i}}^{n_{i}} d \omega=\frac{\prod_{i=0}^{d} n_{i} !}{\left(\sum_{i=0}^{d} n_{i}+d\right) !} d !\left|\hat{K}_{d}\right|
$$

where $\hat{K}_{d}$ is a d-dimensional standard simplex, $\left|\hat{K}_{d}\right|$ denotes its measure, $\left\{n_{i}, 0 \leq i \leq\right.$ $d\}$ is a set of non-negative integers.

These formulas are useful when dealing with finite-element approximations of the boundary-value problems for the exact computation of polynomial integrals in the characteristic Lagrangian basis functions.

For the sake of an example, Table $8.1$ shows the weights and nodes for some popular quadrature formulas in 2D. Table $8.2$ gives some formulas for a tetrahedron. These formulas are symmetric: we must consider all possible permutations of the barycentric coordinates to obtain the full list of nodes. 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-186.jpg?height=198&width=522&top_left_y=128&top_left_x=186)

Fig. 8.3. The barycentric coordinate $\lambda_{i}$ of the point $P$ represents the ratio between the volume of the tetrahedron having as vertices $P$ and the vertices of the face opposite to $N_{i}$ (in the figure, right, we have shadowed the tetrahedron with vertices $P, N_{1}, N_{2}, N_{3}$ opposite $N_{0}$ ) and the total volume of the tetrahedron

Table 8.1. Nodes and weights for the quadrature formulae on triangles. The nodes are expressed through their barycentric coordinates. The weights do not take into account the measure of the reference element (which is equal to $1 / 2$ in this case)

\begin{tabular}{ccccccc}
\hline$n q n$ & \multicolumn{3}{c}{ barycentric coordinates $\lambda_{j}$} & $1 / 3$ & 1 & $w_{j}$ & $r$ \\
\hline 1 & $1 / 3$ & $1 / 3$ & 0 & 3 & 1 & 1 \\
3 & 1 & 0 & $1 / 3$ & 3 & $1 / 3$ & 1 \\
3 & $2 / 3$ & $1 / 3$ & $1 / 3$ & 1 & $-0.5625$ & 2 \\
4 & $1 / 3$ & $1 / 3$ & $0.2$ & 3 & $0.52083$ & \\
& $0.6$ & $0.2$ & $0.10903900907$ & 6 & $1 / 6$ & 2 \\
6 & $0.65902762237$ & $0.23193336855$ & $0.09157621351$ & 3 & $0.10995174366$ & 3 \\
6 & $0.81684757298$ & $0.09157621351$ & $0.44594849092$ & 3 & $0.22338158968$ & \\
& $0.10810301817$ & $0.44594849092$ & $0.3$ & 1 \\
\hline
\end{tabular}

Table 8.2. Nodes and weights for quadrature formulae on tetrahedra. The nodes are expressed using their barycentric coordinates. The weights do not take into account the measure of the reference element (equal to $1 / 6$ in this case)

\begin{tabular}{cccccccc}
\hline$n q n$ & & \multicolumn{2}{c}{ barycentric coordinates $\lambda_{j}$} & & $m$ & $w_{j}$ & $r$ \\
\hline 1 & $1 / 4$ & $1 / 4$ & $1 / 4$ & $1 / 4$ & 1 & 1 & 1 \\
4 & $0.58541020$ & $0.13819660$ & $0.13819660$ & $0.13819660$ & 4 & $1 / 4$ & 2 \\
5 & $1 / 4$ & $1 / 4$ & $1 / 4$ & $1 / 4$ & 1 & $-16 / 20$ & 3 \\
& $1 / 2$ & $1 / 6$ & $1 / 6$ & $1 / 6$ & 4 & $9 / 20$ & \\
\hline
\end{tabular}

For the reader's convenience we have written, next to the total number $n q n$ of nodes, the multiplicity $m$ of each quadrature node, i.e. the number of nodes generated by the permutations. We have also provided the exactness degree $r$, that is the largest positive integer $r$ for which all polynomials of degree $\leq r$ are integrated exactly by the quadrature formula at hand. Let us see two simple examples. Suppose we want to compute

$$
I=\int_{K} f(\mathbf{x}) d \mathbf{x}=\int_{\widehat{K}} \widehat{f}(\widehat{\mathbf{x}})|J|(\widehat{\mathbf{x}}) d \widehat{\mathbf{x}}
$$

Using the weights and nodes of the first row of the table, we obtain

$$
I \simeq \frac{1}{2} \widehat{f}\left(\frac{1}{3}, \frac{1}{3}\right) J\left(\frac{1}{3}, \frac{1}{3}\right)=\operatorname{Area}(K) f(\overline{\mathbf{x}})
$$

where the coefficient $1 / 2$ represents the area of the reference element, and $\bar{x}$ is the node with barycentric coordinates $\lambda_{1}=\lambda_{2}=\lambda_{3}=1 / 3$ corresponding to the center of gravity of the triangle. The corresponding formula is the well-known composite midpoint formula.

To use the formula in the second row we note that $m=3$, hence we have indeed 3 quadrature nodes whose barycentric coordinates are obtained via cyclic permutation:

$$
\left(\lambda_{0}=1, \lambda_{1}=0, \lambda_{2}=0\right),\left(\lambda_{0}=0, \lambda_{1}=1, \lambda_{2}=0\right),\left(\lambda_{0}=0, \lambda_{1}=0, \lambda_{2}=1\right)
$$

Hence for each triangle $K$ we obtain

$$
\begin{aligned}
\int_{K} f(\mathbf{x}) d \mathbf{x} & \simeq \frac{1}{2} \frac{1}{3}[\widehat{f}(0,0)|\operatorname{det} J(0,0)|+\widehat{f}(1,0)|\operatorname{det} J(1,0)|+\widehat{f}(0,1)|\operatorname{det} J(0,1)|] \\
&=\operatorname{Area}(K) \sum_{i=0}^{2} \frac{1}{3} f\left(\mathbf{N}_{i}\right)
\end{aligned}
$$

$\mathbf{N}_{0}, \mathbf{N}_{1}, \mathbf{N}_{2}$ being the vertices of the triangle $K$, corresponding to the barycentric coordinates $(0,0),(1,0)$ and $(0,1)$ respectively. The corresponding formula therefore yields the composite trapezoidal formula. Both formulae have exactness degree equal to $1$.

Other quadrature formulae for the computation of integrals for different finite elements can be found in [Hug00], [Str71], [FSV12].

Remark 8.1. When using quadrilateral or prismatic elements, nodes and weights of the quadrature formulae can be obtained as the tensor product of the Gauss quadrature formulae for the one-dimensional interval, see Sect. 10.2 (and also [CHQZ06]).

\subsection{Storage of sparse matrices}

As seen in Chapter 4, finite element matrices are sparse. The distribution of non-null elements is retained by the so-called sparsity pattern (also called graph) of the matrix. The pattern depends on the computational grid, on the finite element type and on the numbering of the nodes adopted. The efficient storage of a matrix therefore consists in the storage of its non-null elements, according to the positioning given by the pattern. The discretization of different differential problems sharing the same computational grid and the same type of finite elements leads to matrices with the same graph. Hence in an object-oriented programming logic it can be useful to separate the storage of the graph (which can become a "data type" defined by the user, i.e. a class) from the storage of the values of each matrix. In this way, a matrix can be seen as a data structure for the storage of its values, together with a pointer to the graph associated to it. The pointer only stores the position in the memory where the pattern is stored, hence occupying a minimal amount of memory. Different matrices may therefore share the same graph, without useless storage duplications of the pattern.

In practice, there are several techniques to store sparse matrices efficiently, i.e. the position and value of non-null elements. At this juncture we should observe that, in this context, the adjective "efficient" does not only refer to the lower memory occupation that can be achieved, but also to the speed in accessing memory for each element. A storage format requiring the least possible memory waste is likely to be slower in accessing a desired value. Indeed, a higher storage compactness is typically obtained after finding the position in the memory by accessing the data structures that store the graph. The more intermediate passages are necessary, the longer the access time to the desired element will be. Precisely for the need of finding the right compromise, different storage techniques have been proposed in the literature, with different prerogatives. A review of these, with many comments and remarks, can be found e.g. in [FSV12], Appendix A. Here we just recall a widely used format for the storage of sparse square matrices, i.e. the CSR (Compact Sparse Row) format.

We denote by $n$ the size of the matrix to be stored and by $n z$ the number of its non-null elements. In the CSR format, a matrix is identified by three vectors. Two denoted hereafter $R$ and $C$ - have integer values and form the so-called pattern, i.e. the graph of the sparsity of the matrix, which depends on the mesh and the finite element space; the third vector $A$ contains the non-zero entries of the matrix. More precisely, A has size $n z$ and it stores the entries in a row-wise order. The vector $C$ has size $n z$, as well. The entry $C(k)$ contains the column of the entry $A(k)$. Finally, the vector $R$ has size $n$ and contains pointers to the vector $C$, indicating the beginning of each row. In other terms, $R(k)$ contains the position of the vector $J($ and $A)$ where row $k$ of the matrix begins. To be more concrete, we illustrate this on an example (see Fig. 8.4) where $n=5$ and $n z=17$.

$$
A=\left[\begin{array}{lllll}
0 & 1 & 2 & 3 & 4 \\
a & 0 & f & 0 & g \\
0 & b & k & m & 0 \\
h & l & c & 0 & r \\
0 & n & 0 & d & p \\
i & 0 & s & q & e
\end{array}\right] \quad \begin{aligned}
&0 \\
&1 \\
&2 \\
&3 \\
&4
\end{aligned}
$$

We point out that the numbering of rows and columns in matrices and vectors starts from 0, following the $\mathrm{C}++\operatorname{syntax}$.

The vectors representing this matrix in CSR format are reported in Fig. 8.4. Here, the different background colors in the vectors refer to the different rows of the matrix, and the lines connect the pointers of the vector $R$ to the corresponding entries in the other vectors. In this way, if we want to retrieve a nonzero entry $-$ for instance, in the 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-189.jpg?height=424&width=432&top_left_y=112&top_left_x=241)

Fig. 8.4. Vectors representing the CSR format of Matrix A

second row $-$ we start from position $R(1)$ (because of the $\mathrm{C}++$ numbering) through the entry $R(2)-1$, since $R(2)$ points to the beginning of the third row. The entries $A(R(1), \ldots, R(2)-1)$ are the values of the second row, whose corresponding columns are in $C(R(1), \ldots, R(2)-1)$. In this way it is relatively easy to access a matrix rowwise, whereas the column-wise access is more involved. The column-wise format CSC (Compressed Sparse Colum) is similarly formulated. In some libraries, an even more compact format called Modified Sparse Row (MSR) is used for square matrices. In [FSV12] the latter format is extended to accommodate both row-wise and columnwise matrix access with the same computational cost.

\section{$8.4$ Assembly}

The assembly is the sequence of different operations leading to the construction of the matrix associated to the discretized problem. For doing this, we need two types of information:

1. geometric, typically contained in the mesh file;

2. functional, relative to the representation of the solution via finite elements.

In Fig. $8.5$ we report possible reference geometries with their local vertex numbering. Tetrahedra represent the $3 \mathrm{D}$ extension of the triangular elements considered in Chapter 4. Prismatic elements extend in $3 \mathrm{D}$ the quadrilateral geometric elements in 2 D which will be introduced in Chapter 10, see e.g. [Hug00, Chap. 3], for a complete description. 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-190.jpg?height=316&width=506&top_left_y=252&top_left_x=205)

Fig. 8.5. Illustration of some reference elements available in LifeV with (conventional) local numbering of nodes

Geometric and functional information, suitably coded, is then used to construct the matrix of the discretized problem. As opposed to what would seem natural in the definition of Lagrangian finite elements, the matrix is constructed over cyclic permutations of the elements instead of nodes. The reason for this element-oriented approach, as opposed to the node-oriented one, is essentially linked to computational efficiency matters. The analytical expression of a base function associated to a node varies on each element sharing that node. Prior to the computation of the integrals, it would be necessary to cycle over the nodes and detect the analytical expression of the appropriate basis functions to each element. Hence, we would have to cycle on the nodes and locate the analytic expression of the appropriate basis function for each different element, before carrying out the computation of the integrals. In terms of code, this means that the body of the cycle must be filled with conditional branches, that is instructions of the type if...then...elseif...then...else... within the assembly cycle. These are "costly" operations in computational terms, especially if they lie within a cycle (and thus are carried out several times), as is clear from the large number of micro-assembler instructions required in the compilation phase to expand a conditional branch, with respect to any other instruction, see e.g. [HVZ97]. As we will see, by exploiting the additivity of integration, the element-oriented approach allows to bypass this obstacle in a smart way. In particular (see Chap. 4) the construction of the problem matrix can take place in two conceptual steps, within a cycle on the grid elements:

1. construction of the matrix and of the right-hand side that discretize the differential operator on the element at hand (local matrix and vector);

2. update of the global matrix and the right-hand side, by exploiting the additivity of the integration operation.

There also exist different approaches to the problem: in some cases, the matrix is not constructed, but its effects are directly computed when it multiplies a vector, as happens when the linear system is solved by iterative methods; for reasons of space, here we deal with the more standard approach.

As previously noted, the construction of the local matrix is carried out by integrating on the reference element $\widehat{K}$, using suitable quadrature formulae. Once the matrix and known terms have been constructed, the boundary conditions are imposed; in particular, imposing Dirichlet conditions does not necessarily require the technique seen in Sects. 3.2.2 and 4.5, which consisted in removing the degrees of freedom associated to such conditions after the lift is constructed.

This should explain why assembly is a complicated phase. In the following sections we will discuss the above aspects, though in little detail for reasons of space. First, we will treat the data structures for the coding of geometric (Sect. 8.4.1) and functional (Sect. 8.4.2) information. The computation of the geometric mapping between reference element and current element provides the opportunity of introducing isoparametric elements (Sect. 8.4.3). The effective computation of the local matrix and known term and their use in the construction of the global system are treated in Sect. 8.4.4. Finally, in Sect. $8.4 .5$ we will mention implementation techniques for the lifting of the boundary datum.

\subsubsection{Coding geometrical information}

In terms of data structures, the mesh can be seen as a collection of geometric elements and topological information. The former can be constructed by aggregating classes for the definition of points (i.e. zero-dimensional geometric elements), edges (onedimensional geometric elements), faces (2D) and finally volumes (3D).

Starting from base classes coding these geometrical entities, a mesh will be a class collecting the elements. In fact the geometric structure should be supplemented by the following elements:

1. topological information allowing to characterize the elements in the grid, i.e. the connectivity among nodes, with respect to a conventional numbering of the latter. The convention for the possible elements in LifeV is illustrated in Fig. 8.5; to "visit" the elements of a grid efficiently, we can also add to each given element information on the adjacent elements;

2. specific information allowing to locate the degrees of freedom on the boundary; this simplifies handling the boundary condition prescription; note that we typically associate to each boundary geometric element an indicator that will subsequently be associated to a specific boundary condition. Starting from the reference geometric class, we then retrieve the current geometric elements, according to the possible mappings treated in Sect. 8.4.3. For instance Program 2 gives a portion of a class for a linear (affine) tetrahedron built upon a tetrahedric basic reference shape with some functional information on the associated degrees of freedom.

Currently, no standards are available for the mesh file format. Each mesh generator has its own format. Typically, we expect such a file to contain the vertex coordinates, the connectivity associating the vertices to the geometric elements and the list of boundary elements, with corresponding indicator to be used for defining boundary conditions. The functions with the data serving as boundary conditions, instead, are generally assigned separately.

Remark 8.2. Multi-physics or multi-model problems are becoming a relevant component of scientific computation: think for instance of fluid-structure interaction problems, or the coupling of problems where the full (and computationally costlier) differential model is used only in a specific region of interest, and coupled it with simpler models in the remaining regions. These applications and, more generally, the need to develop parallel computation algorithms, have motivated the development of techniques for the solution of differential problems through domain decomposition (see Chap. 19 and the more comprehensive presentations [QV99, TW05]). In this case, the resulting mesh is the collection of subdomain meshes, together with topological information about subdomain interfaces. In this chapter, however, we will refer to single-domain problems only. 

\subsubsection{Coding of functional information}

As seen in Chapter 4 , basis functions are defined on a reference element. For instance, for tetrahedra, this element coincides with the unit simplex (see Fig. 8.5). The coding of a reference element will basically include pointers to functions for determining basis functions and their derivatives.

In Program 3 we report the functions for the definition of linear finite elements on tetrahedra. For the sake of space, we provide the code for the first derivatives of the first basis functions only.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-193.jpg?height=283&width=722&top_left_y=358&top_left_x=94)

Once the reference element is instantiated, functional information will be available both for the representation of the solution and for the definition of the geometric mapping between reference element and current element, as we explain in the following section.

Having defined the geometric element and the type of finite elements we want to use, we are now able to construct the problem's degrees of freedom. This means assigning to each mesh element the numbering of the degrees of freedom lying on the element and the pattern of the local matrix; the latter is generally full, although it can always contain null elements.

\subsubsection{Mapping between reference and physical element}

In Chapter 4 we saw how convenient it is to write basis functions, quadrature formulae and, therefore, compute integrals with respect to a reference element. It can thus be interesting to examine some practical methods to construct and code such coordinate change. For further details, we refer to [Hug00]. Let us now limit ourselves to considering the case of triangular and tetrahedric elements.

A first type of coordinate transformation is the affine one. Basically, the mapping between $\hat{\mathbf{x}}$ and $\mathbf{x}$ can be expressed via a matrix $B$ and a vector $\mathbf{c}$ (see Sect. 4.5.3 and Fig. 8.6)

$$
\mathbf{x}=B \hat{\mathbf{x}}+\mathbf{c}
$$

In this way, we trivially have that $J=B$ (constant on each element). If the node distribution generated by the grid generator is correct, the determinant of $J$ is always 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-194.jpg?height=350&width=548&top_left_y=112&top_left_x=182)

Fig. 8.6. Mapping between the reference tetrahedron and the current one. The top map is affine, the bottom one quadratic

positive, which guarantees there are no degenerate cases (for instance, four vertices on the same plane in a tetrahedron) and that there are no incorrect permutations in the nodes corresponding to the mapping. The expressions of $B$ and $\mathbf{c}$ can be obtained from those of the node coordinates. Indeed, let us suppose that the nodes, numbered locally $1,2,3,4$, of the reference tetrahedron correspond to the nodes of the mesh numbered as $i, k, l, m$, respectively.

We then have:

$$
\left\{\begin{array}{lll}
x_{i}=c_{1} & y_{i}=c_{2} & y_{i}=c_{3} \\
x_{k}=b_{11}+x_{i} & y_{k}=b_{12}+y_{i} & z_{k}=b_{13}+y_{i} \\
x_{l}=b_{21}+x_{i} & y_{l}=b_{22}+y_{i} & z_{l}=b_{23}+y_{i} \\
x_{m}=b_{31}+x_{i} & y_{m}=b_{32}+y_{i} & z_{m}=b_{33}+y_{i}
\end{array}\right.
$$

from which we obtain the expressions for $B$ and $\mathbf{c}$.

However, there exists a more efficient way to represent the transformation: being element-wise linear, it can be represented via the basis functions of linear Lagrangian finite elements. Indeed, we can write:

$$
x=\sum_{j=0}^{3} X_{j} \widehat{\varphi}_{j}(\widehat{x}, \hat{y}, \widehat{z}), y=\sum_{j=0}^{3} Y_{j} \widehat{\varphi}_{j}(\widehat{x}, \widehat{y}, \widehat{z}), z=\sum_{j=0}^{3} Z_{j} \widehat{\varphi}_{j}(\widehat{x}, \widehat{y}, \widehat{z})
$$

The elements of the Jacobian matrix of the transformation are immediately computed:

$$
J=\left[\begin{array}{cc}
\sum_{j=1}^{4} X_{j} \frac{\partial \widehat{\varphi}_{j}}{\partial \widehat{x}} & \sum_{j=1}^{4} X_{j} \frac{\partial \widehat{\varphi}_{j}}{\partial \widehat{y}} & \sum_{j=1}^{4} X_{j} \frac{\partial \widehat{\varphi}_{j}}{\partial \widehat{z}} \\
\sum_{j=1}^{4} Y_{j} \frac{\partial \widehat{\varphi}_{j}}{\partial \widehat{x}} & \sum_{j=1}^{4} Y_{j} \frac{\partial \widehat{\varphi}_{j}}{\partial \widehat{y}} & \sum_{j=1}^{4} Y_{j} \frac{\partial \widehat{\varphi}_{j}}{\partial \widehat{z}} \\
\sum_{j=1}^{4} Z_{j} \frac{\partial \widehat{\varphi}_{j}}{\partial \widehat{x}} & \sum_{j=1}^{4} Z_{j} \frac{\partial \widehat{\varphi}_{j}}{\partial \widehat{y}} & \sum_{j=1}^{4} Z_{j} \frac{\partial \widehat{\varphi}_{j}}{\partial \widehat{z}}
\end{array}\right]
$$



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-195.jpg?height=399&width=390&top_left_y=111&top_left_x=263)

Fig. 8.7. Mapping between the reference quadrilateral and the current element: affine (top), isoparametric (middle), hybrid (bottom). The latter is constructed with 5 nodes, in order to have a biquadratic transformation for the nodes of a single side

When in a Lagrangian finite element the same basis functions are used for the definition of the geometric mapping, we say that we are dealing with iso-parametric elements (see Figs. 8.6 and 8.7). In the case at hand, this is a consequence of having chosen linear finite elements and affine geometric transformations. When we take finite elements of degree higher than 1 , we can consider two kinds of mapping:

- affine finite elements: in this case, the geometric transformation is still described by the affine transformations (8.6), although the functional information relative to the solution is described by quadratic functions of higher degree; the boundary of the discretized domain $\Omega_{h}$, in this case, is still polygonal (polyhedral);

- isoparametric finite elements: the geometric transformation is described by the same basis functions used to represent the solution; hence the elements in the physical space $O x y z$ will generally have curved sides;

The definition of a quadratic mapping starting from the tetrahedric reference element allows for instance to create tetrahedric quadratic geometric elements, coded in the class QuadraticTetra reported in Program $4$.

i = 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-196.jpg?height=138&width=723&top_left_y=112&top_left_x=95)

Having established the type of reference element and the geometrical mappings, it is possible to construct the collection of "current" elements. The current element can be coded as in Program $5$.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-196.jpg?height=878&width=733&top_left_y=357&top_left_x=93)

As it can be seen, the class contains information relating to the reference element, to the geometric mapping that generates it and to the quadrature formula that will be used for the computation of the integrals.

In particular, (8.7) proves to be very efficient in the coding phase, which we report in Program $6$. The computation of the Jacobian is carried out at the quadrature nodes required for the integral computation (Sec. 8.2).
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-197.jpg?height=542&width=722&top_left_y=284&top_left_x=94)

In the case of quadrilateral and prismatic elements, several of the previous concepts can be extended, by referring e.g. to bilinear or biquadratic mappings. However, guaranteeing that the map is invertible is more difficult: for more details, see [FSV12].

There are cases where it can be convenient to use finite elements of different degree with respect to different coordinates. This is possible using quadrilateral structured grids, where we can construct an element having a biquadratic polynomial on one side, and bilinear polynomials on the remaining sides. In the case of an isoparametric coding of the geometrical mapping, this leads to having, say, quadrilateral elements with three straight sides and one curved side. To this end, we point out that [Hug00, Chap. 4], reports the "incremental" implementation of a quadrilateral element that, starting from a four-node bilinear setting, is enriched by other degrees of freedom up to the biquadratic 9 -node element. 

\subsubsection{Construction of local and global systems}

This phase is the core of the construction of the discretization of differential operators. As an example, let us take the code in Program 7, which constructs the discretization of the elliptic differential equation $-\mu \triangle u+\sigma u=f$.

The overall operation is articulated in a cycle over all the elements of the mesh aMesh. After setting to zero the elementary matrix and vector, these structures are filled incrementally, first with the discretization of the stiffness (diffusion) operator and then with the mass operator (reaction). The source subroutine handles the local right-hand side vector. The assemb subroutines handle the update of the computation in the global matrix, as previously indicated in Fig. 8.2.

In this phase, to avoid checking whether a degree of freedom is on the boundary using conditional branches within the loop, we ignore boundary conditions.
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-198.jpg?height=482&width=714&top_left_y=462&top_left_x=102)

Let us see in detail a possible implementation of the local computation and of the global update separately.

\section{Computation of the local matrices}

Program 8 reports the implementation of the computation of the local matrix of the diffusion operator and of the right-hand side of the linear system.

In particular, we first assemble the diagonal contributions and then the extra-diagonal ones of the local matrix, thus looping over the quadrature nodes. The "core" loop operation is: The instruction

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-199.jpg?height=23&width=188&top_left_y=154&top_left_x=107)

updates the term $i, j$ of the local matrix incrementally: upon call of the ensuing subroutine mass(), the contribution of the reaction operator will be added to the one previously computed.

We proceed in a similar way in source for the computation of the local vector of known terms.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-199.jpg?height=913&width=723&top_left_y=327&top_left_x=95)



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-200.jpg?height=682&width=720&top_left_y=114&top_left_x=95)

\section{Update of the global matrix}

Program 9 contains the update of the global matrix starting from the local ones. The crucial point is the identification of the position of the nodes that compose the current element, on which we have just computed the local matrix within the global one. This operation is performed by looking up the dof.localToGlobal Tables, which contain this type of operation.

For the update of the right-hand side, we perform a similar operation. Obviously, the additivity of the integral requires the operation to be performed by adding the different contributions: this explains the $+=$ in the update of the vector (corresponding to $V[i g]=V[i g]+\operatorname{vec}(i)$ and to the analogous term in M.setmatinc, which stands for set matrix incrementally.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-200.jpg?height=139&width=726&top_left_y=1101&top_left_x=94)



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-201.jpg?height=688&width=694&top_left_y=137&top_left_x=97)

\subsubsection{Boundary conditions prescription}

The need to store sparse matrices efficiently must be compensated by the need to access and manipulate the matrix itself, as we have previously noticed for the CSR format, for instance in the phase of setting the boundary conditions. In a finite element code, the matrix is typically assembled regardless of boundary conditions, so as not to introduce conditional branches within the assembly. Boundary conditions are then introduced by modifying the algebraic system. Imposing Neumann and Robin-type conditions basically translates into the computation of suitable boundary integrals (or, in one-dimensional cases, of values evaluated at the boundary). For instance, Program 10 implements the computation of integrals on the surface for Neumann-type conditions specified in function Bcb. The integral requires a suitable quadrature formula that allows to update the known term b. The structure bdLocalToGlobal allows to transfer the information for each boundary element having Neumann degrees of freedom at the global right-hand side. 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-202.jpg?height=976&width=738&top_left_y=118&top_left_x=82)

Handling Dirichlet (essential) boundary conditions is more complex (see Fig. 8.2). There are various strategies for this operation, some of which are treated in [FSV12]. The most coherent approach to what is prescribed by the theory consists in removing the rows and columns referring to the nodes associated to the Dirichlet boundary conditions from the system obtained during assembly, thus correcting the known term by using the values of the Dirichlet datum we want to impose.

In fact, this coincides with the operation of lifting the boundary datum through a piecewise polynomial function of the chosen degree for the finite elements, and whose support is limited to the only layer of elements of the triangulation that face the boundary (see Fig. $4.13$ in Sect. 4.5.1)

This way of proceeding has the advantage of reducing the dimension of the problem to the effective number of degrees of freedom, however its practical implementation is problematic. Indeed, while for $1 D$ problems, due to the natural ordering of degrees of freedom, the optional rows and columns to be removed are always the first and last one, for multi-dimensional problems the implementation involves eliminating rows and columns whose numbering can be arbitrary, a difficult operation to handle efficiently. It must also be noted that this operation substantially modifies the pattern of the matrix, and this can be inconvenient in case we want to share the latter among several matrices in order to save memory. For this reason, we prefer to consider the Dirichlet condition to be imposed at a given node $k_{D}$ as an equation of the form $u_{k_{D}}=g_{k_{D}}$ replacing the $k_{D}$-th row of the original system. To avoid modifying the matrix pattern, this substitution must be inserted by annihilating the extra-diagonal row elements, except for the diagonal one, which is set to 1 , while the corresponding entry in the right-hand side is set to $g_{k_{D}}$.

This operation only requires row-wise access to the matrix, for which the CSR format is particularly efficient.

\subsection{Integration in time}

Among the different methods to integrate in time, we analyzed the $\theta$ method in the previous chapters, and pointed out a number of other methods, in particular BDF (Backward Difference Formulas) methods implemented in LifeV. An introduction of these methods can be found in [QSS07]. We here recall some of their basic aspects.

Given the system of ordinary differential equations:

$$
M \frac{d \mathbf{u}}{d t}=\mathbf{f}-A \mathbf{u}
$$

and the associated initial datum $\mathbf{u}(t=0)=\mathbf{u}_{0}$, a BDF method is an implicit multi-step method of the form

$$
\frac{\alpha_{0}}{\Delta t} M \mathbf{U}^{n+1}+A \mathbf{U}^{n+1}=\mathbf{f}^{n+1}+\sum_{j=1}^{p} \frac{\alpha_{j}}{\Delta t} \mathbf{U}^{n+1-j}
$$

for suitable $p \geq 1$, where the coefficients are determined so that:

$$
\left.\frac{\partial \mathbf{U}}{\partial t}\right|_{t=t^{n+1}}=\frac{\alpha_{0}}{\Delta t} \mathbf{U}^{n+1}-\sum_{j=1}^{p} \frac{\alpha_{j}}{\Delta t} \mathbf{U}^{n+1-j}+\mathscr{O}\left(\Delta t^{p}\right)
$$

Table 8.3. Coefficients $\alpha_{i}$ for the BDF methods $(p=1,2,3)$ and coefficients $\beta_{i}$ for timeextrapolation

\begin{tabular}{cccccccc}
\hline$p$ & $\alpha_{0}$ & $\alpha_{1}$ & $\alpha_{2}$ & $\alpha_{3}$ & $\beta_{0}$ & $\beta_{1}$ & $\beta_{2}$ \\
\hline 1 & 1 & 1 & $-$ & $-$ & 1 & $-$ & $-$ \\
2 & $3 / 2$ & 2 & $-1 / 2$ & $-$ & 2 & $-1$ & $-$ \\
3 & $11 / 6$ & 3 & $-3 / 2$ & $1 / 3$ & 3 & $-3$ & 1 \\
\hline
\end{tabular}

Here, $\Delta t>0$ is the time-step, $t^{n}=n \Delta t$, and $\mathbf{U}^{n}$ stands for $\mathbf{U}$ at time $t^{n}$. In Table $8.3$ (left) we report the coefficients for $p=1$ (implicit Euler method), $2,3$.

If the matrix $A$ is a function of $\mathbf{u}$, that is when problem $(8.8)$ is nonlinear, BDF methods, being implicit, can be very costly, for they require at each time step the solution of the nonlinear algebraic system in $\mathbf{U}^{n+1}$

$$
\frac{\alpha_{0}}{\Delta t} M \mathbf{U}^{n+1}+A\left(\mathbf{U}^{n+1}\right) \mathbf{U}^{n+1}=\mathbf{f}^{n+1}+\sum_{j=1}^{p} \frac{\alpha_{j}}{\Delta t} \mathbf{U}^{n+1-j}
$$

A possible trade-off that significantly reduces computational costs, without switching to a completely explicit method (whose stability properties can in general be unsatisfactory), is to solve the linear system

$$
\frac{\alpha_{0}}{\Delta t} M \mathbf{U}^{n+1}+A\left(\mathbf{U}^{*}\right) \mathbf{U}^{n+1}=\mathbf{f}^{n+1}+\sum_{j=1}^{p} \frac{\alpha_{j}}{\Delta t} \mathbf{U}^{n+1-j}
$$

where $\mathbf{U}^{*}$ approximates $\mathbf{U}^{n+1}$ using the solutions known from the previous steps. We basically set

$$
\mathbf{U}^{*}=\sum_{j=0}^{p} \beta_{j} \mathbf{U}^{n-j}=\mathbf{U}^{n+1}+\mathscr{O}\left(\Delta t^{p}\right)
$$

for suitable "extrapolation" coefficients $\beta_{j}$. The objective is to reduce the computational costs without dramatically reducing neither the region of absolute stability of the implicit scheme nor the overall accuracy of the time-advancing scheme. Table $8.3$ reports the coefficients $\beta_{j}$.

The coding of a $\mathrm{BDF}$ time integrator can at this point be performed using a dedicated class, reported in Program 11, whose members are:

1. the indicator of the order $p$ which also states the dimension of the vectors $\alpha$ and $\beta$; 2. the vectors $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$;

3. the unknowns matrix given by aligning the vectors $\mathbf{U}^{n}, \mathbf{U}^{n-1}, \ldots \mathbf{U}^{n+1-p}$. The size of each vector, i.e. the number of rows of such matrix (which has $p$ columns) is stored in the size index.

Having assembled the matrices $A$ and $M$, the time-advancing scheme will be performed by computing the matrix $\frac{\alpha_{0}}{\Delta t} M+A$, the right-hand side $\mathbf{f}^{n+1}+\sum_{j=1}^{p} \frac{\alpha_{j}}{\Delta t} \mathbf{U}^{n+1-j}$ and solving system (8.8). In particular, in the implementation presented in Program 11 , the function time der computes the term $\sum_{j=1}^{p} \frac{\alpha_{j}}{\Delta t} \mathbf{U}^{n+1-j}$ by accessing the vector $\alpha$ and the unknowns matrix. In case the problem is nonlinear, we can access to the vector $\boldsymbol{\beta}$ via the function extrap ()

Having computed the solution at the new time step, the unknowns matrix has to "make room for it", by shifting all of its columns to the right, so that the first column is the solution just computed. This operation is performed by the function shift right, which basically copies the next-to-last column of unknowns into the last one, the third from the bottom into the second-last one and so on until the solution computed is fully stored.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-205.jpg?height=908&width=726&top_left_y=333&top_left_x=92)



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-206.jpg?height=613&width=706&top_left_y=114&top_left_x=108)

\subsection{A complete example}

We conclude this chapter with the listing of a program written for the solution of the parabolic diffusion-reaction problem:

$$
\begin{cases}\frac{\partial u}{\partial t}-\mu(t) \Delta u+\sigma(t) u=f, & \mathbf{x} \in \Omega, \quad 0<t \leq 10 \\ u=g_{1}, & \mathbf{x} \in \Gamma_{10} \cup \Gamma_{11}, \quad 0<t \leq 10 \\ u=g_{2}, & \mathbf{x} \in \Gamma_{20} \cup \Gamma_{21}, \quad 0<t \leq 10 \\ \nabla u \cdot \mathbf{n}=0, & \mathbf{x} \in \Gamma_{50}, \quad 0<t \leq 10 \\ u=u_{0}, & \mathbf{x} \in \Omega, \quad t=0\end{cases}
$$

where $\Omega$ is a cubic domain and $\partial \Omega=\Gamma_{10} \cup \Gamma_{11} \cup \Gamma_{20} \cup \Gamma_{21} \cup \Gamma_{50}$. Precisely, the numerical codes on the various boundary portions are:

$$
\begin{aligned}
&\Gamma_{20}: x=0,0<y<1,0<z<1 \\
&\Gamma_{21}: x=0,(y=0,0<z<1) \cup(y=1,0<z<1) \\
&\cup \cup(z=0,0<y<1) \cup(z=0,0<y<1)
\end{aligned}
$$



$$
\begin{aligned}
&\Gamma_{10}: x=1,0<y<1,0<z<1 \\
&\Gamma_{11}: x=1,(y=0,0<z<1) \cup(y=1,0<z<1) \\
&\qquad \cup(z=0,0<y<1) \cup(z=0,0<y<1) ; \\
&\Gamma_{50}: \partial \Omega \backslash\left\{\Gamma_{20} \cup \Gamma_{21} \cup \Gamma_{10} \cup \Gamma_{11}\right\} .
\end{aligned}
$$

In particular, $\mu(t)=t^{2}, \sigma(t)=2, g_{1}(x, y, z, t)=g_{2}(x, y, z, t)=t^{2}+x^{2}, u_{0}(x, y, z)=0$, $f=2 t+2 x^{2}$. The exact solution is precisely $t^{2}+x^{2}$ and the test is made on a cubic grid of 6007 elements with quadratic affine tetrahedra, for a total of 9247 degrees of freedom. The time step is $\Delta t=0.5$, the order of the $\mathrm{BDF}$ scheme is 3 .

Program 12 contains the main program for this example (originally based on the library LifeV) and has been enriched by comments to help the reading, although obviously not everything will be immediately clear just by reading the preceding sections. Coherently with the spirit with which this chapter has been designed, we invite the reader to try to write her/his own code or run the tutorials of libraries such as LifeV, FEniCS or OpenFOAM.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-207.jpg?height=148&width=714&top_left_y=492&top_left_x=95)

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-207.jpg?height=583&width=644&top_left_y=633&top_left_x=119)

// Definition of the parameters for the integration in time 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-209.jpg?height=1122&width=642&top_left_y=116&top_left_x=118)



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-210.jpg?height=1125&width=586&top_left_y=111&top_left_x=149)



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-211.jpg?height=773&width=723&top_left_y=137&top_left_x=95)

This is what we have obtained after running the code

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-211.jpg?height=212&width=327&top_left_y=960&top_left_x=97)

Note that the errors are to be attributed exclusively to the linear system's solution: as the exact solution is a parabolic function in time and space, the choice of finite ele- ments of degree 2 and of the BDF scheme of order 3 guarantees that the discretization errors are non-null. Fig. $8.8$ illustrates the results visualized by Medit.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-212.jpg?height=314&width=635&top_left_y=197&top_left_x=139)

Fig. 8.8. Results of the simulation after 5 (left) and 20 (right) time steps 



\section{The finite volume method}

The finite volume method is a very popular method for the space discretization of partial differential problems in conservation form. For an in-depth presentation of the method, we suggest the monographs [LeV02a], [Wes01] and [Tor09].

As a paradigm to describe the method and illustrate its main features, let us consider the following scalar equation

$$
\frac{\partial u}{\partial t}+\operatorname{div}(\mathbf{F}(u))=s(u), \quad \mathbf{x} \in \Omega, t>0
$$

where $u:(\mathbf{x}, t) \rightarrow \mathbb{R}$ denotes the unknown, $\mathbf{x} \in \Omega \subset \mathbb{R}^{d}(d=1,2,3), \mathbf{F}$ is a given vector function, linear or nonlinear, called flux, $s$ is a given source function. If the flux $\mathbf{F}$ contains terms depending on the first derivatives of $u$, the differential problem is a second-order one. The differential equation $(9.1)$ must be completed by the initial condition $u(\mathbf{x}, 0)=u_{0}(\mathbf{x}), \mathbf{x} \in \Omega$ for $t=0$, as well as by suitable boundary conditions, on the whole boundary $\partial \Omega$ if problem (9.1) is a second-order one, or just on a subset $\partial \Omega^{i n}$ of $\partial \Omega$ (the inflow boundary) in the case of first-order problems. As we will see in Chapter 16 (see Sect. $16.1$ and Sect. 16.4), this type of differential equations are called conservation laws.

The diffusion-transport equations that will be addressed in Chapter 13, the pure transport equations of Chapters $14-16$, and the parabolic ones examined in Chapter 5 , can all be considered as special cases of $(9.1)$. Indeed, all partial differential equations deriving from physical conservation laws can be expressed in conservation form.

Typically, the finite volume method operates on equations written in conservation form such as $(9.1)$.

With some additional effort, we can obviously consider the vector case, where the unknown $\mathbf{u}$ and the source $\mathbf{s}$ are vector functions with $p$ components, while the flux $\mathbf{F}$ is now a tensor with dimension $p \times d$. In particular, also the Navier-Stokes equations and the Euler equations for compressible flows that will be considered in Sect. $16.4$ can be rewritten in conservative form. A finite volume approximation of free-surface incompressible flows for real life applications will be discussed in Sect. 17.11.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-214.jpg?height=250&width=528&top_left_y=115&top_left_x=147)

Fig. 9.1. A control volume in $2 \mathrm{D}$ (left) and 3D (right)

\subsection{Some basic principles}

The preliminary step towards a finite volume discretization of $(9.1)$ consists in identifying a set of polyhedra $\Omega_{i} \subset \Omega$ with diameter less than $h$, called control volumes (or control cells), $i=1, \ldots, M$, such that $\cup_{i} \bar{\Omega}_{i}=\bar{\Omega}$ (we will assume for simplicity that the domain $\Omega$ is polygonal, otherwise $\cup_{i} \bar{\Omega}_{i}$ will be its approximation). See Fig. 9.1 for an example of control volume. We will furthermore suppose the cells to be pairwise disjoint, this being the most commonly used case, although such restriction is not required, in principle, by the method.

Equation (9.1) is integrated on each $\Omega_{i}$; using the divergence theorem we obtain the system of ordinary differential equations

$$
\frac{\partial}{\partial t} \int_{\Omega_{i}} u d \Omega+\int_{\partial \Omega_{i}} \mathbf{F}(u) \cdot \mathbf{n}_{i} d \gamma=\int_{\Omega_{i}} s(u) d \Omega, \quad i=1, \ldots, M
$$

We have denoted by $\mathbf{n}_{i}$ the unit outward normal of $\partial \Omega_{i}$. In two dimensions, let us denote by $L_{i}$ the number of straight sides of $\Omega_{i}$ (in Fig. $\left.9.1 L_{i}=5\right)$ and by $\mathbf{n}_{i j}, j=$ $1, \ldots, L_{i}$, the (constant) outward unit normal vector to the side $l_{i j}$ of $\partial \Omega_{i}$. Then (9.2) can be rewritten as

$$
\frac{\partial}{\partial t} \int_{\Omega_{i}} u d \Omega+\sum_{j=1}^{L_{i}} \int_{l_{i j}} \mathbf{F}(u) \cdot \mathbf{n}_{i j} d \gamma=\int_{\Omega_{i}} s(u) d \Omega, \quad i=1, \ldots, M
$$

Several issues have to be addressed:

- which geometrical shape should the control volumes have;

- how to represent the unknown $u$ in each control volume, that is which are its degrees of freedom and where should they be placed;

- how to approximate the (volume and surface) integrals;

- how to represent the flux $\mathbf{F}(u)$ on each side, as a function of the values of the unknown $u$ on the control volumes adjacent to the side.

For the construction of the control volumes, we usually start from a triangulation $\mathscr{T}_{h}$ of the domain into polygons called elements, say $\left\{K_{m}\right\}$, of the same kind. Typically 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-215.jpg?height=327&width=262&top_left_y=112&top_left_x=324)

Fig. 9.2. An example of blockwise structured mesh

these are triangles or quadrilaterals in $2 \mathrm{D}$, tetrahedra or hexahedra in $3 \mathrm{D}$, as we saw in Chapter 4 when using finite elements. The grid can be structured, blockwise-structured (with either disjoint or overlapping blocks), or unstructured. Structured grids are in general bounded to domains of relatively simple shape, in order for the whole domain, or each block in which it is subdivided, to be mapped to a rectangle or a cube. In Fig. $9.2$ we display a block structured grid on the surface of the appendages of a yacht.

Once the domain has been triangulated, we have two possibilities.

In the so-called cell-centered method, the elements $\left\{K_{m}\right\}$ of $\mathscr{T}_{h}$ directly serve as control volumes. Consequently, the unknowns are associated to an internal point on each element, typically the barycenter, called node. However, this apparently natural choice of control volumes has a disadvantage: as there are no nodes lying on the boundary of $\Omega$, imposing the boundary conditions will require special actions, which we will examine later on. To account for such inconvenient, we can construct control volumes around the elements of $\mathscr{T}_{h}$, where we will place the unknowns. This yields to the socalled vertex-centered schemes.

Sometimes, in multifield problems with several unknowns, both techniques are used at the same time to place different unknowns at different nodes. In this case, we will say that staggered grids are used; we will present a remarkable example in Sect. 17.11, devoted to the discretization of Navier-Stokes equations.

A basic example on a structured quadrangular grid is reported in Fig. $9.3$, where we also show the control volumes for cell-centered and vertex-centered schemes. The latter are defined by the squares

$$
\Omega_{i}^{V}=\left\{\mathbf{x} \in \Omega:\left\|\mathbf{x}-\mathbf{x}_{i}\right\|_{\infty}<h / 2\right\}, \quad \Omega_{i}=\Omega_{i}^{V} \cap \Omega,
$$

where $\left\{\mathbf{x}_{i}\right\}$ are the vertices of the squares $\left\{K_{m}\right\}$ of the initial grid $\mathscr{T}_{h}$, which coincide in this case with the nodes of the control volumes, and $h$ is the uniform length of the element edges.

These two choices do not exhaust the options encountered in the practice. Sometimes the variables are placed on each edge (or face, in 3D) of the grid $\mathscr{T}_{h}$, and the 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-216.jpg?height=197&width=525&top_left_y=113&top_left_x=200)

Fig. 9.3. Control volumes (in grey) generated by a partition of a square domain $\Omega$ with square finite elements of edge $h$. Left: cell-centered case. Right: vertex-centered case

corresponding control volume is formed by the elements of $\mathscr{T}_{h}$ adjacent to the edge (or face).

In general terms, a finite volume approach is simple to implement: the discretization cells can be chosen in a very general form, the solution is typically assumed to be a constant function in each control volume, the Neumann boundary conditions are imposed in a natural way, and the very formulation of the problem expresses the local conservation of the amount $\int_{\Omega_{i}} u d \Omega$. The potential drawbacks are the objective difficulty in drawing high-order schemes, the need to treat essential (Dirichlet) boundary conditions, in particular for the cell-centered methods; finally, the mathematical analysis is less simple than in the case of Galerkin methods, as a direct application of variational techniques used for the former is not straightforward.

It should however be mentioned that some special instances of finite volume approximations can be recovered starting from the discontinuous Galerkin method that will be introduced in Chapter 12. See, e.g., [EGH00], [Riv08].

\subsection{Construction of control volumes for vertex-centered schemes}

In the case the original triangulation $\mathscr{T}_{h}$ is made of triangular unstructured elements in $2 \mathrm{D}$ or tetrahedric ones in $3 \mathrm{D}$, the construction of control volumes around the vertices of $\mathscr{T}_{h}$ is not straightforward. In principle, we could choose as control volume $\Omega_{i}$ the set of all elements containing the vertex $\mathbf{x}_{i}$. However, this would generate control volumes with non-null intersection, a permitted-but not desirable-situation.

We can thus take advantage of some geometrical concepts. Let us consider for example a bounded polygonal domain $\Omega \subset \mathbb{R}^{2}$, and let $\left\{\mathbf{x}_{i}\right\}_{i \in \mathscr{P}}$ be a set of points, which we will call nodes, of $\bar{\Omega}$. Here $\mathscr{P}$ denotes a set of indexes. These points are typically the ones where we intend to provide an approximation of the solution $u$. We associate to each node the polygon

$$
\Omega_{i}^{V}=\left\{\mathbf{x} \in \mathbb{R}^{2}:\left|\mathbf{x}-\mathbf{x}_{i}\right|<\left|\mathbf{x}-\mathbf{x}_{j}\right| \forall j \in \mathscr{P}, j \neq i\right\}
$$

with $i \in \mathscr{P}$. The set $\left\{\Omega_{i}^{V}, i \in \mathscr{P}\right\}$ is called Voronoi diagram, or Voronoi tessellation, associated to the set of points $\left\{\mathbf{x}_{i}\right\}_{i \in \mathscr{P}} ; \Omega_{i}^{V}$ is called i-th Voronoi polygon. For 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-217.jpg?height=197&width=380&top_left_y=110&top_left_x=274)

Fig. 9.4. A Voronoi diagram

an example see Fig. 9.4. The polygons thus obtained are convex, but not necessarily bounded (consider for instance the ones adjacent to the boundary). Their vertices are called Voronoi vertices; a vertex is said regular when it is the meeting point of three Voronoi polygons, and degenerate when it is shared by at least four polygons. A Voronoi diagram with only regular vertices is in turn called regular.

At this point, we can define the control volumes $\Omega_{i}$ introduced in the previous section as

$$
\Omega_{i}=\Omega_{i}^{V} \cap \Omega, \quad i \in \mathscr{P}
$$

For each $i \in \mathscr{P}$, we denote by $\mathscr{P}_{i}$ the set of indexes of the nodes adjacent to $\mathbf{x}_{i}$ i.e.

$$
\mathscr{P}_{i}=\left\{j \in \mathscr{P} \backslash\{i\}: \partial \Omega_{i} \cap \partial \Omega_{j} \neq \emptyset\right\}
$$

Moreover, we denote by $l_{i j}=\partial \Omega_{i} \cap \partial \Omega_{j}, j \in \mathscr{P}_{i}$, a side of the boundary of $\Omega_{i}$ shared by an adjacent control volume, and by $m_{i j}$ its length. If the Voronoi diagram is regular, we have $m_{i j}>0$. In this case, if we connect each node $\mathbf{x}_{i}$ with the nodes of $\mathscr{P}_{i}$, we obtain a triangulation of $\Omega$ coinciding with the Delaunay triangulation (see Sect. 6.4.1) of the convex hull of the nodes. In case there are degenerate vertices in the Voronoi tessellation, from this procedure we can still obtain a Delaunay triangulation, provided we triangulate suitably the polygons $\Omega_{i}$ constructed around the degenerate vertices. Clearly, if $\Omega$ is convex, the above-described process directly provides a Delaunay triangulation of $\Omega$, see e.g. Fig. 9.5. The inverse procedure is also possible, noting that the vertices of the Voronoi diagram correspond to the centres of the
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-217.jpg?height=200&width=508&top_left_y=976&top_left_x=197)

Fig. 9.5. Delaunay triangulation (right) obtained from a Voronoi diagram (left). The dots indicate the nodes $\left\{\mathbf{x}_{i}\right\}_{i \in \mathscr{P}}$ 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-218.jpg?height=186&width=516&top_left_y=116&top_left_x=196)

Fig. 9.6. Voronoi diagram (right) obtained starting from a Delaunay triangulation (left)

circles circumscribed to the triangles (the circumcenters) of the corresponding Delaunay triangulation. The triangle axes thus form the sides of the tessellation. The latter therefore represents a possible set of control volumes associated to a given Delaunay triangulation (see e.g. Fig. 9.6).

The Voronoi diagram and the Delaunay triangulation are dual to one another: Voronoi vertices correspond one-to-one to elements (triangles) of the Delaunay triangulation, and, conversely, Delaunay vertices correspond to the polygons of the tessellation, hence to the nodes.

There are two interesting properties which are worth highlighting. The first one is that the center of the circumscribed circle to an acute triangle $K$ lies within the closure of $K$. Hence if the Delaunay triangulation has no obtuse angles, the vertices of the corresponding Voronoi diagram are all contained in $\bar{\Omega}$. The second is that if we denote by $\mathbf{v}_{i}, i=1,2,3$, the vertices of the non-obtuse triangle $K$, and by $\Omega_{i, K}=\Omega_{i} \cap K$ the portion of the control volume $\Omega_{i}$ included in $K$, then we have the following inequalities between the measures of $K$ and $\Omega_{i, K}$

$$
\frac{1}{4}|K| \leq\left|\Omega_{i, K}\right| \leq \frac{1}{2}|K|, \quad i=1,2,3
$$

An alternative to the construction based on the Voronoi diagram, which does not require a Delaunay triangulation, consists in starting from a triangulation $\mathscr{T}_{h}$ of $\Omega$ formed by any kind of triangles including obtuse ones. If $K$ is the generic triangle of $\mathscr{T}_{h}$ with vertices $\mathbf{v}_{i}, i=1,2,3$, we now define

$$
\Omega_{i, K}=\left\{\mathbf{x} \in K: \lambda_{j}(\mathbf{x})<\lambda_{i}(\mathbf{x}), j \neq i\right\}
$$

where $\lambda_{j}$ are the barycentric coordinates of $K$ (see Sect. $4.4 .3$ for their definition). An example is shown in Fig. 9.7. At this point, the control volumes can be defined in the following way

$$
\Omega_{i}=\operatorname{int}\left(\bigcup_{\left\{K: \mathbf{v}_{i} \in \partial K\right\}} \bar{\Omega}_{i, K}\right), \quad i \in \mathscr{P}
$$

where int $(\mathscr{D})$ denotes the interior of the closed set $\mathscr{D}$. The family $\left\{\Omega_{i}, i \in \mathscr{P}\right\}$ defines the so-called median dual grid (sometimes also named Donald diagram). See Fig. $9.8$ for an example. Consequently, we can define the quantities $l_{i j}, m_{i j}$ and $\mathscr{P}_{i}$ as for the Voronoi diagram. Now the elements $l_{i j}$ are not necessarily straight segments. 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-219.jpg?height=187&width=254&top_left_y=109&top_left_x=333)

Fig. 9.7. A triangle $K$, its center of gravity $\mathbf{G}=\frac{1}{3}\left(\mathbf{v}_{1}+\mathbf{v}_{2}+\mathbf{v}_{3}\right)$, and the polygons $\Omega_{i, K}$
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-219.jpg?height=184&width=536&top_left_y=346&top_left_x=196)

Fig. 9.8. Triangulation of the domain (left) and median dual grid, or Donald diagram (right)

\subsection{Discretization of a diffusion-transport-reaction problem}

Let us consider for the sake of an example equation $(9.1)$ where

$$
\mathbf{F}(u)=-\mu \nabla u+\mathbf{b} u, \quad s(u)=f-\sigma u .
$$

This is a time-dependent diffusion-transport-reaction equation written in conservation form, similar to the one described at the beginning of Chapter 13 . The functions $f, \mu, \sigma$ and $\mathbf{b}$ are given, and fulfill the hypotheses made at the beginning of Chapter 13. As in the case of problem (13.1), we will suppose for simplicity that $u$ satisfies a homogeneous Dirichlet boundary condition, $u=0$ on $\partial \Omega$. Let us suppose that $\Omega$ is partitioned by a Voronoi diagram and consider the corresponding Delaunay triangulation (an instance is provided in Fig. 9.5). What follows can in fact be extended to other types of finite volumes; for that, it will be sufficient to consider the set of the inner indexes only, $\mathscr{P}_{i n t}=\left\{i \in \mathscr{P}: \mathbf{x}_{i} \in \Omega\right\}$, because $u$ vanishes on the boundary. Integrating the assigned equation on the control volume $\Omega_{i}$ as we did in (9.3) and using the divergence theorem, we find

$$
\frac{\partial}{\partial t} \int_{\Omega_{i}} u d \Omega+\sum_{j=1}^{L_{i}} \int_{l_{i j}}\left(-\mu \frac{\partial u}{\partial \mathbf{n}_{i j}}+\mathbf{b} \cdot \mathbf{n}_{i j} u\right) d \gamma=\int_{\Omega_{i}}(f-\sigma u) d \Omega, \quad i \in \mathscr{P}_{i n t}
$$

In order to approximate the line integrals, a typical strategy consists in approximating the functions $\mu$ and $\mathbf{b} \cdot \mathbf{n}_{i j}$ using piecewise constants, and precisely

$$
\left.\mu\right|_{l_{i j}} \simeq \mu_{i j}=\text { const }>0,\left.\quad \mathbf{b} \cdot \mathbf{n}_{i j}\right|_{l_{i j}} \simeq b_{i j}=\text { const }
$$



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-220.jpg?height=205&width=347&top_left_y=118&top_left_x=286)

Fig. 9.9. The segment $l_{i j}$

Such constants can represent either the value of the corresponding function at the midpoint of segment $l_{i j}$, or the mean value on the same side, that is

$$
\mu_{i j}=\frac{1}{m_{i j}} \int_{l_{i j}} \mu d \gamma, \quad b_{i j}=\frac{1}{m_{i j}} \int_{l_{i j}} \mathbf{b} \cdot \mathbf{n}_{i j} d \gamma
$$

As far as the normal derivatives are concerned, an option consists in approximating them using incremental ratios of the type

$$
\frac{\partial u}{\partial \mathbf{n}_{i j}} \simeq \frac{u\left(\mathbf{x}_{j}\right)-u\left(\mathbf{x}_{i}\right)}{\left|\mathbf{x}_{j}-\mathbf{x}_{i}\right|}
$$

(see e.g. Fig. 9.9). This formula is exact if $u$ is linear on the segment connecting $\mathbf{x}_{i}$ and $\mathbf{x}_{j}$. Finally, regarding the approximation of the integral of $u$ on $l_{i j}$, we replace $\left.u\right|_{l_{i j}}$ by a constant obtained by a linear convex combination, that is

$$
\left.u\right|_{l_{i j}} \simeq \rho_{i j} u\left(\mathbf{x}_{i}\right)+\left(1-\rho_{i j}\right) u\left(\mathbf{x}_{j}\right)
$$

with $\rho_{i j} \in[0,1]$ a parameter to be defined. Using the previous approximations, and denoting by $u_{i}$ the approximation of the unknown value $u\left(\mathbf{x}_{i}\right)$, we can derive from (9.8) the following approximate equations

$$
\begin{aligned}
m_{i} \frac{d u_{i}}{d t} &+\sum_{j=1}^{L_{i}} m_{i j}\left\{-\mu_{i j} \frac{u_{j}-u_{i}}{\delta_{i j}}+b_{i j}\left[\rho_{i j} u_{i}+\left(1-\rho_{i j}\right) u_{j}\right]\right\} \\
&+m_{i} \sigma_{i} u_{i}=m_{i} f_{i}, \quad i \in \mathscr{P}
\end{aligned}
$$

having denoted by $m_{i}$ the measure of $\Omega_{i}$, by $\sigma_{i}$ and $f_{i}$ the values of $\sigma$ and $f$ at $\mathbf{x}_{i}$ and by $\delta_{i j}$ the distance between $\mathbf{x}_{i}$ and $\mathbf{x}_{j}$. Note that $(9.10)$ can be written in the form

$$
m_{i} \frac{d u_{i}}{d t}+\sum_{j=1}^{L_{i}} m_{i j} H_{i j}\left(u_{i}, u_{j}\right)+m_{i} \sigma_{i} u_{i}=m_{i} f_{i}
$$

where $H_{i j}$ is the so-called numerical flux representing the contribution of the approximation of the flux through the side $l_{i j}$. The concept of numerical flux is relevant also in the context of finite difference schemes for hyperbolic equations, as we will see in Chapters 14 (Sect. 14.3) and 16 (Sect. 16.3). Some of the features of the numerical flux also translate into scheme properties. For instance, to have a conservative scheme, it will be necessary that $H_{i j}\left(u_{i}, u_{j}\right)=-H_{j i}\left(u_{j}, u_{i}\right)$.

\subsection{Analysis of the finite volume approximation}

The system of equations $(9.10)$ can be rewritten in the form of a discrete variational problem by proceeding in the following way. For each $i=1, \ldots, M$, the i-th equation is multiplied by a real number $v_{i}$ then by summing over the index $i$ we obtain

$$
\begin{aligned}
\sum_{i=1}^{M} m_{i} v_{i} \frac{d u_{i}}{d t} &+\sum_{i=1}^{M} v_{i} \sum_{j=1}^{L_{i}} m_{i j}\left\{-\mu_{i j} \frac{u_{j}-u_{i}}{\delta_{i j}}+b_{i j}\left[\rho_{i j} u_{i}+\left(1-\rho_{i j}\right) u_{j}\right]\right\} \\
&+\sum_{i=1}^{M} m_{i} \sigma_{i} v_{i} u_{i}=\sum_{i=1}^{M} m_{i} v_{i} f_{i}
\end{aligned}
$$

Let us now denote by $V_{h}$ the space of piecewise-linear continuous functions with respect to the Delaunay triangulation $\mathscr{T}_{h}$, which vanish at the boundary $\partial \Omega$ (see (4.17)). From a set of values $v_{i}$ we can univocally reconstruct a function $v_{h} \in V_{h}$ that interpolates such values at the nodes $\mathbf{x}_{i}$, that is (see $\left.(4.7)\right)$

$$
v_{h} \in V_{h}: v_{h}\left(\mathbf{x}_{i}\right)=v_{i}, \quad i=1, \ldots, \dot{M}
$$

In a similar way, let $u_{h} \in V_{h}$ be the function interpolating the values $u_{i}$ at $\mathbf{x}_{i}$. Then, (9.12) is rewritten equivalently in the following discrete variational form: for each $t>0$, find $u_{h}=u_{h}(t) \in V_{h}$ such that

$$
\left(\frac{\partial}{\partial t} u_{h}, v_{h}\right)_{h}+a_{h}\left(u_{h}, v_{h}\right)=\left(f, v_{h}\right)_{h} \quad \forall v_{h} \in V_{h}
$$

having introduced the internal scalar product $\left(w_{h}, v_{h}\right)_{h}=\sum_{i=1}^{M} m_{i} v_{i} w_{i}$ and having denoted by $a_{h}\left(u_{h}, v_{h}\right)$ the bilinear form appearing in the left-hand side of $(9.12)$. We have thus interpreted the finite volume approximation as a particular case of the generalized Galerkin method for the assigned problem (see Sect. 10.4.1, in particular (10.47)). As far as the choice of the coefficients $\rho_{i j}$ for the linear combination is concerned, an option is to use $\rho_{i j}=1 / 2$, which corresponds to using a finite difference of the centered type for the convective term. As we will see in Chapter 14, this strategy is adequate when the so called local Péclet number

$$
\mathbb{P e}_{i j}=\frac{b_{i j} \delta_{i j}}{\mu_{i j}}
$$

(see (13.22)) is less than 1 for every pair $i, j$. If this is not the case, a more careful choice of the coefficients $\rho_{i j}$ for the convex combination is required. In general, $\rho_{i j}=$ $\varphi\left(\mathbb{P e}_{i j}\right)$, where $\varphi$ is a function of the local Péclet number with values in $[0,1]$, that can be chosen as follows: if $\varphi(z)=1 / 2[\operatorname{sign}(z)+1]$ gives a stabilization of upwind type, while choosing $\varphi(z)=1-\left(1-z /\left(e^{z}-1\right)\right) / z$ we will have a stabilization of exponential-fitting type. (A similar kind of stabilization will be used in Sect. $13.6$ in the context of finite difference approximation of diffusion-transport equations.) By this choice, we can show that the bilinear form $a_{h}(\cdot, \cdot)$ is $V_{h}$-elliptic, uniformly with respect to $h$, under the usual hypothesis that the coefficients of the problem satisfy the positivity condition $1 / 2 \operatorname{div}(\mathbf{b})+\sigma \geq \beta_{0}=$ const $\geq 0$.

Precisely, in this case, supposing further that $\mu \geq \mu_{0}=$ const $>0$,

$$
a_{h}\left(v_{h}, v_{h}\right) \geq \mu_{0}\left|v_{h}\right|_{\mathrm{H}^{1}(\Omega)}^{2}+\beta_{0}\left(v_{h}, v_{h}\right)_{h}
$$

Moreover, as $\left(v_{h}, v_{h}\right)_{h}$ is uniformly equivalent to the exact $\mathrm{L}^{2}$-scalar product $\left(v_{h}, v_{h}\right)$ for functions of $V_{h}$, this ensures the stability of problem (9.13). Finally, the method is linearly convergent with respect to $h$. In particular

$$
\left\|u-u_{h}\right\|_{\mathrm{H}^{1}(\Omega)} \leq C h\left(\|u\|_{\mathrm{H}^{2}(\Omega)}+|\nabla f|_{\mathrm{L}^{\infty}(\Omega)}\right)
$$

under the assumption that the norms on the right are bounded. For the proof, see e.g. [KA00]. We suggest the same reference for an analysis of other properties of the method, such as monotonicity and conservation.

\subsection{Implementation of boundary conditions}

As previously stated, the differential problem under exam must be completed by suitable boundary conditions. For a problem written in conservation form, natural boundary conditions would be to impose the fluxes, i.e.

$$
\mathbf{F}(u) \cdot \mathbf{n}=h \quad \text { on } \Gamma_{N} \subset \partial \Omega
$$

For their implementation in the framework of finite volumes it is sufficient to act on the numerical flux relating to the boundary sides, imposing

$$
H_{i k}=H\left(u_{i}, u_{k}\right)=h\left(\mathbf{x}_{i k}\right) \quad \text { if } l_{i k} \subset \Gamma_{N}
$$

where $\mathbf{x}_{i k}$ is a suitable point (typically the midpoint) of $l_{i k}$.

On the other hand, essential (Dirichlet) conditions of the form

$$
u=g \quad \text { on } \Gamma_{D} \subset \partial \Omega
$$

are immediate to implement in the context of vertex-centered schemes, for it is sufficient to add the corresponding equation for the nodes lying on $\Gamma_{D}$. As previously noted, the matter is more complicated for cell-centered schemes, as in this case there are no nodes on the boundary. An option is to impose the conditions weakly, in a similar way to what we will illustrate, although in a different context, in Sect. 15.3.1. This is a matter of suitably modifying the numerical fluxes on the sides, imposing

$$
H_{i k}=H\left(u_{i}, g\left(\mathbf{x}_{i k}\right)\right) \quad \text { if } l_{i k} \subset \Gamma_{D}
$$



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-223.jpg?height=180&width=221&top_left_y=115&top_left_x=346)

Fig. 9.10. The numerical flux on the side $l_{i k}$ belonging to the Dirichlet boundary is computed in order to implement the boundary condition

Fig. $9.10$ illustrates the situation for a cell-centered control volume adjacent to the boundary.

In practice, however, Dirichlet boundary conditions for cell-centered finite volumes are often implemented using the so-called ghost nodes. For each side $l_{i k}$ on the boundary, we generate additional nodes, external to the domain, to which the corresponding boundary values are assigned. In this way, the computation of numerical fluxes is formally the same also for the boundary sides. 

\section{Spectral methods}

As we have seen in Chapter 4 , when we approximate boundary-value problems using the finite element method, the order of convergence is anyhow limited by the degree of the polynomials used, also in the case where solutions are very regular. In this chapter we will introduce spectral methods, for which the convergence rate is only limited by the regularity of the solution of the problem (and is exponential for analytical solutions). For a detailed analysis we refer to [CHQZ06, CHQZ07, Fun92, BM92].

\subsection{The spectral Galerkin method for elliptic problems}

The main feature that distinguishes finite elements from spectral methods in their classical "single-domain" version, is that the latter use global polynomials on the computational domain $\Omega$, instead of piecewise polynomials. This is no longer true in the case of the spectral element method.

For each positive integer $N$, we denote by $\mathbb{Q}_{N}$ the space of polynomials with real coefficients and degree less than or equal to $N$ with respect to each of the variables. Thus in one dimension we will denote by

$$
\mathbb{Q}_{N}(I)=\left\{v(x)=\sum_{k=0}^{N} a_{k} x^{k}, \quad a_{k} \in \mathbb{R}\right\}
$$

the space of polynomials of degree $\leq N$ on the interval $I \subset \mathbb{R}$, while in two dimensions,

$$
\mathbb{Q}_{N}(\Omega)=\left\{v(\mathbf{x})=\sum_{k, m=0}^{N} a_{k m} x_{1}^{k} x_{2}^{m}, \quad a_{k m} \in \mathbb{R}\right\}
$$

will denote the same space, but on the open set $\Omega \subset \mathbb{R}^{2}$. We note that while in one dimension $\mathbb{Q}_{N}=\mathbb{P}_{N}$, in several dimensions this does not happen. In particular, $\operatorname{dim} \mathbb{Q}_{N}=$ $(N+1)^{2}$, while, as already seen in Sect. 4.4.1, $\operatorname{dim} \mathbb{P}_{N}=(N+1)(N+2) / 2$.

Suppose we want to approximate the solution $u$ of an elliptic problem which admits the variational formulation (4.1). 


![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-225.jpg?height=144&width=326&top_left_y=112&top_left_x=293)

Fig. 10.1. Acceptable (left) and unacceptable (right) Dirichlet boundaries for the spectral method SM

Using a spectral Galerkin method $(\mathrm{SM})$, the space $V$ will be approximated by a space $V_{N} \subset \mathbb{Q}_{N}$ and the approximate solution will consequently be indicated by $u_{N}$. In particular, if we suppose that $V$ is $\mathrm{H}_{\Gamma_{D}}^{1}(\Omega)$ (the space defined in $(3.27)), V_{N}$ will denote the set of polynomials of $\mathbb{Q}_{N}$ that vanish on the boundary portion $\Gamma_{D}$ where a Dirichlet condition is prescribed, that is

$$
V_{N}=\left\{v_{N} \in \mathbb{Q}_{N}:\left.\quad v_{N}\right|_{\Gamma_{D}}=0\right\}
$$

It is evident that $V_{N} \subset V$. The spectral Galerkin method SM will therefore be formulated on the subspace $V_{N}$. However, there is an issue in the definition of $V_{N}$ : in the multi-dimensional case it is indeed not possible (in general) to require that a polynomial $v_{N}$ vanishes only on an arbitrary part of the boundary of $\Omega$. For instance, if $\Omega$ is the square $(-1,1)^{2}$, it is impossible to construct a polynomial that is null only on a portion of a boundary edge without it being null on the whole edge (see Fig. 10.1). This does not prevent a polynomial from vanishing on one whole side of the square or on all sides without necessarily being null in the whole of $\Omega$ (for instance, $v_{2}(\mathbf{x})=\left(1-x_{1}^{2}\right)\left(1-x_{2}^{2}\right)$ is null only on the boundary of $\Omega$ ).

For this reason, in the two-dimensional case we limit our attention to square domains (or, more generally, to domains that are reducible, through appropriate transformation, \left. to the reference square $\widehat{\Omega}=(-1,1)^{2}\right)$ and we suppose that the boundary's portion $\Gamma_{D}$ is formed by the union of one or more sides of the domain.

However, the spectral method can be extended to the case of a domain $\Omega$ composed by the union of quadrilaterals $\Omega_{k}$, each of which can be reduced to the reference square $\widehat{\Omega}$ via an invertible transformation $\varphi_{k}: \widehat{\Omega} \rightarrow \Omega_{k}$ (see Fig. 10.2), leading to the socalled spectral element method $(S E M)$, that was introduced by A.T. Patera [Pat84]. It is evident that in such a context it will be possible to require that the solution vanish on portions of the boundary given by the union of sides of the quadrilateral, but naturally not on portions of sides (see Fig. 10.2). In the SEM case, the discrete space has the following form

$$
V_{N}^{C}=\left\{v_{N} \in C^{0}(\bar{\Omega}):\left.v_{N}\right|_{\Omega_{k}} \circ \varphi_{k} \in \mathbb{Q}_{N}(\widehat{\Omega})\right\}
$$

Example 10.1. A particularly important two-dimensional mapping is the transfinite interpolation (called Gordon-Hall transformation as well as Coons patch). The mapping $\varphi_{k}$ is in this case expressed as a function of the invertible mappings $\boldsymbol{\pi}_{k}^{(i)}:(-1,1) \rightarrow$ $\Gamma_{i}$ (for $\left.i=1, \ldots, 4\right)$ that define the four sides of the computational domain $\Omega_{k}$ (see 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-226.jpg?height=242&width=367&top_left_y=115&top_left_x=273)

Fig. 10.2. Decomposition of the solution domain and acceptable boundary conditions for the SEM

Fig. 10.3). The transformation takes the following form

$$
\begin{aligned}
\varphi_{k}(\xi, \eta)=& \frac{1-\eta}{2} \pi_{k}^{1}(\xi)+\frac{1+\eta}{2} \pi_{k}^{3}(\xi) \\
&+\frac{1-\xi}{2}\left[\boldsymbol{\pi}_{k}^{4}(\eta)-\frac{1+\eta}{2} \pi_{k}^{4}(1)-\frac{1-\eta}{2} \pi_{k}^{4}(-1)\right] \\
&+\frac{1+\xi}{2}\left[\boldsymbol{\pi}_{k}^{2}(\eta)-\frac{1+\eta}{2} \pi_{k}^{2}(1)-\frac{1-\eta}{2} \pi_{k}^{2}(-1)\right]
\end{aligned}
$$

The transfinite interpolation therefore allows to consider computational domains $\Omega$ characterized by domains with non-straight edges. For more examples of transformations, see [CHQZ07].

The approximation of problem (4.1) using the Galerkin spectral method (SM) is the following

$$
\text { find } u_{N} \in V_{N}: \quad a\left(u_{N}, v_{N}\right)=F\left(v_{N}\right) \quad \forall v_{N} \in V_{N},
$$

while the spectral element one (SEM) will be

$$
\text { find } u_{N} \in V_{N}^{C}: \quad a_{C}\left(u_{N}, v_{N}\right)=F_{C}\left(v_{N}\right) \quad \forall v_{N} \in V_{N}^{C},
$$

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-226.jpg?height=202&width=429&top_left_y=998&top_left_x=247)

Fig. 10.3. The transformation $\varphi_{k}$ in the case of the transfinite interpolation where

$$
a_{C}\left(u_{N}, v_{N}\right)=\sum_{k} a_{\Omega_{k}}\left(u_{N}, v_{N}\right), \quad F_{C}\left(v_{N}\right)=\sum_{k} F_{\Omega_{k}}\left(v_{N}\right)
$$

$a_{\Omega_{k}}(\cdot, \cdot)$ and $F_{\Omega_{k}}(\cdot)$ being the restrictions of $a(\cdot, \cdot)$ and $F(\cdot)$ to $\Omega_{k}$.

Since these methods represent a special instance of the Galerkin method $(4.2)$, the analysis made in Sect. $4.2$ continues to hold and in particular, the existence, uniqueness, stability and convergence results can be applied.

Moreover, it can be proved that for SM and SEM spectral methods the following a priori error estimates hold:

As opposed to what happens for the finite element method, a greater regularity of the solution leads to an increase in convergence rate, even supposing that the polynomial degree $N$ is fixed. In particular, if $u$ is analytical, the order of convergence of the spectral method becomes more than algebraic, i.e. exponential: more precisely,

$$
\exists \gamma>0: \quad\left\|u-u_{N}\right\|_{\mathrm{H}^{1}(\Omega)} \leq C \exp (-\gamma N)
$$

Also in the case where $u$ has finite regularity, it is still possible to obtain from the spectral method the maximal convergence rate allowed by the regularity of the exact solution: this is a clear advantage of spectral methods over finite elements.

The main limitation (in two or three dimensions) of classical spectral methods is that they can only handle simple geometries: rectangles or quadrilaterals which can be mapped into a square via an invertible transformation. However, as previously mentioned, they can be extended, via the SEM, to the case where the domain is given by the union of quadrilaterals, possibly with curved sides.

A further disadvantage of classical spectral methods lies in the fact that the associated stiffness matrix $\mathrm{A}$ is full in the one-dimensional case, or anyhow much less sparse than the one for finite elements in high dimensions, because the basis functions of such methods have global (and not local) support, see Sects. $10.2$ and $10.3$. The associated system of equations is generally more costly to solve.

Finally, the computational cost required to compute the elements of the stiffness matrix of the right-hand side must not be underestimated, as we are dealing with high degree polynomials. We will sort out this issue in the next section by using well-chosen Gaussian numerical integration.

Remark 10.1. In Sect. $10.5$ at the end of this chapter, we will provide the algebraic formulation of the SEM for a one-dimensional problem. In particular, we will introduce the basis functions for the space $V_{N}^{C}$ of composite polynomials.

Remark 10.2. The SEM formulation is not so different from the $p$ version of the finite element method. In both cases, the number of subdomains $\Omega_{k}$ is fixed while the local degree of polynomials (called $N$ in the case of $\mathrm{SEM}, p$ in the finite element case) is increased locally, in order to improve the accuracy of the numerical approximation. For further details, we refer the interested reader to [CHQZ07, Sch98].

\subsection{Orthogonal polynomials and Gaussian numerical integration}

In this section we introduce the mathematical ingredients that allow to construct numerical integration formulae of Gaussian type. As previously anticipated, such formulae are the basis of pseudo-spectral methods, but also of spectral element methods that make use of numerical integration formulae.

\subsubsection{Orthogonal Legendre polynomials}

Let us consider a function $f:(-1,1) \rightarrow \mathbb{R}$. We recall that the space $\mathrm{L}^{2}(-1,1)$ is defined by (see Sect. 2.3.1)

$$
\mathrm{L}^{2}(-1,1)=\left\{f:(-1,1) \rightarrow \mathbb{R}:\|f\|_{\mathrm{L}^{2}(-1,1)}=\left(\int_{-1}^{1} f^{2}(x) d x\right)^{1 / 2}<\infty\right\}
$$

Its scalar product is given by

$$
(f, g)=\int_{-1}^{1} f(x) g(x) d x
$$

The orthogonal Legendre polynomials $L_{k} \in \mathbb{P}_{k}$, for $k=0,1, \ldots$, constitute a sequence for which the following orthogonality property is satisfied

$$
\left(L_{k}, L_{m}\right)= \begin{cases}0 & \text { if } m \neq k \\ \left(k+\frac{1}{2}\right)^{-1} & \text { if } m=k\end{cases}
$$

They are linearly independent and form a basis for $\mathrm{L}^{2}(-1,1)$. Consequently, each function $f \in \mathrm{L}^{2}(-1,1)$ admits the series expansion

$$
f(x)=\sum_{k=0}^{\infty} \widehat{f}_{k} L_{k}(x)
$$

known as Legendre series. This is a modal representation of $f$. The Legendre coefficients $\widehat{f}_{k}$ can easily be computed by exploiting the orthogonality of Legendre polynomials. Indeed, we have

$$
\begin{aligned}
\left(f, L_{k}\right)=\int_{-1}^{1} f(x) L_{k}(x) d x=\int_{-1}^{1}\left(\sum_{i=0}^{\infty} \widehat{f}_{i} L_{i}(x) L_{k}(x)\right) d x \\
&=\sum_{i=0}^{\infty}\left(\int_{-1}^{1} L_{i}(x) L_{k}(x) d x\right) \widehat{f}_{i}=\widehat{f}_{k}\left\|L_{k}\right\|_{\mathrm{L}^{2}(-1,1)}^{2}
\end{aligned}
$$

Hence,

$$
\widehat{f_{k}}=\left(f, L_{k}\right) /\left\|L_{k}\right\|_{\mathrm{L}^{2}(-1,1)}^{2}=\left(k+\frac{1}{2}\right) \int_{-1}^{1} f(x) L_{k}(x) d x
$$

from which the so-called Parseval identity immediately descends

$$
\|f\|_{\mathrm{L}^{2}(-1,1)}^{2}=\sum_{k=0}^{\infty}\left(\widehat{f}_{k}\right)^{2}\left\|L_{k}\right\|_{\mathrm{L}^{2}(-1,1)}^{2}
$$

It is possible to compute the Legendre polynomials recursively via the following three-term relation:

$$
\begin{aligned}
&L_{0}=1, \quad L_{1}=x \\
&L_{k+1}=\frac{2 k+1}{k+1} x L_{k}-\frac{k}{k+1} L_{k-1}, \quad k=1,2, \ldots
\end{aligned}
$$

(In Fig. 10.4, the graphs of the polynomials $L_{k}$, with $k=2, \ldots, 5$, are drawn). The Legendre series of any $f \in \mathrm{L}^{2}(-1,1)$ converges to $f$ in $\mathrm{L}^{2}(-1,1)$ norm. Denoting by

$$
f_{N}(x)=\sum_{k=0}^{N} \widehat{f}_{k} L_{k}(x)
$$

the $N$-th truncation of the Legendre series of $f$, this means that

$$
\lim _{N \rightarrow \infty}\left\|f-f_{N}\right\|_{\mathrm{L}^{2}(-1,1)}=0
$$

that is

$$
\lim _{N \rightarrow \infty}\left\|\sum_{k=N+1}^{\infty} \widehat{f}_{k} L_{k}\right\|_{\mathrm{L}^{2}(-1,1)}=0
$$

Thanks to the Parseval identity, we have that

$$
\left\|f-f_{N}\right\|_{\mathrm{L}^{2}(-1,1)}^{2}=\sum_{k=N+1}^{\infty}\left(\widehat{f}_{k}\right)^{2}\left\|L_{k}\right\|_{\mathrm{L}^{2}(-1,1)}^{2}=\sum_{k=N+1}^{\infty} \frac{\left(\widehat{f}_{k}\right)^{2}}{k+\frac{1}{2}}
$$



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-230.jpg?height=344&width=475&top_left_y=126&top_left_x=222)

Fig. 10.4. The Legendre polynomials of degree $k=2,3,4,5$

hence condition $(10.7)$ is equivalent to

$$
\lim _{N \rightarrow \infty} \sum_{k=N+1}^{\infty} \frac{\left(\widehat{f}_{k}\right)^{2}}{k+\frac{1}{2}}=0
$$

Moreover, it can be proved that if $f \in \mathrm{H}^{s}(-1,1)$, for some $s \geq 1$, then it is possible to find a suitable constant $C_{s}>0$, independent of $N$, such that

$$
\left\|f-f_{N}\right\|_{\mathrm{L}^{2}(-1,1)} \leq C_{s}\left(\frac{1}{N}\right)^{s}\left\|f^{(s)}\right\|_{\mathrm{L}^{2}(-1,1)}
$$

i.e. we have convergence of order $s$, with respect to $1 / N$.

At this point, we can prove that $f_{N}$ is the orthogonal projection of $f$ on $\mathbb{Q}_{N}$ with respect to the scalar product of $\mathrm{L}^{2}(-1,1)$, that is

$$
\left(f-f_{N}, p\right)=0 \quad \forall p \in \mathbb{Q}_{N}
$$

First of all we note that

$$
\left(f-f_{N}, L_{m}\right)=\left(\sum_{k=N+1}^{\infty} \widehat{f}_{k} L_{k}, L_{m}\right)=\sum_{k=N+1}^{\infty} \widehat{f}_{k}\left(L_{k}, L_{m}\right)
$$

Since the polynomials $L_{k}$, with $0 \leq k \leq N$, form a basis for the space $\mathbb{Q}_{N}$, every polynomial $p \in \mathbb{Q}_{N}$ can be expanded with respect to this basis. Equation $(10.8)$ follows noticing that for $m \leq N,\left(L_{k}, L_{m}\right)=0 \quad \forall k \geq N+1$ because of orthogonality.

In particular, from $(10.8)$ it follows that $f_{N}$ is the function which minimizes the distance of $f$ from $\mathbb{Q}_{N}$, that is

$$
\left\|f-f_{N}\right\|_{\mathrm{L}^{2}(-1,1)} \leq\|f-p\|_{\mathrm{L}^{2}(-1,1)} \quad \forall p \in \mathbb{Q}_{N}
$$

For this purpose, we start by observing that

$$
\left\|f-f_{N}\right\|_{\mathrm{L}^{2}(-1,1)}^{2}=\left(f-f_{N}, f-f_{N}\right)=\left(f-f_{N}, f-p\right)+\left(f-f_{N}, p-f_{N}\right)
$$

for each $p \in \mathbb{Q}_{N}$ and that $\left(f-f_{N}, p-f_{N}\right)=0$ by the orthogonality property $(10.8)$. Consequently,

$$
\left\|f-f_{N}\right\|_{\mathrm{L}^{2}(-1,1)}^{2}=\left(f-f_{N}, f-p\right) \quad \forall p \in \mathbb{Q}_{N}
$$

from which, applying the Cauchy-Schwarz inequality, we obtain

$$
\left\|f-f_{N}\right\|_{\mathrm{L}^{2}(-1,1)}^{2} \leq\left\|f-f_{N}\right\|_{\mathrm{L}^{2}(-1,1)}\|f-p\|_{\mathrm{L}^{2}(-1,1)} \quad \forall p \in \mathbb{Q}_{N}
$$

i.e. (10.9).

\subsubsection{Gaussian integration}

Gaussian integration formulae are the ones which, having fixed the number of quadrature nodes, allow to obtain the highest exactness degree (see [QSS07]). The latter is the highest integer $r$ such that all polynomials of degree less than or equal to $r$ are integrated exactly by the formula at hand. We will start by introducing such formulae on the interval $(-1,1)$, and then extend them to the case of a generic interval.

We denote by $N$ the number of nodes. We call Gauss-Legendre quadrature nodes the zeroes $\left\{\bar{x}_{1}, \ldots, \bar{x}_{N}\right\}$ of the Legendre polynomial $L_{N}$. In the presence of such a set of nodes, we will consider the following quadrature formula (called interpolatory of Gauss-Legendre)

$$
I_{N-1}^{G L} f=\int_{-1}^{1} \Pi_{N-1}^{G L} f(x) d x
$$

$\Pi_{N-1}^{G L} f$ being the polynomial of degree $N-1$ interpolating $f$ at the nodes $\bar{x}_{1}, \ldots, \bar{x}_{N}$ We denote by $\bar{\psi}_{k} \in \mathbb{Q}_{N-1}, k=1, \ldots, N$, the characteristic Lagrange polynomials associated to the Gauss-Legendre nodes,

$$
\bar{\psi}_{k}\left(\bar{x}_{j}\right)=\delta_{k j}, \quad j=1, \ldots, N
$$

The quadrature formula (10.10) then takes the following expression

$$
\int_{-1}^{1} f(x) d x \simeq I_{N-1}^{G L} f=\sum_{k=1}^{N} \bar{\alpha}_{k} f\left(\bar{x}_{k}\right), \quad \text { with } \bar{\alpha}_{k}=\int_{-1}^{1} \bar{\psi}_{k}(x) d x
$$

and is called Gauss-Legendre quadrature formula (GL).

To find the nodes $\bar{t}_{k}$ and the weights $\bar{\delta}_{k}$ characterizing such formula on a generic interval $[a, b]$, it will be sufficient to refer to the relation

$$
\bar{t}_{k}=\frac{b-a}{2} \bar{x}_{k}+\frac{a+b}{2}
$$

for the former, while, for the latter, it can easily be verified that

$$
\bar{\delta}_{k}=\frac{b-a}{2} \bar{\alpha}_{k}
$$

The exactness degree of these formulae is equal to $2 N-1$ (and is the maximum possible for formulae with $N-1$ nodes). This means that

$$
\int_{a}^{b} f(x) d x=\sum_{k=1}^{N} \bar{\delta}_{k} f\left(\bar{t}_{k}\right) \quad \forall f \in \mathbb{Q}_{2 N-1}
$$

\subsubsection{Gauss-Legendre-Lobatto formulae}

A feature of the Gauss-Legendre integration formulae is to have all quadrature nodes internal to the integration interval. In the case of differential problems this makes the imposition of boundary conditions on the end points of the interval problematic.

To overcome such difficulty, the so-called Gauss-Lobatto formulae are introduced, particularly the Gauss-Legendre-Lobatto (GLL) formulae. There, nodes, relative to the interval $(-1,1)$, are represented by the end points of the interval themselves and by the maximum and minimum points of the Legendre polynomial of degree $N$, i.e. by the zeroes of the first derivative of the polynomial $L_{N}$.

We denote such nodes by $\left\{x_{0}=-1, x_{1}, \ldots, x_{N-1}, x_{N}=1\right\}$. Therefore, we have

$$
L_{N}^{\prime}\left(x_{i}\right)=0, \quad \text { for } i=1, \ldots, N-1
$$

(In this chapter the symbol "' " denotes a derivative with respect to $x$.) Let $\psi_{i}$ be the corresponding characteristic polynomials, that is

$$
\psi_{i} \in \mathbb{Q}_{N}: \quad \psi_{i}\left(x_{j}\right)=\delta_{i j}, \quad 0 \leq i, j \leq N
$$

whose analytical expression is given by

$$
\psi_{i}(x)=\frac{-1}{N(N+1)} \frac{\left(1-x^{2}\right) L_{N}^{\prime}(x)}{\left(x-x_{i}\right) L_{N}\left(x_{i}\right)}, \quad i=0, \ldots, N
$$

(see Fig. $10.5$ for the graphs of the characteristic polynomials $\psi_{i}$, for $i=0, \ldots, 4$ in the case where $N=4$ ). The functions $\psi_{i}(x)$ are the counterpart of the Lagrangian basis functions $\left\{\varphi_{i}\right\}$ of the finite elements introduced in Sect. 4.3. Given a function $f \in$ $C^{0}([-1,1])$, its interpolation polynomial $\Pi_{N}^{G L L} f \in \mathbb{Q}_{N}$ at the GLL nodes is identified by the relation

$$
\Pi_{N}^{G L L} f\left(x_{i}\right)=f\left(x_{i}\right), \quad 0 \leq i \leq N
$$

It has the following expression

$$
\Pi_{N}^{G L L} f(x)=\sum_{i=0}^{N} f\left(x_{i}\right) \psi_{i}(x)
$$



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-233.jpg?height=317&width=436&top_left_y=126&top_left_x=240)

Fig. 10.5. The characteristic polynomials $\psi_{i}, i=0, \ldots, 4$ of degree 4 corresponding to the Gauss-Legendre-Lobatto nodes

It can be proved, thanks to the non-uniform distribution of the nodes $\left\{x_{i}\right\}$, that $\Pi_{N}^{G L L} f$ converges towards $f$ when $N \rightarrow \infty$. Moreover, the following error estimate is satisfied: if $f \in \mathrm{H}^{s}(-1,1)$, for some $s \geq 1$,

$$
\left\|f-\Pi_{N}^{G L L} f\right\|_{\mathrm{L}^{2}(-1,1)} \leq C_{s}\left(\frac{1}{N}\right)^{s}\left\|f^{(s)}\right\|_{\mathrm{L}^{2}(-1,1)}
$$

where $C_{s}$ is a constant depending on $s$ but not on $N$. More generally (see [CHQZ06]),

$$
\left\|f-\Pi_{N}^{G L L} f\right\|_{\mathrm{H}^{k}(-1,1)} \leq C_{s}\left(\frac{1}{N}\right)^{s-k}\|f\|_{\mathrm{H}^{s}(-1,1)}, \quad s \geq 1, k=0,1
$$

In Fig. $10.6$ (left), we show the convergence curves for the interpolation error of two different functions.

By using $\Pi_{N}^{G L L} f$ instead of $\Pi_{N-1}^{G L L} f$ we can define the following Gauss-LegendreLobatto (GLL) integration formula, in alternative to $(10.10)$

$$
I_{N}^{G L L} f=\int_{-1}^{1} \Pi_{N}^{G L L} f(x) d x=\sum_{k=0}^{N} \alpha_{k} f\left(x_{x}\right)
$$

The new weights are $\alpha_{i}=\int_{-1}^{1} \psi_{i}(x) d x$ and take the following expression

$$
\alpha_{i}=\frac{2}{N(N+1)} \frac{1}{L_{N}^{2}\left(x_{i}\right)}
$$

The GLL formula has exactness degree equal to $2 N-1$, that is it integrates exactly all polynomials of degree $\leq 2 N-1$,

$$
\int_{-1}^{1} f(x) d x=I_{N}^{G L L} f \quad \forall f \in \mathbb{Q}_{2 N-1}
$$


![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-234.jpg?height=236&width=712&top_left_y=117&top_left_x=100)

Fig. 10.6. Behaviour of the interpolation (left) and integration (right) error in the GLL nodes as a function of the degree $N$ for the two functions $f_{1}(x)=\cos (4 \pi x)$ (\bullet) and $f_{2}(x)=$ $4 \cos (4 x) \exp (\sin (4 x))(\mathbf{w})$ on the interval $(-1,1)$

This is the maximum degree obtainable when $N+1$ nodes are used, 2 of which assigned a priori. Moreover, using the interpolation estimate (10.16), the following integration error estimate can be proved: if $f \in \mathrm{H}^{s}(-1,1)$, with $s \geq 1$,

$$
\left|\int_{-1}^{1} f(x) d x-I_{N}^{G L L} f\right| \leq C_{s}\left(\frac{1}{N}\right)^{s}\left\|f^{(s)}\right\|_{L^{2}(-1,1)}
$$

where $C_{s}$ is independent of $N$ but can depend, in general, on $s$. This means that the more regular the function $f$ is, the higher is the order of convergence of the integration formula. In Fig. $10.6$ (right) we report the integration error for two different functions (the same ones considered for the left graph).

If we now consider a generic interval $(a, b)$ instead of $(-1,1)$, nodes and weights in $(a, b)$ take the following expression

$$
t_{k}=\frac{b-a}{2} x_{k}+\frac{a+b}{2}, \quad \delta_{k}=\frac{b-a}{2} \alpha_{k} .
$$

Formula (10.20) generalizes as follows

$$
\int_{a}^{b} f(x) d x \simeq \sum_{k=0}^{N} \delta_{k} f\left(t_{k}\right)
$$

The properties of exactness and accuracy remain unchanged.

\section{$10.3$ G-NI methods in one dimension}

Let us consider the following one-dimensional elliptic problem with homogeneous Dirichlet data

$$
\begin{cases}L u=-\left(\mu u^{\prime}\right)^{\prime}+\sigma u=f, & -1<x<1 \\ u(-1)=0, & u(1)=0\end{cases}
$$

$\mu(x) \geq \mu_{0}>0$ and $\sigma(x) \geq 0$, in order to have an associated bilinear form that is coercive in $\mathrm{H}_{0}^{1}(-1,1)$.

The spectral Galerkin method (SM) is written as

$$
\text { find } u_{N} \in V_{N}: \int_{-1}^{1} \mu u_{N}^{\prime} v_{N}^{\prime} d x+\int_{-1}^{1} \sigma u_{N} v_{N} d x=\int_{-1}^{1} f v_{N} d x \quad \forall v_{N} \in V_{N}
$$

with

$$
V_{N}=\left\{v_{N} \in \mathbb{Q}_{N}: v_{N}(-1)=v_{N}(1)=0\right\}
$$

The G-NI (Galerkin with Numerical Integration) method is obtained by approximating the integrals in (10.23) via the GLL integration formulae. This amounts to substituting the scalar product $(f, g)$ in $\mathrm{L}^{2}(-1,1)$ by the discrete GLL scalar product (for continuous functions)

$$
(f, g)_{N}=\sum_{i=0}^{N} \alpha_{i} f\left(x_{i}\right) g\left(x_{i}\right)
$$

where the $x_{i}$ and the $\alpha_{i}$ are defined according to $(10.11)$ and $(10.19)$. Hence, the $\mathrm{G}$-NI method is written as

$$
\text { find } u_{N}^{*} \in V_{N}:\left(\mu u_{N}^{*}, v_{N}^{\prime}\right)_{N}+\left(\sigma u_{N}^{*}, v_{N}\right)_{N}=\left(f, v_{N}\right)_{N} \quad \forall v_{N} \in V_{N}
$$

Due to the numerical integration, in general $u_{N}^{*} \neq u_{N}$, that is the solutions of the SM and G-NI methods do not coincide.

However, thanks to the exactness property (10.20), we will have

$$
(f, g)_{N}=(f, g) \quad \forall f, g: \quad f g \in \mathbb{Q}_{2 N-1} .
$$

If we consider the particular case where in $(10.22) \mu$ is a constant and $\sigma=0$, the G-NI problem becomes

$$
\mu\left(u_{N}^{* \prime}, v_{N}^{\prime}\right)_{N}=\left(f, v_{N}\right)_{N}
$$

In some very particular cases, the spectral and the G-NI methods coincide. This is for instance the case of $(10.28)$, where $f$ is a polynomial with degree equal at most to $N-1$. It is simple to verify that the two methods coincide thanks to the exactness relation $(10.27)$.

Generalizing to the case of more complex differential formulations having different boundary conditions (Neumann, or mixed), the G-NI problem is written as

$$
\text { find } u_{N}^{*} \in V_{N}: \quad a_{N}\left(u_{N}^{*}, v_{N}\right)=F_{N}\left(v_{N}\right) \quad \forall v_{N} \in V_{N}
$$

where $a_{N}(\cdot, \cdot)$ and $F_{N}(\cdot)$ are obtained starting from the bilinear form $a(\cdot, \cdot)$ and from the known term $F(\cdot)$ of the spectral Galerkin problem, by substituting the exact integrals with the GLL integration formulae. $V_{N}$ is the space of polynomials of degree $N$ that vanish on the boundary points (provided that there are any) on which Dirichlet conditions are imposed. Observe that, due of the fact that the bilinear form $a_{N}(\cdot, \cdot)$ and the functional $F_{N}(\cdot)$ are no longer the ones associated to the initial problem, what we obtain is no longer a Galerkin approximation method, and the relative theoretical results cannot be applied (in particular, the Céa lemma, see Lemma 4.1).

In general, a method derived from a Galerkin method, either spectral or with finite elements, where numerical integrals replace exact ones, will be called generalized Galerkin method $(G G)$. For the corresponding analysis we will resort to the Strang lemma (see Sect. 10.4.1 and also [Cia78, QV94]) .

\subsubsection{Algebraic interpretation of the G-NI method}

The functions $\psi_{i}$, with $i=1,2, \ldots, N-1$, introduced in Sect. 10.2.3 consititute a basis for the space $V_{N}$, as they are all null in $x_{0}=-1$ and $x_{N}=1$. We can therefore provide for the solution $u_{N}^{*}$ of the G-NI problem $(10.29)$ the nodal representation

$$
u_{N}^{*}(x)=\sum_{i=1}^{N-1} u_{N}^{*}\left(x_{i}\right) \psi_{i}(x)
$$

In analogy with the finite-element method, this means we identify the unknowns of our problem with the values taken by $u_{N}^{*}$ at the nodes $x_{i}$ (now coinciding with the Gauss-Legendre-Lobatto nodes). Moreover, for problem (10.29) to be verified for each $v_{N} \in V_{N}$, it will be sufficient that it be verified for each basis function $\psi_{i}$. We will therefore have

$$
\sum_{j=1}^{N-1} u_{N}^{*}\left(x_{j}\right) a_{N}\left(\psi_{j}, \psi_{i}\right)=F_{N}\left(\psi_{i}\right), \quad i=1,2, \ldots, N-1
$$

which we can rewrite

$$
\sum_{j=1}^{N-1} a_{i j} u_{N}^{*}\left(x_{j}\right)=f_{i}, \quad i=1,2, \ldots, N-1
$$

that is, in matrix form,

$$
\mathrm{Au}_{N}^{*}=\mathbf{f}
$$

where

$$
\mathrm{A}=\left(a_{i j}\right) \text { with } \quad a_{i j}=a_{N}\left(\psi_{j}, \psi_{i}\right), \quad \mathbf{f}=\left(f_{i}\right) \quad \text { with } \quad f_{i}=F_{N}\left(\psi_{i}\right),
$$

and where $\mathbf{u}_{N}^{*}$ denotes the vector of unknown coefficients $u_{N}^{*}\left(x_{j}\right)$, for $j=1, \ldots, N-1$. In the particular case of problem (10.26), we would obtain

$$
a_{i j}=\left(\mu \psi_{j}^{\prime}, \psi_{i}^{\prime}\right)_{N}+\alpha_{i} \sigma\left(x_{i}\right) \delta_{i j}, \quad f_{i}=\left(f, \psi_{i}\right)_{N}=\alpha_{i} f\left(x_{i}\right)
$$

for each $i, j=1, \ldots, N-1$. The matrix in $1 \mathrm{D}$ is full due to the presence of the diffusive term. Indeed, the reactive term only contributes to the diagonal. In more dimensions, the matrix $A$ has a block structure, and the diagonal blocks are full. See Fig. 10.7, 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-237.jpg?height=332&width=680&top_left_y=118&top_left_x=114)

Fig. 10.7. Pattern of the matrix $A$ for the G-NI method, the $2 D$ (left) and $3 D$ (right) case: $\mathrm{nz}$ denotes the number of non-null elements in the matrix

reporting the pattern relating to matrix $A$ in $2 \mathrm{D}$ and $3 \mathrm{D}$. Finally, we observe that the condition number we would get in the absence of numerical integration results is, in general, much larger, namely $O\left(N^{4}\right)$. Moreover, the matrix $A$ turns out to be illconditioned, with condition number $O\left(N^{3}\right)$. For the solution of system $(10.30)$ it is therefore convenient to resort, especially in $2 \mathrm{D}$ and $3 \mathrm{D}$, to a suitably preconditioned iterative method. By choosing as a preconditioner the matrix of linear finite elements associated to the same bilinear form $a(\cdot, \cdot)$ and to the GLL nodes, we obtain a preconditioned matrix whose conditioning is independent of $N$ ([CHQZ06]). At the top of Fig. $10.8$ we report the condition number (as a function of $N$ ) of the matrix $A$ and of the matrix obtained by preconditioning $A$ with different preconditioning matrices: the diagonal matrix of $A$, the one obtained from $A$ through the incomplete Cholesky factorization, the one obtained using linear finite elements by approximating integrals with the composite trapezoidal formula, and finally the exact one from finite elements.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-237.jpg?height=242&width=351&top_left_y=934&top_left_x=100)
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-237.jpg?height=242&width=712&top_left_y=934&top_left_x=100tioners At the bottom of Fig. $10.8$ we report the number of necessary iterations for the conjugate gradient method to converge in the different cases.

\subsubsection{Conditioning of the stiffness matrix in the G-NI method}

We seek estimates for the eigenvalues $\lambda^{N}$ of the stiffness matrix $A$ of the G-NI method

$$
A \mathbf{u}=\lambda^{N} \mathbf{u}
$$

In the case of the simple second derivative operator, we have $A=\left(a_{i j}\right)$, with $a_{i j}=$ $\left(\psi_{j}^{\prime}, \psi_{i}^{\prime}\right)_{N}=\left(\psi_{j}^{\prime}, \psi_{i}^{\prime}\right), \psi_{j}$ being the $j$-th characteristic Lagrange function associated to the node $x_{j}$. Then,

$$
\lambda^{N}=\frac{\mathbf{u}^{T} A \mathbf{u}}{\mathbf{u}^{T} \mathbf{u}}=\frac{\left\|u_{x}^{N}\right\|_{\mathrm{L}^{2}(-1,1)}^{2}}{\mathbf{u}^{T} \mathbf{u}}
$$

$u^{N} \in V_{N}$ being the only polynomial of the space $V_{N}$ defined in $(10.24)$ satisfying $u^{N}\left(x_{j}\right)=u_{j}$, for $j=1, \ldots, N-1$, where $\mathbf{u}=\left(u_{j}\right)$. By setting

$$
u_{j}=\int_{-1}^{x_{j}} u_{x}^{N}(s) d s
$$

thanks to the Cauchy-Schwarz inequality we obtain the bound

$$
\left|u_{j}\right| \leq\left(\int_{-1}^{x_{j}}\left|\left(u^{N}\right)^{\prime}(s)\right|^{2} d s\right)^{1 / 2}\left(\int_{-1}^{x_{j}} d s\right)^{1 / 2} \leq \sqrt{2}\left\|\left(u^{N}\right)^{\prime}\right\|_{\mathbf{L}^{2}(-1,1)}
$$

Hence

$$
\mathbf{u}^{T} \mathbf{u}=\sum_{j=1}^{N-1} u_{j}^{2} \leq 2(N-1)\left\|\left(u^{N}\right)^{\prime}\right\|_{\mathrm{L}^{2}(-1,1)}^{2}
$$

which, thanks to $(10.31)$, provides the lower bound

$$
\lambda^{N} \geq \frac{1}{2(N-1)}
$$

An upper bound for $\lambda^{N}$ can be obtained by recurring to the following inverse inequality for algebric polynomials (see [CHQZ06], Sect. 5.4.1)

$$
\forall p \in V_{N}, \quad\left\|p^{\prime}\right\|_{\mathrm{L}^{2}(-1,1)} \leq \sqrt{2} N\left(\int_{-1}^{1} \frac{p^{2}(x)}{1-x^{2}} d x\right)^{1 / 2}
$$

Then

$$
\left\|\left(u^{N}\right)^{\prime}\right\|_{\mathrm{L}^{2}(-1,1)}^{2} \leq 2 N^{2} \int_{-1}^{1} \frac{\left[u^{N}(x)\right]^{2}}{1-x^{2}} d x=2 N^{2} \sum_{j=1}^{N-1} \frac{\left[u^{N}\left(x_{j}\right)\right]^{2}}{1-x_{j}^{2}} \alpha_{j}
$$

where we use the exactness of the GLL integration formula (see $(10.20))$, as $\left[u^{N}\right]^{2} /\left(1-x^{2}\right) \in \mathbb{P}_{2 N-2}$. Since for the coefficients $\alpha_{j}$ the following asymptotic estimate holds: $\alpha_{j} /\left(1-x_{j}^{2}\right) \leq C$, for a suitable constant $C$ independent of $N$, we can conclude, thanks to $(10.31)$ and $(10.34)$, that

$$
\lambda^{N} \leq 2 C N^{2}
$$

It can finally be proved that both estimates (10.32) and (10.35) are optimal as far as the asymptotic behaviour with respect to $N$ is concerned.

\subsubsection{Equivalence between G-NI and collocation methods}

We want to prove that the G-NI method con be interpreted as a collocation method, i.e. one imposing the differential equation only at selected points of the computational interval. Let us consider once again the homogeneous Dirichlet problem $(10.22)$, whose associated G-NI problem is written is the form $(10.26)$.

We would like to counterintegrate by parts equation (10.26), but in order to do that, we must first rewrite the discrete scalar products as integrals. Let $\Pi_{N}^{G L L}: C^{0}([-1,1]) \mapsto$ $\mathbb{Q}_{N}$ be the interpolation operator introduced in Sect. 10.2.3, which maps a continuous function to the corresponding interpolating polynomial at the Gauss-LegendreLobatto nodes.

Since the GLL integration formula uses the values of the function only at the integration nodes and since the function and its G-NI interpolant coincide there, we have

$$
\sum_{i=0}^{N} \alpha_{i} f\left(x_{i}\right)=\sum_{i=0}^{N} \alpha_{i} \Pi_{N}^{G L L} f\left(x_{i}\right)=\int_{-1}^{1} \Pi_{N}^{G L L} f(x) d x
$$

where the latter equality descends from (10.20) as $\Pi_{N}^{G L L} f$ is integrated exactly, being a polynomial of degree $N$.

The discrete scalar product thus becomes a scalar product in $\mathrm{L}^{2}(-1,1)$, in the case where one of the two functions is a polynomial of degree strictly less than $N$, i.e.

$$
(f, g)_{N}=\left(\Pi_{N}^{G L L} f, g\right)_{N}=\left(\Pi_{N}^{G L L} f, g\right) \quad \forall g \in \mathbb{Q}_{N-1}
$$

In this case, indeed, $\Pi_{N}^{G L L} f \in \mathbb{Q}_{N},\left(\Pi_{N}^{G L L} f\right) g \in \mathbb{Q}_{2 N-1}$ and therefore the integral is computed exactly. Integrating by parts the exact integrals, we obtain $^{1}$

$$
\begin{aligned}
\left(\mu u_{N}^{\prime}, v_{N}^{\prime}\right)_{N} &=\left(\Pi_{N}^{G L L}\left(\mu u_{N}^{\prime}\right), v_{N}^{\prime}\right)_{N}=\left(\Pi_{N}^{G L L}\left(\mu u_{N}^{\prime}\right), v_{N}^{\prime}\right) \\
&=-\left(\left[\Pi_{N}^{G L L}\left(\mu u_{N}^{\prime}\right)\right]^{\prime}, v_{N}\right)_{+}\left[\Pi_{N}^{G L L}\left(\mu u_{N}^{\prime}\right) v_{N}\right]_{-1}^{1} \\
&=-\left(\left[\Pi_{N}^{G L L}\left(\mu u_{N}^{\prime}\right)\right]^{\prime}, v_{N}\right)_{N}
\end{aligned}
$$

where the last equality holds because $v_{N}$ vanishes at the boundary and the terms which appear in the scalar product yield a polynomial whose total degree is equal to $2 N-1$.

${ }^{1}$ From now on, for simplicity of notation, we will denote the G-NI solution by $u_{N}$ (instead of

$\left.u_{N}^{*}\right)$, since there is no longer the risk to confuse it with the spectral solution. At this point, we can rewrite the G-NI problem as follows

$$
\text { find } u_{N} \in V_{N}:\left(L_{N} u_{N}, v_{N}\right)_{N}=\left(f, v_{N}\right)_{N} \quad \forall v_{N} \in V_{N},
$$

where we have defined

$$
L_{N} u_{N}=-\left[\Pi_{N}^{G L L}\left(\mu u_{N}^{\prime}\right)\right]^{\prime}+\sigma u_{N}
$$

By imposing that (10.37) is valid for each basis function $\psi_{i}$, we obtain

$$
\left(L_{N} u_{N}, \psi_{i}\right)_{N}=\left(f, \psi_{i}\right)_{N}, \quad i=1,2, \ldots, N-1
$$

Now we examine the $i$-th equation. The first term is

$$
\begin{aligned}
-\left(\left[\Pi_{N}^{G L L}\left(\mu u_{N}^{\prime}\right)\right]^{\prime}, \psi_{i}\right)_{N} &=-\sum_{j=0}^{N} \alpha_{j}\left[\Pi_{N}^{G L L}\left(\mu u_{N}^{\prime}\right)\right]^{\prime}\left(x_{j}\right) \psi_{i}\left(x_{j}\right) \\
&=-\alpha_{i}\left[\Pi_{N}^{G L L}\left(\mu u_{N}^{\prime}\right)\right]^{\prime}\left(x_{i}\right)
\end{aligned}
$$

since $\psi_{i}\left(x_{j}\right)=\delta_{i j}$. Analogously, for the second term we have

$$
\left(\sigma u_{N}, \psi_{i}\right)_{N}=\sum_{j=0}^{N} \alpha_{j} \sigma\left(x_{j}\right) u_{N}\left(x_{j}\right) \psi_{i}\left(x_{j}\right)=\alpha_{i} \sigma\left(x_{i}\right) u_{N}\left(x_{i}\right)
$$

Finally, the right-hand side becomes

$$
\left(f, \psi_{i}\right)_{N}=\sum_{j=0}^{N} \alpha_{j} f\left(x_{j}\right) \psi_{i}\left(x_{j}\right)=\alpha_{i .} f\left(x_{i}\right)
$$

Dividing by $\alpha_{i}$ the equation thus found, we obtain the following equivalent formulation of the G-NI problem

$$
\begin{cases}L_{N} u_{N}\left(x_{i}\right)=f\left(x_{i}\right), & i=1,2, \ldots, N-1 \\ u_{N}\left(x_{0}\right)=0, & u_{N}\left(x_{N}\right)=0\end{cases}
$$

This is called a collocation problem as it is equivalent to placing at the internal nodes $x_{i}$ the assigned differential equation (after approximating the operator $L$ by $\left.L_{N}\right)$, and satisfying the boundary conditions at the boundary nodes.

We now introduce the interpolation derivative, $D_{N}(\Phi)$, of a continuous function $\Phi$, as being the derivative of the interpolating polynomial $\Pi_{N}^{G L L} \Phi$ defined according to (10.14), i.e.

$$
D_{N}(\Phi)=D\left[\Pi_{N}^{G L L} \Phi\right]
$$

$D$ being the symbol of exact differentiation. If we consider the differential operator $L$ and replace all derivatives with the corresponding interpolation derivatives, we obtain a new operator, called pseudo-spectral operator $L_{N}$, that coincides with the one defined in (10.38). It follows that the G-NI method, introduced here as a generalized Galerkin method, can also be interpreted as a collocation method that operates directly on the differential part of the problem, analogously to what happens, for instance, in the case of finite differences. In this sense, finite differences can be considered as a less accurate version of the G-NI method, as the derivatives are approximated using formulae that use a small number of nodal values.

If the initial operator had been

$$
L u=\left(-\mu u^{\prime}\right)^{\prime}+(b u)^{\prime}+\sigma u
$$

then the corresponding pseudo-spectral operator would have been

$$
L_{N} u_{N}=-D_{N}\left(\mu u_{N}^{\prime}\right)+D_{N}\left(b u_{N}\right)+\sigma u_{N}
$$

Had the boundary conditions for problem $(10.22)$ been of Neumann type,

$$
\left(\mu u^{\prime}\right)(-1)=g_{-}, \quad\left(\mu u^{\prime}\right)(1)=g_{+},
$$

the spectral Galerkin method would be formulated as follows

$$
\begin{aligned}
&\text { find } u_{N} \in \mathbb{Q}_{N}: \int_{-1}^{1} \mu u_{N}^{\prime} v_{N}^{\prime} d x+\int_{-1}^{1} \sigma u_{N} v_{N} d x= \\
&\int_{-1}^{1} f v_{N} d x+g_{+} v_{N}(1)-g_{-} v_{N}(-1) \quad \forall v_{N} \in \mathbb{Q}_{N}
\end{aligned}
$$

while the G-NI method would become

$$
\begin{aligned}
&\text { find } u_{N} \in \mathbb{Q}_{N} \quad:\left(\mu u_{N}^{\prime}, v_{N}^{\prime}\right)_{N}+\left(\sigma u_{N}, v_{N}\right)_{N}= \\
&\left(f, v_{N}\right)_{N}+g_{+} v_{N}(1)-g_{-} v_{N}(-1) \quad \forall v_{N} \in \mathbb{Q}_{N}
\end{aligned}
$$

Its interpretation as a collocation method becomes: find $u_{N} \in \mathbb{Q}_{N}$ such that

$$
\begin{aligned}
&L_{N} u_{N}\left(x_{i}\right)=f\left(x_{i}\right), \quad i=1, \ldots, N-1 \\
&\left(L_{N} u_{N}\left(x_{0}\right)-f\left(x_{0}\right)\right)-\frac{1}{\alpha_{0}}\left(\left(\mu u_{N}^{\prime}\right)(-1)-g_{-}\right)=0 \\
&\left(L_{N} u_{N}\left(x_{N}\right)-f\left(x_{N}\right)\right)+\frac{1}{\alpha_{N}}\left(\left(\mu u_{N}^{\prime}\right)(1)-g_{+}\right)=0
\end{aligned}
$$

where $L_{N}$ is defined in (10.38). Note that at the boundary nodes the Neumann condition is satisfied up to the equation residual $L_{N} u_{N}-f$ multiplied by the coefficient of the GLL formula, which is an infinitesimal of order 2 with respect to $1 / N$.

In Fig. $10.9$ (taken from [CHQZ06]) we report the error in the $\mathrm{H}^{1}(-1,1)$-norm (left) and the absolute value of the difference $\left(\mu u_{N}^{\prime}\right)(\pm 1)-g_{\pm}$(right), that can be regarded as the error made on the fulfillment of the Neumann boundary condition, for different values of $N$. Both errors decay exponentially when $N$ increases. Moreover, we report 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-242.jpg?height=236&width=704&top_left_y=116&top_left_x=108)

Fig. 10.9. Error in $\mathrm{H}^{1}(-1,1)$ (left) and error on the Neumann datum (right) for varying $N$

the errors obtained by using the Galerkin finite element approximations of degree $r=1,2,3$.

Finally, it can be useful to observe that the interpolation derivative $(10.40)$ can be represented through a matrix $\mathrm{D} \in \mathbb{R}^{(N+1) \times(N+1)}$, called matrix of the interpolation derivative, associating to any vector $\mathbf{v} \in \mathbb{R}^{N+1}$ of nodal values $v_{i}=\Phi\left(x_{i}\right), i=0, \ldots, N$, the vector $\mathbf{w}=\mathrm{D} \mathbf{v}$ whose components are the nodal values of the polynomial $D_{N}(\Phi)$ i.e. $w_{i}=\left(D_{N}(\Phi)\right)\left(x_{i}\right), i=0, \ldots, N$. The elements of $\mathrm{D}$ are (see [CHQZ06])

$$
D_{i j}=\psi_{j}^{\prime}\left(x_{i}\right)= \begin{cases}\frac{L_{N}\left(x_{i}\right)}{L_{N}\left(x_{j}\right)} \frac{1}{x_{i}-x_{j}}, & i, j=0, \ldots, N, i \neq j \\ -\frac{(N+1) N}{4}, & i=j=0, \\ \frac{(N+1) N}{4}, & i=j=N \\ 0 & \text { otherwise }\end{cases}
$$

where $d_{0}=d_{N}=2$ and $d_{j}=1$ for $j=1, \ldots, N-1$

\subsubsection{G-NI for parabolic equations}

When we consider time-dependent problems, the spectral G-NI method can be used for the spatial approximation. For the discretization of the time derivative we can then apply a finite difference scheme. In this section, we consider one specific instance, the $\theta$-method that was introduced in Sect. 5.1.

The $\theta$-method applied to the G-NI spatial discretization of the homogeneous Dirichlet problem (5.4), defined on the space interval $-1<x<1$, is formulated as follows:

for each $k \geq 0$, find $u_{N}^{k} \in V_{N}=\left\{v_{N} \in \mathbb{Q}_{N}: v_{N}(-1)=v_{N}(1)=0\right\}$ such that

$$
\begin{aligned}
&\left(\frac{u_{N}^{k+1}-u_{N}^{k}}{\Delta t}, v_{N}\right)_{N}+a_{N}\left(\theta u_{N}^{k+1}+(1-\theta) u_{N}^{k}, v_{N}\right) \\
&=\theta\left(f^{k+1}, v_{N}\right)_{N}+(1-\theta)\left(f^{k}, v_{N}\right)_{N} \quad \forall v_{N} \in V_{N}
\end{aligned}
$$

with $u_{N}^{0}=u_{0, N} \in V_{N}$ being a convenient approximation of $u_{0}$ (for instance, the interpolant $\Pi_{N}^{G L L} u_{0}$ introduced in (10.14)). As usual, $(\cdot, \cdot)_{N}$ denotes the discrete scalar product obtained using the Gauss-Legendre-Lobatto (GLL) numerical integration formula, while $a_{N}(\cdot, \cdot)$ is the approximation of the bilinear form $a(\cdot, \cdot)$ obtained by replacing the exact integrals with the above-mentioned numerical integration formula. By proceeding as we did in Sect. $5.4$ for finite element spatial discretizations, it can be proved that also in this case, the $\theta$-method is unconditionally stable if $\theta \geq \frac{1}{2}$, while for $\theta<\frac{1}{2}$ a sufficient condition for absolute stability if

$$
\Delta t \leq C(\theta) N^{-4}
$$

Indeed, the proof can be checked by repeating the same steps we followed earlier in the case of the finite element approximation. In particular, we define the eigenvalueeigenfunction pairs $\left(\lambda_{j}, w_{N}^{j}\right)$ of the bilinear form $a_{N}(\cdot, \cdot)$, for each $j=1, \ldots, N-1$, through the relation

$$
w_{N}^{j} \in V_{N}: a_{N}\left(w_{N}^{j}, v_{N}\right)=\lambda_{j}\left(w_{N}^{j}, v_{N}\right) \quad \forall v_{N} \in V_{N}
$$

Hence

$$
\lambda_{j}=\frac{a_{N}\left(w_{N}^{j}, w_{N}^{j}\right)}{\left\|w_{N}^{j}\right\|_{N}^{2}}
$$

Using the continuity of the bilinear form $a_{N}(\cdot, \cdot)$, we find

$$
\lambda_{j} \leq \frac{M\left\|w_{N}^{j}\right\|_{\mathrm{H}^{1}(-1,1)}^{2}}{\left\|w_{N}^{j}\right\|_{N}^{2}}
$$

We now recall the following inverse inequality for algebraic polynomials ([CHQZ06])

$$
\exists C_{I}>0:\left\|v_{N}^{\prime}\right\|_{\mathrm{L}^{2}(-1,1)}^{2} \leq C_{I}\left\|v_{N}\right\|_{\mathrm{L}^{2}(-1,1)}^{2} \quad \forall v_{N} \in \mathbb{Q}_{N}
$$

Then

$$
\lambda_{j} \leq \frac{C_{I}^{2} M N^{4}\left\|w_{N}^{j}\right\|_{\mathrm{L}^{2}(-1,1)}^{2}}{\left\|w_{N}^{j}\right\|_{N}^{2}}
$$

Recalling the equivalence property (10.54), we conclude that

$$
\lambda_{j} \leq 3 C_{I}^{2} M N^{4} \quad \forall j=1, \ldots, N-1 .
$$

Inequality (10.42) is now obtained using the stability condition $(5.35)$ (with the finite element eigenvalues $\left\{\lambda_{h}^{i}\right\}$ replaced by the $\lambda_{j}$ 's). Moreover, we have the following convergence estimate, for $n \geq 1$ and $\Omega=(-1,1)$,

$$
\begin{array}{r}
\left\|u\left(t^{n}\right)-u_{N}^{n}\right\|_{\mathrm{L}^{2}(\Omega)} \leq \widetilde{C}\left(t^{n}\right)\left[N ^ { - r } \left(\left|u_{0}\right|_{\mathrm{H}^{r}(\Omega)}+\int_{0}^{t^{n}}\left|\frac{\partial u}{\partial t}(s)\right|_{\mathrm{H}^{r}(\Omega)} d s\right.\right. \\
\left.\left.+\left|u\left(t^{n}\right)\right|_{\mathrm{H}^{r}(\Omega)}\right)+\Delta t \int_{0}^{t^{n}}\left\|\frac{\partial^{2} u}{\partial t^{2}}(s)\right\|_{\mathrm{L}^{2}(\Omega)} d s\right]
\end{array}
$$

For the proof, refer to [CHQZ06, Chap. 7]. 

\subsection{Generalization to the two-dimensional case}

Let us consider as a domain the unit square $\Omega=(-1,1)^{2}$. Since $\Omega$ is the tensor product of the one-dimensional interval $(-1,1)$, it is natural to choose as nodes

$$
\mathbf{x}_{i j}=\left(x_{i}, x_{j}\right), \quad i, j=0, \ldots, N
$$

whose coordinates both coincide with the one-dimensional GLL nodes $x_{i}$, while we take as weights the product of the corresponding one-dimensional weights

$$
\alpha_{i j}=\alpha_{i} \alpha_{j}, \quad i, j=0, \ldots, N
$$

The Gauss-Legendre-Lobatto (GLL) integration formula in two dimensions is therefore defined by

$$
\int_{\Omega} f(\mathbf{x}) d \mathbf{x} \simeq \sum_{i, j=0}^{N} \alpha_{i j} f\left(\mathbf{x}_{i j}\right)
$$

while the discrete scalar product is given by

$$
(f, g)_{N}=\sum_{i, j=0}^{N} \alpha_{i j} f\left(\mathbf{x}_{i j}\right) g\left(\mathbf{x}_{i j}\right)
$$

Analogously to the one-dimensional case it can be proved that the integration formula (10.43) is exact whenever the integrand function is a polynomial of degree at most $2 N-1$. In particular, this implies that

$$
(f, g)_{N}=(f, g) \quad \forall f, g \text { such that } f g \in \mathbb{Q}_{2 N-1}
$$

In this section, for each $N, \mathbb{Q}_{N}$ denotes the space of polynomials of degree less than or equal to $N$ with respect to each of the variables, introduced in $(10.2)$.

We now consider as an example the problem

$$
\begin{cases}L u=-\operatorname{div}(\mu \nabla u)+\sigma u=f & \text { in } \Omega=(-1,1)^{2} \\ u=0 & \text { on } \partial \Omega\end{cases}
$$

By assuming that $\mu(\mathbf{x}) \geq \mu_{0}>0$ and $\sigma(\mathbf{x}) \geq 0$, the corresponding bilinear form is coercive in $\mathrm{H}_{0}^{1}(\Omega)$. Its G-NI approximation is given by

$$
\text { find } u_{N} \in V_{N}: \quad a_{N}\left(u_{N}, v_{N}\right)=F_{N}\left(v_{N}\right) \quad \forall v_{N} \in V_{N},
$$

where

$$
\begin{aligned}
&V_{N}=\left\{v \in \mathbb{Q}_{N}:\left.v\right|_{\partial \Omega}=0\right\} \\
&a_{N}(u, v)=(\mu \nabla u, \nabla v)_{N}+(\sigma u, v)_{N}
\end{aligned}
$$

and

$$
F_{N}\left(v_{N}\right)=\left(f, v_{N}\right)_{N} .
$$

As shown in the one-dimensional case, also in higher dimensions the G-NI formulation is equivalent to a collocation method where the operator $L$ is replaced by $L_{N}$, the pseudo-spectral operator obtained by approximating each derivative by the corresponding interpolation derivative $(10.40)$.

In the case of spectral element methods, we will need to generalize the GLL numerical integration formula on each element $\Omega_{k}$. This can be done thanks to the transformation $\varphi_{k}: \Omega \rightarrow \Omega_{k}$ (see Fig. 10.2). Indeed, we can first of all generate the GLL nodes on the generic element $\Omega_{k}$, by setting

$$
\mathbf{x}_{i j}^{(k)}=\boldsymbol{\varphi}_{k}\left(\mathbf{x}_{i j}\right), \quad i, j=0, \ldots, N
$$

then defining the corresponding weights

$$
\alpha_{i j}^{(k)}=\alpha_{i j}\left|\operatorname{det} J_{k}\right|=\alpha_{i j} \frac{\left|\Omega_{k}\right|}{4}, \quad i, j=0, \ldots, N
$$

having denoted by $J_{k}$ the Jacobian of the transformation $\varphi_{k}$ and by $\left|\Omega_{k}\right|$ the measure of $\Omega_{k}$. The GLL integration formula on $\Omega_{k}$ hence becomes

$$
\int_{\Omega_{k}} f(\mathbf{x}) d \mathbf{x} \simeq I_{N, k}^{G L L}(f)=\sum_{i, j=0}^{N} \alpha_{i j}^{(k)} f\left(\mathbf{x}_{i j}^{(k)}\right)
$$

The spectral element formulation with Gaussian numerical integration, which we will denote by SEM-NI, becomes

$$
\text { find } u_{N} \in V_{N}^{C}: \quad a_{C, N}\left(u_{N}, v_{N}\right)=F_{C, N}\left(v_{N}\right) \quad \forall v_{N} \in V_{N}^{C} .
$$

We have set

$$
a_{C, N}\left(u_{N}, v_{N}\right)=\sum_{k} a_{\Omega_{k}, N}\left(u_{N}, v_{N}\right)
$$

where $a_{\Omega_{k}, N}\left(u_{N}, v_{N}\right)$ is the approximation of $a_{\Omega_{k}}\left(u_{N}, v_{N}\right)$ obtained by approximating each integral on $\Omega_{k}$ that appears in its bilinear form via the GLL numerical integration formula in $\Omega_{k}(10.44)$. The term $F_{C, N}$ is defined in a similar way, and precisely $F_{C, N}\left(v_{N}\right)=\sum_{k} F_{\Omega_{k}, N}\left(v_{N}\right)$, where $F_{\Omega_{k}, N}$ is obtained, in turn, by replacing $\int_{\Omega_{k}} f v_{N} d \mathbf{x}$ with the formula $I_{N, k}^{G L L}\left(f v_{N}\right)$ for each $k$.

Remark 10.3. Fig. $10.10$ summarizes rather schematically the origin of the different approximation schemes evoked up to now. In the case of finite differences, we have denoted by $L_{\Delta}$ the discretization of the operator through finite difference schemes applied to the various derivatives appearing in the definition of $L$.

\subsubsection{Convergence of the G-NI method}

As observed in the one-dimensional case, the G-NI method can be considered as a generalized Galerkin method. For the latter, the analysis of convergence is based on the following general result: Lemma $\mathbf{1 0 . 1}$ (Strang). Consider the problem

$$
\text { find } u \in V: \quad a(u, v)=F(v) \quad \forall v \in V
$$

where $V$ is a Hilbert space with norm $\|\cdot\|_{V}, F \in V^{\prime}$ a linear and bounded functional on $V$ and $a(\cdot, \cdot): V \times V \rightarrow \mathbb{R}$ a bilinear, continuous and coercive form 8 on $V$. (1.)

Consider an approximation of (10.46) that can be formulated through the following generalized Galerkin problem (1) (1.)

$$
\text { find } u_{h} \in V_{h}: \quad a_{h}\left(u_{h}, v_{h}\right)=F_{h}\left(v_{h}\right) \quad \forall v_{h} \in V_{h},
$$

1

$\left\{V_{h}, h>0\right\}$ being a family of finite-dimensional subspaces of $V$.

Let us suppose that the discrete bilinear form $a_{h}(\cdot, \cdot)$ is continuous on $V_{h} \times V_{h}$, and uniformly coercive on $V_{h}$, that is (1)

$\exists \alpha^{*}>0$ independent of h such that $a_{h}\left(v_{h}, v_{h}\right) \geq \alpha^{*}\left\|v_{h}\right\|_{V}^{2} \quad \forall v_{h} \in V_{h}$.

Furthermore, let us suppose that $F_{h}$ is a linear and bounded functional on $V_{h}$. Then:

1. there exists a unique solution $u_{h}$ to problem $(10.47)$;

2. such solution depends continuously on the data, i.e. we have

$$
\left\|u_{h}\right\|_{V} \leq \frac{1}{\alpha^{*}} \sup _{v_{h} \in V_{h} \backslash\{0\}} \frac{F_{h}\left(v_{h}\right)}{\left\|v_{h}\right\|_{V}}
$$

3. finally, the following a priori error estimate holds

$$
\begin{aligned}
\left\|u-u_{h}\right\|_{V} \leq & \inf _{w_{h} \in V_{h}}\left\{\left(1+\frac{M}{\alpha^{*}}\right)\left\|u-w_{h}\right\|_{V}\right.\\
&\left.+\frac{1}{\alpha^{*}} \sup _{v_{h} \in V_{h} \backslash\{0\}} \frac{\left|a\left(w_{h}, v_{h}\right)-a_{h}\left(w_{h}, v_{h}\right)\right|}{\left\|v_{h}\right\|_{V}}\right\} \\
&+\frac{1}{\alpha^{*}} \sup _{v_{h} \in V_{h} \backslash\{0\}} \frac{\left|F\left(v_{h}\right)-F_{h}\left(v_{h}\right)\right|}{\left\|v_{h}\right\|_{V}}
\end{aligned}
$$

$M$ being the continuity constant of the bilinear form $a(\cdot, \cdot)$.

Proof. The assumptions of the Lax-Milgram lemma for problem $(10.47)$ are satisfied, so the solution of such problem exists and is unique. Moreover,

$$
\left\|u_{h}\right\|_{V} \leq \frac{1}{\alpha^{*}}\left\|F_{h}\right\|_{V_{h}^{\prime}}
$$

$\left\|F_{h}\right\|_{V_{h}^{\prime}}=\sup _{v_{h} \in V_{h} \backslash\{0\}} \frac{F_{h}\left(v_{h}\right)}{\left\|v_{h}\right\|_{V}}$ being the norm of the dual space $V_{h}^{\prime}$ of $V_{h}$ 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-247.jpg?height=784&width=716&top_left_y=115&top_left_x=97)

Fig. 10.10. Reference frame for the main numerical methods addressed in this book

Let us now prove the error inequality $(10.48)$. Let $w_{h}$ be any function of the subspace $V_{h}$. Setting $\sigma_{h}=u_{h}-w_{h} \in V_{h}$, we have:

$$
\begin{aligned}
\alpha^{*}\left\|\sigma_{h}\right\|_{V}^{2} & \leq a_{h}\left(\sigma_{h}, \sigma_{h}\right) \quad\left[\text { by the coercivity of } a_{h}\right] \\
&=a_{h}\left(u_{h}, \sigma_{h}\right)-a_{h}\left(w_{h}, \sigma_{h}\right) \\
&=F_{h}\left(\sigma_{h}\right)-a_{h}\left(w_{h}, \sigma_{h}\right) \quad[\text { thanks to }(10.47)] \\
&=F_{h}\left(\sigma_{h}\right)-F\left(\sigma_{h}\right)+F\left(\sigma_{h}\right)-a_{h}\left(w_{h}, \sigma_{h}\right) \\
&=\left[F_{h}\left(\sigma_{h}\right)-F\left(\sigma_{h}\right)\right]+a\left(u, \sigma_{h}\right)-a_{h}\left(w_{h}, \sigma_{h}\right) \quad[\text { thanks to }(10.46)] \\
&=\left[F_{h}\left(\sigma_{h}\right)-F\left(\sigma_{h}\right)\right]+a\left(u-w_{h}, \sigma_{h}\right)+\left[a\left(w_{h}, \sigma_{h}\right)-a_{h}\left(w_{h}, \sigma_{h}\right)\right]
\end{aligned}
$$

If $\sigma_{h} \neq 0,(10.49)$ can be divided by $\alpha^{*}\left\|\sigma_{h}\right\|_{V}$, to give

$$
\begin{gathered}
\left\|\sigma_{h}\right\|_{V} \leq \frac{1}{\alpha^{*}}\left\{\frac{\left|a\left(u-w_{h}, \sigma_{h}\right)\right|}{\left\|\sigma_{h}\right\|_{V}}+\frac{\left|a\left(w_{h}, \sigma_{h}\right)-a_{h}\left(w_{h}, \sigma_{h}\right)\right|}{\left\|\sigma_{h}\right\|_{V}}\right. \\
\left.+\frac{\left|F_{h}\left(\sigma_{h}\right)-F\left(\sigma_{h}\right)\right|}{\left\|\sigma_{h}\right\|_{V}}\right\} \\
+\frac{1}{\alpha^{*}}\left\{M\left\|u-w_{h}\right\|_{V}+\sup _{v_{h} \in V_{h} \backslash\{0\}} \frac{\left|a\left(w_{h}, v_{h}\right)-a_{h}\left(w_{h}, v_{h}\right)\right|}{\left\|v_{h}\right\|_{V}}\right. \\
\left.+\sup _{v_{h} \in V_{h} \backslash\{0\}} \frac{\left|F_{h}\left(v_{h}\right)-F\left(v_{h}\right)\right|}{\left\|v_{h}\right\|_{V}}\right\} \quad[\text { by the continuity of } a]
\end{gathered}
$$

If $\sigma_{h}=0$ such inequality is still valid (it states that 0 is smaller than a sum of positive terms), although the proof breaks down.

We can now estimate the error between the solution $u$ of (10.46) and the solution $u_{h}$ of (10.47). Since $u-u_{h}=\left(u-w_{h}\right)-\sigma_{h}$, we obtain

$$
\begin{gathered}
\left\|u-u_{h}\right\|_{V} \leq\left\|u-w_{h}\right\|_{V}+\left\|\sigma_{h}\right\|_{V} \leq\left\|u-w_{h}\right\|_{V} \\
+\frac{1}{\alpha^{*}}\left\{M\left\|u-w_{h}\right\|_{V}+\sup _{v_{h} \in V_{h} \backslash\{0\}} \frac{\left|a\left(w_{h}, v_{h}\right)-a_{h}\left(w_{h}, v_{h}\right)\right|}{\left\|v_{h}\right\|_{V}}\right. \\
\left.+\quad \sup _{v_{h} \in V_{h} \backslash\{0\}} \frac{\left|F_{h}\left(v_{h}\right)-F\left(v_{h}\right)\right|}{\left\|v_{h}\right\|_{V}}\right\} \\
=\left(1+\frac{M}{\alpha^{*}}\right)\left\|u-w_{h}\right\|_{V}+\frac{1}{\alpha^{*}} \sup _{v_{h} \in V_{h} \backslash\{0\}} \frac{\left|a\left(w_{h}, v_{h}\right)-a_{h}\left(w_{h}, v_{h}\right)\right|}{\left\|v_{h}\right\|_{V}} \\
+\frac{1}{\alpha^{*}} \sup _{v_{h} \in V_{h} \backslash\{0\}} \frac{\left|F_{h}\left(v_{h}\right)-F\left(v_{h}\right)\right|}{\left\|v_{h}\right\|_{V}} .
\end{gathered}
$$

If the previous inequality holds $\forall w_{h} \in V_{h}$, it also holds when taking the infimum when $w_{h}$ varies in $V_{h}$. Hence, we obtain (10.48).

By observing the right-hand side of inequality (10.48), we can recognize three different contributions to the approximation error $u-u_{h}:$ the first is the best approximation error, the second is the error deriving from the approximation of the bilinear form $a(\cdot, \cdot)$ using the discrete bilinear form $a_{h}(\cdot, \cdot)$, and the third is the error arising from the approximation of the linear functional $F(\cdot)$ by the discrete linear functional $F_{h}(\cdot)$.

Remark 10.4. If in the preceding proof we choose $w_{h}=u_{h}^{*}, u_{h}^{*}$ being the solution to the Galerkin problem

$$
u_{h}^{*} \in V_{h}: a\left(u_{h}^{*}, v_{h}\right)=F\left(v_{h}\right) \quad \forall v_{h} \in V_{h}
$$

then the term $a\left(u-w_{h}, \sigma_{h}\right)$ is null thanks to $(10.46),(4.1)$. It is therefore possible to obtain the following estimate, alternative to $(10.48)$

$$
\begin{aligned}
\left\|u-u_{h}\right\|_{V} \leq\left\|u-u_{h}^{*}\right\|_{V} \\
&+\frac{1}{\alpha^{*}} \sup _{v_{h} \in V_{h} \backslash\{0\}} \frac{\left|a\left(u_{h}^{*}, v_{h}\right)-a_{h}\left(u_{h}^{*}, v_{h}\right)\right|}{\left\|v_{h}\right\|_{V}} \\
&+\frac{1}{\alpha^{*}} \sup _{v_{h} \in V_{h} \backslash\{0\}} \frac{\left|F\left(v_{h}\right)-F_{h}\left(v_{h}\right)\right|}{\left\|v_{h}\right\|_{V}} .
\end{aligned}
$$

The latter highlights the fact that the error due to the generalized Galerkin method can be bounded by the error of the Galerkin method plus the errors induced by the use of numerical integration for the computation of both $a(\cdot, \cdot)$ and $F(\cdot)$.

We now want to apply Strang's lemma to the G-NI method, to verify its convergence. For simplicity we will only consider the one-dimensional case. Obviously, $V_{h}$ will be replaced by $V_{N}, u_{h}$ by $u_{N}, v_{h}$ by $v_{N}$ and $w_{h}$ by $w_{N}$.

First of all, we begin by computing the error of the GLL numerical integration formula

$$
E\left(g, v_{N}\right)=\left(g, v_{N}\right)-\left(g, v_{N}\right)_{N}
$$

$g$ and $v_{N}$ being a generic continuous function and a generic polynomial of $\mathbb{Q}_{N}$, respectively. By introducing the interpolation polynomial $\Pi_{N}^{G L L} g$ defined according to (10.14), we obtain

$$
\begin{aligned}
E\left(g, v_{N}\right)=&\left(g, v_{N}\right)-\left(\Pi_{N}^{G L L} g, v_{N}\right)_{N} \\
=&{\left(g, v_{N}\right)-\left(\Pi_{N-1}^{G L L} g, v_{N}\right)+\underbrace{(\overbrace{\Pi_{N-1}^{G L L}}^{\in \mathbb{Q}_{N-1}}}_{\in \mathbb{Q}}, \overbrace{v_{N}}^{\in \mathbb{Q}_{N}})}_{=} &\left(g, v_{N}\right)-\left(\Pi_{N-1}^{G L L} g, v_{N}\right) \\
&+\left(\Pi_{N-1}^{G L L} g, v_{N}\right)_{N}-\left(\Pi_{N}^{G L L} g, v_{N}\right)_{N}[\text { by }(10.27)] \\
=&\left(g-\Pi_{N-1}^{G L L} g, v_{N}\right)+\left(\Pi_{N-1}^{G L L} g-\Pi_{N}^{G L L} g, v_{N}\right)_{N}
\end{aligned}
$$

The first summand of the right-hand side can be bounded from above using the CauchySchwarz inequality as follows

$$
\left|\left(g-\Pi_{N-1}^{G L L} g, v_{N}\right)\right| \leq\left\|g-\Pi_{N-1}^{G L L} g\right\|_{L^{2}(-1,1)}\left\|v_{N}\right\|_{L^{2}(-1,1)}
$$

To find an upper bound for the second summand, we must first introduce the two following lemmas, for the proof of which we refer to [CHQZ06]: 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-250.jpg?height=193&width=723&top_left_y=380&top_left_x=95)

By using first (10.53) and then (10.54) we obtain

$$
\begin{aligned}
&\left|\left(\Pi_{N-1}^{G L L} g-\Pi_{N}^{G L L} g, v_{N}\right)_{N}\right| \leq\left\|\Pi_{N-1}^{G L L} g-\Pi_{N}^{G L L} g\right\|_{N}\left\|v_{N}\right\|_{N} \\
&\quad \leq 3\left[\left\|\Pi_{N-1}^{G L L} g-g\right\|_{L^{2}(-1,1)}+\left\|\Pi_{N}^{G L L} g-g\right\|_{L^{2}(-1,1)}\right]\left\|v_{N}\right\|_{L^{2}(-1,1)}
\end{aligned}
$$

Using such inequality and (10.51), from $(10.50)$ we can obtain the following upper bound

$$
\left|E\left(g, v_{N}\right)\right| \leq\left[4\left\|\Pi_{N-1}^{G L L} g-g\right\|_{L^{2}(-1,1)}+3\left\|\Pi_{N}^{G L L} g-g\right\|_{L^{2}(-1,1)}\right]\left\|v_{N}\right\|_{L^{2}(-1,1)}
$$

Using the interpolation estimate (10.17), we have that

$$
\left|E\left(g, v_{N}\right)\right| \leq C\left[\left(\frac{1}{N-1}\right)^{s}+\left(\frac{1}{N}\right)^{s}\right]\|g\|_{\mathrm{H}^{s}(-1,1)}\left\|v_{N}\right\|_{\mathrm{L}^{2}(-1,1)}
$$

provided that $g \in \mathrm{H}^{s}(-1,1)$, for some $s \geq 1$. Finally, as for each $N \geq 2,1 /(N-1) \leq$ $2 / N$, the Gauss-Legendre-Lobatto integration error results to be bound as

$$
\left|E\left(g, v_{N}\right)\right| \leq C\left(\frac{1}{N}\right)^{s}\|g\|_{\mathrm{H}^{s}(-1,1)}\left\|v_{N}\right\|_{L^{2}(-1,1)}
$$

for each $g \in \mathrm{H}^{s}(-1,1)$ and for each polynomial $v_{N} \in \mathbb{Q}_{N}$.

At this point we are ready to evaluate the various contributions that intervene in (10.48). We anticipate that this analysis will be carried out in the case where suitable simplifying hypotheses are introduced on the differential problem under exam. We begin with the simplest term, i.e. the one associated with the functional $F$, supposing to consider a problem with homogeneous Dirichlet boundary conditions, in order to obtain $F\left(v_{N}\right)=\left(f, v_{N}\right)$ and $F_{N}\left(v_{N}\right)=\left(f, v_{N}\right)_{N}$. Provided that $f \in \mathrm{H}^{s}(-1,1)$ for some $s \geq 1$, then,

$$
\begin{aligned}
&\sup _{v_{N} \in V_{N} \backslash\{0\}} \frac{\left|F\left(v_{N}\right)-F_{N}\left(v_{N}\right)\right|}{\left\|v_{N}\right\|_{V}}=\sup _{v_{N} \in V_{N} \backslash\{0\}} \frac{\left|\left(f, v_{N}\right)-\left(f, v_{N}\right)_{N}\right|}{\left\|v_{N}\right\|_{V}} \\
&=\sup _{v_{N} \in V_{N} \backslash\{0\}} \frac{\left|E\left(f, v_{N}\right)\right|}{\left\|v_{N}\right\|_{V}} \leq \sup _{v_{N} \in V_{N} \backslash\{0\}} \frac{C\left(\frac{1}{N}\right)^{s}\|f\|_{\mathrm{H}^{s}(-1,1)}\left\|v_{N}\right\|_{\mathrm{L}^{2}(-1,1)}}{\left\|v_{N}\right\|_{V}} \\
&\leq C\left(\frac{1}{N}\right)\|f\|_{\mathrm{H}^{s}(-1,1)}
\end{aligned}
$$

having exploited relation (10.55) and having bounded the norm in $\mathrm{L}^{2}(-1,1)$ by that in $\mathrm{H}^{S}(-1,1)$.

As for the contribution arising from the approximation of the bilinear form,

$$
\sup _{v_{N} \in V_{N} \backslash\{0\}} \frac{\left|a\left(w_{N}, v_{N}\right)-a_{N}\left(w_{N}, v_{N}\right)\right|}{\left\|v_{N}\right\|_{V}}
$$

we cannot explicitly evaluate it without referring to a particular differential problem. We then choose, as an example, the one-dimensional diffusion-reaction problem (10.22), supposing moreover that $\mu$ and $\sigma$ are constant. Incidentally, such problem satisfies homogeneous Dirichlet boundary conditions, in accordance with what was requested for deriving estimate (10.56). In such case, the associated bilinear form is

$$
a(u, v)=\left(\mu u^{\prime}, v^{\prime}\right)+(\sigma u, v)
$$

while its G-NI approximation is given by

$$
a_{N}(u, v)=\left(\mu u^{\prime}, v^{\prime}\right)_{N}+(\sigma u, v)_{N}
$$

We must then evaluate

$$
a\left(w_{N}, v_{N}\right)-a_{N}\left(w_{N}, v_{N}\right)=\left(\mu w_{N}^{\prime}, v_{N}^{\prime}\right)-\left(\mu w_{N}^{\prime}, v_{N}^{\prime}\right)_{N}+\left(\sigma w_{N}, v_{N}\right)-\left(\sigma w_{N}, v_{N}\right)_{N}
$$

Since $w_{N}^{\prime} v_{N}^{\prime} \in \mathbb{Q}_{2 N-2}$, if we suppose that $\mu$ is constant, the product $\mu w_{N}^{\prime} v_{N}^{\prime}$ is integrated exactly by the GLL integration formula, that is $\left(\mu w_{N}^{\prime}, v_{N}^{\prime}\right)-\left(\mu w_{N}^{\prime}, v_{N}^{\prime}\right)_{N}=0$. We now observe that

$$
\left(\sigma w_{N}, v_{N}\right)-\left(\sigma w_{N}, v_{N}\right)_{N}=E\left(\sigma w_{N}, v_{N}\right)=E\left(\sigma\left(w_{N}-u\right), v_{N}\right)+E\left(\sigma u, v_{N}\right)
$$

and therefore, using $(10.55)$, we obtain

$$
\left|E\left(\sigma\left(w_{N}-u\right), v_{N}\right)\right| \leq C\left(\frac{1}{N}\right)\left\|\sigma\left(w_{N}-u\right)\right\|_{\mathrm{H}^{1}(-1,1)}\left\|v_{N}\right\|_{L^{2}(-1,1)}
$$



$$
\left|E\left(\sigma u, v_{N}\right)\right| \leq C\left(\frac{1}{N}\right)^{s}\|\sigma u\|_{\mathrm{H}^{s}(-1,1)}\left\|v_{N}\right\|_{\mathrm{L}^{2}(-1,1)}
$$

On the other hand, since $\sigma$ is also constant, setting $w_{N}=\Pi_{N}^{G L L} u$ and using $(10.17)$, we obtain

$$
\left\|\sigma\left(w_{N}-u\right)\right\|_{\mathrm{H}^{1}(-1,1)} \leq C\left\|u-\Pi_{N}^{G L L} u\right\|_{\mathrm{H}^{1}(-1,1)} \leq C\left(\frac{1}{N}\right)^{s-1}\|u\|_{\mathrm{H}^{s}(-1,1)}
$$

Hence,

$$
\sup _{v_{N} \in V_{N} \backslash\{0\}} \frac{\left|a\left(w_{N}, v_{N}\right)-a_{N}\left(w_{N}, v_{N}\right)\right|}{\left\|v_{N}\right\|_{V}} \leq C^{*}\left(\frac{1}{N}\right)^{s}\|u\|_{\mathrm{H}^{s}(-1,1)}
$$

We still need to estimate the first summand of (10.48). Having chosen $w_{N}=\Pi_{N}^{G L L} u$ and exploiting (10.17) again, we obtain that

$$
\left\|u-w_{N}\right\|_{V}=\left\|u-\Pi_{N}^{G L L} u\right\|_{\mathrm{H}^{1}(-1,1)} \leq C\left(\frac{1}{N}\right)^{s}\|u\|_{\mathrm{H}^{s+1}(-1,1)}
$$

provided that $u \in \mathrm{H}^{s+1}(-1,1)$, for a suitable $s \geq 1$. To conclude, thanks to $(10.56)$, (10.57) and (10.58), from (10.48) applied to the G-NI approximation of problem (10.22), and under the previous hypotheses, we find the following error estimate

$$
\left\|u-u_{N}\right\|_{\mathrm{H}^{1}(-1,1)} \leq C\left(\frac{1}{N}\right)^{s}\left(\|f\|_{\mathrm{H}^{s}(-1,1)}+\|u\|_{\mathrm{H}^{s+1}(-1,1)}\right)
$$

The convergence analysis just carried out for the model problem (10.22) can be generalized (with a few technical difficulties) to the case of more complex differential problems and different boundary conditions.

Example 10.2 (Problem with regularity depending on a parameter). Let us consider the following (trivial but instructive) problem

$$
\begin{cases}-u^{\prime \prime}=0, & x \in(0,1] \\ -u^{\prime \prime}=-\alpha(\alpha-1)(x-1)^{\alpha-2}, & x \in(1,2) \\ u(0)=0, & u(2)=1\end{cases}
$$

with $\alpha \in \mathbb{N}$. The exact solution is null in $(0,1)$ and equals $(x-1)^{\alpha}$ for $x \in(1,2)$. Thus it belongs to $\mathrm{H}^{\alpha}(0,2)$, but not to $\mathrm{H}^{\alpha+1}(0,2)$. We report in Table $10.1$ the behaviour of the error in $\mathrm{H}^{1}(0,2)$ norm with respect to $N$ using a G-NI method for three different values of $\alpha$. As it can be seen, when the regularity increases, so does the order of convergence of the spectral method with respect to $N$, as stated by the theory. In the same table we report the results obtained using linear finite elements (this time $N$ denotes the number of elements). The order of convergence of the finite element method remains linear in either case. Table 10.1. Behaviour of the error of the G-NI spectral method for varying polynomial degree $N$ and solution regularity index (left). Behaviour of the error of the linear finite element method for varying number of intervals $N$ and solution regularity index (right)

\begin{tabular}{cccc}
\hline$N$ & $\alpha=2$ & $\alpha=3$ & $\alpha=4$ \\
\hline 4 & $0.5931$ & $0.2502$ & $0.2041$ \\
8 & $0.3064$ & $0.0609$ & $0.0090$ \\
16 & $0.1566$ & $0.0154$ & $7.5529 \cdot 10^{-4}$ \\
32 & $0.0792$ & $0.0039$ & $6.7934 \cdot 10^{-5}$ \\
\hline
\end{tabular}

\begin{tabular}{ccc}
\hline$N$ & $\alpha=2$ & $\alpha=3$ \\
\hline 4 & $0.4673$ & $0.5768$ \\
8 & $0.2456$ & $0.3023$ \\
16 & $0.1312$ & $0.1467$ \\
32 & $0.0745$ & $0.0801$ \\
\hline
\end{tabular}
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-253.jpg?height=214&width=718&top_left_y=364&top_left_x=96)

Fig. 10.11. The grid (left) and the solution isolines obtained using the spectral finite element method (right) for the problem in Example $10.3$

Example 10.3. Let us take the second example illustrated in Sect. 4.6.3, this time using the spectral element method. Let us consider a partition of the domain into four spectral elements of degree 8 as shown in the left of Fig. 10.11. The solution obtained (Fig. 10.11, left) does not exhibit any inaccuracy in proximity of the origin, as opposed to the solution obtained using finite elements in the absence of grid adaptivity (compare with Fig. 4.24, left).

\subsection{G-NI and SEM-NI methods for a one-dimensional model problem}

Let us consider the one-dimensional diffusion-reaction problem

$$
-\left[\left(1+x^{2}\right) u^{\prime}(x)\right]^{\prime}+\cos \left(x^{2}\right) u(x)=f(x), \quad x \in(-1,1),
$$

together with mixed-type boundary conditions

$$
u(-1)=0, \quad u^{\prime}(1)=1
$$

The goal of this section is to discuss in detail how to formulate the G-NI and SEMNI approximations. For the former, we will also provide the corresponding matrix formulation as well as a stability analysis. 

\subsubsection{The G-NI method}

The weak formulation of problem $(10.59)$ is:

$$
\text { find } \quad u \in V: a(u, v)=F(v) \quad \forall v \in V,
$$

$V=\left\{v \in \mathrm{H}^{1}(-1,1): v(-1)=0\right\}, a: V \times V \longrightarrow \mathbb{R}$ and $F: V \longrightarrow \mathbb{R}$ being the bilinear form and the linear functional, respectively, defined by

$$
\begin{gathered}
a(u, v)=\int_{-1}^{1}\left(1+x^{2}\right) u^{\prime}(x) v^{\prime}(x) d x+\int_{-1}^{1} \cos \left(x^{2}\right) u(x) v(x) d x \\
F(v)=\int_{-1}^{1} f(x) v(x) d x+2 v(1)
\end{gathered}
$$

The spectral-Galerkin formulation (SM) takes the following form

$$
\text { find } u_{N} \in V_{N} \quad \text { such that } \quad a\left(u_{N}, v_{N}\right)=F\left(v_{N}\right) \quad \forall v_{N} \in V_{N}
$$

with

$$
V_{N}=\left\{v_{N} \in \mathbb{Q}_{N}: v_{N}(-1)=0\right\} \subset V
$$

In order to obtain the corresponding G-NI formulation, it is sufficient to approximate in (10.60) all scalar products on $\mathrm{L}^{2}(-1,1)$ with the GLL discrete scalar product defined in (10.25). We then have

$$
\text { find } u_{N}^{*} \in V_{N}: a_{N}\left(u_{N}^{*}, v_{N}\right)=F_{N}\left(v_{N}\right) \quad \forall v_{N} \in V_{N} \text {, }
$$

having set

$$
\begin{aligned}
a_{N}(u, v) &=\left(\left(1+x^{2}\right) u^{\prime}, v^{\prime}\right)_{N}+\left(\cos \left(x^{2}\right) u, v\right)_{N} \\
&=\sum_{i=0}^{N}\left(1+x_{i}^{2}\right) u^{\prime}\left(x_{i}\right) v^{\prime}\left(x_{i}\right) \alpha_{i}+\sum_{i=1}^{N} \cos \left(x_{i}^{2}\right) u\left(x_{i}\right) v\left(x_{i}\right) \alpha_{i}
\end{aligned}
$$

and

$$
F_{N}(v)=(f, v)_{N}+2 v(1)=\sum_{i=1}^{N} f\left(x_{i}\right) v\left(x_{i}\right) \alpha_{i}+2 v(1)
$$

Note that this requires $f$ to be continuous. We observe that the index $i$ of the last sum in (10.63) and of the sum in (10.64) starts from 1 , instead of 0, since $v\left(x_{0}\right)=v(-1)=0$. Moreover, the formulations SM (10.60) and G-NI (10.62) never coincide. Consider, for instance, the diffusive term $\left(1+x^{2}\right)\left(u_{N}^{*}\right)^{\prime} v_{N}^{\prime}:$ this is a polynomial of degree $2 N$. Since the GLL integration formula has exactness degree $2 \mathrm{~N}-1$, the discrete scalar product (10.25) will not return the exact value of the corresponding continuous scalar product $\left(\left(1+x^{2}\right)\left(u_{N}^{*}\right)^{\prime}, v_{N}^{\prime}\right)$

To obtain the matrix formulation of the G-NI approximation, we denote by $\psi_{i}$, for $i=1, \ldots, N$, the characteristic polynomials associated to all GLL nodes except to the one where a Dirichlet boundary condition is assigned, $x_{0}=-1$. Such polynomials constitute a basis for the space $V_{N}$ introduced in (10.61). This allows us, in the first place, to write the solution $u_{N}^{*}$ of the G-NI formulation as

$$
u_{N}^{*}(x)=\sum_{j=1}^{N} u_{N}^{*}\left(x_{j}\right) \psi_{j}(x)
$$

Secondly, we can choose in (10.62) $v_{N}=\psi_{i}, i=1, \ldots, N$, obtaining

$$
a_{N}\left(u_{N}^{*}, \psi_{i}\right)=F_{N}\left(\psi_{i}\right), \quad i=1, \ldots, N,
$$

i.e.

$$
\sum_{j=1}^{N} u_{N}^{*}\left(x_{j}\right) a_{N}\left(\psi_{j}, \psi_{i}\right)=F_{N}\left(\psi_{i}\right), \quad i=1, \ldots, N
$$

In matrix form,

$$
A \mathbf{u}_{N}^{*}=\mathbf{f}
$$

having $\mathbf{u}_{N}^{*}=\left(u_{N}^{*}\left(x_{i}\right)\right), A=\left(a_{i j}\right)$, with

$$
\begin{aligned}
a_{i j}=a_{N}\left(\psi_{j}, \psi_{i}\right) &=\sum_{k=0}^{N}\left(1+x_{k}^{2}\right) \psi_{j}^{\prime}\left(x_{k}\right) \psi_{i}^{\prime}\left(x_{k}\right) \alpha_{k}+\sum_{k=1}^{N} \cos \left(x_{k}^{2}\right) \psi_{j}\left(x_{k}\right) \psi_{i}\left(x_{k}\right) \alpha_{k} \\
&=\sum_{k=0}^{N}\left(1+x_{k}^{2}\right) \psi_{j}^{\prime}\left(x_{k}\right) \psi_{i}^{\prime}\left(x_{k}\right) \alpha_{k}+\cos \left(x_{i}^{2}\right) \alpha_{i} \delta_{i j}
\end{aligned}
$$

and

$$
\begin{aligned}
\mathbf{f}=\left(f_{i}\right), \text { con } f_{i} &=F_{N}\left(\psi_{i}\right)=\left(f, \psi_{i}\right)_{N}+2 \psi_{i}(1) \\
&=\sum_{k=1}^{N} f\left(x_{k}\right) \psi_{i}\left(x_{k}\right) \alpha_{k}+2 \psi_{i}(1) \\
&= \begin{cases}\alpha_{i} f\left(x_{i}\right) & \text { for } \quad i=1, \ldots, N-1, \\
\alpha_{N} f(1)+2 & \text { for } i=N\end{cases}
\end{aligned}
$$

We recall that the matrix $A$, besides being ill-conditioned, is full due to the presence of the diffusive term.

Finally, we can verify that the G-NI method (10.62) can be reformulated as a suitable collocation method. To this end, we wish to rewrite the discrete formulation (10.62) in continuous form in order to counterintegrate by parts, i.e. to return to the initial differential operator. In order to do this, we will resort to the interpolation operator $\Pi_{N}^{G L L}$ defined in (10.15), recalling in addition that the discrete scalar product (10.25) coincides with the continuous one on $\mathrm{L}^{2}(-1,1)$ if the product of the two integrand functions is a polynomial of degree $\leq 2 N-1$ (see (10.36)). We then accurately rewrite the first summand of $a_{N}\left(u_{N}^{*}, v_{N}\right)$, ignoring the $*$ to simplify the notation. Thanks to $(10.36)$ and integrating by parts, we have

$$
\begin{aligned}
&\left(\left(1+x^{2}\right) u_{N}^{\prime}, v_{N}^{\prime}\right)_{N} \\
&\quad=\left(\Pi_{N}^{G L L}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right), v_{N}^{\prime}\right)_{N}=\left(\Pi_{N}^{G L L}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right), v_{N}^{\prime}\right) \\
&\quad=-\left(\left[\Pi_{N}^{G L L}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right)\right]^{\prime}, v_{N}\right)+\Pi_{N}^{G L L}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right)(1) v_{N}(1) \\
&\quad=-\left(\left[\Pi_{N}^{G L L}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right)\right]^{\prime}, v_{N}\right)_{N}+\Pi_{N}^{G L L}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right)(1) v_{N}(1)
\end{aligned}
$$

Hence, we can reformulate $(10.62)$ as

$$
+\left(2-\Pi_{N}^{G L L}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right)(1)\right) v_{N}(1) \quad \forall v_{N} \in V_{N}
$$

$$
\text { find } u_{N} \in V_{N}:\left(L_{N} u_{N}, v_{N}\right)_{N}=\left(f, v_{N}\right)_{N}
$$

with

$$
L_{N} u_{N}=-\left[\Pi_{N}^{G L L}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right)\right]^{\prime}+\cos \left(x^{2}\right) u_{N}=-D_{N}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right)+\cos \left(x^{2}\right) u_{N}
$$

$D_{N}$ being the interpolation derivative introduced in (10.40). We now choose $(10.65)$ $v_{N}=\psi_{i}$. For $i=1, \ldots, N-1$, we have

$$
\begin{aligned}
&\left(L_{N} u_{N}, \psi_{i}\right)_{N}=\left(-\left[\Pi_{N}^{G L L}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right)\right]^{\prime}, \psi_{i}\right)_{N}+\left(\cos \left(x^{2}\right) u_{N}, \psi_{i}\right)_{N} \\
&=-\sum_{j=1}^{N-1} \alpha_{j}\left[\Pi_{N}^{G L L}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right)\right]^{\prime}\left(x_{j}\right) \psi_{i}\left(x_{j}\right)+\sum_{j=1}^{N-1} \alpha_{j} \cos \left(x_{j}^{2}\right) u_{N}\left(x_{j}\right) \psi_{i}\left(x_{j}\right) \\
&=-\alpha_{i}\left[\Pi_{N}^{G L L}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right)\right]^{\prime}\left(x_{i}\right)+\alpha_{i} \cos \left(x_{i}^{2}\right) u_{N}\left(x_{i}\right)=\left(f, \psi_{i}\right)_{N} \\
&=\sum_{j=1}^{N-1} \alpha_{j} f\left(x_{j}\right) \psi_{i}\left(x_{j}\right)=\alpha_{i} f\left(x_{i}\right)
\end{aligned}
$$

that is, exploiting the definition of the $L_{N}$ operator and dividing everything by $\alpha_{i}$,

$$
L_{N} u_{N}\left(x_{i}\right)=f\left(x_{i}\right), \quad i=1, \ldots, N-1
$$

Having set $v_{N}=\psi_{N}$ in $(10.65)$, we obtain instead

$$
\begin{aligned}
\left(L_{N} u_{N}, \psi_{N}\right)_{N} &=-\alpha_{N}\left[\Pi_{N}^{G L L}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right)\right]^{\prime}\left(x_{N}\right)+\alpha_{N} \cos \left(x_{N}^{2}\right) u_{N}\left(x_{N}\right) \\
&=\left(f, \psi_{N}\right)_{N}+2-\Pi_{N}^{G L L}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right)(1) \\
&=\alpha_{N} f\left(x_{N}\right)+2-\Pi_{N}^{G L L}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right)(1)
\end{aligned}
$$

or, dividing all by $\alpha_{N}$,

$$
L_{N} u_{N}\left(x_{N}\right)=f\left(x_{N}\right)+\frac{1}{\alpha_{N}}\left(2-\Pi_{N}^{G L L}\left(\left(1+x^{2}\right) u_{N}^{\prime}\right)(1)\right)
$$

Equations (10.66) and (10.67) therefore provide the collocation in all the nodes (except the potential boundary ones where Dirichlet conditions are assigned) of the given differential problem, after approximating of the differential operator $L$ using operator $L_{N}$.

Finally, we analyze the stability of formulation (10.62). Since we are dealing with a generalized Galerkin-type of approach, we will have to resort to the Strang Lemma 10.1. This guarantees that, for the solution $u_{N}^{*}$ of $(10.62)$, the estimate

$$
\left\|u_{N}^{*}\right\|_{V} \leq \frac{1}{\alpha^{*}} \sup _{v_{N} \in V_{N} \backslash\{0\}} \frac{\left|F_{N}\left(v_{N}\right)\right|}{\left\|v_{N}\right\|_{V}}
$$

holds, $\alpha^{*}$ being the (uniform) coercivity constant associated to the discrete bilinear form $a_{N}(\cdot, \cdot)$. We apply this result to problem (10.59), by computing first of all $\alpha^{*}$. By exploiting the definition $(10.53)$ of the discrete norm $\|\cdot\|_{N}$ and the equivalence relation (10.54), we have

$$
\begin{aligned}
a_{N}\left(u_{N}, u_{N}\right) &=\left(\left(1+x^{2}\right) u_{N}^{\prime}, u_{N}^{\prime}\right)_{N}+\left(\cos \left(x^{2}\right) u_{N}, u_{N}\right)_{N} \\
& \geq\left(u_{N}^{\prime}, u_{N}^{\prime}\right)_{N}+\cos (1)\left(u_{N}, u_{N}\right)_{N}=\left\|u_{N}^{\prime}\right\|_{N}^{2}+\cos (1)\left\|u_{N}\right\|_{N}^{2} \\
& \geq\left\|u_{N}^{\prime}\right\|_{\mathrm{L}^{2}(-1,1)}^{2}+\cos (1)\left\|u_{N}\right\|_{\mathrm{L}^{2}(-1,1)}^{2} \geq \cos (1)\left\|u_{N}\right\|_{V}^{2}
\end{aligned}
$$

having moreover exploited the relations

$$
\begin{aligned}
\min _{j}\left(1+x_{j}^{2}\right) & \geq \min _{x \in[-1,1]}\left(1+x^{2}\right)=1 \\
\min _{j} \cos \left(x_{j}^{2}\right) & \geq \min _{x \in[-1,1]} \cos \left(x^{2}\right)=\cos (1)
\end{aligned}
$$

This allows us to identify $\alpha^{*}$ using the value $\cos (1)$. At this point, we can evaluate the quotient $\left|F_{N}\left(v_{N}\right)\right| /\left\|v_{N}\right\|_{V}$ in $(10.68)$. Indeed, we have

$$
\begin{aligned}
&\left|F_{N}\left(v_{N}\right)\right|=\left|\left(f, v_{N}\right)_{N}+2 v_{N}(1)\right| \leq\|f\|_{N}\left\|v_{N}\right\|_{N}+2\left|v_{N}(1)\right| \\
&\leq \sqrt{3}\|f\|_{N}\left\|v_{N}\right\|_{V}+2\left|\int_{-1}^{1} v_{N}^{\prime}(x) d x\right| \leq \sqrt{3}\|f\|_{N}\left\|v_{N}\right\|_{V}+2 \sqrt{2}\left\|v_{N}\right\|_{V}
\end{aligned}
$$

having once more used the equivalence property $(10.54)$ together with the CauchySchwarz inequality in its discrete (10.52) and continuous (3.7) versions. We can thus conclude that

$$
\frac{\left|F_{N}\left(v_{N}\right)\right|}{\left\|v_{N}\right\|_{V}} \leq \sqrt{3}\|f\|_{N}+2 \sqrt{2}
$$

that is, returning to the stability estimate $(10.68)$,

$$
\left\|u_{N}^{*}\right\|_{V} \leq \frac{1}{\cos (1)}\left[\sqrt{3}\|f\|_{N}+2 \sqrt{2}\right] .
$$

Finally, we note that $\|f\|_{N} \leq 2\|f\|_{C^{0}([-1,1])} \forall f \in C^{0}([-1,1])$. 

\subsubsection{The SEM-NI method}

Starting from problem (10.59), we now want to consider its SEM-NI formulation, i.e. a spectral element formulation that uses the integration formulae of type GLL in each element. Moreover, we propose to provide a basis for the space where such formulation will be implemented.

We first introduce a partition of the interval $(-1,1)$ in $M(\geq 2)$ disjoint sub-intervals $\Omega_{m}=\left(\bar{x}_{m-1}, \bar{x}_{m}\right)$, with $m=1, \ldots, M$, denoting by $h_{m}=\bar{x}_{m}-\bar{x}_{m-1}$ the width of the $\mathrm{m}-$ th interval, and setting $h=\max _{m} h_{m}$. The SEM formulation of problem $(10.59)$ takes the form

$$
\text { find } \quad u_{N} \in V_{N}^{C}: a\left(u_{N}, v_{N}\right)=F\left(v_{N}\right) \quad \forall v_{N} \in V_{N}^{C},
$$

with

$$
V_{N}^{C}=\left\{v_{N} \in C^{0}([-1,1]):\left.v_{N}\right|_{\Omega_{m}} \in \mathbb{Q}_{N}, \forall m=1, \ldots, M, v_{N}(-1)=0\right\}
$$

We note that the functional space $V_{N}^{C}$ of the SEM approach loses the "global" nature that is instead typical of a SM formulation. Similarly to what happens in the case of finite element approximations, we now have piecewise polynomial functions. By exploiting the partition $\left\{\Omega_{m}\right\}$, we can rewrite formulation (10.69) in the following way

$$
\text { find } \quad u_{N} \in V_{N}^{C}: \sum_{m=1}^{M} a_{\Omega_{m}}\left(u_{N}, v_{N}\right)=\sum_{m=1}^{M} F_{\Omega_{m}}\left(v_{N}\right) \quad \forall v_{N} \in V_{N}^{C}
$$

where

$$
\begin{aligned}
a_{\Omega_{m}}\left(u_{N}, v_{N}\right) &=\left.a\left(u_{N}, v_{N}\right)\right|_{\Omega_{m}} \\
&=\int_{x_{m-1}}^{\bar{x}_{m}}\left(1+x^{2}\right) u_{N}^{\prime}(x) v_{N}^{\prime}(x) d x+\int_{x_{m-1}}^{\bar{x}_{m}} \cos \left(x^{2}\right) u_{N}(x) v_{N}(x) d x
\end{aligned}
$$

while

$$
F_{\Omega_{m}}\left(v_{N}\right)=\left.F\left(v_{N}\right)\right|_{\Omega_{m}}=\int_{x_{m-1}}^{\bar{x}_{m}} f(x) v_{N}(x) d x+2 v_{N}(1) \delta_{m M}
$$

The SEM-NI formulation can be obtained at this point by approximating in $(10.70)$ the continuous scalar products by the discrete GLL scalar product $(10.25)$ :

$$
\text { find } u_{N}^{*} \in V_{N}^{C}: \sum_{m=1}^{M} a_{N, \Omega_{m}}\left(u_{N}^{*}, v_{N}\right)=\sum_{m=1}^{M} F_{N, \Omega_{m}}\left(v_{N}\right) \quad \forall v_{N} \in V_{N}^{C},
$$

where

$$
\begin{gathered}
a_{N, \Omega_{m}}(u, v)=\left(\left(1+x^{2}\right) u^{\prime}, v^{\prime}\right)_{N, \Omega_{m}}+\left(\cos \left(x^{2}\right) u, v\right)_{N, \Omega_{m}} \\
F_{N, \Omega_{m}}(v)=(f, v)_{N, \Omega_{m}}+2 v(1) \delta_{m M} \\
(u, v)_{N, \Omega_{m}}=\sum_{i=0}^{N} u\left(x_{i}^{(m)}\right) v\left(x_{i}^{(m)}\right) \alpha_{i}^{(m)}
\end{gathered}
$$

$x_{i}^{(m)}$ being the $i$-th GLL node of the sub-interval $\Omega_{m}$ and $\alpha_{i}^{(m)}$ the corresponding integration weight.

Starting from the reference element $\widehat{\Omega}=(-1,1)$ (which, in the case under exam, coincides with the domain $\Omega$ of problem (10.59)) and calling

$$
\varphi_{m}(\xi)=\frac{h_{m}}{2} \xi+\frac{\bar{x}_{m}+\bar{x}_{m-1}}{2}, \quad \xi \in[-1,1]
$$

the affine map from $\widehat{\Omega}$ into $\Omega_{m}$, for $m=1, \ldots, M$, we will have

$$
x_{i}^{(m)}=\varphi_{m}\left(x_{i}\right), \quad \alpha_{i}^{(m)}=\frac{h_{m}}{2} \alpha_{i}, \quad i=0, \ldots, N
$$

that is $x_{i}^{(m)}$ is the image, through the mapping $\varphi_{m}$, of the i-th GLL node of $\widehat{\Omega}$.

We introduce, on each $\Omega_{m}$, the set $\left\{\psi_{i}^{(m)}\right\}_{i=0}^{N}$ of basis functions, such that

$$
\psi_{i}^{(m)}(x)=\psi_{i}\left(\varphi_{m}^{-1}(x)\right) \quad \forall x \in \Omega_{m}
$$

$\psi_{i}$ being the characteristic polynomial introduced in (10.12) and (10.13) associated to node $x_{i}$ of GLL in $\widehat{\Omega}$. Having now a basis for each sub-interval $\Omega_{m}$, we can write the solution $u_{N}$ of the SEM on each $\Omega_{m}$ as

$$
u_{N}(x)=\sum_{i=0}^{N} u_{i}^{(m)} \psi_{i}^{(m)}(x) \quad \forall x \in \Omega_{m}
$$

where $u_{i}^{(m)}=u_{N}\left(x_{i}^{(m)}\right)$.

Since we want to define a global basis for the space $V_{N}^{C}$, we start by defining the basis functions associated to the internal nodes of $\Omega_{m}$, for $m=1, \ldots, M$. For this purpose, it will be sufficient to extend trivially, outside $\Omega_{m}$, each basis function $\psi_{i}^{(m)}$, yielding

$$
\widetilde{\psi}_{i}^{(m)}(x)= \begin{cases}\psi_{i}^{(m)}(x), & x \in \Omega_{m} \\ 0, & \text { otherwise }\end{cases}
$$

These are, overall, $(N-1) M$ functions that behave as shown in Fig. 10.12. For each end node $\bar{x}_{m}$ of the $\Omega_{m}$ sub-domains, with $m=1, \ldots, M-1$, we define instead the basis function

$$
\psi_{m}^{*}(x)= \begin{cases}\psi_{N}^{(m)}(x), & x \in \Omega_{m} \\ \psi_{0}^{(m+1)}(x), & x \in \Omega_{m+1} \\ 0, & \text { otherwise }\end{cases}
$$

obtained by "pasting" $\psi_{N}^{(m)}$ and $\psi_{0}^{(m+1)}$ together (see Fig. 10.13). In particular, we observe that $\psi_{0}^{*}$ is not needed, since a homogeneous Dirichlet condition is assigned at $\bar{x}_{0}=-1$, whereas we need $\psi_{M}^{*}$ that we indicate with $\psi_{N}^{(M)}$. Thus, by the choice of 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-260.jpg?height=164&width=603&top_left_y=113&top_left_x=153)

Fig. 10.12. basis function $\widetilde{\psi}_{i}^{(m)}$ associated to the internal node $x_{i}^{(m)}$

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-260.jpg?height=198&width=719&top_left_y=333&top_left_x=95)

Fig. 10.13. basis function $\psi_{m}^{*}$ associated to the internal node $\bar{x}_{m}$

boundary conditions made, there exist $M$ basis functions associated to the endpoints of the sub-intervals $\Omega_{m}$. (Had Dirichlet conditions been applied at both endpoints of $\Omega$, we would have had the $(M-1)$ functions $\left.\psi_{m}^{*}, m=1, \ldots, M-1 .\right)$

Hence, we have $n=(N-1) M+M$ basis functions for the space $V_{N}^{C}$ altogether. Each function $u_{N} \in V_{N}^{C}$ can then be expressed in the following way

$$
u_{N}(x)=\sum_{m=1}^{M} u_{m}^{\Gamma} \psi_{m}^{*}(x)+\sum_{m=1}^{M} \sum_{i=1}^{N-1} u_{i}^{(m)} \widetilde{\psi}_{i}^{(m)}(x)
$$

with $u_{m}^{\Gamma}=u_{N}\left(\bar{x}_{m}\right)$ and $u_{i}^{(m)}$ defined as in (10.72). This way, the Dirichlet boundary condition is respected.

\subsection{Spectral methods on triangles and tetrahedra}

As we have seen, the use of spectral methods on quadrilaterals in two dimensions (or parallelepipeds in three dimensions) is made possible by the use of tensor products of one-dimensional functions (on the reference interval $[-1,1])$ and of the onedimensional Gaussian numerical integration formulae. Since a few years, however, we are witnessing a growth of interest in the use of spectral-type methods also on geometries that do not have tensor product structure, such as, for instance, triangles in $2 \mathrm{D}$ and tetrahedra, prisms or pyramids in $3 \mathrm{D}$.

We briefly describe Dubiner's pioneering idea [Dub91] to introduce polynomial bases of high degree on triangles, later extended in $[\mathrm{KS} 05]$ to the three-dimensional case. 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-261.jpg?height=262&width=548&top_left_y=119&top_left_x=176)

Fig. 10.14. Transformation of the reference triangle $\widehat{T}$ into the reference square $\widehat{Q}$. The slanting segments are transformed into vertical segments

We consider the reference triangle

$$
\widehat{T}=\left\{\left(x_{1}, x_{2}\right) \in \mathbb{R}^{2}:-1<x_{1}, x_{2} ; x_{1}+x_{2}<0\right\}
$$

and the reference square

$$
\widehat{Q}=\left\{\left(\xi_{1}, \xi_{2}\right) \in \mathbb{R}^{2}:-1<\xi_{1}, \xi_{2}<1\right\}
$$

The transformation

$$
\left(x_{1}, x_{2}\right) \rightarrow\left(\xi_{1}, \xi_{2}\right), \quad \xi_{1}=2 \frac{1+x_{1}}{1-x_{2}}-1, \quad \xi_{2}=x_{2}
$$

is a bijection between $\widehat{T}$ and $\widehat{Q}$. Its inverse is given by

$$
\left(\xi_{1}, \xi_{2}\right) \rightarrow\left(x_{1}, x_{2}\right), \quad x_{1}=\frac{1}{2}\left(1+\xi_{1}\right)\left(1-\xi_{2}\right)-1, \quad x_{2}=\xi_{2}
$$

As highlighted in Fig. 10.14, the mapping $\left(x_{1}, x_{2}\right) \rightarrow\left(\xi_{1}, \xi_{2}\right)$ sends the ray in $\widehat{T}$ issuing from the vertex $(-1,1)$ and passing through the point $\left(x_{1},-1\right)$ to the vertical segment of $\widehat{Q}$ of equation $\xi_{1}=x_{1}$. The latter therefore becomes singular in $(-1,1)$. For this reason we call $\left(\xi_{1}, \xi_{2}\right)$ the collapsed Cartesian coordinates of the point of the triangles having coordinates $\left(x_{1}, x_{2}\right)$. We denote by $\left\{J_{k}^{(\alpha, \beta)}(\xi), k \geq 0\right\}$ the family of Jacobi polynomials that are orthogonal with respect to the weight $w(\xi)=(1-\xi)^{\alpha}(1+\xi)^{\beta}$, for $\alpha, \beta \geq 0$. Hence,

$$
\forall k \geq 0, \quad J_{k}^{(\alpha, \beta)} \in \mathbb{P}_{k} \quad \text { and } \quad \int_{-1}^{1} J_{k}^{(\alpha, \beta)}(\xi) J_{m}^{(\alpha, \beta)}(\xi) w(\xi) d \xi=0 \quad \forall m \neq k
$$

We observe that, for $\alpha=\beta=0, J_{k}^{(0,0)}$ coincides with the $k$-th Legendre polynomial $L_{k}$ For each pair of integers $\mathbf{k}=\left(k_{1}, k_{2}\right)$ we define the so-called warped tensor product basis on $\widehat{Q}$

$$
\Phi_{\mathbf{k}}\left(\xi_{1}, \xi_{2}\right)=\Psi_{k_{1}}\left(\xi_{1}\right) \Psi_{k_{1}, k_{2}}\left(\xi_{2}\right)
$$

with $\Psi_{k_{1}}\left(\xi_{1}\right)=J_{k_{1}}^{(0,0)}\left(\xi_{1}\right)$ and $\Psi_{k_{1}, k_{2}}\left(\xi_{2}\right)=\left(1-\xi_{2}\right)^{k_{1}} J_{k_{2}}^{\left(2 k_{1}+1,0\right)}\left(\xi_{2}\right)$. Note that $\Phi_{\mathbf{k}}$ is a polynomial of degree $k_{1}$ in $\xi_{1}$ and $k_{1}+k_{2}$ in $\xi_{2}$. By now applying mapping $(10.73)$, we find the following function defined on $\widehat{T}$

$$
\varphi_{\mathbf{k}}\left(x_{1}, x_{2}\right)=\Phi_{\mathbf{k}}\left(\xi_{1}, \xi_{2}\right)=J_{k_{1}}^{(0,0)}\left(2 \frac{1+x_{1}}{1-x_{2}}-1\right)\left(1-x_{2}\right)^{k_{1}} J_{k_{2}}^{\left(2 k_{1}+1,0\right)}\left(x_{2}\right)
$$

This is a polynomial of total degree $k_{1}+k_{2}$ in the variables $x_{1}, x_{2}$, i.e. $\varphi_{\mathrm{k}} \in \mathbb{P}_{k_{1}+k_{2}}(\widehat{T})$. The orthogonality of the Jacobi polynomials (10.74), for each $m \neq k$, allows to prove that

$$
\begin{aligned}
&\int_{\widehat{T}} \varphi_{\mathbf{k}}\left(x_{1}, x_{2}\right) \varphi_{\mathbf{m}}\left(x_{1}, x_{2}\right) d x_{1} d x_{2}=\frac{1}{2}\left(\int_{-1}^{1} J_{k_{1}}^{(0,0)}\left(\xi_{1}\right) J_{m_{1}}^{(0,0)}\left(\xi_{1}\right) d \xi_{1}\right) \\
&\quad\left(\int_{-1}^{1} J_{k_{2}}^{\left(2 k_{1}+1,0\right)}\left(\xi_{2}\right) J_{m_{2}}^{\left(2 m_{1}+1,0\right)}\left(\xi_{2}\right)\left(1-\xi_{2}\right)^{k_{1}+m_{1}+1} d \xi_{2}\right)=0
\end{aligned}
$$

Hence, $\left\{\varphi_{\mathrm{k}}: 0 \leq k_{1}, k_{2}, k_{1}+k_{2} \leq N\right\}$ constitutes an orthogonal (modal) basis for the space of polynomials $\mathbb{P}_{N}(\widehat{T})$, with dimension $\frac{1}{2}(N+1)(N+2)$.

The orthogonality property is undoubtedly convenient as it allows to diagonalize the mass matrix (see Chap. 5). However, with the modal basis described above, imposing the boundary conditions (in case the computational domain is a triangle $\widehat{T}$ ), as well as satisfying the continuity conditions on the interelements (in case spectral element methods with triangular elements are used) results to be uncomfortable. A possible remedy consists in adapting such basis by generating a new one, which we will denote by $\left\{\varphi_{\mathbf{k}}^{b a}\right\} ;$ ba stands for boundary adapted. In order to obtain it, we will start by replacing the one-dimensional Jacobi basis $J_{k}^{(\alpha, 0)}(\xi)$ (with $\alpha=0$ or $\left.2 k+1\right)$ with the adapted basis constituted by:

- two boundary functions: $\frac{1+\xi}{2}$ and $\frac{1-\xi}{2}$;

- $(N-1)$ bubble functions: $\left(\frac{1+\xi}{2}\right)\left(\frac{1-\xi}{2}\right) J_{k-2}^{(\alpha, \beta)}(\xi), k=2, \ldots, N$, for suitable fixed $\alpha, \beta \geq 1$.

These one-dimensional bases are then used as in (10.75) instead of the non-adapted Jacobi polynomials. This way, we find vertex-type, edge-type and bubble functions. Precisely:

- vertex-type functions:

$$
\begin{array}{ll}
\Phi^{V_{1}}\left(\xi_{1}, \xi_{2}\right)=\left(\frac{1-\xi_{1}}{2}\right)\left(\frac{1-\xi_{2}}{2}\right) & \left(\text { vertex } V_{1}=(-1,-1)\right), \\
\Phi^{V_{2}}\left(\xi_{1}, \xi_{2}\right)=\left(\frac{1+\xi_{1}}{2}\right)\left(\frac{1-\xi_{2}}{2}\right) & \left(\text { vertex } V_{2}=(1,-1)\right), \\
\Phi^{V_{3}}\left(\xi_{1}, \xi_{2}\right)=\frac{1+\xi_{2}}{2} & \left(\text { vertex } V_{3}=(-1,1)\right) ;
\end{array}
$$


![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-263.jpg?height=998&width=562&top_left_y=116&top_left_x=175)

Fig. 10.15. Basis functions of degree $N=5$ : boundary-adapted bases on the square (first from the top) and on the triangle (second from top) associated to the values $\beta=1$ and $\delta=0$; Jacobi basis $J_{k}^{(\alpha, \beta)}$ on the square (second from the bottom) corresponding to the values $\alpha=\beta=0$ (Legendre case); Dubiner basis functions $\left\{\Phi_{\mathbf{k}}\right\}$ on the triangle (bottom) - edge-type functions:

$$
\begin{array}{ll}
\Phi_{K_{1}}^{V_{1} V_{2}}\left(\xi_{1}, \xi_{2}\right)=\left(\frac{1-\xi_{1}}{2}\right)\left(\frac{1+\xi_{1}}{2}\right) J_{k_{1}-2}^{(\beta, \beta)}\left(\xi_{1}\right)\left(\frac{1-\xi_{2}}{2}\right)^{k_{1}}, & 2 \leq k_{1} \leq N \\
\Phi_{K_{2}}^{V_{1} V_{3}}\left(\xi_{1}, \xi_{2}\right)=\left(\frac{1-\xi_{1}}{2}\right)\left(\frac{1-\xi_{2}}{2}\right)\left(\frac{1+\xi_{2}}{2}\right) J_{k_{2}-2}^{(\beta, \beta)}\left(\xi_{2}\right), & 2 \leq k_{2} \leq N \\
\Phi_{K_{2}}^{V_{2} V_{3}}\left(\xi_{1}, \xi_{2}\right)=\left(\frac{1+\xi_{1}}{2}\right)\left(\frac{1-\xi_{2}}{2}\right)\left(\frac{1+\xi_{2}}{2}\right) J_{k_{2}-2}^{(\beta, \beta)}\left(\xi_{2}\right), & 2 \leq k_{2} \leq N
\end{array}
$$

- bubble-type functions:

$$
\begin{aligned}
&\Phi_{k_{1}, k_{2}}^{\beta}\left(\xi_{1}, \xi_{2}\right)=\left(\frac{1-\xi_{1}}{2}\right)\left(\frac{1+\xi_{1}}{2}\right) J_{k_{1}-2}^{(\beta, \beta)}\left(\xi_{1}\right) \\
&\left(\frac{1-\xi_{2}}{2}\right)^{k_{1}}\left(\frac{1+\xi_{2}}{2}\right) J_{k_{2}-2}^{\left(2 k_{1}-1+\delta, \beta\right)}\left(\xi_{2}\right) \\
&2 \leq k_{1}, k_{2}, k_{1}+k_{2} \leq N
\end{aligned}
$$

Although the choice $\beta=\delta=2$ ensures the orthogonality of the bubble functions, generally we prefer the choice $\beta=1, \delta=0$, which guarantees a good sparsity of the mass and stiffness matrices and an acceptable condition number for the stiffness matrix for second-order differential operators.

In Fig. 10.15 we report some examples of bases on triangles corresponding to different choices of $\beta$ and $\delta$ and different values of the degree $N$

Using these modal bases, we can now set up a spectral Galerkin approximation for a boundary-value problem set on the triangle $\widehat{T}$, or a SEM-type method on a domain $\Omega$ partitioned in triangular elements. We refer the interested reader to [CHQZ06, CHQZ07, KS05].

\section{$10.7$ Exercises}

1. Prove inequality $(10.52)$.

2. Prove property $(10.54)$.

3. Write the weak formulation of problem

$$
\left\{\begin{array}{l}
-\left((1+x) u^{\prime}(x)\right)^{\prime}+u(x)=f(x), \quad 0<x<1 \\
u(0)=\alpha, \quad u(1)=\beta
\end{array}\right.
$$

and the linear system resulting from its discretization using the G-NI method.

4. Approximate the problem

$$
\left\{\begin{array}{l}
-u^{\prime \prime}(x)+u^{\prime}(x)=x^{2}, \quad-1<x<1 \\
u(-1)=1, \quad u^{\prime}(1)=0
\end{array}\right.
$$

using the G-NI method and analyze its stability and convergence.

5. Write the G-NI approximation of the problem

$$
\left\{\begin{array}{l}
L u(x)=-\left(\mu(x) u^{\prime}(x)\right)^{\prime}+(b(x) u(x))^{\prime}+\sigma(x) u(x)=f(x), \quad-1<x<1 \\
\mu(\pm 1) u^{\prime}(\pm 1)=0
\end{array}\right.
$$

Find the conditions on the data under which the pseudo-spectral approximation is stable. Moreover, verify that the following relations hold:

$$
\begin{aligned}
&L_{N} u_{N}\left(x_{j}\right)=f\left(x_{j}\right), \quad j=1, \ldots, N-1 \\
&\mu(1) u_{N}^{\prime}(1)=\alpha_{N}\left(f-L_{N} u_{N}\right)(1) \\
&\mu(-1) u_{N}^{\prime}(-1)=-\alpha_{0}\left(f-L_{N} u_{N}\right)(-1)
\end{aligned}
$$

$L_{N}$ being the pseudo-spectral operator defined in $(10.41)$.

6. Consider the problem

$$
\begin{cases}-\mu \Delta u+\mathbf{b} \cdot \nabla u-\sigma u=f & \text { in } \Omega=(-1,1)^{2} \\ u(\mathbf{x})=u_{0} & \text { for } x_{1}=-1 \\ u(\mathbf{x})=u_{1} & \text { for } x_{1}=1 \\ \nabla u(\mathbf{x}) \cdot \mathbf{n}(\mathbf{x})=0 & \text { for } x_{2}=-1 \text { and } x_{2}=1\end{cases}
$$

where $\mathbf{x}=\left(x_{1}, x_{2}\right)^{T}, \mathbf{n}$ is the outgoing normal of $\Omega, \mu=\mu(\mathbf{x}), \mathbf{b}=\mathbf{b}(\mathbf{x}), \sigma=\sigma(\mathbf{x})$, $f=f(\mathbf{x})$ are assigned functions, and $u_{0}, u_{1}$ are given constants.

Provide sufficient conditions on the data to guarantee the existence and uniqueness of the weak solution, and give an a priori estimate. Then approximate the weak problem using the G-NI method, providing an analysis of its stability and convergence.

7. Prove the stability condition (10.42) in the case of the pseudo-spectral approximation of the equation (5.4) (replacing the interval $(0,1)$ with $(-1,1)$ ).

[Solution: follow a similar procedure to that explained in Sect. $5.4$ for the finite element solution and invoke the properties reported in Lemmas $10.2$ and 10.3.]

8. Consider the parabolic heat equation

$$
\begin{cases}\frac{\partial u}{\partial t}-\frac{\partial^{2} u}{\partial x^{2}}=0, & -1<x<1, t>0 \\ u(x, 0)=u_{0}(x), & -1<x<1 \\ u(-1, t)=u(1, t)=0, & t>0\end{cases}
$$

Approximate it using the G-NI method in space and the implicit Euler method in time and its stability study. Chapter 11

\section{Isogeometric analysis}

Isogeometric Analysis - commonly abbreviated as IGA - is a strategy for the spatial approximation of PDEs based on the so-called isogeometric concept. Extensively developed in the last years starting from the seminal work of T.J.R. Hughes and collaborators [HCB05] in 2005 , IGA originally aimed at restoring the centrality of the geometric representation of the computational domain in the numerical approximation of PDEs. IGA was developed with the promise to close the current gap between Computer Aided Design (CAD) procedures and computational modeling of PDEs (as outlined e.g. in Sec. 8.1) by recognizing that the meshing procedure of the computational domain is indeed a major bottleneck in Engineering practice. Since CAD systems mostly employ B-splines or NURBS [dB01, PT97] basis functions for the geometric representation of the computational domains, IGA considers the same Bsplines or NURBS bases for the construction of the finite dimensional space in which the approximate solutions of the PDEs lay; this strategy is commonly indicated as NURBS-based IGA [CHB09, HCB05]. Other, more general or flexible geometric representations based on splines, as e.g. T-splines, locally refined splines, etc.. [BCC $^{+} 10$, BCS10, SBV $^{+} 11$, SLSH12, SZBN03, TSHH17], can be used for IGA, even if their development and usage are still not straightforward and represent active research fields.

While the original, driving factor in the IGA development has been the efficiency of the whole design-through-analysis computational pipeline in parallel with the geometric accuracy - namely "exactness" - in the representation of the computational domain, the employment of B-splines and NURBS basis functions is especially suited for the spatial approximation of several families of PDEs, also in virtue of the regularity properties of these basis functions.

The most widespread IGA construction is NURBS-based IGA in the framework of the Galerkin method [CHB09, EBBH09, HCB05]. We refer the interested reader to e.g. $\left[\mathrm{ABC}^{+} 08\right.$, ABBS17, ABKF11, BBHH10, BCZH06, BHS12, BSV14, BVS $^{+} 12$, CHR07, DBH12, DCV $^{+} 16$, dFRV11, GCBH08, $\left.S D S^{+} 12, Z B G^{+} 07\right]$ for an overview of applications to problems in Computational Mechanics, a list which is far from being exhaustive.

In this chapter we provide a brief overview of NURBS-based IGA in the framework of the Galerkin method starting from the definition of B-splines and NURBS basis functions and geometries, the isogeometric concept, the Galerkin method, and the approximation properties of the method.

\subsection{B-splines and NURBS}

B-splines and NURBS (Non Rational Uniform B-splines) - a generalization of Bsplines - are widely used in CAD. B-splines are a particular family of splines functions - constituted by piecewise polynomials - with minimal support for a given polynomial degree.

\subsubsection{B-splines basis functions}

Univariate B-splines basis functions are built over a parameter domain $\widehat{\Omega} \subset \mathbb{R}$ starting from the so-called knot vector $\Xi$, a set of non-decreasing real values in the parameter space $\mathbb{R}$. Specifically, $\Xi=\left\{\widehat{\xi}_{1}, \widehat{\xi}_{2}, \ldots, \hat{\xi}_{n+p+1}\right\}$, where $\widehat{\xi}_{i}$ for $i=1, \ldots, n+p+1$ is the $i$ th knot, $n$ is the number of basis functions composing the B-splines basis, and $p \geq 0$ the polynomial degree; in this manner, the parameter domain $\Omega$ reads as $\widehat{\Omega}=\left(\widehat{\xi}_{1}, \widehat{\xi}_{n+p+1}\right)$. As $\widehat{\xi}_{1} \leq \widehat{\xi}_{2} \leq \cdots \leq \hat{\xi}_{n+p+1}$, knots can be repeated for which a multiplicity $m_{j} \geq 1$ associated to each knot value $j=1, \ldots, \tilde{n}$, for some $\tilde{n} \leq n+p+1$; by convention, the multiplicity of each knot value is $1 \leq m_{j} \leq p+1$. In particular, we consider the case of open knot vectors, for which the multiplicity of the first and last knot values in $\Xi$ is $m_{1}=m_{\tilde{n}}=p+1$. Pairs of knots delimit knot spans $\left(\widehat{\xi}_{i}, \widehat{\xi}_{i+1}\right)$ in $\widehat{\Omega}$, for $i=1, \ldots, n+p ;$ in particular, knot spans of null size are allowed by construction in the case knot values have multiplicity $m_{j} \geq 1$. For open knot vectors, the knot spans internal to $\widehat{\Omega}$, i.e. $\left\{\left(\widehat{\xi}_{i}, \widehat{\xi}_{i+1}\right)\right\}_{i=p+1}^{n}$ can be interpreted as mesh elements whose number is $n_{e l}$; then, one has the relation $n=n_{e l}+p$ among the number of such mesh elements, the number of basis functions, and the polynomial degree. Finally, we denote by $\widehat{h}$ the characteristic mesh size of the elements partitioning $\widehat{\Omega}$.

For a given knot vector $\Xi=\left\{\widehat{\xi}_{1}, \widehat{\xi}_{2}, \ldots, \widehat{\xi}_{n+p+1}\right\}$, the univariate $\mathrm{B}-$ splines basis functions $\widehat{N}_{i}: \widehat{\Omega} \rightarrow \mathbb{R}$, for $i=1, \ldots, n$, are piecewise polynomials of degree $p$ built by means of the Cox-de Boor recursion formula [PT97]:

$$
\begin{aligned}
&\widehat{N}_{i, 0}(\xi)= \begin{cases}1 & \text { for } \xi \in\left[\widehat{\xi}_{i}, \widehat{\xi}_{i+1}\right) \\
0 & \text { otherwise }\end{cases} \\
&\widehat{N}_{i, k}(\xi)=\frac{\xi-\widehat{\xi}_{i}}{\widehat{\xi}_{i+k}-\widehat{\xi}_{i}} \widehat{N}_{i, k-1}(\xi)+\frac{\widehat{\xi}_{i+1}-\xi}{\widehat{\xi}_{i+k+1}-\widehat{\xi}_{i+i}} \widehat{N}_{i+1, k-1}(\xi) \quad \text { for } k=1, \ldots p
\end{aligned}
$$

when the ratio $\frac{0}{0}$ appears in the above formula, it is, by convention, replaced by 0 . For the sake of simplicity, we denote the basis functions $\widehat{N}_{i, p}(\xi)$ simply as $\widehat{N}_{i}(\xi)$, the dependence on the degree $p$ being understood. The properties of B-splines are uniquely determined from the knot vector $\Xi$, including their number $n$ and degree $p$, as well as their support and regularity, which are strictly related to the multiplicity of the knot values. In particular, each basis function $\widehat{N}_{i}$ has support in $p+1$ knot spans, regardless of their size; moreover, $p+1$ basis functions have support in each knot span (or mesh element). B-splines basis functions are positive definite, specifically one has $\widehat{N}_{i}(\xi) \geq 0$ for all $\xi \in \widehat{\Omega}$ and for any $i=1, \ldots, n ;$ in addition, they constitute a partition of unity, i.e. $\sum_{i=1}^{n} \widehat{N}_{i}(\xi) \equiv 1$ for all $\xi \in \widehat{\Omega}$. B-splines basis functions are piecewise polynomials, hence they are $C^{\infty}$-continuous in each knot span (mesh element), but only $C^{p-m}$-continuous across each knot value of multiplicity $m \geq 1$. This last property is widely exploited in NURBS-based IGA as it is peculiar of these basis functions. For example, B-splines of degree $p=1$ are only $C^{0}-$ continuous across each mesh element (internal knot value) in $\Omega$. Basis functions of degree $p=2$ are either $C^{0}-$ or $C^{1}-$ continuous across mesh elements depending on the multiplicity $m=2$ or $m=1$ of the associated internal knot value, respectively. In general, basis functions of degree $p$ are $C^{0}-, C^{1}-, \ldots$, or $C^{p-1}-$ continuous across mesh elements depending on the multiplicity $m=p, p-1, \ldots, 1$ of the associated knot values, respectively. Examples of univariate B-splines basis functions are reported in Fig. 11.1.

Bivariate, trivariate, and in general multivariate B-splines basis functions of dimension $\kappa \geq 1$ are built through the tensor product rule. The knot vectors $\Xi^{\alpha}=\left\{\widehat{\xi}_{1} \alpha\right.$ \left.$\widehat{\xi}_{2} \alpha, \ldots, \widehat{\xi}_{n_{\alpha}+p_{\alpha}+1}^{\alpha}\right\}$ are defined for each parametric direction $\alpha=1, \ldots, \kappa$, with $n_{\alpha}$ and $p_{\alpha}$ the corresponding number of basis functions and degree; then, univariate B-splines basis functions $\widehat{N}_{i}^{\alpha}: \widehat{\Omega} \rightarrow \mathbb{R}$ are built separately from each knot vector $\Xi^{\alpha}$ for $\alpha=$ $1, \ldots, \kappa$. By using the tensor product combination of knot vectors, the parameter domain reads $\widehat{\Omega}=\otimes_{\alpha=1}^{\kappa}\left(\widehat{\xi}_{1}^{\alpha}, \widehat{\xi}_{n_{\alpha}+p_{\alpha+1}}^{\alpha}\right) \subset \mathbb{R}^{\kappa}$ with mesh elements $\otimes_{\alpha=1}^{\kappa}\left(\widehat{\xi}_{i_{\alpha}}^{\alpha}, \widehat{\xi}_{i_{\alpha}+1}^{\alpha}\right) \subset$ $\mathbb{R}^{\kappa}$, whose characteristic mesh size is denoted with $\widehat{h}$. By introducing the multi-index $\mathbf{i}=\left(i_{1}, \ldots, i_{\kappa}\right)$ for some $1 \leq i_{\alpha} \leq n_{\alpha}$ and $1 \leq \alpha \leq \kappa$, the multivariate B-splines basis functions $\widehat{N}_{\mathbf{i}}: \widehat{\Omega} \rightarrow \mathbb{R}$ are built by exploiting the tensor product rule as

$$
\widehat{N}_{\mathbf{i}}(\xi)=\prod_{\alpha=1}^{\kappa} \widehat{N}_{i \alpha}^{\alpha}\left(\xi^{\alpha}\right)
$$

where $\xi=\left(\xi^{1}, \ldots, \xi^{\kappa}\right) \in \widehat{\Omega}$. The total number of multivariate $\mathrm{B}-$ splines basis functions is $n=\prod_{\alpha=1}^{\kappa} n_{\alpha}$; by assuming a suitable reordering, the multi-index notation is dropped henceforth in favor of the scalar one, for which multivariate B-splines are indicated as $\widehat{N}_{i}: \widehat{\Omega} \rightarrow \mathbb{R}$ for $i=1, \ldots, n$. At this stage, we simply recall that multivariate B-splines basis functions are positive and still form a partition of unity in $\Omega$. Regarding the properties of support and regularity of these basis functions across the mesh elements edges, these are inherited from the independent univariate basis functions along each parametric direction $\alpha=1, \ldots, \kappa$ from which they are built. 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-269.jpg?height=1026&width=712&top_left_y=129&top_left_x=100)

Fig. 11.1. Examples of univariate B-splines basis functions; knot vectors $\Xi$, degree $p$, number of elements $n_{e l}$, number of basis functions $n$, and global continuity $C^{k}$ are indicated 

\subsubsection{B-splines curves, surfaces, and solids}

B-splines geometries are obtained as geometrical mappings from the parameter domain $\widehat{\Omega}$ in the parameter space $\mathbb{R}^{\kappa}\left(\widehat{\Omega} \subset \mathbb{R}^{\kappa}\right)$ into the physical space $\mathbb{R}^{d}$, with $1 \leq$ $\kappa \leq d$. For example, for $\kappa=1$ and $d=3$, one obtains curves in 3D, for $\kappa=2$ and $d=3$ a surface, while for $\kappa=d=3$ a solid object. Other than from the B-splines basis functions $\left\{\widehat{N}_{i}(\xi)\right\}_{i=1}^{n}$, obtained from the knot vectors $\Xi^{\alpha}$ for $\alpha=1, \ldots, \kappa, \mathrm{B}-$ splines geometries are built from the control points $\left\{\mathbf{P}_{i}\right\}_{i=1}^{n} \in \mathbb{R}^{d}$, a set of points in the physical space. Each control point is associated to a B-splines basis function. Control points are ordered accordingly based on the tensor product rule, a construction which identifies the so-called control polygon in $\mathbb{R}^{d}$. Then, a B-splines geometry is determined by the geometrical mapping $\phi: \widehat{\Omega} \rightarrow \mathbb{R}^{d}$, with $\phi: \xi \mapsto \mathbf{x}$, as

$$
\phi(\xi)=\sum_{i=1}^{n} \mathbf{P}_{i} \widehat{N}_{i}(\xi)
$$

which defines the physical domain $\Omega$ identifying the geometry in the physical space $\mathbb{R}^{d}$. We illustrate in Figs. $11.2$ and $11.3$ some examples of B-splines geometries.

Regularity properties of a B-splines geometry are inherited from those of the Bsplines basis functions which they are built from, other than the location of control points in the physical space. We remark that smooth graphical representations of $\Omega$ known as (high order) geometric continuity - can be obtained by particular choices of the control points positions even if the B-splines basis functions are only $C^{0}$-continuous. However, particular choices of the control points positions may lead to degenerate ge-

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-270.jpg?height=298&width=455&top_left_y=802&top_left_x=212)

Fig. 11.2. B-splines curve in $2 \mathrm{D}(\kappa=1$ and $d=2)$ obtained from $n=6 \mathrm{~B}$-splines basis functions of degree $p=2$, built from the knot vector $\Xi=\left\{\{0\}^{3}, 1 / 4,1 / 2,3 / 4,\{1\}^{3}\right\}$, and the control points $\mathbf{P}_{1}=(1,0)^{T}, \mathbf{P}_{2}=(1 / 2,1)^{T}, \mathbf{P}_{3}=(0,1)^{T}, \mathbf{P}_{4}=(-3 / 4,1 / 2)^{T}, \mathbf{P}_{5}=(-1,0)^{T}$, and $\mathbf{P}_{6}=(0,-1 / 4)^{I}$. The control points and control polygon are highlighted in red 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-271.jpg?height=341&width=435&top_left_y=137&top_left_x=236)

Fig. 11.3. B-splines surface in $3 \mathrm{D}(\kappa=2$ and $d=3)$ obtained from $n=8$ bivariate B-splines basis functions. These are built from the univariate B-splines associated to the knot vectors $\Xi^{1}=\left\{\{0\}^{2},\{1\}^{2}\right\}$ and $\Xi^{2}=\left\{\{0\}^{3}, \frac{1}{2},\{1\}^{3}\right\}$ and have degrees 1 and 2, respectively. The control points are $\mathbf{P}_{1}=(1,0,0)^{T}, \mathbf{P}_{2}=(1 / 2,1 / 4,0)^{T}, \mathbf{P}_{3}=(1 / 4,1 / 2,0)^{T}$, and $\mathbf{P}_{4}=(0,1,0)^{T}$ along the parametric direction $\alpha=1$, while $\mathbf{P}_{5}=(3 / 2,0,0)^{T}, \mathbf{P}_{6}=(5 / 4,3 / 4,1)^{T}, \mathbf{P}_{7}=$ $(3 / 4,5 / 4,1)^{T}$, and $\mathbf{P}_{8}=(0,3 / 2,0)^{T}$ along the parametric direction $\alpha=2$. The control points and control polygon are highlighted in red

ometric representations which however we do not consider in this chapter [PT97].

\subsubsection{NURBS basis functions and geometries}

Geometric representations based on B-splines are flexible but do not allow to exactly represent conic sections, a wide family of geometries commonly used in computer graphics and design including circular arcs, cylindrical shapes, and spheres. To achieve both flexibility and accuracy in geometric representations, NURBS have been introduced [PT97] and nowadays represent a widely used tool in computer graphics.

Univariate NURBS basis functions are obtained by associating to an univariate Bsplines basis $\left\{\widehat{N}_{i}(\xi)\right\}_{i=1}^{n}$, built from the knot vector $\Xi$, a set of $n$ real numbers called weights, say $\left\{w_{i}\right\}_{i=1}^{n} \in \mathbb{R}$. By introducing the weighting function $W(\xi)=\sum_{j=1}^{n} \widehat{N}_{j}(\xi) w_{j}$, the univariate NURBS basis functions $\widehat{R}_{i}: \widehat{\Omega} \rightarrow \mathbb{R}$, with $\widehat{\Omega} \subset \mathbb{R}$, read

$$
\widehat{R}_{i}(\xi)=\frac{\widehat{N}_{i}(\xi) w_{i}}{W(\xi)}=\frac{\widehat{N}_{i}(\xi) w_{i}}{\sum_{j=1}^{n} \widehat{N}_{j}(\xi) w_{j}} \quad \text { for } i=1, \ldots, n
$$

Each NURBS basis function is a piecewise rational function defined in the parameter domain $\Omega$; even if they are not piecewise polynomials, the degree $p$ is conventionally referred to as the polynomial degree of the B-splines basis functions from which 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-272.jpg?height=280&width=690&top_left_y=129&top_left_x=100)

Fig. 11.4. Examples of univariate NURBS basis functions with knot vector $\Xi=$ $\left\{\{0\}^{3}, 1 / 4,1 / 2,3 / 4,\{1\}^{3}\right\}$, degree $p=2$, number of elements $n_{e l}=4$, number of basis functions $n=6$, and globally $C^{1}$-continuous; NURBS weights are indicated in the vector $\mathbf{w}$

these are generated. In this chapter, we will assume that the weights are strictly positive, i.e. $w_{i}>0$ for all $i=1, \ldots, n$, whence the properties of NURBS are immediately inherited from those of the generating B-splines, including the regularity (continuity) properties; as a matter of fact, B-splines are a particular case of NURBS for which the weights are all equal, by convention $w_{i}=1$ for all $i=1, \ldots, n$. Some examples of univariate NURBS basis functions are reported in Fig. 11.4.

By exploiting the tensor product construction of multivariate B-splines basis functions, which straightforwardly extends to the corresponding weights, and by using a suitable reordering of the basis functions, multivariate NURBS basis functions $\widehat{R}_{i}$ : $\widehat{\Omega} \rightarrow \mathbb{R}$, with $\widehat{\Omega} \subset \mathbb{R}^{\kappa}$, read

$$
\widehat{R}_{i}(\xi)=\frac{\widehat{N}_{i}(\xi) w_{i}}{W(\xi)}=\frac{\widehat{N}_{i}(\xi) w_{i}}{\sum_{j=1}^{n} \widehat{N}_{j}(\xi) w_{j}} \quad \text { for } i=1, \ldots, n
$$

Analogously to B-splines, by associating to each basis $\left\{\widehat{R}_{i}(\xi)\right\}_{i=1}^{n}$ a set of control points $\left\{\mathbf{P}_{i}\right\}_{i=1}^{n}$, a NURBS geometry is determined by the geometric mapping $\phi: \widehat{\Omega} \rightarrow$ $\mathbb{R}^{d}$ as W

$$
\phi(\xi)=\sum_{i=1}^{n} \mathbf{P}_{i} \widehat{R}_{i}(\xi)
$$

with the physical domain $\Omega$ identified by the NURBS mapping. An example of NURBS geometry is shown in Fig. $11.5$.

In this chapter, we simply consider B-splines and NURBS geometric representations for which $\kappa \equiv d$, i.e. the parametric and physical spaces coincide. We assume that the Jacobian $\widehat{J}(\xi)=\frac{d \phi}{d \xi}(\xi)$ of the geometric mapping $(11.1)$ or $(11.2)$ is well defined in $\widehat{\Omega}$ and that det $(\widehat{J}(\xi))>0$ a.e. in $\widehat{\Omega}$ with the measure of the set $\{\xi \in \widehat{\Omega}: \operatorname{det}(\widehat{J}(\xi))=0\}$ equal to zero in the topology of $\mathbb{R}^{d}$. Finally, we assume that the mapping (11.1) is invertible a.e. in $\widehat{\Omega}$ and that the inverse of $\widehat{J}$ exists and is regular in each element of 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-273.jpg?height=347&width=285&top_left_y=138&top_left_x=311)

Fig. 11.5. Example of a NURBS surface: the cylindrical shell. The control points and control net are highlighted in red

the mesh; the inverse of the geometric mapping is $\phi^{-1}: \mathbf{x} \mapsto \xi$. The geometric mapping (11.2) also allows to map the mesh in the parametric domain $\widehat{\Omega}$, whose characteristic size is $h$, into the mesh in the physical domain $\Omega$ with characteristic size $h$; an example is depicted in Fig. 11.6.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-273.jpg?height=303&width=520&top_left_y=876&top_left_x=197)

Fig. 11.6. NURBS geometric mapping $\phi(\xi)$ from the parameter domain $\widehat{\Omega} \subset \mathbb{R}^{2}$ into the physical domain $\Omega \subset \mathbb{R}^{2}$; the physical mesh is determined by the parametric one through $\phi(\xi)$ 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-274.jpg?height=620&width=692&top_left_y=129&top_left_x=101)

Fig. 11.7. Example of successive $h$-refinements originating from an univariate B-splines basis of degree $p=2$ with $n_{e l}=1$ and $h_{0}=1$ (top-left); the sequentially $h$-refined $\mathrm{B}$-splines bases give rise to sequences of nested function spaces

\subsubsection{NURBS function spaces and $h p k$-refinements}

The NURBS basis $\left\{\widehat{R}_{i}(\xi)\right\}_{i=1}^{n}$ in the parameter domain $\widehat{\Omega}$ defines a NURBS function space over $\widehat{\Omega}$, say $\widehat{\mathscr{N}_{h}}$, of dimension $n$

$$
\widehat{\mathscr{N}_{h}}=\operatorname{span}\left\{\widehat{R}_{i}(\xi), \quad i=1, \ldots, n\right\}
$$

Because of the invertibility of the geometric mapping $(11.2)($ for $\kappa=d)$, NURBS basis functions can be defined in the physical domain $\Omega$ (the geometry itself) as $R_{i}$ : $\Omega \rightarrow \mathbb{R}$, with $R_{i}(\mathbf{x})=\widehat{R}_{i}\left(\phi^{-1}(\mathbf{x})\right)$. Hence, the NURBS function space over the physical domain $\Omega \subset \mathbb{R}^{d}$ reads

$$
\mathscr{N}_{h}=\operatorname{span}\left\{R_{i}(\mathbf{x}), \quad i=1, \ldots, n\right\}=\operatorname{span}\left\{\widehat{R}_{i}\left(\phi^{-1}(\mathbf{x})\right), \quad i=1, \ldots, n\right\}
$$


![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-275.jpg?height=638&width=690&top_left_y=128&top_left_x=98)

Fig. 11.8. Example of successive $p$-refinements originating from an univariate B-splines basis of degree $p_{0}=1$ with $n_{e l}=2$ (top-left); the sequentially $p$-refined B-splines bases give rise to sequences of nested function spaces

NURBS spaces can be enriched by means of three different strategies. The socalled $h$ - and $p$-refinements are analogous to the ones performed with $C^{0}$-continuous Lagrangian basis functions which stand at the basis of both the Finite Element and Spectral Element Methods presented in Chapters 4 and 10 , respectively. A third one however, called $k$-refinement and peculiar of NURBS basis functions, is particularly useful for the construction of regular function spaces [CHB09, HCB05]. We provide hereafter a short and simplified overview of the $h p k$-refinements for univariate $\mathrm{B}-$ splines basis functions, while we refer the reader to $\left[\mathrm{ABC}^{+} 08\right.$, CHR07, EBBH09, HRS08] for a more comprehensive description of the topic.

- $h$-refinement (mesh refinement). It consists of inserting additional knots in the original knot vector $\Xi$, without replicating existing knot values. Each new knot increases the number of new basis functions by a single unit. A uniform knot insertion yields a uniform mesh refinement. An example is reported in Fig. 11.7. - $p$-refinement (increasing the polynomial degree). The polynomial degree $p$ of the B-splines basis functions is increased to $p+1$ by increasing the multiplicity of each knot value in $\Xi$ by one unit; consequently, the number of basis functions is increased by the number of actual mesh elements $n_{e l}$. The regularity of the original basis functions is preserved along the application of the procedure. An example is highlighted in Fig. 11.8.

- $k$-refinement. It consists of performing, on a knot vector without internal knots, firstly order elevations followed by knot insertions. The regularity of the basis functions is maximum, $C^{p-1}$, across each internal knot value and their number only increases by one unit for each $k$-refinement step. An example is depicted in Fig. $11.9$.

We remark that $h$ - and $p$-refinements generate sequences of enriched NURBS function spaces $\mathscr{N}_{h_{1}}, \mathscr{N}_{h_{2}}, \ldots$ which are nested into each other and the original one $\mathscr{N}_{h}$, i.e. $\mathscr{N}_{h} \subset \mathcal{N}_{h_{1}} \subset \mathscr{S}_{h_{2}} \subset \cdots$. Conversely, $k$-refinements provide sequences of NURBS
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-276.jpg?height=616&width=694&top_left_y=539&top_left_x=100)

Fig. 11.9. Example of three levels of $k$-refinements applied to an univariate B-splines basis of degree $p=1$ with $n_{e l}=1$ (top-left); the $k$-refined B-splines bases do not give rise to sequences of nested function spaces function spaces $\mathscr{N}_{h_{1}}, \mathscr{N}_{h_{2}}, \ldots$ with functions endowed with increasing regularity, but not nested into each other, i.e. $\mathscr{N}_{h} \subset \mathscr{N}_{h_{1}}, \mathscr{N}_{h} \subset \mathscr{N}_{h_{2}}, \ldots$, but $\mathscr{N}_{h_{1}} \not \subset \mathscr{N}_{h_{2}}$, etc.

Refinements or, equivalently, enrichments of NURBS function spaces are typically used as a starting point to enhance the representations of the geometry at hand, e.g. to add more detailed features by moving control points. If the enriched NURBS basis is also used for a geometric representation of $\Omega$, a key point is that such geometric representation is preserved while performing the refinement. This means that $\Omega$ must be equally represented by the original NURBS basis $\left\{\widehat{R}_{i}(\xi)\right\}_{i=1}^{n}$ and the enriched one $\left\{\overline{\widehat{R}}_{i}(\xi)\right\}_{i=1}^{\pi}$. Specifically, in order to preserve the geometric mapping along the enrichment of the NURBS basis functions, both a new set of control points $\left\{\bar{P}_{i}\right\}_{i=1}^{n}$ and of NURBS weights $\left\{\bar{w}_{i}\right\}_{i=1}^{\pi}$ must be contextually generated such that, from Eq. (11.2), one has $\phi(\xi)=\sum_{i=1}^{n} \bar{P}_{i} \widehat{R}_{i}(\xi)=\sum_{i=1}^{\pi} \bar{P}_{i} \bar{R}_{i}(\xi)$

Concepts about $h p k$-refinements straightforwardly extend to multivariate NURBS basis functions.

\subsubsection{Construction of NURBS geometries: an example}

We consider as example the construction of a curve, specifically an arc centered in the origin, spanning 90 degrees in $\mathbb{R}^{2}$, and featuring a smooth kink. We start by constructing the arc that will be later "adapted" to our curve. Albeit this is not the unique option to build the curve, we hereby summarize its construction according to this principle in the following steps, also depicted in Fig. 11.10:

- Step 1 . We build the univariate B-splines basis functions of degree $p=2$ from the knot vector $\Xi=\left\{\{0\}^{3},\{1\}^{3}\right\}$ to guarantee a minimum regularity to the basis functions; this yields $n=3$ B-splines basis functions $\left\{\widehat{N}_{i}(\xi)\right\}_{i=1}^{n}$. Correspondingly, we place in the physical space $\mathbb{R}^{2} n=3$ control points $\mathbf{P}_{1}=(1,0)^{T}, \mathbf{P}_{2}=(1,1)^{T}$, and $\mathbf{P}_{3}=(0,1)^{T}$; this yields a B-splines curve, but not yet the arc since this cannot be exactly represented under this construction.

- Step 2(a). In order to obtain the arc, we associate the non-unitary weight $w_{2}=\frac{1}{\sqrt{2}}$ to the B-splines basis function $\widehat{N}_{2}(\xi)$, thus yielding a NURBS basis $\left\{\widehat{R}_{i}(\xi)\right\}_{i=1}^{3}$ - Step $2(\mathrm{~b})$. By using the same control points of step 2 , we obtain the arc spanning 90 degrees and centered in the origin; we notice that in this case $w_{2}=\arccos \left(\frac{90^{\circ}}{2}\right)$.

- Step $3(a)$. As we need to modify the arc with a smooth kink, we enrich the NURBS basis $\left\{\widehat{R}_{i}(\xi)\right\}_{i=1}^{3}$ by inserting new knots in the knot vector $\Xi$, thus yielding $\bar{\Xi}=$ $\left\{\{0\}^{3}, \frac{9}{21}, \frac{10}{21}, \frac{11}{21}, \frac{12}{21},\{1\}^{3}\right\}$. In virtue of this $h$-refinement step, we obtain a new NURBS basis $\left\{\bar{R}_{i}(\xi)\right\}_{i=1}^{\pi}$ of degree $p=2, \bar{n}=7$, and globally $C^{1}-$ continuous;

- Step 3 (b). Following step 3(a), a new set of control points $\left\{\overline{\mathbf{P}}_{i}\right\}_{i=1}^{\pi}$ is generated to preserve the geometric representation of the arc. 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-278.jpg?height=268&width=328&top_left_y=129&top_left_x=116)

Step
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-278.jpg?height=568&width=688&top_left_y=130&top_left_x=136)

Step 2(b)

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-278.jpg?height=282&width=329&top_left_y=436&top_left_x=496)
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-278.jpg?height=594&width=684&top_left_y=436&top_left_x=140)

- Step 4. Finally, the position of the control point $\bar{P}_{4}$ is modified, e.g. as $\bar{P}_{4}=\left(\frac{1}{2}, \frac{1}{2}\right)^{T}$, to obtain the desired curve. 

\subsection{The isogeometric concept}

The commonly indicated NURBS-based IGA [CHB09, HCB05] relies on the very same NURBS (or B-splines) basis functions first used to represent the computational domain of a PDE also to build later the finite dimensional trial space where the numerical solution is sought for. This is specifically referred to as the isogeometric concept [CHB09].

Let us consider for example the linear elliptic problem of Eq. (4.1) in weak formulation. According to the isogeometric concept, the computational domain $\Omega \subset \mathbb{R}^{d}$ is represented by means of NURBS basis functions $-$ exactly in most of the cases of practical interest - through the NURBS geometric mapping $\phi: \Omega \rightarrow \mathbb{R}^{d}$ of Eq. (11.2), where $\phi(\xi)=\sum_{i=1}^{n} \mathbf{P}_{i} \widehat{R}_{i}(\xi) ; \widehat{\Omega} \subset \mathbb{R}^{d}$ is called parameter domain. Then, we look for an approximate solution in the parameter domain $\widehat{u}_{h}: \widehat{\Omega} \rightarrow \mathbb{R}$ such that

$$
\widehat{u}_{h}(\xi)=\sum_{i=1}^{n} U_{i} \widehat{R}_{i}(\xi)
$$

where $\left\{U_{i}\right\}_{i=1}^{n}$ is the so-called set of control variables. Since we assume that the NURBS mapping (11.2) is invertible a.e. in $\widehat{\Omega}$, we write the approximate solution in the computational (physical) domain $u_{h}: \Omega \rightarrow \mathbb{R}$ as

$$
u_{h}(\mathbf{x})=\sum_{i=1}^{n} U_{i} R_{i}(\mathbf{x})=\sum_{i=1}^{n} U_{i} \widehat{R}_{i}\left(\phi^{-1}(\mathbf{x})\right)
$$

It follows that $\widehat{u}_{h} \in \widehat{\mathscr{N}_{h}}$ and, equivalently, $u_{h} \in \mathscr{N}_{h}$, with $\widehat{\mathscr{N}_{h}}$ and $\mathscr{N}_{h}$ the NURBS spaces of Eqs. (11.3) and (11.4), respectively. Thanks to the invertibility of the NURBS mapping $(11.2)$, we have $u_{h}(\mathbf{x})=\widehat{u}_{h}\left(\phi^{-1}(\mathbf{x})\right)$ for which $\widehat{u}_{h}(\xi)$ is often simply indicated as $u_{h}(\mathbf{x})$

According to the isogeometric concept, the approximation properties of the NURBS space are inherited from those of the NURBS basis functions already used to represent the computational domain $\Omega$ wherein the solution of the PDE is defined. We remark however that the solution $u_{h}$ can be sought in a NURBS space $\overline{\mathscr{N}}_{h}$ enriched with respect to the space $\mathscr{N}_{h}$ already used to represent $\Omega$ provided that $\overline{\mathscr{N}}_{h} \subset \mathscr{N}_{h}$; such enrichment is obtained by means of suitable $h p k$-refinements.

We observe that the isogeometric mapping is quite general and not strictly limited to the NURBS mapping (11.2); indeed, other geometric mappings can be used, e.g. those based on other types of splines [TSHH17] or T-splines [BCC $^{+} 10$, SLSH12], provided that the associated basis is complete and $H^{1}(\Omega)$-conformal for a second order elliptic PDE.

The strategy of using the same representation, through suitable basis functions, of the trial solution $u_{h}$ and the computational domain has already been extensively exploited in the so-called isoparametric finite element method [Cia78, Hug00]. However, in this case, the basis functions $-$ let them be for example polynomials of degree $r$, say $\left\{\varphi_{i}^{r}(\mathbf{x})\right\}_{i=1}^{N_{r}} \in \mathbb{P}_{r}-$ are determined by the choice of the finite element approximation, or equivalently by the finite element space $X_{h}^{r}$ (4.38). Then, the computational domain $\bar{\Omega}$ is approximated by the computational mesh $\Omega_{h}=\operatorname{int}\left(\cup_{K \in \mathscr{T}_{h}} K\right)$, intended as 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-280.jpg?height=152&width=694&top_left_y=110&top_left_x=106)

Fig. 11.11. Isoparametric finite elements in $\mathbb{R}^{2}$; mappings from the reference element $\widehat{K}$ into the mesh element $K$

the union of non-overlapping mesh elements $K ;$ see Fig. 4.9. According to the isoparametric concept, each mesh element $K \in \mathscr{T}_{h}$ is built from the reference one, say $\widehat{K}$, by means of a geometric mapping in the form of Eq. (11.2) by using the polynomial basis $\left\{\widehat{\varphi}_{i}^{r}(\xi)\right\}_{i=1}^{N_{r}} \in \mathbb{P}_{r}$. In this manner, if $r \geq 2$, mesh elements as triangles in $\mathbb{R}^{2}$ may be endowed with "curved" edges to better approximate the computational domain $\Omega$. For example, if $r=1$, we have $\mathbb{P}_{1}=\left\{p\left(\xi_{1}, \xi_{2}\right)=a+b \xi_{1}+c \xi_{2}\right.$, with $\left.a, b, c \in \mathbb{R}\right\}$, while for $r=2$, we have $\mathbb{P}_{2}=\left\{p\left(\xi_{1}, \xi_{2}\right)=a+b \xi_{1}+c \xi_{2}+d \xi_{1} \xi_{2}+e \xi_{1}^{2}+f \xi_{2}^{2}\right.$, with $a, b, c$, $d, e, f \in \mathbb{R}\}$, with obvious choice of the basis functions $\left\{\widehat{\varphi}_{i}^{r}(\xi)\right\}_{i=1}^{N_{r}} \in \mathbb{P}_{r}$. If the geometric mapping is based on polynomials of degree $r=1$, then the reference triangle $\widehat{K}$ is mapped into a mesh element $K$ as a triangle with straight edges according to a mapping $\phi^{1}(\xi)$ using the basis $\left\{\widehat{\varphi}_{i}^{1}(\xi)\right\}_{i=1}^{3}$. Instead, if $r=2$, the reference triangle $\widehat{K}$ can be mapped into a mesh element $K$ with curved edges (sections of parabola) through a mapping $\phi^{2}(\xi)$ using $\left\{\widehat{\varphi}_{i}^{1}(\xi)\right\}_{i=1}^{6} ;$ see Fig. 11.11. As a matter of fact, the finite element method presented in Chapter 4 is a particular case of the isoparametric finite element method just introduced for which the geometric mapping of each mesh element $K$ from $\widehat{K}$ is $\phi^{1}(\xi)-$ i.e. using the basis functions $\left\{\widehat{\varphi}_{i}^{1}(\xi)\right\}_{i=1}^{3} \in \mathbb{P}_{1}-$ regardless of the polynomial degree $r \geq 1$ used to build the finite element space $X_{h}^{r}$.

We remark that the isoparametric concept can also be exploited with the spectral element method presented in Sec. $10.1$ and Fig. 10.2 for square mesh elements, as well as, with the Gordon-Hall transformation introduced in Example $10.1$ and Fig. 10.3.

Differently from the isoparametric concept, the isogeometric one essentially lets the geometrical representation of $\Omega$ to determine the basis functions for the trial solution. This choice naturally leads to incorporate the exact geometric representation of $\Omega$ in the computation of the solution of the PDE, whereas the isoparametric concepts generally generates a new computational domain $\Omega_{h}$, suitable for the "analysis", which is only an approximation of $\Omega$. Another distinguishing feature of the isogeometric concept is that the computational domain $\Omega$ is exactly represented as a whole, in general; conversely, in the isoparametric one, the geometric mapping generates a set of mesh elements $K \in \mathscr{T}_{h}$ whose union determines $\Omega_{h}$.

\section{$11.3$ NURBS-based IGA: the Galerkin method}

We briefly introduce IGA with NURBS basis functions in the framework of the Galerkin method as presented in Chapter $3$. 

\subsubsection{The Poisson problem}

The NURBS function space $\mathscr{N}_{h}$ of Eq. (11.4) is suitable for approximating the space $H^{1}(\Omega)$ according to Property $4.1$. In addition, we define

$$
\dot{\mathscr{N}}_{h}=\left\{v_{h} \in \mathscr{N}_{h}:\left.v_{h}\right|_{\partial \Omega}=0\right\}
$$

and we indicate with $N_{h}$ its dimension, i.e. $N_{h}=\operatorname{dim}\left(\mathscr{N}_{h}\right)$. For example, if $d=$ $1, \mathscr{\mathscr { N }}_{h}=\operatorname{span}\left\{R_{i}(x), \quad i=2, \ldots, n-1\right\}$ by observing that univariate NURBS basis functions are interpolatory at the images of the knot values $\xi_{1}$ and $\xi_{n+p+1}$. More in general, for $d=1,2$ or 3, by assuming a suitable reordering of the $n$ NURBS basis functions such that $R_{i}(\mathbf{x})=0$ for all $\mathbf{x} \in \partial \Omega$ with index $i=N_{h}+1, \ldots, n$, we can write $\mathscr{N}_{h}=\operatorname{span}\left\{R_{i}(\mathbf{x}), \quad i=1, \ldots, N_{h}\right\}$. Again, the NURBS space $\dot{\mathscr{N}}_{h}$ is suitable for approximating the space $H_{0}^{1}(\Omega)$.

Let us consider the Poisson problem (3.1) defined in the computational domain $\Omega \subset \mathbb{R}^{d}$, for $d=1,2,3$, and endowed with the Dirichlet boundary conditions (3.2) on $\partial \Omega$ in the homogeneous case, i.e. $g=0$ on $\partial \Omega \subset \mathbb{R}^{d-1}$. In this respect, we assume that the computational domain $\Omega$ is exactly represented by the NURBS basis functions of the space $\mathscr{N}_{h}$ through the geometric mapping $\phi(\xi)$ of Eq. (11.2); as a matter of fact, according to the isogeometric concept, the NURBS space $\mathscr{N}_{h}$ - together with the corresponding control points $\left\{\mathbf{P}_{i}\right\}_{i=1}^{n} \in \mathbb{R}^{d}-$ is specifically built to exactly represent $\Omega$ and typically enriched through $h p k$-refinements preserving its geometric representation. Then, by referring to the weak formulation (3.18), for which the solution $u \in H_{0}^{1}(\Omega)$, and by setting $V_{h}=\mathscr{\mathscr { N }}_{h}$, we obtain the NURBS-based IGA approximation of the problem in the framework of the Galerkin method

$$
\text { find } u_{h} \in V_{h}: \int_{\Omega} \nabla u_{h} \cdot \nabla v_{h} d \Omega=\int_{\Omega} f v_{h} d \Omega \quad \forall v_{h} \in V_{h} .
$$

We remark that the approximate solution $u_{h}(11.7)$ of the previous problem is rewritten as

$$
u_{h}(\mathbf{x})=\sum_{i=1}^{N_{h}} U_{i} R_{i}(\mathbf{x})
$$

in virtue of the reordering of the NURBS basis functions previously introduced.

Remark 11.1. The NURBS basis $\left\{R_{i}\right\}_{i=1}^{n}$ is a modal basis as opposed to the finite element Lagrangian basis that is instead nodal. In other words, if we assume that a control point $\mathbf{P}_{i}$ lays in $\bar{\Omega}$, the value taken by the approximate solution $u_{h}$ in such control point does not coincide in general with the corresponding control variable $U_{i}$, i.e. $u_{h}\left(\mathbf{P}_{i}\right) \neq U_{i}$. Moreover, some of the control points used to build the computational domain may even lay outside $\bar{\Omega}$, as seen e.g. in Fig. 11.6. In those cases, the approximate solution at these points is not defined.

Let us now consider the Poisson problem (3.1) with non-homogeneous Dirichlet boundary conditions $(3.2)$ on $\partial \Omega$ for which $g \in L^{2}(\partial \Omega)$ is a non-zero function. Again, we resort to the homogeneous case through a lifting of the boundary datum $g$ and its discrete counterpart. Let us denote with $\mathscr{N}_{h}^{\partial \Omega}$ the space formed by the trace functions on $\partial \Omega$ of functions of $\mathscr{N}_{h}$, which incidentally is still a NURBS space. As we consider NURBS basis functions built from open knot vectors, we write the approximation of $g$ on $\partial \Omega$ as

$$
g_{h}(\mathbf{x})=\sum_{i=N_{h}+1}^{n} G_{i} R_{i}(\mathbf{x}) \quad \forall \mathbf{x} \in \partial \Omega
$$

where the non-zero basis functions at the boundary $\partial \Omega$ are reordered to carry indexes $i=N_{h}+1, \ldots, n$ and $\left\{G_{i}\right\}_{i=N_{h}+1}^{n}$ is the set of corresponding control variables. As the NURBS basis functions of $\mathscr{N}_{h} \partial \Omega$ are not interpolatory, the control variables $\left\{G_{i}\right\}_{i=N_{h}+1}^{n}$ can not be determined in general through interpolation of the datum $g$ on $\partial \Omega$. However, a non-rigorous, but efficient approach consists in "interpolating" the datum at the control points, i.e. in setting $G_{i} \approx g\left(\mathbf{P}_{i}\right)$, if $\mathbf{P}_{i} \in \partial \Omega$, for $i=N_{h}+$ $1, \ldots, n$ [CHB09]. Alternatively, the approximate datum $g_{h}$ can be built by means of the $L^{2}(\partial \Omega)$ projection of $g$ onto the NURBS space $\mathscr{N}_{h}^{\partial \Omega}$. Once the approximate datum $g_{h}$ is built, its lifting $R_{g_{h}} \in \mathscr{N}_{h}$ is constructed as

$$
R_{g_{h}}(\mathbf{x})=\sum_{i=N_{h}+1}^{n} G_{i} R_{i}(\mathbf{x}) \quad \forall \mathbf{x} \in \Omega
$$

By setting $V_{h}=\dot{\mathscr{N}}_{h}$, the NURBS-based IGA approximation of the problem reads

$$
\text { find } \stackrel{\circ}{u}_{h} \in V_{h}: \int_{\Omega} \nabla \stackrel{\circ}{u}_{h} \cdot \nabla v_{h} d \Omega=\int_{\Omega} f v_{h} d \Omega \quad \forall v_{h} \in V_{h}
$$

with the approximate solution $u_{h}$ recovered as $u_{h}=\circ{u}_{h}+R_{g_{h}}$

We observe that Neumann, Robin, and in general natural boundary conditions (see Secs. $3.3$ and $3.4$ ) are embedded in the weak formulation of the problem and are treated in the same manner as other Galerkin methods.

Remark 11.2. For computational purposes, problems approximated by IGA are recast in the parametric domain $\Omega$ by means of the so-called "pull-back" operation thanks to the invertibility of the NURBS geometric mapping $\phi(\xi)$ of Eq. (11.2) [CHB09]; see also Fig. 11.6. For the Poisson problem (11.6), by setting $\widehat{V}_{h}=\widehat{\mathscr{N}_{h}}$ from Eq. (11.3) with obvious choice of notation, we obtain

$$
\text { find } \widehat{u}_{h} \in \widehat{V}_{h}: \int_{\widehat{\Omega}}\left(\widehat{J}^{-T} \nabla \widehat{u}_{h}\right) \cdot\left(\widehat{J}^{-T} \nabla \widehat{v}_{h}\right) \widehat{j} d \widehat{\Omega}=\int_{\widehat{\Omega}} \widehat{f} \widehat{v}_{h} \widehat{j} d \widehat{\Omega} \quad \forall \widehat{v}_{h} \in \widehat{V}_{h}
$$

where $\widehat{f}=f \circ \phi, \widehat{J}: \widehat{\Omega} \rightarrow \mathbb{R}^{d \times d}$ is the Jacobian of the geometric mapping, and $\widehat{j}=\operatorname{det}(\widehat{J})$ its determinant, which is positive a.e. in $\widehat{\Omega} \subset \mathbb{R}^{d}$, as in this case we assumed $\kappa=d$. 

\subsubsection{Algebraic aspects}

Let us refer for simplicity to the Poisson problem (11.6) in weak formulation with homogeneous Dirichlet boundary conditions on $\partial \Omega$. By expressing the approximate solution $u_{h}$ as in Eq. (11.7) and then fulfilling Eq. (11.6) for each test functions $v_{h}$ equal to the NURBS basis functions of $V_{h}$, we have the equivalent problem

$$
\text { find }\left\{U_{j}\right\}_{j=1}^{N_{h}}: \sum_{j=1}^{N_{h}} U_{j} \int_{\Omega} \nabla R_{j} \cdot \nabla R_{i} d \Omega=\int_{\Omega} f R_{i} d \Omega \quad \forall i=1, \ldots, N_{h}
$$

This yields a linear system in the form of Eq. (4.46), i.e.

$$
A \mathbf{u}=\mathbf{f}
$$

where the stiffness matrix $A \in \mathbb{R}^{N_{h} \times N_{h}}$ is such that

$$
A=\left[A_{i j}\right] \quad \text { with } A_{i j}=\int_{\Omega} \nabla R_{j} \cdot \nabla R_{i} d \Omega \text {, }
$$

the solution vector $\mathbf{u} \in \mathbb{R}^{N_{h}}$ contains the control variables, i.e $\mathbf{u}=\left[U_{j}\right]$, and the source $\mathbf{f} \in \mathbb{R}^{N_{h}}$ vector reads $\mathbf{f}=\left[f_{i}\right]$ with $f_{i}=\int_{\Omega} f R_{i} d \Omega$

At the computational level, the assembling of the matrix $A$ and vector $f$ is "pulledback" into the parametric domain $\Omega$, as anticipated in Remark 11.2. Moreover, such calculation is made inexactly, using suitable quadrature formulas. Typically, the GaussLegendre quadrature rules [QSS07] are applied element by element on the mesh with the rule-of-thumb of using $p+1$ quadrature nodes per parametric direction. Nevertheless, more efficient quadrature rules, tailored for NURBS basis functions and exploiting their properties of high order continuity across mesh edges, have been proposed e.g. in $\left[\mathrm{ACH}^{+} 12, \mathrm{HRS} 10\right]$. We remark however that, albeit the NURBS geometric representation of the computational domain $\Omega$ is exact, the assembly of $A$ and $f$ is not in general; indeed, quadrature rules as the Gauss-Legendre one allow the exact evaluation of the integrals involved in the weak formulation of the problem see e.g. Eq. (11.10) - only for B-splines, which are piecewise polynomials, but not for NURBS, which are instead rational functions. Still, quadrature errors do not harm in most of the cases the overall accuracy of NURBS-based IGA. Indeed, they are generally negligible or decay at very high rates, for example when using $p+1$ GaussLegendre quadrature nodes per parametric direction (this is again a rule-of-thumb).

The properties of the stiffness matrix $A$ (11.10) directly follow from those of the NURBS basis functions used for its construction - as discussed in Sec. $11.1$ - and the nature of the differential problem to be approximated. In particular, univariate NURBS basis functions are sequentially ordered in $\mathbb{R}$; similarly, bivariate and trivariate NURBS basis functions correspondingly possess an ordered structure in virtue of their construction based on the tensor product rule of univariate basis functions. This infers that the NURBS mesh is structured and all the connectivity structures (as the so-called ID array) are simultaneously determined when the NURBS basis is built. It follows that the matrix $A$ is banded. In general, the matrix $A$ is also sparse; indeed, 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-284.jpg?height=298&width=281&top_left_y=131&top_left_x=103)

$1 D, C^{1}-$ continuous, $n=7$
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-284.jpg?height=640&width=694&top_left_y=130&top_left_x=100)

$2 D, C^{1}-$ continuous, $n=49$

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-284.jpg?height=323&width=289&top_left_y=467&top_left_x=510)

Fig. 11.12. Examples of stiffness matrices for NURBS basis functions of degree $p=2$ in $1 D$, $2 D$, and $3 D$ with $n_{e l}=5$ along each parametric direction

by recalling the notation of Sec. 11.1.1, this occurs if $n_{\alpha} \gg p_{\alpha}$ for $\alpha=1, \ldots, \kappa$. The maximum number of non-zero entries in each row of $A$ is

$$
\prod_{\alpha=1}^{\kappa}\left(2 p_{\alpha}+1\right)
$$

where $p_{\alpha}$ is the polynomial degree along each parametric direction. This is independent of the continuity order of the NURBS basis functions $-$ i.e. let them be $C^{0}$ or $C^{p-1}$ - as their support only depends on the polynomial degree $p$. Hence, the maximum size of band of the stiffness matrix is the same for the isoparametric finite element method on structured meshes and for NURBS-based IGA.

We report some examples of stiffness matrices assembled by means of NURBS basis functions in Fig. 11.12.

The stiffness matrix $A=\left[a\left(R_{j}, R_{i}\right)\right]$ of Eq. (11.10) stems from the NURBS-based IGA approximation of the Poisson problem in the framework of the Galerkin method; 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-285.jpg?height=578&width=700&top_left_y=124&top_left_x=98)

Fig. 11.13. Condition number $K_{2}(A)$ of the stiffness matrix $A-$ with Dirichlet boundary conditions $-$ in the $1 D$ case, $\Omega=(0,1)$, for the spectral element method with numerical integration (SEM-NI) and for $C^{p-1}$-continuous NURBS-based IGA under $h$-refinement for different values of $p$. The number of degrees of freedom is $N_{h}=p / h+1$ for SEM and $N_{h}=1 / h+p$ for IGA with $C^{p-1} \mathrm{~B}$-splines

hence, it is positive definite as the bilinear form $a(\cdot, \cdot)$ is coercive. In addition, $A$ is symmetric since the form $a(\cdot, \cdot)$ is symmetric. Moreover, its conditioning number behaves as in Eq. (4.50), $K_{2}(A) \simeq C h^{-2}$, provided that $h$ is "sufficiently" small, with the constant $C$ dependent on the polynomial degree $p$ and the order of continuity of the NURBS basis functions. We report in Fig. $11.13$ some examples to illustrate the behavior of the condition number under $h$-refinement.

Regarding the condition number $K_{2}(A)$ under $p$-refinement, this may "quickly" grow with $p$ for $C^{p-1}$-continuous NURBS basis functions. Specifically, for a stiffness matrix $A$ assembled for a $d$-dimensional problem, with $d \geq 2$, the following bound $K_{2}(A) \leq C(h) p^{2 d+2} 4^{d p}$ (which however does not seem to be sharp) was provided in [GSD14]. In particular, the condition number of the matrix $A$ associated to NURBS-based IGA does not enjoy the same properties of the spectral element method with or without numerical integration presented in Sec. 10.3.1, for which one has in- 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-286.jpg?height=278&width=694&top_left_y=130&top_left_x=100)

Fig. 11.14. Condition number $K_{2}(A)$ of the stiffness matrix $A-$ with Dirichlet boundary conditions $-$ in the $1 D$ case, $\Omega=(0,1)$, for the spectral element method with numerical integration (SEM-NI) under $h$-refinement and $C^{p-1}$-continuous NURBS-based IGA under $k$-refinement for $h=1$ and $h=1 /$ 32. The number of degrees of freedom is $N_{h}=p / h+1$ for SEM and $N_{h}=1 / h+p$ for IGA with $C^{p-1}$ B-splines

stead $K_{2}(A)=O\left(p^{4}\right)$ and $O\left(p^{3}\right)$, respectively. We report in Fig. $11.14$ an example illustrating the behavior of the condition number under $p$-refinement for a $1 D$ case.

\subsubsection{A priori error estimates}

We provide the error estimates associated to NURBS-based IGA in the framework of the Galerkin method under $h$-refinement. Specifically, we refer to the Poisson problem of Sec. 11.3.1, for which $u \in H_{0}^{1}(\Omega)$, by assuming that $f$ and $\partial \Omega$ are "sufficiently" regular.

We report, from $\left[\mathrm{BBC}^{+} 06\right]$ and [TDQ14] a result for the a priori error estimate in $H^{1}$ norm. Its proof follows analogously to its finite element counterparts of Theorems $4.3$ and 4.6; however, it makes use of an operator that is not interpolatory, but rather a suitable $L^{2}$ projector on the NURBS subspace $\mathscr{N}_{h}$.

Theorem $\mathbf{1 1 . 3}$ (A priori error estimate in $H^{1}$ norm). Let $u \in H^{r+1}(\Omega)$, with $r \geq 0$, be the solution of the Poisson problem and $u_{h} \in V_{h}$ its NURBS-based IGA approximation using the NURBS space of basis function of degree $p$ and at least $C^{k_{m i n}}$-continuous in $\Omega$, with $k_{\min } \geq 0$. Then, the following error estimate under $h$-refinement holds

$$
\left\|u-u_{h}\right\|_{H^{1}(\Omega)} \leq C h^{s}\|u\|_{H^{r+1}(\Omega)}
$$

where $s=\min \{p, r\}$ and $C$ is a positive constant independent of both the mesh size $h$ and the solution $u$.

By using the Aubin-Nitsche's argument as in Sec. 4.5.4 and Theorem $4.7$ for the finite element method, we have the following $L^{2}$-error estimate for the Poisson problem. Theorem $11.4$ (A priori error estimate in $L^{2}$ norm). Under the same assumptions of Theorem 11.3, the following error estimate under h-refinement holds

$$
\left\|u-u_{h}\right\|_{L^{2}(\Omega)} \leq C h^{s+1}\|u\|_{H^{r+1}(\Omega)}
$$

where $s=\min \{p, r\}$ and $C$ is a positive constant independent of both the mesh size $h$ and the solution $u$.

The previous results (11.11) and (11.12) show that the errors associated to the NURBS-based IGA approximation converge under $h$-refinement with orders $\beta$ or $\gamma$, respectively, which are determined only by the polynomial degree $p$ of the NURBS space $\mathscr{N}_{h}$ and the regularity of the solution $u$. The convergence orders therefore do not depend on the regularity of the NURBS basis functions of $\mathscr{N}_{h}$, provided that these are at least $C^{0}-$ continuous in $\Omega$. Moreover, the convergence rates are the same that one would obtain by approximating $u$ by means of the finite element method. Specifically, if $u \in H^{r}(\Omega)$ for a sufficiently large $r(r \geq p+1)$, then the convergence rates of the errors under $h$-refinement are $p$ and $p+1$ in norms $H^{1}$ and $L^{2}$, respectively.

Results for a priori error estimates under $p$ - and $k$-refinements are presented in [BBRS11]. Here, we limit ourselves to recall that the errors associated to NURBSbased IGA behave as for the $h p$-version of the finite element method, i.e. as the spectral element method of Chapter 10. Specifically, a priori error estimates under $p$ refinement are characterized in a similar fashion of Theorem 10.1; in this respect, if the exact solution $u$ of the PDE is analytical, then the order of convergence of the errors associated to NURBS-based IGA is more than algebraic, i.e. it can be interpreted as being exponential (Sec. 10.1).

\subsubsection{A numerical example: the Poisson problem}

We consider hereby the solution of a simple Poisson problem in order to highlight some of the features and properties of NURBS-based IGA.
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-287.jpg?height=296&width=640&top_left_y=907&top_left_x=113)

Fig. 11.15. Poisson problem: computational domain $\Omega$ (left) and exact solution $u$ (right) 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-288.jpg?height=240&width=714&top_left_y=121&top_left_x=98)

Fig. 11.16. Poisson problem: sequence of $h$-refined meshes of $\Omega$

Let us start by considering the NURBS geometry in Fig. 11.15, which it is exactly represented by means of NURBS basis functions of degree $p=2$ (here we use basis functions with the same polynomial degree $p$ along the two parametric directions $\alpha=$ 1,2 ). Such NURBS geometry defines the computational domain $\Omega=(1,2) \times(0, \pi / 2)$ in polar coordinates $(r, \theta)$.

We consider now the Poisson problem (3.13) with homogeneous Dirichlet boundary conditions. The source term $f$ is chosen such that the exact solution of the Poisson problem in $\Omega$ is $u=\sin \left(\pi / 3\left(r^{2}-1\right)\right) \sin (4 \theta)$; the polar coordinates are used with the sole purpose of representing it in a compact fashion. The exact solution $u$ is depicted in Fig. $11.15$.

We solve the Poisson problem by means of NURBS-based IGA using NURBS basis functions of degrees $p=2$ and 3 which are $C^{p-1}$ and $C^{0}-$ continuous in $\Omega$. Specifically, we evaluate the $H^{1}$ and $L^{2}$-errors under $h$-refinement, i.e. over sequences of successively refined meshes, as in Fig. 11.16, which lead to sequences of enriched, but nested NURBS function spaces $\mathscr{N}_{h}$ as described in Sec. 11.1.4. We therefore consider NURBS basis functions as those depicted in the univariate case in Fig. 11.1. In Fig. $11.17$ we report the $H^{1}$ and $L^{2}$-errors under $h$-refinement for which we highlight the convergence rates expected from the corresponding a priori error estimates of Theorems $11.3$ and $11.4$. We remark in particular that the convergence rates of the $H^{1}$ and $L^{2}$-errors are $p$ and $p+1$, respectively; this result is independent of the order of continuity of the NURBS basis functions, let them be either $C^{p-1}$ or $C^{0}-$ continuous. However, the number $N_{h}$ of basis functions involved in the computation is much larger for the $C^{0}$ NURBS than for their $C^{p-1}$ counterpart, for a given number of mesh elements (and hence $h$ ). High order continuous NURBS $\left(C^{p-1}\right)$ require therefore a smaller number of degrees of freedom $N_{h}$ to achieve the same levels of errors of the $C^{0}$ basis functions.

\subsubsection{Eigenvalue analysis}

We consider the solution of an eigenvalue problem as a further test of comparison between the approximation properties of NURBS-based IGA, the spectral element method with numerical integration, and the standard finite element method. Let us 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-289.jpg?height=630&width=738&top_left_y=129&top_left_x=100)

Fig. 11.17. Poisson problem: behavior of the $H^{1}$ and $L^{2}$-errors under $h$-refinement for NURBS basis functions of degrees $p=2$ and 3 which are $C^{p-1}$ and $C^{0}$-continuous; the errors are plotted both against the characteristic mesh size $h$ and the number $N_{h}$ of degrees of freedom

introduce the $1 \mathrm{D}$ eigenvalue problem in $\Omega=(0,1)$

$$
\text { find } \omega_{n} \in \mathbb{R} \text { and } u_{n} \in V: a\left(u_{n}, v\right)=\omega_{n}^{2} m\left(u_{n}, v\right) \quad \forall v \in V,
$$

where $a(w, v)=\int_{0}^{1} w^{\prime} v^{\prime} d x, m(w, v)=\int_{0}^{1} w v d x$, and $V=H^{1}(0,1)$, which admits an infinite number of eigenvalues $\omega_{n}^{2}$, with $\omega_{n}=n \pi$ for $n=0,1,2 \ldots \ldots$ By setting the finite dimensional space $V_{h}=\mathscr{A}_{h}$, the NURBS space of Eq. (11.4) with dimension $N_{h}$, we obtain the NURBS-based IGA approximation of the eigenvalue problem

$$
\text { find } \omega_{n, h} \in \mathbb{R} \text { and } u_{n, h} \in V_{h}: a\left(u_{n, h}, v_{h}\right)=\omega_{n, h}^{2} m\left(u_{n, h}, v_{h}\right) \quad \forall v \in V_{h}
$$

The approximate eigenvalues are $\left\{\omega_{n, h}^{2}\right\}_{n=0}^{N_{h}}$, which we assume to be ordered (increasingly) in $\mathbb{R}$. As our NURBS-based IGA formulation is a Galerkin method $-$ either with exact integration for NURBS or with a rule-of-thumb Gauss-Legendre quadrature for B-splines $-$ one obtains for this symmetric eigenvalue problem ([Cia78, Hug00]) that

$$
\omega_{n} \leq \omega_{n, h} \quad \forall n=0,1, \ldots, N_{h}
$$



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-290.jpg?height=261&width=330&top_left_y=135&top_left_x=102)

SEM-NI

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-290.jpg?height=281&width=329&top_left_y=427&top_left_x=103)

$C^{p-1}$-continuous B-splines, IC

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-290.jpg?height=262&width=330&top_left_y=135&top_left_x=461)

FEM

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-290.jpg?height=286&width=331&top_left_y=428&top_left_x=460)

Fig. 11.18. Normalized spectra for the spectral element method with numerical integration (SEM-NI), the finite element method (FEM), and both $C^{p-1}$ - and $C^{0}$-continuous NURBSbased IGA (with B-splines) for different values of $p$. The number of degrees of freedom is $N_{h}=p n_{e l}+1$ for SEM, FEM, and $C^{0}$ NURBS-based IGA, while $N_{h}=n_{e l}+p$ for $C^{p-1}$ IGA; we vary $n_{e l}-$ yielding $h=1 / n_{e l}$ for $\Omega=(0,1)-$ such that $N_{h} \simeq 1024$, for all the methods under consideration, for $p=1, \ldots, 5$

This property is not guaranteed for methods employing non-exact quadrature formulas or methods not enjoying the properties of the Galerkin framework, as for the spectral element method with numerical integration of Sec. 10.3; indeed, the latter is no longer a Galerkin approximation method when the bilinear forms $a(\cdot, \cdot)$ and $m(\cdot, \cdot)$ are evaluated in an approximate fashion.

We report in Fig. $11.18$ the normalized spectra, i.e. the plots of the ratios $\frac{\omega_{n, h}}{\omega_{n}}$ vs. $\frac{n}{N_{h}}$ for $n=1, \ldots, N_{h}$, where $N_{h}$ is the dimension of the finite dimensional space $V_{h}$. We compare the spectra obtained by means of NURBS-based IGA with both $C^{p-1}$ and $C^{0}$ B-splines basis functions of degree $p=1, \ldots, 5$, as well as through the corresponding approximations based on the spectral element method with numerical integration and the standard finite element method. Albeit normalized, our comparison comprises about the same number of degrees of freedom $N_{h}$ for all the approximation methods, which is obtained by suitably changing the number of mesh elements $n_{e l}$ for each values of the polynomial degree $p$ such that $N_{h} \simeq 1024$. As we can observe, IGA using $C^{p-1}$-continuous B-splines provides by far the best approximation of the whole spectrum, with accuracy improving by elevating the degree $p$ of the basis functions. Conversely, the spectral element method with numerical integration, the standard finite element method, and IGA with $C^{0}$-continuous B-splines exhibit approximations worsening in the largest part of the spectrum as the polynomial degree $p$ increases.

\section{$11.4$ Current developments and perspectives}

IGA is nowadays a very active research field both in terms of applications and methodological developments. For the latter, we limit ourselves to mention the exploitation of the properties of high order continuity of the B-splines and NURBS basis functions in the framework of the Galerkin method ([CHR07, EBBH09]), which are particularly suited for high order PDEs [BDQ15, TDQ14], PDEs involving sharp but smooth interfaces as phase field models $\left[\mathrm{GCBH} 08, \mathrm{GHNC} 10, \mathrm{GN} 12, \mathrm{LDE}^{+} 13, \mathrm{LH} 16\right]$, and problems of linear wave propagation (e.g., elastodynamics equations) for which controlling numerical dissipation and dispersion is crucial for accurate simulations [CHR07, CRBH06, DJQ15, HER14, HRS08].

The "exactness" of the geometric representation guaranteed by NURBS is particularly suited for problems in computational fluid dynamics involving boundary layers [BCZH06, HAB 11], mechanical problems undergoing large deformations [BBHH11, $\left.\mathrm{LEB}^{+} 10\right]$, contact problems in structural analysis [LWH14], beams and shell analyses [BBHH10, DS11, KBH $^{+} 10$, LZZ13], and in general surface and geometric PDEs [BDQ15, BDQ16, DQ15]. Shape optimization is another notable example in which the NURBS geometric representation is easily exploited [KSWB14, NAG10, NG13, Qia10, WFC08]. Indeed, NURBS control point and weights provide a direct access to control the shape of the computational domain wherein the PDEs are defined; we refer the reader to Chapter 18 for an overview of optimization problems.

Computational efficiency and accuracy in IGA approximations are also pursued through more efficient quadrature formulas for B-splines and NURBS basis functions to be used in the framework of the Galerkin method $\left[\mathrm{ACH}^{+} 12, \mathrm{HCSH} 17, \mathrm{HRS} 10,\right.$, SHH14]. In this respect, collocation techniques for the approximation of PDEs are also seeing a remarkable development $\left[\mathrm{ABH}^{+} 10\right.$, BLR12, GL16, MST17, RH15, SER $^{+} 13$ ].

Other than NURBS, more general geometric representations have been considered and are currently under development. Among these, significant efforts are dedicated to use and fit geometric representations based on T-splines [SZBNO3] - largely used in the computer graphics industry $-$ into the isogeometric concept, due to their flexibility with respect to NURBS basis functions $\left[\mathrm{BCC}^{+} 10, \mathrm{BCS} 10\right]$. Adaptive IGA [DJS10, KVvdZvB14, SDS $^{+} 12$, VGJS11] is also a very active research field and involves, other than hierarchical refinements, the development of hierarchical, truncated hierarchical, and locally refined B-splines [BC13, GJS12, JKD14] as well as hierarchical and modified T-splines [ESLT15, SLSH12, WZLH17]. 

\section{Discontinuous element methods (DG and mortar)}

Up to now we have considered Galerkin methods with subspaces of continuous polynomial functions, either within the finite element method (Chapter 3 ) or the spectral element method (Chapter 10). This chapter deals with approximation techniques based on subspaces of polynomials that are discontinuous between elements. We will, in particular, introduce the so-called Discontinuous Galerkin method (DG) and the mortar method. We will carry out this for the Poisson problem first, and then generalize to the case of diffusion and transport problems (see Chapter 13). To maintain the presentation general we will consider a partition of the computational domain into disjoint subdomains that may be either finite or spectral elements.

\subsection{The discontinuous Galerkin method (DG) for the Poisson problem}

Let us consider the Poisson problem together with homogeneous Dirichlet boundary conditions (3.13) in a domain $\Omega \subset \mathbb{R}^{2}$ divided in the union of $M$ disjoint elements $\Omega_{m}$, $m=1, \ldots, M$. We wish to attain an alternative weak formulation to the usual one, that will serve as starting point for the DG method. To simplify the discussion we assume the exact solution to be sufficiently regular, for instance $u \in H_{0}^{1}(\Omega) \cap H^{2}(\Omega)$, so that all operations below make sense. Define the space

$$
W^{0}=\left\{v \in W:\left.v\right|_{\partial \Omega}=0\right\}
$$

where

$$
W=\left\{v \in L^{2}(\Omega):\left.v\right|_{\Omega_{m}} \in H^{1}\left(\Omega_{m}\right), m=1, \ldots, M\right\}
$$

By Green's formula we have, for every $v \in W^{0}$,

$$
\sum_{m=1}^{M}(-\Delta u, v)_{\Omega_{m}}=\sum_{m=1}^{M}\left((\nabla u, \nabla v)_{\Omega_{m}}-\int_{\partial \Omega_{m}} v \nabla u \cdot \mathbf{n}_{m}\right)
$$

where $\mathbf{n}_{m}$ is the outward unit normal to $\partial \Omega_{m}$ and $(\cdot, \cdot)_{\Omega_{m}}$ denotes the scalar product of $L^{2}\left(\Omega_{m}\right)$. 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-293.jpg?height=252&width=358&top_left_y=113&top_left_x=276)

Fig. 12.1. An "edge" e separating two neighbouring subdomains (or elements)

Calling $\mathscr{E}_{\delta}$ the union of all internal edges, i.e. the interfaces separating the subdomains (outermost edges may be neglected as $v$ vanishes on them), we can rearrange terms to obtain

$$
-\sum_{m=1}^{M} \int_{\partial \Omega_{m}} v \nabla u \cdot \mathbf{n}_{m}=-\left.\sum_{e \in \mathscr{E}_{\delta}} \int_{e}\left(v^{+} \nabla u^{+} \cdot \mathbf{n}^{+}+v^{-} \nabla u^{-} \cdot \mathbf{n}^{-}\right)\right|_{e}
$$

in which the signs "' $+$ " and " $-$ " label the information according to the two possible normal orientations (see, e.g., Fig. 12.1).

We will use the following notation to denote mean values and jumps on elements' edges:

$$
\begin{gathered}
\{v\}=\frac{v^{+}+v^{-}}{2}, \quad[v]=v^{+} \mathbf{n}^{+}+v^{-} \mathbf{n}^{-} \\
\mathfrak{\{} \nabla w\}=\frac{(\nabla w)^{+}+(\nabla w)^{-}}{2}, \quad \llbracket \nabla w \rrbracket=(\nabla w)^{+} \cdot \mathbf{n}^{+}+(\nabla w)^{-} \cdot \mathbf{n}^{-}
\end{gathered}
$$

Notice how the above convention guarantees that the definition of the jump operator will not depend on how subdomains (elements) are numbered. A little algebraic manipulation eventually gives

$$
\begin{aligned}
v^{+} \nabla u^{+} \cdot \mathbf{n}^{+}+v^{-} \nabla u^{-} \cdot \mathbf{n}^{-}=& 2[v] \cdot\{\nabla u\}-\left(v^{+} \nabla u^{-} \cdot \mathbf{n}^{+}+v^{-} \nabla u^{+} \cdot \mathbf{n}^{-}\right) \\
=& 2[v] \cdot\{\nabla u\}+2 \llbracket \nabla u \rrbracket\{v\} \\
&-\left(v^{+} \nabla u^{+} \cdot \mathbf{n}^{+}+v^{-} \nabla u^{-} \cdot \mathbf{n}^{-}\right)
\end{aligned}
$$

and so

$$
v^{+} \nabla u^{+} \cdot \mathbf{n}^{+}+v^{-} \nabla u^{-} \cdot \mathbf{n}^{-}=[v] \cdot\{\{\nabla u\}+\llbracket \nabla u \rrbracket\{v\} .
$$

Using (12.3) and (12.4), from (12.2) we obtain that the solution to the Poisson problem (3.13) satisfies: $u \in W^{0}$ s.t.

$$
\sum_{m=1}^{M}(\nabla u, \nabla v)_{\Omega_{m}}-\sum_{e \in \mathscr{E}_{\delta}} \int_{e}([v] \cdot\{\mathfrak{u} \nabla u\}+\llbracket \nabla u \rrbracket\{v\})=\sum_{m=1}^{M}(f, v)_{\Omega_{m}} \quad \forall v \in W^{0}
$$

Now we introduce the discrete space

$$
W_{\delta}=\left\{v_{\delta} \in W:\left.v_{\delta}\right|_{\Omega_{m}} \in \mathbf{P}_{r}\left(\Omega_{m}\right), m=1, \ldots, M\right\}
$$

$\mathbf{P}_{r}\left(\Omega_{m}\right)$ being a space of "polynomials" on $\Omega_{m}$. More precisely, $\mathbf{P}_{r}\left(\Omega_{m}\right)=\mathbb{P}_{r}$ if $\Omega_{m}$ is a simplex (2D triangle or 3 D tetrahedron), while $\mathbf{P}_{r}\left(\Omega_{m}\right)=\mathbb{Q}_{r} \circ F_{m}\left(\Omega_{m}\right)$ if $\Omega_{m}$ is a spectral element (a quadrilateral in $2 \mathrm{D}$, a parallelepiped in $3 \mathrm{D}$, cf. Ch. 10). Here $F_{m}$ is the map that transforms $\Omega_{m}$ into the unit cube $\Omega=[-1,1]^{d}(d=2,3)$. At last, let $W_{\delta}^{0}$ be the following subspace of $W_{\delta}$

$$
W_{\delta}^{0}=\left\{v_{\delta} \in W_{\delta}:\left.v_{\delta}\right|_{\delta \Omega}=0\right\}
$$

Note that the term $\llbracket \nabla u \rrbracket\{v\}$ in (12.3) is null because if $u \in H_{0}^{1}(\Omega) \cap H^{2}(\Omega)$ then $\llbracket \nabla u \rrbracket=0$ on every edge $e \in \mathscr{E}_{\delta}$. This fact together with expression $(12.1)$ motivates the following DG approximation for problem (3.13): find $u_{\delta} \in W_{\delta}^{0}$ satisfying

$$
\begin{aligned}
\sum_{m=1}^{M}\left(\nabla u_{\delta}, \nabla v_{\delta}\right)_{\Omega_{m}}-&\left.\sum_{e \in \mathscr{E}_{\delta}} \int_{e}\left[v_{\delta}\right] \cdot\left\{\nabla u_{\delta}\right\}-\tau \sum_{e \in \mathscr{E}_{\delta}} \int_{e}\left[u_{\delta}\right] \cdot \mathfrak{\{} \nabla v_{\delta}\right\} \\
&+\sum_{e \in \mathscr{E}_{\delta}} \gamma|e|^{-1} \int_{e}\left[u_{\delta}\right] \cdot\left[v_{\delta}\right]=\sum_{m=1}^{M}\left(f, v_{\delta}\right)_{\Omega_{m}} \quad \forall v_{\delta} \in W_{\delta}^{0}
\end{aligned}
$$

where $\gamma=\gamma(r)$ is a suitable positive constant (depending on the local polynomial degree), $|e|$ is the length of $e \in \mathscr{E}_{\delta}$ and $\tau$ is a suitable fixed number. The additional new terms $\tau\left[u_{\delta}\right] \cdot\left\{\left\{\nabla v_{\delta}\right\}\right.$ and $\gamma|e|^{-1}\left[u_{\delta}\right] \cdot\left[v_{\delta}\right]$ do not undermine strong consistency (since $[u]=0$ if $u$ is the exact Poisson solution), beside warranting greater generality and improved stability features.

Formulation (12.5), introduced at the end of the $70 \mathrm{~s}$, is called Interior Penalty (IP) ([Whe78, Arn82]). In case $\tau=1$, the method preserves the symmetry and the resulting formulation is known as SIPG method (Symmetric Interior Penalty Galerkin) [Whe78, Arn82]. For $\tau \neq 1$ the bilinear form is no longer symmetric, and the special values $\tau=-1$ and $\tau=0$ respectively lead to the NIPG method $($ Non-symmetric Interior Penalty Galerkin) [RWG99] and the IIPG method (Incomplete Interior Penalty Galerkin) [DSW04]. Whereas the former is stable for any given $\gamma>0$, SIPG and IIPG require, in order to reach a stable formulation, a sufficiently large penalty parameter $\gamma$

Several variants of formulation (12.5) have been proposed within the context of approximations by finite elements. Here we will only briefly describe the most classical situations, and refer to the article [ABCM02] of Arnold, Brezzi, Cockburn, Marini both for a general overview and a detailed study of stability and convergence.

A first version consists in replacing the last term on the left side of $(12.5)$ with the following stabilization term

$$
\sum_{e \in \mathscr{E}_{\delta}} \gamma \int_{e} r_{e}\left(\left[u_{\delta}\right]\right) \cdot r_{e}\left(\left[v_{\delta}\right]\right)
$$

Above $r_{e}(\cdot)$ is a suitable extension operator that, from the jump of a function $\left[v_{\delta}\right]$ across $e \in \mathscr{E}_{\delta}$, generates a function $r_{e}\left(\left[v_{\delta}\right]\right)$ with non-zero support on the elements having $e$ as edge. See [BRM $^{+}$97] and [ABCM02] for full details.

A second variant (cf. [Ste98]) replaces the averages $\{\nabla w\}$ in $(12.5)$ by the averages with relaxation

$$
\{\nabla w\}_{\theta}=\theta \nabla w^{+}+(1-\theta) \nabla w^{-}, \quad 0 \leq \theta \leq 1
$$

Up to this point we have imposed the homogeneous Dirichlet condition "strongly". In order to add the boundary constraints, say $u=g$ on $\partial \Omega$, in weak form ("à la Nitsche" [Nit71]), as is more natural for DG-like approximations, we write the discrete formulation (12.5) in $W_{\delta}$ rather than in $W_{\delta}^{0}$, and add on the left side the following contributions to the boundary edges $e \subseteq \partial \Omega$

$$
\begin{aligned}
-\sum_{e \subseteq \partial \Omega} \int_{e} v_{\delta} \nabla u_{\delta} \cdot \mathbf{n}-\tau \sum_{e \subseteq \partial \Omega} \int_{e}\left(u_{\delta}-g_{\delta}\right) \nabla v_{\delta} \cdot \mathbf{n} \\
&+\sum_{e \subseteq \partial \Omega} \gamma|e|^{-1} \int_{e}\left(u_{\delta}-g_{\delta}\right) v_{\delta}, \quad u_{\delta}, v_{\delta} \in W_{\delta}
\end{aligned}
$$

The positive constant $\gamma=\gamma(r)$ is the same of $(12.5)$, and $g_{\delta}$ is a convenient approximation of $g$. The first term, arising naturally from integration by parts, ensures the method is strongly consistent, while the second term makes the formulation symmetric if $\tau=1$ and non-symmetric if $\tau=-1,0$. The last term penalizes the trace of the discrete solution $u_{\delta}$ and makes it "approach" the Dirichlet datum. Observe once again that these terms do not affect the method's strong consistency.

The DG formulation with boundary conditions imposed weakly thus becomes: find $u_{\delta} \in W_{\delta}$ such that

$$
\begin{aligned}
&\sum_{m=1}^{M}\left(\nabla u_{\delta}, \nabla v_{\delta}\right)_{\Omega_{m}}-\sum_{e \in \mathscr{E}_{\delta}} \int_{e}\left[v_{\delta}\right] \cdot\left\{\left\{\nabla u_{\delta}\right\}-\tau \sum_{e \in \mathscr{E}_{\delta}} \int_{e}\left[u_{\delta}\right] \cdot\left\{\left\{\nabla v_{\delta}\right\}+\sum_{e \in \mathscr{E}_{\delta}} \gamma|e|^{-1} \int_{e}\left[u_{\delta}\right] \cdot\left[v_{\delta}\right]\right.\right. \\
&-\sum_{e \subseteq \partial \Omega} \int_{e} v_{\delta} \nabla u_{\delta} \cdot \mathbf{n}-\tau \sum_{e \subseteq \partial \Omega} \int_{e} u_{\delta} \nabla v_{\delta} \cdot \mathbf{n}+\sum_{e \subseteq \partial \Omega} \gamma|e|^{-1} \int_{e} u_{\delta} v_{\delta} \\
&=\sum_{m=1}^{M}\left(f, v_{\delta}\right)_{\Omega_{m}}-\tau \sum_{e \subseteq \partial \Omega} \int_{e} g_{\delta} \nabla v_{\delta} \cdot n+\sum_{e \subseteq \partial \Omega} \gamma|e|^{-1} \int_{e} g_{\delta} v_{\delta} \quad \forall v_{\delta} \in W_{\delta}
\end{aligned}
$$

We shall refer to the latter formulation as the DG-N method (N for Nitsche). Clearly, if the Dirichlet datum $g$ is zero the last two terms on the right will not show up.

Concerning the accuracy of method $(12.7)$ for discretizing the Poisson problem (3.13) with homogeneous Dirichlet boundary conditions, let us introduce the so-called energy norm

$$
\left\|u_{\delta}\right\|=\left(\sum_{m=1}^{M} \int_{\Omega_{m}}\left|\nabla u_{\delta}\right|^{2}+\sum_{e \in \mathscr{E}_{\delta}} \gamma|e|^{-1} \int_{e}\left[u_{\delta}\right]^{2}+\sum_{e \subseteq \partial \Omega} \gamma|e|^{-1} \int_{e}\left|u_{\delta}\right|^{2}\right)^{1 / 2}
$$

For formulation (12.5), where boundary conditions are imposed strongly, the last term is missing. It can be proved that if the exact solution is sufficiently regular, the SIPG method $(\tau=1)$ converges with optimal convergence rate both for the $L^{2}(\Omega)$ norm and for (12.8), as long as the penalty parameter $\gamma$ is large enough. Better said, for finite elements of degree $r$ one has

$$
h\left\|u-u_{\delta}\right\|+\left\|u-u_{\delta}\right\|_{L^{2}(\Omega)} \leq C h^{r+1}|u|_{H^{r+1}(\Omega)}
$$

where $C$ is an appropriate positive number that depends on $r$ (for a proof see [ABCM02], for example). As always, $r$ is the polynomial degree employed on each element $\Omega_{m}$. For the non-symmetric methods NIPG and IIPG, as these schemes are not strongly consistent on the adjoint problem, one cannot get optimal $L^{2}$ estimates. In many cases, nevertheless, both methods exhibit optimal rates of convergence when the degree of the approximation is odd and grids are sufficiently regular (see e.g. [OBB98]).

For all variants of the DG-N method we have seen one can prove that if $u \in$ $H^{s+1}(\Omega), s \geq 1$, and if the polynomial degree $r$ satisfies $r \geq s$, the error can be estimated in energy norm (12.8) as follows

$$
\left\|\left.\left|u-u_{\delta} \| \leq C\left(\frac{h}{r}\right)^{s} r^{1 / 2}\right| u\right|_{H^{s+1}(\Omega)}\right.
$$

where $C$ is a suitable positive constant that does not depend on $r$. For the SIPG method $(\tau=1)$ and the IIPG method $(\tau=0)$ estimate (12.10) holds as long as the penalty parameter $\gamma$ is taken large enough. In particular, convergence in $r$ is exponential when the exact solution $u$ is analytic. Let us also remark, by comparison with the known results for spectral elements, that (12.10) is sub-optimal with respect to the approximation degree $r$, due to the presence of the factor $r^{1 / 2}$. More details can be found in [RWG99, RWG01, HSS02, PS03], for instance.

In certain special situations one can attain optimal estimates in $r$. For the twodimensional case with quadrilateral grids, for example, [GS05] provides optimal estimates in energy norm under the assumption that the solutions belongs locally to an enriched Sobolev space. Different estimates, were proved in [SW10] without extra regularity hypotheses, but under homogeneous Dirichlet boundary conditions.

We close the section by observing that sometimes formulation (12.7) is stable even without the penalty term for jumps, i.e. choosing $\gamma=0$ for the internal edges and for the external ones as well. Rivière, Wheeler and Girault [RWG99] proved that the nonsymmetric version $(\tau=-1)$, known in the literature as the Baumann-Oden method [OBB98], is stable and provides optimal estimates of the error (in energy norm) if the approximation degree $r$ satisfies $r \geq 2$. In that case one uses a special interpolation $u^{I}$ of $u$, called Morley interpolation, for which $\left\{\nabla_{h}\left(u-u^{I}\right)\right\}=0$ on each edge. In the article of Brezzi and Marini [BM06] (see also [BS08]) it was proved that the BaumannOden method (in its non-symmetric incarnation, with $\tau=-1$ ) in two dimensions with triangular grids is stable, provided we add to the space of linear polynomials a bubble function for each element. The Baumann-Oden method was shown in [ABM09] to be stable (always in 2 dimensions) when adding to linear polynomials $n-2$ bubbles 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-297.jpg?height=292&width=718&top_left_y=115&top_left_x=95)

Fig. 12.2. Study of the convergence of method (12.5) $\left(\tau=1, \gamma=10 r^{2}\right)$. Left: errors in energy norm (12.8) and $L^{2}(\Omega)$ norm $(r=1$, structured triangular grids). Right: errors in energy norm from biquadratic and bi-cubic elements on a sequence of Cartesian grids

for each element, for decompositions involving $n$-gons (polygons with $n$ edges). At last, we mention that Burman et al. [BEMS07] proved the 1-dimensional symmetric variant $(\tau=1)$ need not be stabilized if $r \geq 2$.

To learn more on DG-type methods the reader should consult, for example, [Riv08], [HW08], [ABCM02], [Woh01].

We present now some numerical results obtained by the discontinuous Galerkin method the homogeneous Dirichlet problem (3.13) on $\Omega=(0,1)^{2}$, where the forcing term $f$ is such that the exact solution reads $u\left(x_{1}, x_{2}\right)=\left(x_{1}-x_{1}^{2}\right) \exp \left(3 x_{1}\right) \sin \left(2 \pi x_{2}\right)$. We discussed method $(12.5)$ with $\tau=1$ and penalty constant $\gamma=10 r^{2}$. This choice makes sure that the SIPG method is well posed. Then the $\left\{\Omega_{m}\right\}$ are nothing but the finite elements (triangles) and $r$ is the polynomial degree on each element. In the ensuing numerical experiments $\mathscr{E}_{\delta}$ is thus the union of all inner edges in the grid. Errors were computed in $L^{2}(\Omega)$ norm and in the energy norm (12.8). Fig. $12.2$ (left) shows the (normalized) errors computed on a sequence of triangular grids made by linear elements $(r=1)$. As predicted by (12.9), the error tends to zero linearly in energy norm and quadratically in $L^{2}(\Omega)$. In Fig. $12.2$ (right) we can read the (normalized) errors computed on a sequence of Cartesian grids with biquadratic elements $(r=2)$ and bi-cubic ones $(r=3)$. The approximation error in norm $(12.8)$ tends to zero when $h \rightarrow 0$, and the convergence order equals $r$.

In the framework of spectral elements we can attain a DG-SEM formulation starting from a partition of $\Omega$ in quadrilaterals, using formulation (12.7), and replacing the volume integrals $(\cdot, \cdot)_{\Omega_{m}}$ with local GLL quadrature formulae; similarly for the integrals extended over the edges of spectral elements. 

\subsection{The mortar method}

An alternative to the DG technique is based on the so-called mortar method, originating in the framework of spectral element methods (SEM).

Let us consider again the Poisson problem (3.13) in a domain $\Omega \subset \mathbb{R}^{2}$ with homogeneous Dirichlet boundary conditions.

Define on $\Omega$ a partition into pairwise-disjoint non-empty open subregions $\Omega_{i} \subset \Omega$, $i=1, \ldots, M$ such that $\bar{\Omega}=\cup_{i=1}^{M} \bar{\Omega}_{i}$. Then let $\Gamma_{i j}=\Gamma_{j i}=\partial \Omega_{i} \cap \partial \Omega_{j}$ be the interface between $\Omega_{i}$ and $\Omega_{j}, 1 \leq i \neq j \leq M$, and define $\Gamma=\cup_{i j} \Gamma_{i j}$ to be their union (Fig. 12.3).

Solving (3.13) by a mortar method means finding a discrete solution $u_{\delta}$ that, inside every subregion $\Omega_{i}$, is continuous and polynomial (globally or locally), and that fulfills a continuity condition on the interface $\Gamma$, called weak or integral: namely, that for every $i, j$ with $1 \leq i \neq j \leq M$

$$
\int_{\Gamma_{i j}}\left(\left.u_{\delta}\right|_{\Omega_{i}}-\left.u_{\delta}\right|_{\Omega_{j}}\right) \psi=0 \quad \forall \psi \in \tilde{\Lambda}
$$

where $\tilde{\Lambda}$ is a suitable finite-dimensional space that depends on the discretization chosen on the $\Omega_{i}$. To (12.11) are then added strong continuity constraints at certain points lying on the interface $\Gamma$.

Note that equations (12.11) do not force the solution's jump to vanish on the interface, but they prescribe that its $L^{2}$ projection on $\tilde{\Lambda}$ be zero. Consequently, in contrast to what happens for an approximation of Galerkin type (see Chaps. 4 and 10 ), $u_{\delta} \notin H_{0}^{1}(\Omega)$ in general, but rather $u_{\delta} \in W($ see $(12.1))$.

To simplify the discussion, let us consider a partition of $\Omega$ in $M=2$ subregions $\Omega_{1}, \Omega_{2}$ and call $\Gamma$ the interface, so that $\Gamma=\Gamma_{12}=\Gamma_{21}$. In $\Omega_{i}, i=1,2$, we define a further partition $\mathscr{T}_{i}=\cup_{k} T_{i, k}$ in triangles or quadrilaterals $T_{i, k}$ as explained in Sect. $6.2$.

If the elements $T_{i, k}$ are quadrilaterals we also require each $T_{i, k}$ to be the image of the reference element $\widehat{T}=(-1,1)^{2}$ under a smooth bijection $\varphi_{i, k}$, see Sect. 10.1. Given polynomial interpolation degrees $N_{i} \geq 1$ in each $\Omega_{i}$, define

$$
V_{N_{i}}\left(T_{i, k}\right)=\left\{v \in C^{0}\left(\overline{T_{i, k}}\right): v \circ \varphi_{i, k} \in \mathbb{Q}_{N_{i}}(\widehat{T})\right\}
$$

where $\mathbb{Q}_{N}(\widehat{T})$ is the space of degree $N$ polynomials in every variable in the reference element $T$ (cf. definition (10.2)).
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-298.jpg?height=194&width=580&top_left_y=1004&top_left_x=166)

Fig. 12.3. Two possible partitions of the domain $\Omega$, with only one interface $\Gamma_{i j}$ drawn For a triangular partition, instead, we consider the finite-element spaces of Sect. $4.5$, and for $i=1,2$ and any $T_{i, k} \in \mathscr{T}_{i}$ set

$$
X_{\delta_{i}}\left(T_{i, k}\right)=\left\{\begin{array}{l}
\mathbb{P}_{r_{i}}\left(T_{i, k}\right) \text { for finite elements of degree } r_{i} \text { on triangles, } \\
V_{N_{i}}\left(T_{i, k}\right) \text { for spectral elements of degree } N_{i} \text { on quadrilaterals. }
\end{array}\right.
$$

The parameter $\delta_{i}$ implicitly depends on the degree $\left(r_{i}\right.$ or $\left.N_{i}\right)$ and on the maximum diameter $h$ of the elements of $\mathscr{T}_{i}$.

The finite-dimensional spaces induced by the local discretizations in $\Omega_{i}, i=1,2$, are then

$$
V_{i, \delta_{i}}=\left\{v_{\delta} \in C^{0}\left(\bar{\Omega}_{i}\right):\left.v_{\delta}\right|_{T_{i, k}} \in X_{\delta_{i}}\left(T_{i, k}\right), \forall T_{i, k} \in \mathscr{T}_{i}\right\}
$$

and the discrete solution $u_{\delta}$ must be seeked in

$$
Y_{\delta}=\left\{v_{\delta} \in L^{2}(\Omega): v_{\delta}^{(i)}=v_{\delta \mid \Omega_{i}} \in V_{i, \delta_{i}}, \text { for } i=1,2\right\}
$$

As the space $Y_{\delta}$ does not retain the information on how to match the functions $v_{\delta}^{(i)}$ on the interface, we must introduce a subspace $V_{\delta} \subset Y_{\delta}$ of functions satisfying $(12.11)$ and search for the mortar solution $u_{\delta}$ inside $V_{\delta}$.

Let us observe first that the choice of mesh and polynomial degree in one subdomain is completely independent of the choice in the other subdomain, as Figure $12.4$ explains.

On the left we have a spectral discretization in either $\Omega_{i}$, where the edges of the elements of $\bar{\Omega}_{1} \cap \Gamma$ and $\bar{\Omega}_{2} \cap \Gamma$ coincide but the degrees $N_{1}, N_{2}$ differ (hence interpolating nodes are different from quadrature nodes, too). In such cases we conventionally speak of polynomial nonconformity.

The middle Figure $12.4$ shows a discretization (by spectral elements in both spaces $\Omega_{i}$ ) with the same polynomial degree in every spectral element of $\Omega_{1}$ and $\Omega_{2}$, but now the edges of spectral elements in $\bar{\Omega}_{1} \cap \Gamma$ and $\bar{\Omega}_{2} \cap \Gamma$ do not coincide. We are then in presence of geometrical nonconformity.

At last, in Figure $12.4$, right, we have a discretization by spectral elements on $\Omega_{1}$ and triangular finite elements in $\Omega_{2}$.

Indicate with $\mathscr{M}_{i}$ the set of nodes induced by the discretization chosen in $\Omega_{i}$. In the spectral case these are images on every $T_{i, k}$ of the Gauss-Legendre-Lobatto points defined on $\hat{T}$ (see Sect. 10.2.3). The same discretizations induce two distinct sets of nodes (not necessarily disjoint) on $\Gamma$ which we indicate by $\mathscr{M}_{i}^{\Gamma}=\mathscr{M}_{i} \cap \Gamma$, and two sets $\mathscr{U}_{i}^{\Gamma}$ of degrees of freedom on $\Gamma$, whose elements are the values of the functions $u_{\delta}^{(i)}$ at the nodes of $\mathscr{M}_{i}^{\Gamma}$.

Of the two sets of degrees of freedom on $\Gamma$, one, called mortar or master set, is picked to play an active role in the problem's formulation, meaning that its degrees of freedom are primal unknowns for the problem. The other set, called non-mortar or slave, characterizes the space $\tilde{\Lambda}$ onto which the continuity condition projects. The degrees of freedom of the sets $\mathscr{U}_{1}^{\Gamma}$ and $\mathscr{U}_{2}^{\Gamma}$ will depend on each other via a linear relationship dictated by the integral conditions (12.11).

Label with $m, s \in\{1,2\}$ the master set $\mathscr{U}_{m}^{\Gamma}$ and the slave set $\mathscr{U}_{s}^{\Gamma}$ respectively, and let $\mathscr{N}_{\text {master }}, \mathscr{N}_{\text {slave }}$ be their cardinalities. The subscripts $m$ and $s$ pass on to subdomains, 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-300.jpg?height=222&width=716&top_left_y=112&top_left_x=97)

Fig. 12.4. Left: discretization by quadrilateral spectral elements with nonconformity of polynomial type; middle: geometric nonconformity by spectral elements; right: discretization by spectral elements and finite elements

polynomial degrees and all other quantities in the picture, so for instance we will have $\Omega_{m}, N_{m}$ and so on for the master domain, $\Omega_{s}, N_{s}$ for the slave domain. In our study $\mathscr{U}_{1}^{\Gamma}$ plays the master role, while $\mathscr{U}_{2}^{\Gamma}$ that of the slave.

The choice of finite or spectral elements for the discretization of the problem requires separate arguments from now on. So let us suppose to only have either spectral elements or finite elements in both $\Omega_{1}, \Omega_{2}$, for the time being; we will see in Section $12.8$ how to treat the mixed case.

\subsubsection{Characterization of the space of constraints by spectral elements}

Denote by $\mathscr{E}_{s}^{\Gamma}$ the collection of edges of spectral elements in the slave domain $\Omega_{s}$ that lie on $\Gamma$, and set

$$
\tilde{\Lambda}_{\delta}=\operatorname{span}\left\{\psi \in L^{2}(\Gamma):\left.\psi\right|_{e} \in \mathbb{P}_{N_{s}-2} \quad \forall e \in \mathscr{E}_{s}^{\Gamma}\right\}
$$

and

$$
P_{\delta}=\left\{\mathbf{p} \in \mathscr{M}_{s}^{\Gamma}: \mathbf{p} \text { is the endpoint of an edge } e \in \mathscr{E}_{s}^{\Gamma}\right\}
$$

For the definition of $\widetilde{\Lambda}_{\delta}$ to make any sense we have to take $N_{s} \geq 2$. (The case $N_{s}=1$ can be reduced to the finite element formulation of type $\mathbb{Q}_{1}$.)

We want to characterize the space $\widetilde{\Lambda}_{\delta}$ in terms of a basis whose $L^{2}(\Gamma)$-functions have support on one edge only $e \in \mathscr{E}_{s}^{\Gamma}$, and that on this edge coincide with Lagrange's characteristic polynomials of degree $N_{s}-2$ associated to the $N_{s}-1$ Gauss-Legendre quadrature nodes on $e$ (see [CHQZ06, formula (2.3.10)]). Figure 12.5, left, shows a function $\psi_{l}$ of the basis of $\widetilde{\Lambda}_{\delta}$ supported on the second edge of $\mathscr{E}_{s}^{\Gamma}$ and associated to the first Gauss-Legendre node in $e$.

It is straightforward to see that the dimension of $\widetilde{\Lambda}_{\delta}$ equals $\left(N_{s}-1\right)$ times the cardinality of $\mathscr{E}_{s}^{\Gamma}$, and that $\operatorname{dim}\left(\widetilde{\Lambda}_{\delta}\right)+\operatorname{dim}\left(P_{\delta}\right)=\mathscr{N}_{\text {slave }}$. In the example of Fig. $12.6$ we fixed $N_{s}=4$, so $\operatorname{dim}\left(P_{\delta}\right)=3, \operatorname{dim}\left(\widetilde{\Lambda}_{\delta}\right)=6$ and $\mathscr{N}_{\text {slave }}=9$. 

\subsubsection{Characterization of the space of constraints by finite elements}

Now we denote by $\mathscr{E}_{s}^{\Gamma}$ the set of edges of $\Omega_{s}$ that lie on $\Gamma$, as in Fig. 12.7, left, and by $\mathscr{E}_{s, \delta}^{\Gamma}$ the edges of the triangles $T_{s, k}$ of the slave set on $\Gamma$ (Figure $12.7$, right $)$.

The set $P_{\delta}$ is defined as in (12.15), whilst the projection space of the solution's jump is

$$
\begin{gathered}
\tilde{\Lambda}_{\delta}=\operatorname{span}\left\{\psi \in L^{2}(\Gamma):\left.\psi\right|_{e} \in \mathbb{P}_{r_{s}}(e) \quad \forall e \in \mathscr{E}_{s, \delta}^{\Gamma} \text { such that } \bar{e} \cap P_{\delta}=\emptyset\right. \\
\left.\left.\psi\right|_{e} \in \mathbb{P}_{r_{s}-1}(e) \quad \forall e \in \mathscr{E}_{s, \delta}^{\Gamma} \text { such that } \bar{e} \cap P_{\delta} \neq \emptyset\right\}
\end{gathered}
$$

where $\mathbb{P}_{r}(e)$ are degree $r$ polynomials in one variable on the interval $e$.

Figure $12.7$, right, depicts a generic function of the space $\widetilde{\Lambda}_{\delta}$ over a rectification of the interface $\Gamma$.

To characterize a basis for (12.16) in presence of finite elements $\mathbb{P}_{1}$, we indicate by $\mathbf{x}_{j}^{e}\left(j=1, \ldots, N_{s}+1\right)$ the nodes belonging to $\mathscr{M}_{s}^{\Gamma} \cap \bar{e}$, where $e$ is an edge of $\mathscr{E}_{s}^{\Gamma}$ (see Figure $12.7$, left ).
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-301.jpg?height=150&width=624&top_left_y=588&top_left_x=147)

Fig. 12.5. Left: a function in $\widetilde{\Lambda}_{\delta}$ for a spectral element discretization; right: functions in $\widetilde{\Lambda}_{\delta}$ for a finite element discretization

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-301.jpg?height=137&width=432&top_left_y=816&top_left_x=239)

Fig. 12.6. The black dots on $\Gamma$ represent the nodes of the slave set $\mathscr{M}_{s}^{\Gamma}$

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-301.jpg?height=172&width=303&top_left_y=1009&top_left_x=166)
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-301.jpg?height=172&width=576&top_left_y=1009&top_left_x=166) space (12.16) on a rectification of the interface $\Gamma$ The basis functions of $\widetilde{\Lambda}_{\delta}$ are functions in $L^{2}(\Gamma)$ that have one edge $e \in \mathscr{E}_{s}^{\Gamma}$ as support, and are associated to the inner nodes of $e$; for $j=3, \ldots, N_{s}-1$ they coincide with the piecewise-linear functions of the Lagrangian basis associated to nodes $\mathbf{x}_{j}^{e}$, while for $j=2$ (resp. $j=N_{s}$ ) the function $\psi_{j}$ equals 1 on the segment $\left[\mathbf{x}_{1}^{e}, \mathbf{x}_{2}^{e}\right]$ (resp. $\left.\left[\mathbf{x}_{N_{e}}^{e}, \mathbf{x}_{N_{e}+1}^{e}\right]\right)$ and on the rest of $e$ it coincides with the piecewise-linear characteristic Lagrange function for node $\mathbf{x}_{2}^{e}\left(\right.$ resp. $\left.\mathbf{x}_{N_{\mathrm{s}}}^{e}\right)$.

Figure $12.5$, right, shows basis functions $\psi_{l}$ of the space (12.16), with support on the second edge of $\mathscr{E}_{s}^{\Gamma}$.

\subsection{Mortar formulation for the Poisson problem}

At this juncture we can characterize the space $V_{\delta}$ in which to look for the mortar solution to problem (3.13). We will say a function $v_{\delta} \in Y_{\delta}$ satisfies the mortar conditions if:

$$
\begin{cases}\int_{\Gamma}\left(v_{\delta}^{(m)}-v_{\delta}^{(s)}\right) \psi=0 & \forall \psi \in \widetilde{\Lambda}_{\delta} \\ v_{\delta}^{(m)}(\mathbf{p})=v_{\delta}^{(s)}(\mathbf{p}) & \forall \mathbf{p} \in P_{\delta}\end{cases}
$$

and we also set

$$
V_{\delta}=\left\{v_{\delta} \in Y_{\delta}: v_{\delta} \text { satisfies conditions (12.17) and } v_{\delta}=0 \text { on } \partial \Omega\right\} \text {. }
$$

The mortar formulation of $(3.13)$ is thus

$$
\text { find } u_{\delta} \in V_{\delta}: \quad \sum_{i=1}^{2} a_{i}\left(u_{\delta}^{(i)}, v_{\delta}^{(i)}\right)=\sum_{i=1}^{2} \int_{\Omega_{i}} f v_{\delta}^{(i)} \quad \forall v_{\delta} \in V_{\delta}
$$

where $a_{i}\left(u_{\delta}^{(i)}, v_{\delta}^{(i)}\right)$ is the restriction to $\Omega_{i}$ of the bilinear form $a\left(u_{\delta}, v_{\delta}\right)$, or possibly of a discretization of it by quadrature formulae of Gaussian type whenever a Galerkin-type formulation with numerical integration is used (G-NI, see Chapter 10 ).

The mortar solution therefore satisfies a weak continuity condition on each slave edge $\left(e \in \mathscr{E}_{S}^{\Gamma}\right)$ of the interface $\Gamma$, and a pointwise matching condition at the endpoints $\left(\mathbf{p} \in P_{\delta}\right)$ of slave edges. In [Bel99, BM94] an alternative mortar formulation was proposed where conditions $(12.17)_{2}$ are absent. This formulation is not very favourable, computationally-speaking, for domains in $\mathbb{R}^{3}$.

When dealing with a spectral approximation, if the partitions in $\Omega_{m}$ and $\Omega_{s}$ are geometrically conforming on $\Gamma$ and the spectral interpolation degree $N_{m}$ on the master domain is not larger than the slave degree $N_{s}$, then conditions (12.17) force strong continuity on all of $\Gamma$. Only if $N_{m}>N_{s}$ is the solution $u_{\delta}$ discontinuous on the interface. The literature labels as mortar or nonconforming only this latter approximation, in which the discrete solution $u_{\delta}$ does not belong to the same space as the continuous solution.

For finite element approximations, even when the interpolation degrees in $\Omega_{m}$ and $\Omega_{s}$ are equal, we have nonconformity each time the sets $\mathscr{M}_{m}^{\Gamma}$ and $\mathscr{M}_{s}^{\Gamma}$ differ. Whenever we adopt the spectral element discretization it can be proved ([BMP94]) that if the solution $u$ of the continuous problem (3.13) and the function $f$ are regular enough on each subdomain $\Omega_{i}$, i.e. $u_{\mid \Omega_{i}} \in H^{\sigma_{i}}\left(\Omega_{i}\right)$ with $\sigma_{i}>\frac{3}{2}$ and $f_{\mid \Omega_{i}} \in H^{\rho_{i}}\left(\Omega_{i}\right)$ with $\rho_{i}>1, i=1, \ldots, M$, then

$$
\left\|u-u_{\delta}\right\| \leq C\left(\sum_{i=1}^{M} N_{i}^{1-\sigma_{i}}\left\|u_{\mid \Omega_{i}}\right\|_{H^{\sigma_{i}}\left(\Omega_{i}\right)}+N_{i}^{-\rho_{i}}\left\|f_{\mid \Omega_{i}}\right\|_{H^{\rho_{i}}\left(\Omega_{i}\right)}\right)
$$

where $\|v\|$ represents the so-called $H^{1}$ broken norm, meaning

$$
\|v\|=\left(\sum_{i=1}^{M}\left\|v_{\mid \Omega_{i}}\right\|_{H^{1}\left(\Omega_{i}\right)}^{2}\right)^{1 / 2}
$$

For finite elements, calling $h_{i}$ the maximum diameter of the triangles $T_{i, k}$, one can prove that if $u_{\mid \Omega_{i}} \in H^{\sigma_{i}}\left(\Omega_{i}\right)$ with $\sigma_{i}>\frac{3}{2}$, then

$$
\left\|u-u_{\delta}\right\| \leq C \sum_{i=1}^{M} h_{i}^{\min \left\{\sigma_{i}, r_{i}+1\right\}-1}\left\|u_{\mid \Omega_{i}}\right\|_{H^{\sigma_{i}}\left(\Omega_{i}\right)}
$$

\subsection{Choosing basis functions}

To solve problem (12.19), let us discuss how one can define the basis functions $v_{\delta} \in$ $V_{\delta}$. Denote with

- $\varphi_{k^{\prime}}^{(m)}, k^{\prime}=1, \ldots, \mathscr{N}_{\Omega_{m}}$, Lagrange's characteristic functions in $\Omega_{m}$ associated to the nodes of $\mathscr{M}_{m} \backslash \mathscr{M}_{m}^{\Gamma} ;$ these belong to the space $V_{m, \delta_{m}}$ and vanish identically on $\Gamma$ - $\varphi_{k^{\prime \prime}}^{(s)}, k^{\prime \prime}=1, \ldots, \mathscr{N}_{\Omega_{s}}$, Lagrange's characteristic functions in $\Omega_{s}$ associated to the nodes of $\mathscr{M}_{s} \backslash \mathscr{M}_{s}^{\Gamma} ;$ they belong to $V_{s, \delta_{s}}$ and are null on $\Gamma$;

- $\mu_{k}^{(m)}, k=1, \ldots, \mathscr{N}_{\text {master }}$, Lagrange's characteristic functions in $\Omega_{m}$ associated to the nodes of $\mathscr{M}_{m}^{\Gamma} ;$ they belong to $V_{m, \delta_{m}}$ and vanish at the nodes of $\mathscr{M}_{m} \backslash \mathscr{M}_{m}^{\Gamma}$

- $\mu_{j}^{(s)}, j=1, \ldots, \mathscr{N}_{\text {slave }}$, Lagrange's characteristic functions in $\Omega_{s}$ associated to the nodes of $\mathscr{M}_{s}^{\Gamma} ;$ these belong to $V_{s, \delta_{s}}$ and are zero on the nodes of $\mathscr{M}_{s} \backslash \mathscr{M}_{s}^{\Gamma}$;

- $\mu_{k}, k=1, \ldots, \mathscr{N}_{\text {master }}$, the basis functions associated to active (or master) nodes of $\Gamma$ and thus defined

$$
\mu_{k}= \begin{cases}\mu_{k}^{(m)} \in V_{m, \delta_{m}} & \text { such that } \mu_{k}^{(m)}(\mathbf{x})=0 \quad \forall \mathbf{x} \in \mathscr{M}_{m} \backslash \mathscr{M}_{m}^{\Gamma} \\ \tilde{\mu}_{k}^{(s)} \in V_{s, \delta_{s}} & \text { such that } \tilde{\mu}_{k}^{(s)}(\mathbf{x})=0 \quad \forall \mathbf{x} \in \mathscr{M}_{s} \backslash \mathscr{M}_{s}^{\Gamma}\end{cases}
$$

where $\tilde{\mu}_{k}^{(s)}$ are $\mathscr{N}_{\text {master }}$ functions in $V_{s, \delta_{s}}$ that we can write as linear combinations of the $\mu_{j}^{(s)}$ via a rectangular matrix $\Xi=\left[\xi_{j k}\right]$

$$
\tilde{\mu}_{k}^{(s)}=\sum_{j=1}^{\mathscr{N}_{\text {slave }}} \xi_{j k} \mu_{j}^{(s)}, \quad \text { for } k=1, \ldots, \mathscr{N}_{\text {master }}
$$



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-304.jpg?height=128&width=698&top_left_y=116&top_left_x=106)

Fig. 12.8. Restriction to the interface of three functions $\mu_{k}$ for a spectral element approximation. In grey the trace associated to master degrees of freedom, in black the slave trace

It is easy to check that

$$
V_{\delta}=\operatorname{span}\left\{\varphi_{k^{\prime}}^{(m)}, \varphi_{k^{\prime \prime}}^{(s)}, \mu_{k}\right\}
$$

Figure $12.8$ shows the restrictions to $\Gamma$ of three different functions $\mu_{k}$ for a spectral element discretization. On the left we have the function $\mu_{k}$ associated to a node in $\mathscr{M}_{m}^{\Gamma}$ but not in $\mathscr{M}_{s}^{\Gamma}$ for a geometrically nonconforming partition; on the right the same but for a conforming partition. In the middle the function $\mu_{k}$ associated to a node in $\mathscr{M}_{m}^{\Gamma} \cap \mathscr{M}_{s}^{\Gamma}$

While we can eliminate the functions $\varphi_{k^{\prime}}^{(m)}$ and $\varphi_{k^{\prime \prime}}^{(s)}$ associated to the nodes of $\partial \Omega$ by using the Dirichlet conditions, we keep all $\mu_{k}^{(m)}, \mu_{j}^{(s)}$ and $\mu_{k}$, because the functions $\tilde{\mu}_{k}^{(s)}$ also depend on the $\mu_{j}^{(s)}$ associated to the nodes of $\partial \Omega \cap \partial \Gamma$ if the Dirichlet conditions are non-homogeneous.

As the functions $\varphi_{k^{\prime}}^{(m)}$ and $\varphi_{k^{\prime \prime}}^{(s)}$ vanish on $\Gamma$, imposing $v_{\delta} \in V_{\delta}$, i.e. the mortar conditions $(12.17)$, is the same as asking that for $k=1, \ldots, \mathscr{N}_{\text {master }}$

$$
\begin{cases}\int_{\Gamma}\left(\mu_{k}^{(m)}-\tilde{\mu}_{k}^{(s)}\right) \psi_{\ell}=0 & \forall \psi_{\ell} \in \widetilde{\Lambda}_{\delta} \\ \mu_{k}^{(m)}(\mathbf{p})=\tilde{\mu}_{k}^{(s)}(\mathbf{p}) & \forall \mathbf{p} \in P_{\delta}\end{cases}
$$

Equivalently, by $(12.23)$,

$$
\begin{cases}\sum_{j=1}^{N_{\text {slave }}} \xi_{j k} \int_{\Gamma} \mu_{j}^{(s)} \psi_{\ell}=\int_{\Gamma} \mu_{k}^{(m)} \psi_{\ell} \quad \forall \psi_{\ell} \in \widetilde{\Lambda}_{\delta} \\ \sum_{j=1}^{\mathscr{S}_{\text {lave }}} \xi_{j k} \mu_{j}^{(s)}(\mathbf{p})=\mu_{k}^{(m)}(\mathbf{p}) \quad \forall \mathbf{p} \in P_{\delta}\end{cases}
$$

still for $k=1, \ldots, \mathscr{N}_{\text {master }}$.

System (12.26) may be rewritten in the matrix form

$$
\sum_{j=1}^{\mathscr{N}_{\text {slave }}} \xi_{j k} P_{\ell j}=\Phi_{\ell k}
$$

where $P$ is a square matrix of dimension $\mathscr{N}_{\text {slave }}$, and $\Phi$ a rectangular matrix with $\mathscr{A}_{\text {slave }}$ rows and $\mathscr{N}_{\text {master }}$ columns whose entries arise from the relations in (12.26). $P$ is non-singular, because the pair $\mathbb{P}_{N-2}-\mathbb{P}_{N}$ satisfies an inf-sup condition in case of spectral element approximation (see [BM92]); and similarly for finite elements ([Bel99]), where we have an inf-sup condition on $\Lambda_{\delta}-\mathbb{P}_{1}$.

Therefore the matrix $\Xi$ can be found by solving the linear system

$$
P \Xi=\Phi \text {. }
$$

The computation of the entries $P_{\ell j}=\int_{\Gamma} \mu_{j}^{(s)} \psi_{\ell}$ and $\Phi_{\ell k}=\int_{\Gamma} \mu_{k}^{(m)} \psi_{\ell}$ using suitably accurate quadrature formulae is crucial in order to ensure optimal error estimates (see (12.20) and (12.21)).

\subsection{Choosing quadrature formulae for spectral elements}

The entries $P_{\ell j}$ depend only on the discretization in the slave domain, so we may rewrite

$$
P_{\ell j}=\int_{\Gamma} \mu_{j}^{(s)} \psi_{\ell}=\sum_{e \in \mathscr{E}_{S}^{\Gamma}} \int_{e} \mu_{j}^{(s)} \psi_{\ell}
$$

and use the GLL quadrature formulae with $N_{s}+1$ nodes on each edge $e \in \mathscr{E}_{s}^{\Gamma}$. These formulae are exact to degree $2 N_{s}-1$, and since $\left.\mu_{j}^{(s)}\right|_{e} \in \mathbb{P}_{N_{s}}$ and $\left.\psi_{\ell}\right|_{e} \in \mathbb{P}_{N_{s}-2}$, they compute the terms $P_{\ell j}$ exactly.

To compute the elements $\Phi_{\ell k}$ we need to specify whether there is geometric conformity on $\Gamma$ or not. If yes, the set $\mathscr{E}_{m}^{\Gamma}$ (the edges of the elements in $\Omega_{m}$ lying on $\Gamma$ ) coincides with $\mathscr{E}_{s} \Gamma$ and we can write

$$
\Phi_{\ell k}=\int_{\Gamma} \mu_{k}^{(m)} \psi_{\ell}=\sum_{e \in \mathscr{E}_{s}^{\Gamma}} \int_{e} \mu_{k}^{(m)} \psi_{\ell}
$$

Since $\left.\mu_{k}^{(m)}\right|_{e} \in \mathbb{P}_{N_{m}}$ and $\left.\psi_{\ell}\right|_{e} \in \mathbb{P}_{N_{s}-2}$ on each edge $e \in \mathscr{E}_{s}^{\Gamma}$, to compute exactly the integrals on $e \in \mathscr{E}_{s}^{\Gamma}$ we can use the Gauss-Legendre quadrature formulae on $N_{q}+1$ nodes with $N_{q}=\max \left\{N_{s}, N_{m}\right\}$, because these formulae are exact to degree $2 N_{q}+1$

In a geometrically nonconforming setting, on the other hand, $\mathscr{E}_{s}^{\Gamma}$ and $\mathscr{E}_{m}^{\Gamma}$ do not coincide and composite integration over the edges of $\mathscr{E}_{s} \Gamma$ (or $\left.\mathscr{E}_{m}^{\Gamma}\right)$ always induces a big quadrature error. Suppose, in fact, we choose a partition associated to $\mathscr{E}_{m}^{\Gamma}$ to compute the integrals $\Phi_{\ell k}$. For each $e \in \mathscr{E}_{m}^{\Gamma}$ we have $\left.\mu_{k}^{(m)}\right|_{e} \in \mathbb{P}_{N_{m}}$, but not necessarily $\left.\psi_{\ell}\right|_{e} \in \mathbb{P}_{N_{s}-2}$; quite the opposite, actually, for $\psi_{\ell}$ might be discontinuous on $e \in \mathscr{E}_{m}^{\Gamma}$ (Figure 12.9). A similar thing happens if we take the partition of $\mathscr{E}_{s}^{\Gamma}$ instead of that of $\mathscr{E}_{m}^{\top}$.

So let us build a new partition $\mathscr{E}_{f}^{\Gamma}$, that will be finer than either $\mathscr{E}_{m}^{\Gamma}$ and $\mathscr{E}_{s}^{\Gamma}$ and whose every edge $\tilde{e} \in \mathscr{E}_{f}^{\Gamma}$ is contained in one edge only of $\mathscr{E}_{m}^{\Gamma}$ and one only of $\mathscr{E}_{s}^{\Gamma}$ (Figure 12.9). Then we can write

$$
\Phi_{\ell k}=\int_{\Gamma} \mu_{k}^{(m)} \psi_{\ell}=\sum_{\tilde{e} \in \mathscr{E}_{f}^{\Gamma}} \int_{\tilde{e}} \mu_{k}^{(m)} \psi_{\ell}
$$



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-306.jpg?height=189&width=453&top_left_y=107&top_left_x=233)

Fig. 12.9. The partitions $\mathscr{E}_{s}^{\Gamma}, \mathscr{E}_{m}, \mathscr{E}_{f}^{\Gamma}$ on $\Gamma$ and a function $\left.\psi_{\ell}\right|_{e}$ in case of spectral element approximation with $N_{s}=3$

and since $\left.\mu_{k}^{(m)}\right|_{e} \in \mathbb{P}_{N_{m}}$ and $\left.\psi_{\ell}\right|_{\tilde{e}} \in \mathbb{P}_{N_{s}-2}$ on each $\tilde{e} \in \mathscr{E}_{f}^{\Gamma}$, we can use on the edges $\tilde{e}$ a Gauss-Legendre quadrature formula (see [CHQZ06] over $N_{q}+1$ nodes with $N_{q}=$ $\max \left\{N_{s}, N_{m}\right\}$, and so compute the $\Phi_{\ell k}$ exactly.

\subsection{Choosing quadrature formulae for finite elements}

Let us consider the case of finite elements $\mathbb{P}_{1}$, and recall that $\mathscr{E}_{s} \Gamma$ denotes the set of edges of $\Gamma$, while $\mathscr{E}_{s, \delta}^{\Gamma}$ is the set of edges of the triangles $T_{s, k} \in \mathscr{T}_{s}$ lying on $\Gamma$ (Figure 12.7).

The elements $P_{\ell j}$, depending only on the discretization of the slave domain, are now

$$
P_{\ell j}=\int_{\Gamma} \mu_{j}^{(s)} \psi_{\ell}=\sum_{e \in \mathscr{E}_{s, \delta}^{\Gamma}} \int_{e} \mu_{j}^{(s)} \psi_{\ell}
$$

and as on each edge $e \in \mathscr{E}_{s, \delta}^{\Gamma}$ the product $\mu_{j}^{(s)} \psi_{\ell}$ is polynomial of degree $\leq 2$, we can integrate exactly on edges $e \in \mathscr{E}_{s, \delta}^{\Gamma}$ using Simpson's formula (see, e.g., [QSS07]).

To compute the elements $\Phi_{\ell k}$ we proceed in analogy to what we did in the spectral case without geometric conformity on $\Gamma$.

Build a partition $\mathscr{E}_{f}^{\Gamma}$ finer than $\mathscr{E}_{m}$ and $\mathscr{E}_{s, \delta}^{\Gamma}$, such that each side $\tilde{e} \in \mathscr{E}_{f}^{\Gamma}$ lies only on one edge of $\mathscr{E}_{m}^{\Gamma}$ and one edge of $\mathscr{E}_{s, \delta}^{\Gamma}$, with the result that

$$
\Phi_{\ell k}=\int_{\Gamma} \mu_{k}^{(m)} \psi_{\ell}=\sum_{\tilde{e} \in \mathscr{E}_{f}^{\Gamma}} \int_{\tilde{e}} \mu_{k}^{(m)} \psi_{\ell}
$$

Both $\left.\mu_{k}^{(m)}\right|_{\tilde{e}}$ and $\left.\psi_{\ell}\right|_{\tilde{e}}$ have degree not exceeding 1 on each $\tilde{e} \in \mathscr{E}_{f}^{\Gamma}$, and we can integrate exactly on each edge $\tilde{e}$ via Simpson's formula.

When finite elements of higher degree appear, the procedure is similar, with the proviso of replacing Simpson's formula with a more accurate one, like a Gauss-Legendre formula.

The case of quadrilateral finite elements of type $\mathbb{Q}_{1}$ is treated in the same manner of finite elements $\mathbb{P}_{1}$, because the traces on $\Gamma$ of Lagrangian basis functions $\mathbb{Q}_{1}$ and $\mathbb{P}_{1}$ coincide, and the space $\Lambda_{\delta}$ is defined alike for both elements. 

\subsection{Solving the linear system of the mortar method}

The coefficients $\xi_{i j}$ found by solving system $(12.28)$ ensure that the functions $\mu_{k}$ of (12.22)-(12.23) satisfy the constraints of the space $V_{\delta}$, and once the master degrees of freedom $\lambda_{k}^{(m)} \in \mathscr{U}_{m}$ are known it is possible to compute the slave degrees of freedom $\lambda_{j}^{(s)} \in \mathscr{U}_{s}$ using

$$
\boldsymbol{\lambda}^{(s)}=\boldsymbol{\Xi} \boldsymbol{\lambda}^{(m)}
$$

where $\boldsymbol{\lambda}^{(s)}=\left[\lambda_{j}^{(s)}\right]_{j=1}^{\mathscr{S}_{\text {slave }}}$ and $\boldsymbol{\lambda}^{(m)}=\left[\lambda_{k}^{(m)}\right]_{k=1}^{\mathscr{S}_{\text {master }}}$.

When the discretization is conforming on $\Gamma$, the matrix $\Xi$ coincides with the identity matrix of dimension $\mathscr{N}_{\text {master }}=\mathscr{A}_{\text {slave }}$.

By (12.24) every function in $V_{\delta}$ can be written

$$
v_{\delta}(\mathbf{x})=\sum_{k^{\prime}=1}^{\mathscr{K}_{1}} u_{k^{\prime}}^{(m)} \varphi_{k^{\prime}}^{(m)}(\mathbf{x})+\sum_{k^{\prime \prime}=1}^{\mathscr{N}_{2}} u_{k^{\prime \prime}}^{(s)} \varphi_{k^{\prime \prime}}^{(s)}(\mathbf{x})+\sum_{k=1}^{\mathscr{Y}_{\text {master }}} \lambda_{k}^{(m)} \mu_{k}(\mathbf{x})
$$

Now varying $v_{\delta} \in \operatorname{span}\left\{\varphi_{k^{\prime}}^{(m)}, \varphi_{k^{\prime \prime}}^{(s)}, \mu_{k}\right\}$ and defining vectors $\mathbf{u}^{(m)}=\left[u_{k^{\prime}}^{(m)}\right]^{T}, \mathbf{u}^{(s)}=$ $\left[u_{k^{\prime \prime}}^{(s)}\right]^{T}$, the mortar system (12.19) reads

$$
\left[\begin{array}{ccc}
A_{m m} & 0 & A_{m, \Gamma_{m}} \\
0 & A_{s s} & A_{s, \Gamma_{s}} \Xi \\
A_{\Gamma_{m}, m} & \Xi^{T} A_{\Gamma_{s}, s} & A_{\Gamma_{m}, \Gamma_{m}}+\Xi^{T} A_{\Gamma_{s}, \Gamma_{s}} \Xi
\end{array}\right]\left[\begin{array}{c}
\mathbf{u}^{(m)} \\
\mathbf{u}^{(s)} \\
\boldsymbol{\lambda}^{(m)}
\end{array}\right]=\left[\begin{array}{c}
\mathbf{f}_{m} \\
\mathbf{f}_{s} \\
\mathbf{f}_{\Gamma_{m}}+\Xi^{T} \mathbf{f}_{\Gamma_{s}}
\end{array}\right]
$$

Above, for $i \in\{m, s\}$, we introduced the matrices $\left(A_{i i}\right)_{j k}=a_{i}\left(\varphi_{k}^{(i)}, \varphi_{j}^{(i)}\right),\left(A_{i, \Gamma_{i}}\right)_{j k}=$ $a_{i}\left(\mu_{k}^{(i)}, \varphi_{j}^{(i)}\right),\left(A_{\Gamma_{i}, i}\right)_{k j}=a_{i}\left(\varphi_{j}^{(i)}, \mu_{k}^{(i)}\right),\left(A_{\Gamma_{i}, \Gamma_{i}}\right)_{k \ell}=a_{i}\left(\mu_{\ell}^{(i)}, \mu_{k}^{(i)}\right)$ and the vectors $\left(\mathbf{f}_{i}\right)_{j}=$ $\int_{\Omega_{i}} f \varphi_{j}^{(i)},\left(\mathbf{f}_{\Gamma_{i}}\right)_{\ell}=\int_{\Omega_{i}} f \mu_{\ell}^{(i)}$ for the basis functions associated to the nodes of $\Omega_{i}$ not on the boundary $\partial \Omega$.

The matrix $\Xi$ depends solely on the chosen discretization and is built once the discretization's parameters have been fixed.

System (12.30) can be solved by one of the direct of iterative methods seen in Chapter 7. Instead of solving the overall system (12.30), one can solve its Schur complement for the vector $\boldsymbol{\lambda}^{(m)}$, which consists in eliminating the unknowns $\mathbf{u}^{(m)}, \mathbf{u}^{(s)}$ from the system (see Section 19.3.1 for a thorough description). Let us define the following matrices (called local Schur complements)

$$
\Sigma_{i}=A_{\Gamma_{i}, \Gamma_{i}}-A_{\Gamma_{i}, i} A_{i i}^{-1} A_{i, \Gamma_{i}}, \quad \text { for } i=m, s
$$

and set

$$
\Sigma=\Sigma_{m}+\Xi^{T} \Sigma_{s} \Xi, \quad \chi=\left(\mathbf{f}_{\Gamma_{m}}-A_{m m}^{-1} \mathbf{f}_{m}\right)+\Xi^{T}\left(\mathbf{f}_{\Gamma_{s}}-A_{s s}^{-1} \mathbf{f}_{s}\right)
$$

Now we follow the recipe:

- compute master degrees of freedom on $\Gamma$ by solving

$$
\Sigma \boldsymbol{\lambda}^{(m)}=\boldsymbol{\chi}
$$

- determine slave degrees of freedom on $\Gamma$ using the linear relationship (12.29); - solve problems $A_{i i} \mathbf{u}^{(i)}=\mathbf{f}_{i}-A_{i, \Gamma_{i}} \boldsymbol{\lambda}^{(i)}, i=1,2$, independently. This is equivalent to solving two Dirichlet problems with prescribed trace on $\Gamma$.

Equation (12.32) is the discrete counterpart to the Steklov-Poincaré equation (19.26) (see Chap. 19) that expresses the continuity of fluxes through the interface, rather than strong continuity if the discretization on $\Gamma$ is conforming, or weak continuity if the formulation on $\Gamma$ is of mortar type.

System (12.32) is typically solved by iterative methods (such as the Conjugate Gradient, Bi-CGStab or GMRES), since local Schur complements $\Sigma_{i}$ are not assembled explicitly due to the presence of the matrices $A_{i i}^{-1}$.

Various preconditioners have been proposed for the algebraic system resulting from the mortar formulation; for example in [AMW99] a preconditioner for (12.32) is based on the decomposition of the space of mortar traces in the direct sum of subspaces associated to the traces of the interfaces (in case of many subdomains) and on a coarse space that allows to reduce the lower frequences of the error. At present, as there are only two subdomains and one interface, we have preconditioned system (12.32) with the matrix $\Sigma_{m}$ defined in (12.31). For a spectral element discretization it turns out to be optimal, in the sense that the number of iterations required by the iterative method to solve (12.32) up to a given tolerance is independent of the degrees $N_{i}(i=m, s)$ on the master and slave domains (Figure 12.13). For finite element approximations the preconditioner $\Sigma_{m}$ lowers the number of iterations needed for the method to converge, but now this number does depend on the discretization parameter $h$, as one can see from the results of Figure $12.14$.

\subsection{The mortar method for combined finite and spectral elements}

Until this point we have looked at situations where the spaces $X_{i, \delta_{i}}(i=m, s)$ of $(12.12)$ are of the same kind on both domains $\Omega_{m}, \Omega_{s}$, that is to say both of spectral type or of finite element type. Now we consider the case when we choose $X_{m, \delta_{m}}$ of finite element type while $X_{s, \delta_{s}}$ of spectral element type, or the other way around.

First of all, notice that the spaces $V_{i, \delta_{i}}, i=m, s$, are naturally defined by the $X_{i, \delta_{i}}$. The definition of the space of constraints $\Lambda_{\delta}$ is strictly related to the discretization adopted on the slave space, so $\widetilde{\Lambda}_{\delta}$ will be defined as in (12.14) if the discretization in $\Omega_{s}$ is spectral, or as in (12.16) with finite elements. The corresponding basis functions $\psi_{l}$ will abide by the definitions of Sections $12.2 .1$ or $12.2 .2$ respectvely.

Now we need to accurately compute the integrals appearing in (12.26) that define the entries of the matrices $P$ and $\Phi$.

Computing the $P_{\ell j}$ is only a matter of the discretization chosen on the slave domain, so it is carried out as explained in Section $12.5$ (for spectral elements in $\Omega_{s}$ ) 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-309.jpg?height=122&width=714&top_left_y=123&top_left_x=100)

Fig. 12.10. The restrictions to $\Gamma$ of $\mu_{k}^{(m)}, \mu_{j}^{(s)}$ and the functions $\psi_{l}$ in the spectral master / slave finite elements case

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-309.jpg?height=120&width=232&top_left_y=339&top_left_x=103)

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-309.jpg?height=123&width=233&top_left_y=337&top_left_x=346)
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-309.jpg?height=126&width=712&top_left_y=334&top_left_x=102) / spectral slave case

or Section $12.6$ (for finite elements). Computing the $\Phi_{\ell k}$ requires more care, for it involves both discretizations in $\Omega_{s}$ (via the functions $\psi_{l}$ ) and in $\Omega_{m}$ (via the $\left.\mu_{k}^{(m)}\right)$. We will keep the two situations separate and discuss only finite elements of type $\mathbb{P}_{1}$.

Case 1: spectral master / slave finite elements.

The restrictions of the functions $\mu_{k}^{(m)}$ to the edges $e$ of $\mathscr{E}_{m}^{\Gamma}$ are polynomials of degree $N_{m}$, while the restrictions of the $\psi_{l}$ to the edges of $\mathscr{E}_{s, \delta}^{\Gamma}$ are polynomials of degree 1 at most (Figure 12.10). Let us produce a finer partition $\mathscr{E}_{f}^{\Gamma}$ than either $\mathscr{E}_{m}^{\Gamma}$ and $\mathscr{E}_{s, \delta}^{\Gamma}$ so that on every $\tilde{e} \in \mathscr{E}_{f}^{\Gamma}$ the restrictions of $\mu_{k}^{(m)}, \psi_{l}$ are polynomials. The degree of the product $\mu_{k}^{(m)} \psi_{l}$ is at most $N_{m}+1$, and to compute each integral $\int_{\tilde{e}} \mu_{k}^{(m)} \psi_{l}$ exactly we can use Gauss-Legendre quadrature formulae on $N_{q}+1$ quadrature nodes in $\tilde{e}$, with $N_{q}=N_{m} / 2$ if $N_{m}$ is even and $N_{q}=\left(N_{m}+1\right) / 2$ if $N_{m}$ is odd.

Case 2: master finite elements / spectral slave.

The restrictions of the $\mu_{k}^{(m)}$ to the edges $e$ of $\mathscr{E}_{m, \delta}^{\Gamma}$ (the set of all edges of the triangles $T_{m, k}$ on $\Gamma$ ) are polynomials of degree one at most, while the restrictions of the $\psi_{l}$ to the edges of $\mathscr{E}_{s}^{\Gamma}$ are polynomials of degree $N_{s}-2$ (Figure 12.11). We generate a partition $\mathscr{E}_{f}^{\Gamma}$ that is finer than $\mathscr{E}_{m, \delta}^{\Gamma}$ and than $\mathscr{E}_{s}^{\Gamma}$, so that on every $\tilde{e} \in \mathscr{E}_{f}^{\Gamma}$ the restrictions of $\mu_{k}^{(m)}$ and $\psi_{l}$ are polynomial. The degree of the product $\mu_{k}^{(m)} \psi_{l}$ is at most $N_{s}+1$, and to compute each $\int_{\tilde{e}} \mu_{k}^{(m)} \psi_{l}$ exactly we may employ Gauss-Legendre quadrature formulae with $N_{q}+1$ quadrature nodes in $\tilde{e}$, where $N_{q}=N_{s} / 2$ if $N_{s}$ is even and $N_{q}=\left(N_{s}+1\right) / 2$ if odd.

Once $P$ and $\Phi$ have been computed, we use $(12.28)$ to compute $\Xi$ and then solve the linear system (12.30) (or, equivalently, (12.32), as described in Section 12.7).

Concerning the analysis of the approximation error we have optimal convergence ([BMP94]), a result that generalizes estimates $(12.20)$ and (12.21). Among the do- mains $\Omega_{i}(i=1, \ldots, M)$ we distinguish those with spectral discretization $\Omega_{i}^{\text {es }}, i=$ $1, \ldots, M^{e s}$, from those with finite element discretization $\Omega_{i}^{e f}, i=1, \ldots, M^{e f}$.

If the solution $u$ to the continuous problem (3.13) and the function $f$ are regular enough on each subdomain $\Omega_{i}$, i.e. $u_{\mid \Omega_{i}} \in H^{\sigma_{i}}\left(\Omega_{i}\right)$ with $\sigma_{i}>\frac{3}{2}$ and $f_{\mid \Omega_{i}^{e s}} \in H^{\rho_{i}}\left(\Omega_{i}^{e s}\right)$ with $\rho_{i}>1$ for all $i=1, \ldots, M^{e s}$, then

$$
\begin{gathered}
\left\|u-u_{\delta}\right\| \leq C\left(\sum_{i=1}^{M^{e s}} N_{i}^{1-\sigma_{i}}\left\|u_{\mid \Omega_{i}}\right\|_{H^{\sigma_{i}}\left(\Omega_{i}^{e s}\right)}+N_{i}^{-\rho_{i}}\left\|f_{\mid \Omega_{i}}\right\|_{H^{\rho_{i}}\left(\Omega_{i}^{e s}\right)}\right. \\
\left.\quad+\sum_{i=1}^{M^{e f}} h_{i}^{\min \left\{\sigma_{i}, r_{i}+1\right\}-1}\left\|u_{\mid \Omega_{i}}\right\|_{H^{\sigma_{i}\left(\Omega_{i}^{\text {ef }}\right)}}\right)
\end{gathered}
$$

\subsection{Generalization of the mortar method to multi-domain decompositions}

Suppose we decompose the domain $\Omega$ in more than two subdomains. The previous sections' study of one interface must be repeated for every single interface of the decomposition. Hence for every interface $\Gamma_{i j}$ we have to choose which between $\Omega_{i}$ and $\Omega_{j}$ is the master and which the slave, and then we must impose system $(12.25)$ on $\Gamma_{i j}$. So for every interface $\Gamma_{i j}$ there is a "local" constraint space $\widetilde{\Lambda}_{\delta}$ that depends on the slave domain chosen on $\Gamma_{i j}$, and the overall, global space of constraints will be the Cartesian product of the local ones. At the vertices of the subdomains $\Omega_{i}$ lying on the closure of $\Gamma$ one imposes a continuity condition, in analogy to $(12.25)_{2}$. Observe that a domain might be master for one interface and slave for another one, as in Figure 12.3, right: $\Omega_{3}$ could for example be master for $\Gamma_{36}$ and slave for $\Gamma_{35}$ and $\Gamma_{32}$.

The problems arising from a complicated decomposition crop up concretely in the construction of the matrices $P$ and $\Phi$, in the procedure for solving efficiently $P \Xi=\Phi$, and in the choice of a good preconditioner for the final algebraic system. The mortar formulation should therefore be limited to the case of a small number of interfaces.

\subsection{Numerical results for the mortar method}

Consider the Poisson problem

$$
\begin{cases}-\Delta u=f & \text { in } \Omega=(0,2)^{2} \\ u=g & \text { on } \partial \Omega\end{cases}
$$

where $f$ and $g$ are such that the exact solution reads $u(x, y)=\sin (\pi x y)+1$. Subdivide $\Omega$ in two subdomains $\Omega_{1}=(0,1) \times(0,2)$ and $\Omega_{2}=(1,2) \times(0,2)$, on both of which we introduce a further uniform partition into rectangles, and then discretize by spectral elements.

Figure $12.12$ displays the errors in broken norm between the mortar solution and the exact one, once $\Omega_{1}$ has been appointed master. On the left the slave's degree $N_{s}=$ 
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-311.jpg?height=244&width=712&top_left_y=114&top_left_x=94)

Fig. 12.12. Errors in broken norm for the solution to problem (12.34). $\Omega_{s}=2 \times 2$ spectral elements. On the left the degree $N_{s}=14$ on the slave domain $\Omega_{s}$ is fixed, on the right the degree $N_{m}=14$ on the master domain $\Omega_{m}$ is fixed

$N_{2}=14$ is fixed, and the master's degree $N_{m}=N_{1}$ varies, whilst on the right $N_{1}=14$ is fixed in the master domain and the slave degree varies. The two curves refer to different partitions on the subdomains: the first one is geometrically conforming with $2 \times 2$ spectral elements in each $\Omega_{i}$, the second has $2 \times 3$ spectral elements in $\Omega_{m}$ and $2 \times 2$ in $\Omega_{s}$. In both cases the error converges exponentially until the error in the domain with fixed spectral degree prevails.

Figure $12.13$ gives the number of iterations the preconditioned Bi-CGStab method needs in order to solve system (12.32), using $\Sigma_{m}$ as preconditioner and with fixed tolerance $\varepsilon=10^{-12}$ in the stopping test. Note that for a conforming discretization, convergence is reached after one iteration, while in the nonconforming setting more iterations are necessary, although their number is independent of the polynomial degrees $N_{m}$ and $N_{s}$.

Figure $12.14$ shows the numerical results for an approximation of problem (12.34) by finite elements $\mathbb{P}_{1}$, both in the master domain $\Omega_{1}$ and in the slave domain $\Omega_{2}$. The functions $f, g$ and the subdomains are as in the previous case. In both $\Omega_{i}$ we assumed

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-311.jpg?height=232&width=338&top_left_y=928&top_left_x=99)
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-311.jpg?height=232&width=708&top_left_y=928&top_left_x=98tral elements. The degree is fixed on the slave domain $\Omega_{s}$ on the left, on the master domain $\Omega_{m}$ on the right 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-312.jpg?height=309&width=433&top_left_y=118&top_left_x=237)

Fig. 12.14. Absolute errors in norm $H^{1}$ and in broken norm for the mortar approximation spectral master /slave finite elements

uniform triangulations made of $2 n_{i} \times 2 n_{i}$ triangles, with $n_{m} \neq n_{s}:$ precisely, $n_{m}=2 k$ and $n_{s}=3(k+2)$ for $k=5,10,20,40$.

In Figure $12.14$ one can read the absolute errors in norm $H^{1}\left(\Omega_{i}\right)$ and the brokennorm error between the exact solution and the mortar solution, as the mesh size $h_{i}$ varies: they decrease linearly with respect to $h_{i}$, in agreement with estimate $(12.21)$. The number of iterations required by the preconditioned Bi-CGStab method, with preconditioner $\Sigma_{m}$ and with given tolerance $\varepsilon=10^{-12}$ in the stopping test, is independent of $h_{i}$, and turns out to be $\leq 6$.

In Figure $12.15$ we have the absolute errors in norm $H^{1}\left(\Omega_{i}\right)$ and the error in broken norm between exact and mortar solutions, as the mesh size $h_{s}$ varies in $\Omega_{s}$, relative to the approximation of problem (12.34) with spectral elements on the master domain and finite elements $\left(\mathbb{P}_{1}\right.$ or $\left.\mathbb{Q}_{2}\right)$ on the slave domain. The functions $f, g$ and the subdomains are defined as in the previous cases.
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-312.jpg?height=250&width=716&top_left_y=898&top_left_x=94)

Fig. 12.15. Absolute errors in norm $H^{1}$ and broken norm for the mortar approximation spectral master /slave finite elements. On the left $\mathbb{Q}_{6}-\mathbb{P}_{1}$, on the right $\mathbb{Q}_{8}-\mathbb{Q}_{2}$. In both cases the error line in broken norm overlaps and practically hides the curve in norm $H^{1}\left(\Omega_{s}\right)$ 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-313.jpg?height=310&width=433&top_left_y=119&top_left_x=237)

Fig. 12.16. Absolute errors in norm $H^{1}$ and broken norm for the mortar approximation master finite elements/spectral slave

The errors on the left refer to a partition of $\Omega_{m}$ in $3 \times 3$ spectral elements of degree $N_{m}=6$, of $\Omega_{s}$ in $2 n_{s} \times 2 n_{s}$ equal triangles, with $n_{s}=20,40,80,160\left(h_{s}=2 / n_{s}\right)$. The errors on the right refer to a partition of $\Omega_{m}$ in $3 \times 3$ spectral elements of degree $N_{m}=8$, and of $\Omega_{s}$ in $n_{s} \times n_{s}$ equal quadrilaterals, with $n_{s}=10,20,40,80\left(h_{s}=2 / n_{s}\right)$.

We remark that the error in the slave domain of finite elements decreases like $h_{s}^{r_{s}}$ ( $r_{s}$ is the polynomial degree of the finite elements), whereas the error in the master domain does not reach the spectral case's accuracy because it is sensitive of the worse accuracy on the slave domain. However, it decreases as $h_{s}^{r_{s}+1}$ and the error in broken norm agrees with estimate (12.21).

The preconditioned Bi-CGStab method with preconditioner $\Sigma_{m}$ needs a number of iterations, given a tolerance $\varepsilon=10^{-12}$ in the stopping test, that decreases slightly with $h_{s}$ in both tests, and ranges from 8 iterations for $h_{s}=1 / 10$ to 5 for $h_{s}=1 / 40$

In Figure $12.16$ the numerical results for the approximation of problem (12.34) are shown, with finite elements $\mathbb{P}_{1}$ on the master domain and spectral elements on the slave domain. The functions $f, g$ and the subdomains are defined as in above cases. The domain $\Omega_{s}$ is divided in $4 \times 4$ spectral elements of degree $N_{s}=6$, while in $\Omega_{m}$ we have uniform triangulations of $2 n_{m} \times 2 n_{m}$ triangles, with $n_{m}=20,40,80,160$.

In particular one can see the behaviour of absolute errors in norm $H^{1}\left(\Omega_{i}\right)$ and broken-norm, between the exact and the mortar solution, as the mesh-size $h_{s}$ varies in $\Omega_{s}$. With mortar approximation on master domain and spectral approximation on slave domain, both errors in $\Omega_{m}$ and $\Omega_{s}$ decrease linearly with $h_{m}$. Here, too, the number of iterations of the preconditioned Bi-CGStab method with preconditioner $\Sigma_{m}$, given a tolerance $\varepsilon=10^{-12}$ in the stopping test, does not depend on $h_{i}$ and is $\leq 8$ in all tests. 

\section{Diffusion-transport-reaction equations}

In this chapter we consider problems of the following form:

$$
\begin{cases}L u=-\operatorname{div}(\mu \nabla u)+\mathbf{b} \cdot \nabla u+\sigma u=f & \text { in } \Omega \\ u=0 & \text { on } \partial \Omega\end{cases}
$$

where $\mu, \sigma, f$ and $\mathbf{b}$ are given functions (or constants). In the most general case, we will suppose that $\mu \in \mathrm{L}^{\infty}(\Omega)$ with $\mu(\mathbf{x}) \geq \mu_{0}>0, \sigma \in \mathrm{L}^{2}(\Omega)$ with $\sigma(\mathbf{x}) \geq 0$ a.e. in $\Omega, \mathbf{b} \in\left[\mathrm{L}^{\infty}(\Omega)\right]^{2}$ with $\operatorname{div}(\mathbf{b}) \in \mathrm{L}^{2}(\Omega)$, and $f \in \mathrm{L}^{2}(\Omega)$.

In many practical applications, the diffusion term $-\operatorname{div}(\mu \nabla u)$ is dominated by the convection term $\mathbf{b} \cdot \nabla u$ (also called transport term) or by the reaction term $\sigma u$ (also called the absorption term when $\sigma$ is non-negative). In such cases, as we will see, the solution can give rise to boundary layers, that is regions, generally close to the boundary of $\Omega$, where the solution is characterized by strong gradients.

To derive such models, and to capture the analogy with random walk processes, see e.g. [Sal08, Chap. 2.]

In this chapter we analyze the conditions ensuring the existence and uniqueness of the solution to problem (13.1). We also consider the Galerkin method, illustrate its difficulties in providing stable solutions in the presence of boundary layers, and finally propose alternative discretization methods for the approximation of $(13.1)$.

\subsection{Weak problem formulation}

Let $V=\mathrm{H}_{0}^{1}(\Omega)$. By introducing the bilinear form $a: V \times V \mapsto \mathbb{R}$,

$$
a(u, v)=\int_{\Omega} \mu \nabla u \cdot \nabla v d \Omega+\int_{\Omega} v \mathbf{b} \cdot \nabla u d \Omega+\int_{\Omega} \sigma u v d \Omega \quad \forall u, v \in V
$$

the weak formulation of problem (13.1) becomes

$$
\text { find } u \in V: \quad a(u, v)=(f, v) \quad \forall v \in V \text {. }
$$

In order to prove the existence and uniqueness of the solution of (13.3) we will put ourselves in the condition to apply the Lax-Milgram lemma.

To verify the coercivity of the bilinear form $a(\cdot, \cdot)$, we proceed separately on the single terms composing $(13.2)$.

For the first term we have

$$
\int_{\Omega} \mu \nabla v \cdot \nabla v d \Omega \geq \mu_{0}\|\nabla v\|_{\mathrm{L}^{2}(\Omega)}^{2}
$$

As $v \in \mathrm{H}_{0}^{1}(\Omega)$, the Poincaré inequality holds (see $\left.(2.13)\right)$; then

$$
\|v\|_{\mathrm{H}^{1}(\Omega)}^{2}=\|v\|_{\mathrm{L}^{2}(\Omega)}^{2}+\|\nabla v\|_{\mathrm{L}^{2}(\Omega)}^{2} \leq\left(1+C_{\Omega}^{2}\right)\|\nabla v\|_{\mathrm{L}^{2}(\Omega)}^{2}
$$

and therefore it follows from (13.4) that

$$
\int_{\Omega} \mu \nabla v \cdot \nabla v d \Omega \geq \frac{\mu_{0}}{1+C_{\Omega}^{2}}\|v\|_{\mathrm{H}^{1}(\Omega)}^{2}
$$

We now move to the convective term. Using Green's formula (3.16) yields

$$
\begin{aligned}
\int_{\Omega} v \mathbf{b} \cdot \nabla v d \Omega &=\frac{1}{2} \int_{\Omega} \mathbf{b} \cdot \nabla\left(v^{2}\right) d \Omega=-\frac{1}{2} \int_{\Omega} v^{2} \operatorname{div}(\mathbf{b}) d \Omega+\frac{1}{2} \int_{\partial \Omega} \mathbf{b} \cdot \mathbf{n} v^{2} d \gamma \\
&=-\frac{1}{2} \int_{\Omega} v^{2} \operatorname{div}(\mathbf{b}) d \Omega
\end{aligned}
$$

as $v=0$ on $\partial \Omega$, whence

$$
\int_{\Omega} v \mathbf{b} \cdot \nabla v d \Omega+\int_{\Omega} \sigma v^{2} d \Omega=\int_{\Omega} v^{2}\left(-\frac{1}{2} \operatorname{div}(\mathbf{b})+\sigma\right) d \Omega
$$

The last integral is certainly positive if we suppose that

$$
-\frac{1}{2} \operatorname{div}(\mathbf{b})+\sigma \geq 0 \quad \text { a.e. in } \Omega
$$

Consequently, the bilinear form $a(\cdot, \cdot)$ is coercive, as

$$
a(v, v) \geq \alpha\|v\|_{\mathrm{H}^{1}(\Omega)}^{2} \quad \forall v \in V, \quad \text { with } \quad \alpha=\frac{\mu_{0}}{1+C_{\Omega}^{2}}
$$

To prove that the bilinear form $a(\cdot, \cdot)$ is continuous, that is it satisfies $(2.6)$, we bound the first term on the right-hand side of $(13.2)$ as follows

$$
\begin{aligned}
\left|\int_{\Omega} \mu \nabla u \cdot \nabla v d \Omega\right| & \leq\|\mu\|_{\mathrm{L}^{\infty}(\Omega)}\|\nabla u\|_{\mathrm{L}^{2}(\Omega)}\|\nabla v\|_{\mathrm{L}^{2}(\Omega)} \\
& \leq\|\mu\|_{\mathrm{L}^{\infty}(\Omega)}\|u\|_{\mathrm{H}^{1}(\Omega)}\|v\|_{\mathrm{H}^{1}(\Omega)}
\end{aligned}
$$

We have used the Hölder and Cauchy-Schwarz inequalities (see Sect. $2.5$ ), as well as the inequality $\|\nabla w\|_{\mathrm{L}^{2}(\Omega)} \leq\|w\|_{\mathrm{H}^{1}(\Omega)} \forall w \in H^{1}(\Omega)$. For the second term, proceeding in a similar way we find

$$
\begin{aligned}
&\left|\int_{\Omega} v \mathbf{b} \cdot \nabla u d \Omega\right| \begin{array}{l}
\leq\|\mathbf{b}\|_{\mathrm{L}^{\infty}(\Omega)}\|v\|_{\mathrm{L}^{2}(\Omega)}\|\nabla u\|_{\mathrm{L}^{2}(\Omega)} \\
& \leq\|\mathbf{b}\|_{\mathrm{L}^{\infty}(\Omega)}\|v\|_{\mathrm{H}^{1}(\Omega)}\|u\|_{\mathrm{H}^{1}(\Omega)}
\end{array}
\end{aligned}
$$

Finally, for the third term we have, thanks again to the Cauchy-Schwarz inequality,

$$
\left|\int_{\Omega} \sigma u v d \Omega\right| \leq C^{2}\|\sigma\|_{\mathrm{L}^{2}(\Omega)}\|u v\|_{\mathrm{L}^{2}(\Omega)} \leq C^{2}\|\sigma\|_{\mathrm{L}^{2}(\Omega)}\|u\|_{\mathrm{H}^{1}(\Omega)}\|v\|_{\mathrm{H}^{1}(\Omega)}
$$

Indeed, $\|u v\|_{\mathrm{L}^{2}(\Omega)} \leq\|u\|_{\mathrm{L}^{4}(\Omega)}\|v\|_{\mathrm{L}^{4}(\Omega)} \leq C^{2}\|u\|_{\mathrm{H}^{1}(\Omega)}\|v\|_{\mathrm{H}^{1}(\Omega)}$, having applied inequality (2.18) and exploited inclusions (2.19), with $C$ being the inclusion constant. Summing (13.7), (13.8) and (13.9) term by term, the continuity property $(2.6)$ follows by taking, e.g.,

$$
M=\|\mu\|_{L^{\infty}(\Omega)}+\|\mathbf{b}\|_{\mathrm{L}^{\infty}(\Omega)}+C^{2}\|\sigma\|_{\mathrm{L}^{2}(\Omega)}
$$

On the other hand, the right-hand side of (13.3) defines a bounded and linear functional thanks to the Cauchy-Schwarz inequality and to the Poincaré inequality (2.13).

As the Lax-Milgram lemma hypotheses are verified (see Lemma 3.1), it follows that the solution of the weak problem (13.3) exists and is unique. Moreover, the following a priori estimates hold

$$
\|u\|_{\mathrm{H}^{1}(\Omega)} \leq \frac{1}{\alpha}\|f\|_{\mathrm{L}^{2}(\Omega)}, \quad\|\nabla u\|_{\mathrm{L}^{2}(\Omega)} \leq \frac{C_{\Omega}}{\mu_{0}}\|f\|_{\mathrm{L}^{2}(\Omega)},
$$

as consequences of $(13.4),(13.6)$ and $(2.4)$. The first follows from Corollary $3.1$, the second one can easily be proven starting from equation $a(u, u)=(f, u)$ and using the Cauchy-Schwarz and Poincaré inequalities as well as (13.4) and (13.5).

The Galerkin approximation of problem (13.3) is

$$
\text { find } u_{h} \in V_{h}: \quad a\left(u_{h}, v_{h}\right)=\left(f, v_{h}\right) \quad \forall v_{h} \in V_{h},
$$

where $\left\{V_{h}, h>0\right\}$ is a suitable family of subspaces of $\mathrm{H}_{0}^{1}(\Omega)$. By replicating the proof carried out above for the exact problem (13.3), the following estimates can be proved:

$$
\left\|u_{h}\right\|_{\mathrm{H}^{1}(\Omega)} \leq \frac{1}{\alpha}\|f\|_{\mathrm{L}^{2}(\Omega)}, \quad\left\|\nabla u_{h}\right\|_{\mathrm{L}^{2}(\Omega)} \leq \frac{C_{\Omega}}{\mu_{0}}\|f\|_{\mathrm{L}^{2}(\Omega)}
$$

These prove, in particular, that the gradient of the discrete solution (as well as that of the weak solution $u$ ) could be as large as $\mu_{0}$ is small.

Moreover, the Galerkin error inequality (4.10) gives

$$
\left\|u-u_{h}\right\|_{V} \leq \frac{M}{\alpha} \inf _{v_{h} \in V_{h}}\left\|u-v_{h}\right\|_{V}
$$

By the definitions of $\alpha$ and $M$ (see (13.6) and (13.10)), the upper-bounding constant $M / \alpha$ becomes as large (and, correspondingly, the estimate (13.12) meaningless) as the ratio $\|\mathbf{b}\|_{L^{\infty}(\Omega)} /\|\mu\|_{L^{\infty}(\Omega)}$ (resp. the ratio $\left.\|\sigma\|_{\mathrm{L}^{2}(\Omega)} /\|\mu\|_{L^{\infty}(\Omega)}\right)$ grows, which happens when the convective (resp. reactive) term dominates over diffusive one.

In such cases the Galerkin method can give inaccurate solutions, unless - as we will see $-$ an extremely small discretization step $h$ is used.

Remark 13.1. Problem (13.1) is known as the non-conservative form of the diffusiontransport(-reaction) problem, the conservative form being

$$
\begin{cases}L u=\operatorname{div}(-\mu \nabla u+\mathbf{b} u)+\sigma u=f & \text { in } \Omega \\ u=0 & \text { on } \partial \Omega\end{cases}
$$

If $\mathbf{b}$ is constant, the two formulations (13.1) and (13.13) are equivalent. The bilinear form associated to (13.13) is

$$
a(u, v)=\int_{\Omega}(\mu \nabla u-\mathbf{b} u) \cdot \nabla v d \Omega+\int_{\Omega} \sigma u v d \Omega \quad \forall u, v \in V
$$

It can be easily verified that the condition which ensures the coercivity of this bilinear form is

$$
\frac{1}{2} \operatorname{div}(\mathbf{b})+\sigma \geq 0 \quad \text { a.e. in } \Omega
$$

Under these assumptions, the conclusions drawn for problem (13.1) (and for its approximations) also hold for problem (13.13).

In order to evaluate more precisely the behaviour of the numerical solution provided by the Galerkin method, we analyze a one-dimensional problem.

\subsection{Analysis of a one-dimensional diffusion-transport problem}

Let us consider the following one-dimensional diffusion-transport problem

$$
\begin{cases}-\mu u^{\prime \prime}+b u^{\prime}=0, & 0<x<1 \\ u(0)=0, & u(1)=1\end{cases}
$$

$\mu$ and $b$ being two positive constants.

Its weak formulation is

$$
\text { find } u \in \mathrm{H}^{1}(0,1): \quad a(u, v)=0 \quad \forall v \in \mathrm{H}_{0}^{1}(0,1)
$$

with $u(0)=0, u(1)=1$, and $a(u, v)=\int_{0}^{1}\left(\mu u^{\prime} v^{\prime}+b u^{\prime} v\right) d x$. Following what indicated in Sect. 3.2.2, we can reformulate (13.17) by introducing a suitable lifting (or extension) of the boundary data. In this particular case, we can choose $R_{g}=x$. Having then set $\stackrel{0}{u}=u-R_{g}=u-x$, we can reformulate (13.17) in the following way

$$
\text { find } \stackrel{\circ}{u} \in \mathrm{H}_{0}^{1}(0,1): \quad a(\dot{u}, v)=F(v) \quad \forall v \in \mathrm{H}_{0}^{1}(0,1)
$$

where $F(v)=-a(x, v)=-\int_{0}^{1} b v d x$ represents the contribution due to the data lifting. We define the global Péclet number as the ratio

$$
\mathbb{P e}_{g}=\frac{b L}{2 \mu},
$$

$L$ being the linear dimension of the domain ( 1 in our case). This ratio provides a measure of how the convective term dominates the diffusive one. As such it plays the same role as the Reynolds number in the Navier-Stokes equations, which we will see in Chapter 17. For a negative $b$, its absolute value should be used in the previous definition.

We start by computing the exact solution of such problem. Its associated characteristic equation

$$
-\mu \lambda^{2}+b \lambda=0
$$

has two roots, $\lambda_{1}=0$ and $\lambda_{2}=b / \mu$. The general solution is therefore

$$
u(x)=C_{1} e^{\lambda_{1} x}+C_{2} e^{\lambda_{2} x}=C_{1}+C_{2} e^{\frac{b}{\mu} x}
$$

By imposing the boundary conditions we find the constants $C_{1}$ and $C_{2}$, and therefore the solution

$$
u(x)=\frac{\exp \left(\frac{b}{\mu} x\right)-1}{\exp \left(\frac{b}{\mu}\right)-1}
$$

Using the Taylor expansion for the exponentials, if $b / \mu \ll 1$ we obtain

$$
u(x)=\frac{1+\frac{b}{\mu} x+\cdots-1}{1+\frac{b}{\mu}+\cdots-1} \simeq \frac{\frac{b}{\mu} x}{\frac{b}{\mu}}=x
$$

Thus, the solution lies near the straight line interpolating the boundary data (which is the solution corresponding to the case $b=0$ ).

Conversely, if $b / \mu \gg 1$ the exponentials are very large, hence

$$
u(x) \simeq \frac{\exp \left(\frac{b}{\mu} x\right)}{\exp \left(\frac{b}{\mu}\right)}=\exp \left(-\frac{b}{\mu}(1-x)\right)
$$

and the solution is close to zero on almost all of the interval, except for a neighborhood of the point $x=1$, where it tends to 1 exponentially. Such neighborhood has a width of the order of $\mu / b$ and is therefore very small: the solution exhibits a boundary layer of width $\mathscr{O}\left(\frac{\mu}{b}\right)$ in proximity of $x=1$ (see Fig. 13.1), where the derivative behaves like $b / \mu$, and is therefore unbounded if $\mu \rightarrow 0$.

Let us now suppose to use the Galerkin finite element method with piecewiselinear polynomials to approximate (13.17): find $u_{h} \in X_{h}^{1}$ s.t.

$$
\begin{cases}a\left(u_{h}, v_{h}\right)=0 & \forall v_{h} \in \dot{X}_{h}^{1} \\ u_{h}(0)=0, u_{h}(1)=1\end{cases}
$$



![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-319.jpg?height=350&width=430&top_left_y=116&top_left_x=240)

Fig. 13.1. Behaviour of the solution of problem (13.16) when varying the ratio $b / \mu$. For completeness, we also highlight the solutions relating to the case where $b$ is negative

where, denoting by $x_{i}$, for $i=0, \ldots M$, the vertices of the partition introduced on $(0,1)$, we have set, coherently with (4.14),

$$
\begin{gathered}
X_{h}^{r}=\left\{v_{h} \in C^{0}([0,1]):\left.v_{h}\right|_{\left[x_{i-1}, x_{i}\right]} \in \mathbb{P}_{r}, \quad i=1, \ldots, M\right\} \\
\dot{X}_{h}^{r}=\left\{v_{h} \in X_{h}^{r}: v_{h}(0)=v_{h}(1)=0\right\}
\end{gathered}
$$

for $r \geq 1$. Having chosen, for each $i=1, \ldots, M-1, v_{h}=\varphi_{i}$ (the i-th basis function of $X_{h}^{1}$ ), we have

$$
\int_{0}^{1} \mu u_{h}^{\prime} \varphi_{i}^{\prime} d x+\int_{0}^{1} b u_{h}^{\prime} \varphi_{i} d x=0
$$

Put differently, if we suppose the support of $\varphi_{i}$ to be equal to $\left[x_{i-1}, x_{i+1}\right]$ and writing

$$
\begin{gathered}
\mu\left[u_{i-1} \int_{x_{i-1}}^{x_{i}} \varphi_{i-1}^{\prime} \varphi_{i}^{\prime} d x+u_{i} \int_{x_{i-1}}^{x_{i+1}}\left(\varphi_{i}^{\prime}\right)^{2} d x+u_{i+1} \int_{x_{i}}^{x_{i+1}} \varphi_{i+1}^{\prime} \varphi_{i}^{\prime} d x\right] \\
+b\left[u_{i-1} \int_{x_{i-1}}^{x_{i}} \varphi_{i-1}^{\prime} \varphi_{i} d x+u_{i} \int_{i-1}^{x_{i+1}} \varphi_{i}^{\prime} \varphi_{i} d x+u_{i+1} \int_{x_{i}}^{x_{i+1}} \varphi_{i+1}^{\prime} \varphi_{i} d x\right]=0,
\end{gathered}
$$
$u_{h}=\sum_{j=1}^{M-1} u_{j} \varphi_{j}(x)$, we have

$\forall i=1, \ldots, M-1$. If the partition is uniform, that is $x_{0}=0$ and $x_{i}=x_{i-1}+h$, with $i=1, \ldots, M$, observing that $\varphi_{i}^{\prime}(x)=\frac{1}{\hbar}$ if $x_{i-1}<x<x_{i}, \varphi_{i}^{\prime}(x)=-\frac{1}{\hbar}$ if $x_{i}<x<x_{i+1}$, for $i=1, \ldots, M-1$, we obtain

$$
\mu\left(-u_{i-1} \frac{1}{h}+u_{i} \frac{2}{h}-u_{i+1} \frac{1}{h}\right)+b\left(-u_{i-1} \frac{1}{h} \frac{h}{2}+u_{i+1} \frac{1}{h} \frac{h}{2}\right)=0
$$

that is

$$
\frac{\mu}{h}\left(-u_{i-1}+2 u_{i}-u_{i+1}\right)+\frac{1}{2} b\left(u_{i+1}-u_{i-1}\right)=0, \quad i=1, \ldots, M-1
$$

Rearranging the terms we find

$$
\left(\frac{b}{2}-\frac{\mu}{h}\right) u_{i+1}+\frac{2 \mu}{h} u_{i}-\left(\frac{b}{2}+\frac{\mu}{h}\right) u_{i-1}=0, \quad i=1, \ldots, M-1
$$

Dividing by $\mu / h$ and defining the local (or "grid") Péclet number

$$
\mathbb{P e}=\frac{|b| h}{2 \mu}
$$

we finally have

$$
(\mathbb{P e}-1) u_{i+1}+2 u_{i}-(\mathbb{P e}+1) u_{i-1}=0, \quad i=1, \ldots, M-1
$$

This is a linear difference equation that admits exponential solutions of the form $u_{i}=$ $\rho^{i}$ (see [QSS07]). Replacing such expression into (13.23), we obtain

$$
(\mathbb{P e}-1) \rho^{2}+2 \rho-(\mathbb{P e}+1)=0
$$

from which we get the two roots

$$
\rho_{1,2}=\frac{-1 \pm \sqrt{1+\mathbb{P e}^{2}-1}}{\mathbb{P e}-1}=\left\{\begin{array}{l}
(1+\mathbb{P e}) /(1-\mathbb{P e}) \\
1
\end{array}\right.
$$

Thanks to the linearity of (13.23), the general solution of such equation takes the form

$$
u_{i}=A_{1} \rho_{1}^{i}+A_{2} \rho_{2}^{i}
$$

with $A_{1}$ and $A_{2}$ two arbitrary constants. By imposing the boundary conditions $u_{0}=0$ and $u_{M}=1$, we find

$$
A_{1}=-A_{2} \text { and } A_{2}=\left(1-\left(\frac{1+\mathbb{P e}}{1-\mathbb{P e}}\right)^{M}\right)^{-1}
$$

To conclude, the solution of problem (13.20) has the following nodal values

$$
u_{i}=\frac{1-\left(\frac{1+\mathbb{P e}}{1-\mathbb{P e}}\right)^{i}}{1-\left(\frac{1+\mathbb{P e}}{1-\mathbb{P e}}\right)^{M}}, \quad i=0, \ldots, M
$$

We observe that, if $\mathbb{P e}>1$, the term within brackets is negative and the approximate solution becomes oscillatory, whereas the exact solution that is monotone! This phenomenon is displayed in Fig. 13.2 where the solution of (13.23), for different values of 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-321.jpg?height=334&width=423&top_left_y=113&top_left_x=247)

Fig. 13.2. Finite element solution of the diffusion-transport problem (13.16) with $\mathbb{P e}_{g}=50$ for different values of the local Péclet number

the local Péclet number, is compared to the exact solution for a case where the global Péclet number is equal to $50$. As it can be observed, the higher the Péclet number gets, the more the behaviour of the approximate solution differs from that of the exact solution, with oscillations that become more and more noticeable in proximity of the boundary layer.

The most obvious remedy to this misbehaviour would be to choose a sufficiently small grid-size $h$, in order to ensure $\mathbb{P e}<1$. However, this strategy is not always convenient: for instance, if $b=1$ and $\mu=1 / 5000$, we should take $h<1 / 2500$, that is introduce at least 2500 intervals on $(0,1) !$ In particular, such strategy would require an unreasonably high number of nodal points for boundary-value problems in several dimensions. A more suitable remedy consists in using an a-priori adaptive procedure that refines the grid only in proximity of the boundary layer. Several strategies are availabel for this purpose. Among the better known, we mention the so-called type B (for Bakhvâlov) or type S (for Shishkin) grids. See e.g. [GRS07].

Alternative grid adaptive strategies, both a-priori and a-posteriori, especially useful for multidimensional problems, are those described in Sect. 4.6.

\subsection{Analysis of a one-dimensional diffusion-reaction problem}

Let us now consider a one-dimensional diffusion-reaction problem:

$$
\begin{cases}-\mu u^{\prime \prime}+\sigma u=0, & 0<x<1 \\ u(0)=0, & u(1)=1\end{cases}
$$

with $\mu$ and $\sigma$ positive constants, whose solution is

$$
u(x)=\frac{\sinh (\alpha x)}{\sinh (\alpha)}=\frac{e^{\alpha x}-e^{-\alpha x}}{e^{\alpha}-e^{-\alpha}}, \text { with } \alpha=\sqrt{\sigma / \mu}
$$

Also in this case, if $\sigma / \mu \gg 1$ there is a boundary layer for $x \rightarrow 1$, with thickness of order $\sqrt{\mu / \sigma}$, where the first derivative becomes unbounded for $\mu \rightarrow 0$ (note, for instance, the exact solution for the case displayed in Fig. 13.3). Also in this case, it is interesting to define the global Péclet number, which takes the form

$$
\mathbb{P e}_{g}=\frac{\sigma L^{2}}{6 \mu}
$$

$L$ still being the linear dimension of the domain ( 1 in our case).

The Galerkin finite element approximation of $(13.24)$ reads

$$
\text { find } u_{h} \in X_{h}^{r} \text { such that } a\left(u_{h}, v_{h}\right)=0 \quad \forall v_{h} \in \dot{X}_{h}^{r} \text {, }
$$

for $r \geq 1$, with $u_{h}(0)=0$ and $u_{h}(1)=1$ and $a\left(u_{h}, v_{h}\right)=\int_{0}^{1}\left(\mu u_{h}^{\prime} v_{h}^{\prime}+\sigma u_{h} v_{h}\right) d x$. Equivalently, by setting $\stackrel{\circ}{u}_{h}=u_{h}-x$, and $F\left(v_{h}\right)=-a\left(x, v_{h}\right)=-\int_{0}^{1} \sigma x v_{h} d x$, we have

$$
\text { find } \stackrel{\circ}{u}_{h} \in V_{h} \text { such that } a\left(\dot{u}_{h}, v_{h}\right)=F\left(v_{h}\right) \quad \forall v_{h} \in V_{h},
$$

with $V_{h}=\dot{X}_{h}^{r}$. For the sake of simplicity, let us consider problem (13.25) with piecewise linear elements (that is $r=1$ ) on a uniform partition. The equation associated to the generic basis function $v_{h}=\varphi_{i}, i=1, \ldots, M-1$, is

$$
\int_{0}^{1} \mu u_{h}^{\prime} \varphi_{i}^{\prime} d x+\int_{0}^{1} \sigma u_{h} \varphi_{i} d x=0
$$

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-322.jpg?height=332&width=426&top_left_y=822&top_left_x=245)

Fig. 13.3. Comparison between the numerical solution and the exact solution of the diffusionreaction problem (13.24) with $\mathbb{P e}_{g}=200$. The numerical solution has been obtained using the Galerkin-linear finite elements method on uniform grids By carrying out our computation in a similar way to what we did in the previous section, and observing that

$$
\int_{x_{i-1}}^{x_{i}} \varphi_{i-1} \varphi_{i} d x=\frac{h}{6}, \quad \int_{x_{i-1}}^{x_{i+1}} \varphi_{i}^{2} d x=\frac{2}{3} h, \quad \int_{x_{i}}^{x_{i+1}} \varphi_{i} \varphi_{i+1} d x=\frac{h}{6}
$$

we obtain

$$
\mu\left(-u_{i-1} \frac{1}{h}+u_{i} \frac{2}{h}-u_{i+1} \frac{1}{h}\right)+\sigma\left(u_{i-1} \frac{h}{6}+u_{i} \frac{2}{3} h+u_{i+1} \frac{h}{6}\right)=0
$$

that is

$$
\left(\frac{h}{6} \sigma-\frac{\mu}{h}\right) u_{i+1}+\left(\frac{2}{3} \sigma h+\frac{2 \mu}{h}\right) u_{i}+\left(\frac{h}{6} \sigma-\frac{\mu}{h}\right) u_{i-1}=0
$$

Dividing by $\mu / h$ and defining the following local Péclet number

$$
\mathbb{P e}=\frac{\sigma h^{2}}{6 \mu}
$$

we finally have

$$
(\mathbb{P e}-1) u_{i+1}+2(1+2 \mathbb{P e}) u_{i}+(\mathbb{P e}-1) u_{i-1}=0, \quad i=1, \ldots, M-1
$$

This three-term difference equation admits the following solutions for each $i=$ $0, \ldots, M$

$$
u_{i}=\frac{\left[\frac{1+2 \mathbb{P e}+\sqrt{3 \mathbb{P e}(\mathbb{P e}+2)}}{1-\mathbb{P e}}\right]^{i}-\left[\frac{1+2 \mathbb{P e}-\sqrt{3 \mathbb{P e}(\mathbb{P e}+2)}}{1-\mathbb{P e}}\right]^{i}}{\left[\frac{1+2 \mathbb{P e}+\sqrt{3 \mathbb{P e}(\mathbb{P e}+2)}}{1-\mathbb{P e}}\right]^{M}-\left[\frac{1+2 \mathbb{P} e-\sqrt{3 \mathbb{P e}(\mathbb{P e}+2)}}{1-\mathbb{P e}}\right]^{M}}
$$

again oscillatory when $\mathbb{P e}>1$.

The problem is therefore critical when $\frac{\sigma}{\mu} \gg 1$, that is when the diffusion coefficient is very small with respect to the reaction one (see the example reported in Fig. 13.3).

\section{$13.4$ Finite elements and finite differences (FD)}

We want to analyze the behaviour of the finite difference method (FD, in short) applied to the solution of diffusion-transport and diffusion-reaction problems, and highlight analogies and differences with the finite element method (FE, in short). We will limit ourselves to the one-dimensional case and we will consider a uniform mesh.

Let us consider problem (13.16) once more and let us approximate it via finite differences. In order to generate a local discretization error of the same magnitude for both terms, we will approximate the derivatives by using the following centred incremental ratios:

$$
\begin{aligned}
u^{\prime}\left(x_{i}\right) &=\frac{u\left(x_{i+1}\right)-u\left(x_{i-1}\right)}{2 h}+\mathscr{O}\left(h^{2}\right), & i=1, \ldots, M-1 \\
u^{\prime \prime}\left(x_{i}\right) &=\frac{u\left(x_{i+1}\right)-2 u\left(x_{i}\right)+u\left(x_{i-1}\right)}{h^{2}}+\mathscr{O}\left(h^{2}\right), & i=1, \ldots, M-1
\end{aligned}
$$

In both cases, as highlighted, the remainder is an infinitesimal with respect to the step size $h$, as it can be easily proven by invoking the truncated Taylor series (see, e.g., [QSS07]). By replacing in (13.16) the exact derivatives with these incremental ratios (thus ignoring the infinitesimal error), we find the following scheme

$$
\left\{\begin{array}{l}
-\mu \frac{u_{i+1}-2 u_{i}+u_{i-1}}{h^{2}}+b \frac{u_{i+1}-u_{i-1}}{2 h}=0, \quad i=1, \ldots, M-1 \\
u_{0}=0, \quad u_{M}=1
\end{array}\right.
$$

For each $i$, the unknown $u_{i}$ provides an approximation for the nodal value $u\left(x_{i}\right)$. Multiplying by $h$, we obtain the same equation (13.21) obtained using the finite element method with piecewise linear polynomials on the same uniform grid.

Let us now consider the diffusion-reaction problem (13.24). Proceeding in an analogous way, its approximation using finite differences yields

$$
\left\{\begin{array}{l}
-\mu \frac{u_{i+1}-2 u_{i}+u_{i-1}}{h^{2}}+\sigma u_{i}=0, \quad i=1, \ldots, M-1 \\
u_{0}=0, \quad u_{M}=1 .
\end{array}\right.
$$

The above equation is different from (13.27), which was obtained using linear finite elements: instead the reaction term, appearing in (13.32) with the diagonal contribution $\sigma u_{i}$, yields in (13.27) the sum of three different contributions

$$
\sigma\left(u_{i-1} \frac{h}{6}+u_{i} \frac{2}{3} h+u_{i+1} \frac{h}{6}\right)
$$

Hence the two methods $\mathrm{FE}$ and $\mathrm{FD}$ are not equivalent in this case. We observe that the solution obtained using the FD scheme (13.32) does not display oscillations, whichever value is chosen for the discretization step $h$. As a matter of fact, the solution of $(13.32)$ is

$$
u_{i}=\left(\rho_{1}^{M}-\rho_{2}^{M}\right)^{-1}\left(\rho_{1}^{i}-\rho_{2}^{i}\right)
$$

with

$$
\rho_{1,2}=\frac{\gamma}{2} \pm\left(\frac{\gamma^{2}}{4}-1\right)^{\frac{1}{2}} \text { and } \gamma=2+\frac{\sigma h^{2}}{\mu}
$$

The i-th powers now have a positive basis, guaranteeing a monotone behaviour of the sequence $\left\{u_{i}\right\}$. This differs from what we have seen in Sect. $13.3$ for the $\mathrm{FE}$, for which it is necessary to choose $h \leq \sqrt{\frac{6 \mu}{\sigma}}$ to guarantee that the local Péclet number $(13.28)$ is less than 1. See the example reported in Fig. 13.4 for a comparison between a finite element approximation and a finite difference approximation. 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-325.jpg?height=331&width=425&top_left_y=113&top_left_x=244)

Fig. 13.4. Comparison between the numerical solutions of the one-dimensional diffusiontransport equation (13.24) with $\mathbb{P e}_{g}=2000$ obtained using the Galerkin-linear finite element method (FEM) and the finite difference method (FD), for different values of the local Péclet number

\subsection{The mass-lumping technique}

In the case of the reaction-diffusion problem, we can obtain the same result as with finite differences by using linear finite elements, provided that we resort to the socalled mass-lumping technique, thanks to which the mass matrix

$$
\mathrm{M}=\left(m_{i j}\right), \quad m_{i j}=\int_{0}^{1} \varphi_{j} \varphi_{i} d x
$$

which is tridiagonal, is approximated using a diagonal matrix $\mathrm{M}_{L}$, called condensed or lumped matrix. To this end we use the following trapezoidal quadrature formula on each interval $\left(x_{i}, x_{i+1}\right)$, for each $i=0, \ldots, M-1$

$$
\int_{x_{i}}^{x_{i+1}} f(x) d x \simeq \frac{h}{2}\left(f\left(x_{i}\right)+f\left(x_{i+1}\right)\right)
$$

Thanks to the properties of finite element basis functions, we then find:

$$
\begin{aligned}
&\int_{x_{i-1}}^{x_{i}} \varphi_{i-1} \varphi_{i} d x & \simeq \frac{h}{2}\left[\varphi_{i-1}\left(x_{i-1}\right) \varphi_{i}\left(x_{i-1}\right)+\varphi_{i-1}\left(x_{i}\right) \varphi_{i}\left(x_{i}\right)\right]=0 \\
&\int_{x_{i-1}}^{x_{i+1}} \varphi_{i}^{2} d x & =2 \int_{x_{i-1}}^{x_{i}} \varphi_{i}^{2} d x \simeq 2 \frac{h}{2}\left[\varphi_{i}^{2}\left(x_{i-1}\right)+\varphi_{i}^{2}\left(x_{i}\right)\right]=h \\
&\int_{x_{i}}^{x_{i+1}} \varphi_{i} \varphi_{i+1} d x & \simeq \frac{h}{2}\left[\varphi_{i}\left(x_{i}\right) \varphi_{i+1}\left(x_{i}\right)+\varphi_{i}\left(x_{i+1}\right) \varphi_{i+1}\left(x_{i+1}\right)\right]=0
\end{aligned}
$$

Using the previous formulae to approximate the mass matrix coefficients, we get to the following diagonal matrix $\mathrm{M}_{L}$ whose elements are the sums of the elements of each row of $\mathrm{M}$, i.e.

$$
\mathrm{M}_{L}=\operatorname{diag}\left(\widetilde{m}_{i i}\right), \quad \text { with } \quad \widetilde{m}_{i i}=\sum_{j=i-1}^{i+1} m_{i j}
$$

Note that, thanks to the following partition of unity property of the basis functions

$$
\sum_{j=0}^{M} \varphi_{j}(x)=1 \quad \forall x \in[0,1]
$$

the elements of $\mathrm{M}_{L}$ take the following expression on the interval $[0,1]$

$$
\tilde{m}_{i i}=\int_{0}^{1} \varphi_{i} d x, \quad i=0, \ldots, M
$$

Their values are reported in Exercise 3 for finite elements of degree $1,2,3$. If the terms of order zero are replaced in the following way

$$
\int_{0}^{1} \sigma u_{h} \varphi_{i} d x=\sigma \sum_{j=1}^{M-1} u_{j} \int_{0}^{1} \varphi_{j} \varphi_{i} d x=\sigma \sum_{j=1}^{M-1} m_{i j} u_{j} \simeq \sigma \tilde{m}_{i i} u_{i}
$$

the finite element problem produces solutions coinciding with those of finite differences, hence monotone solutions for each value of $h$. Moreover, replacing $\mathrm{M}$ with $\mathrm{M}_{L}$ does not reduce the order of accuracy of the method.

The process of mass lumping (13.33) can be generalized to the two-dimensional case when linear elements are used. For quadratic finite elements, instead, the abovementioned procedure consisting in summing by rows would generate a singular mass matrix $\mathrm{M}_{L}$ (see Example 13.1). An alternative diagonalization strategy consists in using the matrix $\mathrm{M}=\operatorname{diag}\left(\widehat{m}_{i i}\right)$ with elements

$$
\widehat{m}_{i i}=\frac{m_{i i}}{\sum_{j} m_{j j}}
$$

In the one-dimensional case, for linear and quadratic finite elements, the matrices $\widehat{\mathrm{M}}$ and $\mathrm{M}_{L}$ coincide, while they differ for cubic elements (see Exercise 3). The matrix $\mathrm{M}$ is non-singular also for Lagrangian finite elements of high order, while it can turn out to be singular when using non-Lagrangian finite elements, for instance when using hierarchical bases. In the latter case, we resort to more sophisticated mass-lumping procedures. Indeed, a number of diagonalization techniques able to generate non-singular matrices have been elaborated also for finite elements of high degree. See for example [CJRT01]. Example 13.1. The mass matrix for the $\mathbb{P}_{2}$ finite elements on the reference triangle with vertices $(0,0),(1,0)$ and $(0,1)$ is given by

$$
\mathrm{M}=\frac{1}{180}\left[\begin{array}{rrrrrr}
6 & -1 & -1 & 0 & -4 & 0 \\
-1 & 6 & -1 & 0 & 0 & -4 \\
-1 & -1 & 6 & -4 & 0 & 0 \\
0 & 0 & -4 & 32 & 16 & 16 \\
-4 & 0 & 0 & 16 & 32 & 16 \\
0 & -4 & 0 & 16 & 16 & 32
\end{array}\right]
$$

while the lumped mass matrices are given by

$$
\begin{aligned}
&\mathrm{M}_{L}=\frac{1}{180} \operatorname{diag}(000606060) \\
&\widehat{\mathbf{M}}=\frac{1}{114} \operatorname{diag}(666323232)
\end{aligned}
$$

As it can be noticed the matrix $\mathrm{M}_{L}$ is singular.

The mass-lumping technique is also used in other contexts, for instance in the solution of parabolic problems (see Chap. 5) when finite-element spatial discretizations and finite-difference explicit temporal discretizations (e.g., the forward-Euler method) are used. In such case, lumping the mass matrix that arises from the discretization of the temporal derivative can conduct to the solution of a diagonal system, with corresponding reduction of the computational cost.

\section{$13.6$ Decentred FD schemes and artificial diffusion}

The comparative analysis with finite differences allowed us to find a remedy to the oscillatory behaviour of finite element solutions in the case of a diffusion-reaction problem. We now wish to find a remedy for the case of the diffusion-transport problem (13.16) as well.

Let us consider finite differences. The oscillations in the numerical solution arise from the fact that we use a centred finite difference (CFD) scheme for the discretization of the transport term. Since the latter is non-symmetric, this suggests to discretize the first derivative at a point $x_{i}$ with a decentred incremental ratio where the value at $x_{i-1}$ intervenes if the field is positive, and at $x_{i+1}$ in the opposite case.

This technique is called upwinding and the resulting scheme, called upwind scheme (FDUP, in short) in the case $b>0$ is written as

$$
-\mu \frac{u_{i+1}-2 u_{i}+u_{i-1}}{h^{2}}+b \frac{u_{i}-u_{i-1}}{h}=0, \quad i=1, \ldots, M-1
$$

(See Fig. $13.5$ for an example of application of the upwind scheme). The price to pay is a reduction of the order of convergence, because the decentred incremental ratio 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-328.jpg?height=333&width=423&top_left_y=112&top_left_x=247)

Fig. 13.5. Solution obtained using the centred (CFD) and upwind (UPFD) finite difference scheme for the one-dimensional diffusion-transport equation (13.16) with $\mathbb{P e}_{g}=50$ and two different local PÃl'clet numbers. Also in the presence of high local Péclet numbers, one can notice the stabilizing effect of the artificial diffusion introduced by the upwind scheme, inevitably accompanied by a loss of accuracy

introduces a local discretization error $\mathscr{O}(h)$ as opposed to $\mathscr{O}\left(h^{2}\right)$ (see $\left.(13.30)\right)$, in the CFD case.

We now observe that

$$
\frac{u_{i}-u_{i-1}}{h}=\frac{u_{i+1}-u_{i-1}}{2 h}-\frac{h}{2} \frac{u_{i+1}-2 u_{i}+u_{i-1}}{h^{2}}
$$

that is, the decentred incremental ratio to approximate the first derivative can be written as the sum of a centred incremental ratio plus a term proportional to the discretization of the second derivative, still with a centred incremental ratio. Thus, the upwind scheme can be reinterpreted as a centred finite difference scheme where an artificial diffusion term proportional to $h$ has been introduced. As a matter of fact, (13.35) is equivalent to

$$
-\mu_{h} \frac{u_{i+1}-2 u_{i}+u_{i-1}}{h^{2}}+b \frac{u_{i+1}-u_{i-1}}{2 h}=0, \quad i=1, \ldots, M-1
$$

where $\mu_{h}=\mu(1+\mathbb{P e}), \mathbb{P e}$ being the local Péclet number introduced in (13.22). Scheme (13.36) corresponds to the discretization using a CFD scheme of the perturbed problem

$$
-\mu_{h} u^{\prime \prime}+b u^{\prime}=0
$$

The viscosity "correction" $\mu_{h}-\mu=\mu \mathbb{P e}=\frac{b h}{2}$ is called numerical viscosity or artificial viscosity. The new local Péclet number associated to the scheme (13.36) is

$$
\mathbb{P e}^{*}=\frac{b h}{2 \mu_{h}}=\frac{\mathbb{P e}}{(1+\mathbb{P e})}
$$

so $\mathbb{P e}^{*}<1$ for all possible values of $h>0$. As we will see in the next section, this interpretation allows to extend the upwind technique to finite elements, and also to the two-dimensional case where, incidentally, the notion of decentred differentiation is not obvious.

More generally, in a CFD scheme of the form (13.36) we can use the following numerical viscosity coefficient

$$
\mu_{h}=\mu(1+\phi(\mathbb{P e}))
$$

where $\phi$ is a suitable function of the local Péclet number that must satisfy the property $\lim _{t \rightarrow 0+} \phi(t)=0$. It can be easily observed that if $\phi=0$, we obtain the $\mathrm{CFD}$ method

(13.31), while if $\phi(t)=t$, we obtain the upwind UPFD method (13.35) (or (13.36)).

Different choices of $\phi$ lead to different schemes. For instance, setting

$$
\phi(t)=t-1+B(2 t)
$$

where $B$ is the so-called Bernoulli function defined as

$$
B(t)=\frac{t}{e^{t}-1} \quad \text { if } t>0, \quad \text { and } \quad B(0)=1
$$

we obtain the exponential fitting scheme, generally attributed to Scharfetter and Gummel or to Iljin (in fact, it was originally introduced by Allen and Southwell [AS55]). See also Sect. 13.8.7 for more on this method.

Having denoted by $\phi^{U}$, resp. $\phi^{S G}$, the two functions determined by the choices $\phi(t)=t$ and $\phi(t)=t-1-B(2 t)$, we observe that $\phi^{S G} \simeq \phi^{U}$ if $\mathbb{P e} \rightarrow+\infty$, while $\phi^{S G}=\mathscr{O}\left(\mathbb{P e}^{2}\right)$ and $\phi^{U}=\mathscr{O}(\mathbb{P e})$ if $\mathbb{P e} \rightarrow 0^{+}($see Fig. 13.6 $)$

It can be verified that for each given $\mu$ and $b$ the Scharfetter-Gummel scheme is a second order scheme (with respect to $h$ ) and, because of this, it is sometimes called upwind scheme with optimal viscosity. In fact, it can also be verified, in the case where $f$ is constant $-$ more generally, it is sufficient that $f$ is constant on each interval

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-329.jpg?height=288&width=356&top_left_y=909&top_left_x=279)

Fig. 13.6. The functions $\phi^{U}$ (solid line) and $\phi^{S G}$ (etched line) versus the local Péclet number 

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-330.jpg?height=289&width=400&top_left_y=126&top_left_x=255)

Fig. 13.7. Comparison between the exact solution and those obtained by the upwind scheme (UPFD) and the Scharfetter and Gummel one (SGFD) in the case where $\mathbb{P e}_{g}=50$

$\left.\left[x_{i}, x_{i+1}\right]\right)-$ that the numerical solution produced by this scheme is nodally exact. This means that it coincides exactly with the solution $u$ at each discretization node inside the interval $(0,1)$, that is we have

$$
u_{i}=u\left(x_{i}\right) \text { for } i=1, \ldots, M-1
$$

independently of the choice of $h$ (see Fig. 13.7).

We observe that the local Péclet number associated with the coefficient (13.38) is

$$
\mathbb{P e}^{*}=\frac{b h}{2 \mu_{h}}=\frac{\mathbb{P e}}{(1+\phi(\mathbb{P e}))}
$$

and is therefore always less than 1 , for each value of $h$.

Remark 13.2. The matrix associated with the upwind and the exponential fitting scheme is an M-matrix regardless of the value of $h$; hence, the numerical solution has a monotone behaviour (see [QSS07, Chap. 1]).

\subsection{Eigenvalues of the diffusion-transport equation}

Let us consider the operator $L u=-\mu u^{\prime \prime}+b u^{\prime}$ associated to problem (13.16) on a generic interval $(\alpha, \beta)$. Its eigenvalues $\lambda$ solve $L u=\lambda u, \alpha<x<\beta, u(\alpha)=u(\beta)=0$, $u$ being an eigenfunction. Such eigenvalues, in general, will be complex because of the presence of the first-order term $b u^{\prime}$. Supposing $\mu>0$ constant (and $b$ variable, a priori), we have

$$
\int_{\alpha}^{\beta}|u|^{2} d x \operatorname{Re}(\lambda)=\int_{\alpha}^{\beta} L u \bar{u} d x=\mu \int_{\alpha}^{\beta}\left|u^{\prime}\right|^{2} d x-\frac{1}{2} \int_{\alpha}^{\beta} b^{\prime}|u|^{2} d x
$$

It can be inferred that if $\mu$ is small and $b^{\prime}$ is strictly positive, the real part of $\lambda$ is not necessarily positive. However, thanks to the Poincaré inequality (2.13) we have

$$
\int_{\alpha}^{\beta}\left|u^{\prime}\right|^{2} d x \geq C_{\alpha, \beta} \int_{\alpha}^{\beta}|u|^{2} d x
$$

with $C_{\alpha, \beta}$ being a positive constant depending on $\beta-\alpha$; we deduce from (13.40) that

$$
\operatorname{Re}(\lambda) \geq C_{\alpha, \beta} \mu-\frac{1}{2} b_{\max }^{\prime}
$$

where $b_{\max }^{\prime}=\max _{\alpha \leq s \leq \beta} b^{\prime}(s)$. In particular, let us observe that

$$
\operatorname{Re}(\lambda)>0 \quad \text { if } b \text { is constant or if } \quad b^{\prime}(x) \leq 0 \quad \forall x \in[\alpha, \beta]
$$

The same kind of lower bound can be obtained for the eigenvalues associated to the Galerkin-finite element approximation of the problem at hand. The latter are the solution of the problem

$$
\text { find } \lambda_{h} \in \mathbb{C}, u_{h} \in V_{h}: \int_{\alpha}^{\beta} \mu u_{h}^{\prime} v_{h}^{\prime} d x+\int_{\alpha}^{\beta} b u_{h}^{\prime} v_{h} d x=\lambda_{h} \int_{\alpha}^{\beta} u_{h} v_{h} d x \forall v_{h} \in V_{h}
$$

where $V_{h}=\left\{v_{h} \in X_{h}^{r}: v_{h}(\alpha)=v_{h}(\beta)=0\right\}$. To prove this, it suffices to take again $v_{h}=\bar{u}_{h}$ in (13.42) and proceed as previously.

We can instead obtain an upper bound by choosing again $v_{h}=\bar{u}_{h}$ in (13.42) and taking the modulus in both members:

$$
\left|\lambda_{h}\right| \leq \frac{\mu\left\|u_{h}^{\prime}\right\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2}+\|b\|_{\mathrm{L}^{\infty}(\alpha, \beta)}\left\|u_{h}^{\prime}\right\|_{\mathrm{L}^{2}(\alpha, \beta)}\left\|u_{h}\right\|_{\mathrm{L}^{2}(\alpha, \beta)}}{\left\|u_{h}\right\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2}}
$$

By using the inverse inequality (4.52) in the one-dimensional case

$$
\exists C_{I}=C_{I}(r)>0: \forall v_{h} \in X_{h}^{r}, \quad\left\|v_{h}^{\prime}\right\|_{\mathrm{L}^{2}(\alpha, \beta)} \leq C_{I} h^{-1}\left\|v_{h}\right\|_{\mathrm{L}^{2}(\alpha, \beta)}
$$

we easily find that

$$
\left|\lambda_{h}\right| \leq \mu C_{I}^{2} h^{-2}+\|b\|_{\mathrm{L}^{\infty}(\alpha, \beta)} C_{I} h^{-1}
$$

If, instead, we use a Legendre G-NI spectral approximation of the same problem on the usual reference interval $(-1,1)$ (see Sect. 10.3), the eigenvalue problem takes the following form:

$$
\left\{\begin{array}{l}
\text { find } \lambda^{N} \in \mathbb{C}, u_{N} \in \mathbb{P}_{N}^{0}: \\
\left(\mu u_{N}^{\prime}, v_{N}^{\prime}\right)_{N}+\left(b u_{N}^{\prime}, v_{N}\right)_{N}=\lambda^{N}\left(u_{N}, v_{N}\right)_{N} \quad \forall v_{N} \in \mathbb{P}_{N}^{0}
\end{array}\right.
$$

with $\mathbb{P}_{N}^{0}$ now being the space of algebraic polynomials of degree $N$ vanishing at $x=$ $\pm 1$, and $(\cdot, \cdot)_{N}$ the discrete GLL scalar product defined in $(10.25)$. We will suppose, for simplicity, that $b$ is also constant. Taking $v_{N}=\bar{u}_{N}$, we obtain

$$
\operatorname{Re}\left(\lambda^{N}\right)=\frac{\mu\left\|u_{N}^{\prime}\right\|_{\mathrm{L}^{2}(-1,1)}^{2}}{\left\|u_{N}\right\|_{N}^{2}}
$$

and so $\operatorname{Re}\left(\lambda^{N}\right)>0$. Thanks to the Poincaré inequality (13.41) (which holds in the interval $(-1,1)$ with constant $C_{\alpha, \beta}=\pi^{2} / 4$ ), we obtain the lower bound

$$
\operatorname{Re}\left(\lambda^{N}\right)>\mu \frac{\pi^{2}}{4} \frac{\left\|u_{N}\right\|_{\mathrm{L}^{2}(-1,1)}^{2}}{\left\|u_{N}\right\|_{N}^{2}}
$$

As $u_{N}$ is a polynomial of degree at most $N$, thanks to $(10.54)$ we obtain

$$
\operatorname{Re}\left(\lambda^{N}\right)>\mu \frac{\pi^{2}}{12}
$$

Instead, using the following inverse inequality for algebraic polynomials

$$
\exists C>0: \forall v_{N} \in \mathbb{P}_{N}, \quad\left\|v_{N}^{\prime}\right\|_{L^{2}(-1,1)} \leq C N^{2}\left\|v_{N}\right\|_{L^{2}(-1,1)}
$$

(see [CHQZ06]) and once again (10.54), we find

$$
\operatorname{Re}\left(\lambda^{N}\right)<C \mu N^{4}
$$

In fact, if $N>1 / \mu$, we can prove that the moduli of the eigenvalues of the diffusiontransport problem (13.44) behave like those of the pure diffusion problem, that is (see Sect. $10.3 .2$ )

$$
C_{1} N^{-1} \leq\left|\lambda^{N}\right| \leq C_{2} N^{2}
$$

For proofs and more details, see [CHQZ06, Sect. 4.3.3].

\section{$13.8$ Stabilization methods}

The Galerkin method introduced in the previous sections provides a centred approximation of the transport term. A possible way to use a decentred approximation consists in choosing test functions $v_{h}$ in a different space from the one $u_{h}$ belong to: by doing so, we obtain a method called Petrov-Galerkin, for which the analysis based on the Céa lemma no longer holds. We will analyze this approach more in detail in Sect. 13.8.2. In this section we will deal instead with the methods of stabilized finite elements.

More precisely, instead of using the Galerkin finite element method (13.26) for the approximation of (13.12), we consider the generalized Galerkin method

$$
\text { find } \stackrel{\circ}{u}_{h} \in V_{h}: a_{h}\left(\dot{u}_{h}, v_{h}\right)=F_{h}\left(v_{h}\right) \quad \forall v_{h} \in V_{h},
$$

where

$$
a_{h}\left(\dot{u}_{h}, v_{h}\right)=a\left(\dot{u}_{h}, v_{h}\right)+b_{h}\left(\dot{u}_{h}, v_{h}\right) \text { and } F_{h}\left(v_{h}\right)=F\left(v_{h}\right)+G_{h}\left(v_{h}\right)
$$

The additional terms $b_{h}\left(\dot{u}_{h}, v_{h}\right)$ and $G_{h}\left(v_{h}\right)$ have the purpose of eliminating (or at least reducing) the numerical oscillations produced by the Galerkin method (when the grid is not fine enough) and are therefore named stabilization terms. The latter depend parametrically on $h$.

Remark 13.3. We want to point out that the term "stabilization" is in fact inexact. The Galerkin method is indeed already stable, in the sense of the continuity of the solution with respect to the data of problem (see what has been proved, e.g. in Sect. $13.1$ for problem (13.1)). In this case, stabilization must be understood as the aim of reducing (ideally, eliminating) the oscillations in the numerical solution when $\mathbb{P e}>1$.

Let us now see several possible ways to choose the stabilization terms.

\subsubsection{Artificial diffusion and decentred finite element schemes}

Based on what we have seen for finite differences, we can apply the Galerkin method to problem (13.16) (whose weak formulation is (13.18)) by replacing the viscosity coefficient $\mu$ with a new coefficient $\mu_{h}=\mu(1+\phi(\mathbb{P e}))$. This way, we end up adding to the original viscosity term $\mu$ an artificial (or numerical) viscosity equal to $\mu \phi(\mathbb{P e})$, which depends on the discretization step $h$ through the local Péclet number Pe.

This corresponds to choosing in (13.47) $G_{h}\left(v_{h}\right)=0$ and

$$
b_{h}\left(\dot{u}_{h}, v_{h}\right)=\mu \phi(\mathbb{P e}) \int_{0}^{1} \stackrel{u}_{h} v_{h}^{\prime} d x
$$

Since

$$
a_{h}\left(\dot{u}_{h}, \stackrel{\circ}{u}_{h}\right) \geq \mu_{h}\left|\stackrel{\circ}{u}_{h}\right|_{\mathrm{H}^{1}(\Omega)}^{2}
$$

and $\mu_{h} \geq \mu$, we can say that problem $(12.47)-(12.48)$ has a larger coercivity constant than the standard Galerkin formulation which corresponds to taking $a_{h}=a$ and $F_{h}=F$ in $(13.46)$.

The following result provides an a priori estimate of the error made by approximating the solution of problem (13.18) with that of $(13.46),(13.47),(13.48)$.

![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-333.jpg?height=265&width=722&top_left_y=968&top_left_x=97)

Proof. We can take advantage of Strang's lemma, previously introduced in Sect. $10.4 .1$, thanks to which we obtain

$$
\begin{aligned}
\left\|\stackrel{\circ}{u}-\stackrel{u}_{h}\right\|_{\mathrm{H}^{1}(\Omega)} & \leq \inf _{w_{h} \in V_{h}}\left\{\left(1+\frac{M}{\mu_{h}}\right)\left\|\stackrel{\circ}{u}-w_{h}\right\|_{\mathrm{H}^{1}(\Omega)}\right.\\
&\left.+\frac{1}{\mu_{h}} \sup _{v_{h} \in V_{h}, v_{h} \neq 0} \frac{\left|a\left(w_{h}, v_{h}\right)-a_{h}\left(w_{h}, v_{h}\right)\right|}{\left\|v_{h}\right\|_{\mathrm{H}^{1}(\Omega)}}\right\} .
\end{aligned}
$$

We choose $w_{h}=P_{h}^{r} \dot{u}$, the orthogonal projection of $\dot{u}$ on $V_{h}$ with respect to the scalar product $\int_{0}^{1} u^{\prime} v^{\prime} d x$ of $\mathrm{H}_{0}^{1}(\Omega)$, that is

$$
P_{h}^{r} \stackrel{\circ}{u} \in V_{h}: \int_{0}^{1}\left(P_{h}^{r} \stackrel{\circ}{u}-\stackrel{u}{u}\right)^{\prime} v_{h}^{\prime} d x=0 \quad \forall v_{h} \in V_{h}
$$

It can be proved that (see [QV94, Chap. 3])

$$
\left\|\left(P_{h}^{r} \stackrel{u}\right)^{\prime}\right\|_{L^{2}(\Omega)} \leq\left\|(\dot{u})^{\prime}\right\|_{\mathrm{L}^{2}(\Omega)} \quad \text { and } \quad\left\|P_{h}^{r} \stackrel{\circ}{u}-\stackrel{\circ}{u}\right\|_{\mathrm{H}^{1}(\Omega)} \leq C h^{r}\|\stackrel{\circ}{u}\|_{\mathrm{H}^{r+1}(\Omega)}
$$

$C$ being a constant independent of $h$. Thus, we can bound the first addendum of the right-hand side in (13.50) by $\left(C / \mu_{h}\right) h^{r}\|\stackrel{\circ}{u}\|_{\mathrm{H}^{r+1}(\Omega)}$. Now, thanks to (13.48), we obtain

$$
\frac{1}{\mu_{h}} \frac{\left|a\left(w_{h}, v_{h}\right)-a_{h}\left(w_{h}, v_{h}\right)\right|}{\left\|v_{h}\right\|_{\mathrm{H}^{1}(\Omega)}} \leq \frac{\mu}{\mu_{h}} \phi(\mathbb{P e}) \frac{1}{\left\|v_{h}\right\|_{\mathrm{H}^{1}(\Omega)}}\left|\int_{0}^{1} w_{h}^{\prime} v_{h}^{\prime} d x\right|
$$

Using the Cauchy-Schwarz inequality, and observing that

$$
\left\|v_{h}^{\prime}\right\|_{\mathrm{L}^{2}(\Omega)} \leq\left\|v_{h}\right\|_{\mathrm{H}^{1}(\Omega)}, \quad\left\|\left(P_{h}^{r} \stackrel{\circ}\right)^{\prime}\right\|_{\mathrm{L}^{2}(\Omega)} \leq\left\|P_{h}^{r} \stackrel{\circ}{u}\right\|_{\mathrm{H}^{1}(\Omega)} \leq\|\dot{u}\|_{\mathrm{H}^{1}(\Omega)},
$$

we obtain

$$
\frac{1}{\mu_{h}} \sup _{v_{h} \in V_{h}, v_{h} \neq 0} \frac{\left|a\left(P_{h}^{r} \dot{u}, v_{h}\right)-a_{h}\left(P_{h}^{r} \stackrel{u}, v_{h}\right)\right|}{\left\|v_{h}\right\|_{\mathrm{H}^{1}(\Omega)}} \leq \frac{\phi(\mathbb{P e})}{1+\phi(\mathbb{P e})}\|\dot{u}\|_{\mathrm{H}^{1}(\Omega)}
$$

Inequality (13.49) is therefore proved. Proof. We obtain (13.51) from (13.49) remembering that $\phi(\mathbb{P e}) \rightarrow 0$ for any given $\mu$ when $h \rightarrow 0$. To obtain (13.52) it is sufficient to observe that, in the upwind case, $\phi^{U}(\mathbb{P e})=\mathbb{P e}$, so

$$
\mu(1+\phi(\mathbb{P e}))=\mu+\frac{b}{2} h \quad \text { and } \quad \frac{\phi(\mathbb{P e})}{1+\phi(\mathbb{P e})}=\frac{h}{h+2 \mu / b}
$$

For the Scharfetter and Gummel method, $\phi^{S G}(\mathbb{P e}) \simeq \phi^{U}(\mathbb{P e})$ for a given $h$ and $\mu$ tending to 0 .

In particular, for a given $\mu$, the stabilized method generates an error that decays linearly in $h$ (irrespectively of the degree $r$ ) when using the upwind viscosity, while with an artificial viscosity of Scharfetter and Gummel type, the convergence rate becomes quadratic if $r \geq 2$. This result follows from estimate (13.51), recalling that $\phi^{U}(\mathbb{P e})=\mathscr{O}(h)$ while $\phi^{S G}(\mathbb{P e})=\mathscr{O}\left(h^{2}\right)$ for a fixed $\mu$ and for $h \rightarrow 0$

\subsubsection{The Petrov-Galerkin method}

An equivalent way to write the generalized Galerkin problem (13.46) with numerical viscosity is to reformulate it as a Petrov-Galerkin method, that is a method where the space of test functions is different from the space where the solution is sought. Precisely, the approximation takes the following form

$$
\text { find } \stackrel{\circ}{u}_{h} \in V_{h}: \quad a\left(\dot{u}_{h}, v_{h}\right)=F\left(v_{h}\right) \quad \forall v_{h} \in W_{h},
$$

where $W_{h} \neq V_{h}$, while the bilinear form $a(\cdot, \cdot)$ is the same as in the initial problem. It can be verified that in the case of linear finite elements, that is for $r=1$, problem (13.46)-(13.48) can be rewritten as (13.53), where $W_{h}$ is the space generated by the functions $\psi_{i}(x)=\varphi_{i}(x)+B_{i}^{\alpha}$ (see Fig. 13.8, right). Here the $B_{i}^{\alpha}=\alpha B_{i}(x)$ are the so-called bubble functions, with

$$
B_{i}(x)= \begin{cases}g\left(1-\frac{x-x_{i-1}}{h}\right), & x_{i-1} \leq x \leq x_{i} \\ -g\left(\frac{x-x_{i}}{h}\right), & x_{i} \leq x \leq x_{i+1} \\ 0 & \text { otherwise }\end{cases}
$$


![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-336.jpg?height=230&width=700&top_left_y=119&top_left_x=106)

Fig. 13.8. Example of a bubble function $B_{i}$ and of a basis function $\psi_{i}$ of the space $W_{h}$

and $g(\xi)=3 \xi(1-\xi)$, with $0 \leq \xi \leq 1$ (see Fig. 13.8, left) [ZT00]. In the case of upwind finite differences we have $\alpha=1$, while in the case of the Scharfetter-Gummel scheme we have $\alpha=\operatorname{coth}(\mathbb{P e})-1 / \mathbb{P e}$. Note that the test functions lose their symmetry feature (with respect to the usual piecewise linear basis functions) under the effect of the convective field.

\subsubsection{The artificial diffusion and streamline-diffusion methods in the two-dimensional case}

The upwind artificial-viscosity method can be generalized to the case where we consider a two- or a three-dimensional problem of the type (13.1). In such case, it will suffice to modify the Galerkin approximation (13.11) by adding to the bilinear form (13.2) a term like

$$
Q h \int_{\Omega} \nabla u_{h} \cdot \nabla v_{h} d \Omega \quad \text { for a chosen } Q>0
$$

which corresponds to adding the artificial diffusion term $-Q h \Delta u$ to the initial problem (13.1). The corresponding method is called upwind artificial diffusion. This way an additional diffusion is introduced, not only in the direction of the field $\mathbf{b}$, as one should rightly do in order to stabilize the oscillations generated by the Galerkin method, but also in the orthogonal direction, which is not at all necessary. For instance, if we consider the two-dimensional problem

$$
-\mu \Delta u+\frac{\partial u}{\partial x}=f \quad \text { in } \Omega, \quad u=0 \quad \text { on } \partial \Omega
$$

where the transport field is given by the vector $\mathbf{b}=[1,0]^{T}$, the artificial diffusion term we would add is

$$
-Q h \frac{\partial^{2} u}{\partial x^{2}} \quad \text { and not } \quad-Q h \Delta u=-Q h\left(\frac{\partial^{2} u}{\partial x^{2}}+\frac{\partial^{2} u}{\partial y^{2}}\right)
$$

More generally, we can add the following stabilization term

$$
-Q h \operatorname{div}[(\mathbf{b} \cdot \nabla u) \mathbf{b}]=-Q h \operatorname{div}\left(\frac{\partial u}{\partial \mathbf{b}} \mathbf{b}\right), \text { with } Q=|\mathbf{b}|^{-1}
$$

In the Galerkin problem the latter yields the following term

$$
b_{h}\left(u_{h}, v_{h}\right)=Q h\left(\mathbf{b} \cdot \nabla u_{h}, \mathbf{b} \cdot \nabla v_{h}\right)=Q h\left(\frac{\partial u_{h}}{\partial \mathbf{b}}, \frac{\partial v_{h}}{\partial \mathbf{b}}\right)
$$

The resulting discrete problem is therefore a modification of the Galerkin problem (13.11), called streamline-diffusion problem, and reads

$$
\text { find } u_{h} \in V_{h}: a_{h}\left(u_{h}, v_{h}\right)=\left(f, v_{h}\right) \quad \forall v_{h} \in V_{h}
$$

where

$$
a_{h}\left(u_{h}, v_{h}\right)=a\left(u_{h}, v_{h}\right)+b_{h}\left(u_{h}, v_{h}\right)
$$

Basically, we are adding a term proportional to the second derivative in the direction of the field $\mathbf{b}$ (also called streamline). Note that, in this case, the artificial viscosity coefficient is actually a tensor. As a matter of fact, the stabilization term $b_{h}(\cdot, \cdot)$ can be seen as the bilinear form associated to the operator $-\operatorname{div}\left(\boldsymbol{\mu}_{a} \nabla u\right)$ with $\left[\boldsymbol{\mu}_{a}\right]_{i j}=Q h b_{i} b_{j}$, $b_{i}$ being the $i$-th component of $\mathbf{b}$.

Although the term (13.55) is less diffusive than (13.54), the accuracy is only $\mathscr{O}(h)$ also for the streamline-diffusion method. More accurate stabilization methods are described in Sects. 13.8.6, $13.8 .8$ and 13.8.9. To introduce them we will need some definitions that we will anticipate in Sects. 13.8.4 and 13.8.5.

\subsubsection{Consistency and truncation error for the Galerkin and generalized Galerkin methods}

Let us consider a generalized Galerkin problem of the form (13.47), and replace $\stackrel{u}_{h}$ by $u_{h}$ to recover more familiar notations. Note that this formulation can refer to a problem in any spatial dimension. We define a functional of the variable $v_{h}$

$$
\tau_{h}\left(u ; v_{h}\right)=a_{h}\left(u, v_{h}\right)-F_{h}\left(v_{h}\right)
$$

whose norm

$$
\tau_{h}(u)=\sup _{v_{h} \in V_{h}, v_{h} \neq 0} \frac{\left|\tau_{h}\left(u ; v_{h}\right)\right|}{\left\|v_{h}\right\|_{V}}
$$

is called the truncation error associated to the generalized Galerkin method (13.46). In accordance with the definitions given in Sect. 1.2, the generalized Galerkin method is said to be consistent if $\lim _{h \rightarrow 0} \tau_{h}(u)=0$.

Moreover, we will say that it is strongly (or fully) consistent if the truncation error (13.57) is non-zero for each value of $h$.

The standard Galerkin method is strongly consistent, as seen in Chap. 4 , since

$$
\tau_{h}\left(u ; v_{h}\right)=a\left(u, v_{h}\right)-F\left(v_{h}\right)=0 \quad \forall v_{h} \in V_{h}
$$

Instead, the generalized Galerkin method is only consistent (in general) as long as $a_{h}-a$ and $F_{h}-F$ "tend to zero" when $h$ tends to zero, as guaranteed by Strang's lemma.

Concerning the upwind and streamline-diffusion methods, we have

$$
\begin{aligned}
\tau_{h}\left(u ; v_{h}\right) &=a_{h}\left(u, v_{h}\right)-F\left(v_{h}\right) \\
&=a_{h}\left(u, v_{h}\right)-a\left(u, v_{h}\right)=\left\{\begin{array}{l}
Q h\left(\nabla u, \nabla v_{h}\right) \quad(\text { Upwind }), \\
Q h\left(\frac{\partial u}{\partial \mathbf{b}}, \frac{\partial v_{h}}{\partial \mathbf{b}}\right) \quad \text { (Streamline-Diffusion) }
\end{array}\right.
\end{aligned}
$$

hence both are consistent but not strongly consistent. Remarkable instances of strongly consistent methods will be introduced and analyzed in Sect. 13.8.6

\subsubsection{Symmetric and skew-symmetric part of an operator}

Let $V$ be a Hilbert space and $V^{\prime}$ its dual. We will say that an operator $L: V \rightarrow V^{\prime}$ is symmetric if

$$
v^{\prime}\langle L u, v\rangle_{V}={ }_{V}\langle u, L v\rangle_{V^{\prime}} \quad \forall u, v \in V,
$$

skew-symmetric when

$$
{ }_{V^{\prime}}\langle L u, v\rangle_{V}=-_{V}\langle u, L v\rangle_{V^{\prime}} \quad \forall u, v \in V
$$

An operator can be split into the sum of a symmetric part $L_{S}$ and a skew-symmetric part $L_{S S}$,

$$
L u=L_{S} u+L_{S S} u
$$

Let us consider, for instance, the following diffusiont-transport-reaction operator

$$
L u=-\mu \Delta u+\operatorname{div}(\mathbf{b} u)+\sigma u, \quad \mathbf{x} \in \Omega \subset \mathbb{R}^{d}, d \geq 2
$$

operating on the space $V=H_{0}^{1}(\Omega)$. Since

$$
\begin{aligned}
\operatorname{div}(\mathbf{b} u) &=\frac{1}{2} \operatorname{div}(\mathbf{b} u)+\frac{1}{2} \operatorname{div}(\mathbf{b} u) \\
&=\frac{1}{2} \operatorname{div}(\mathbf{b} u)+\frac{1}{2} u \operatorname{div}(\mathbf{b})+\frac{1}{2} \mathbf{b} \cdot \nabla u
\end{aligned}
$$

we can split $L$ the following way

$$
L u=\underbrace{-\mu \Delta u+\left[\sigma+\frac{1}{2} \operatorname{div}(\mathbf{b})\right] u}_{L_{S} u}+\underbrace{\frac{1}{2}[\operatorname{div}(\mathbf{b} u)+\mathbf{b} \cdot \nabla u]}_{L_{S S} u}
$$

Note that the reaction coefficient has become $\sigma^{*}=\sigma+\frac{1}{2} \operatorname{div}(\mathbf{b})$. We can verify that the two parts are symmetric resp. skew-symmetric. Indeed, integrating twice by parts, we obtain, $\forall u, v \in V$,

$$
\begin{aligned}
v^{\prime}\left\langle L_{S} u, v\right\rangle_{V} &=\mu(\nabla u, \nabla v)+\left(\sigma^{*} u, v\right) \\
&=-\mu_{V}\langle u, \Delta v\rangle_{V^{\prime}}+\left(u, \sigma^{*} v\right) \\
&=v\left\langle u, L_{S} v\right\rangle_{V^{\prime}} \\
\qquad \begin{aligned}
v^{\prime}\left\langle L_{S S} u, v\right\rangle_{V} &=\frac{1}{2}(\operatorname{div}(\mathbf{b} u), v)+\frac{1}{2}(\mathbf{b} \cdot \nabla u, v) \\
&=-\frac{1}{2}(\mathbf{b} u, \nabla v)+\frac{1}{2}(\nabla u, \mathbf{b} v) \\
&=-\frac{1}{2}(u, \mathbf{b} \cdot \nabla v)-\frac{1}{2}(u, \operatorname{div}(\mathbf{b} v)) \\
&=-_{V}\left\langle u, L_{S S} v\right\rangle_{V^{\prime}}
\end{aligned}
\end{aligned}
$$

where we have indicated by $(\cdot, \cdot)$ the scalar product of $L^{2}(\Omega)$.

Remark 13.4. We recall that any matrix A can be decomposed into the sum

$$
\mathrm{A}=\mathrm{A}_{S}+\mathrm{A}_{S S}
$$

where

$$
A_{S}=\frac{1}{2}\left(A+A^{T}\right)
$$

is a symmetric matrix called the symmetric part of $\mathrm{A}$ and

$$
\mathrm{A}_{S S}=\frac{1}{2}\left(\mathrm{~A}-\mathrm{A}^{T}\right)
$$

is a skew-symmetric matrix called the skew-symmetric part of $\mathrm{A}$.

\subsubsection{Strongly consistent methods (GLS, SUPG)}

We consider a diffusion-transport-reaction problem that we write in the abstract form $L u=f$ in $\Omega$, with $u=0$ on $\partial \Omega$. Let us consider the corresponding weak formulation (13.3) with $a(\cdot, \cdot)$ being the bilinear form associated to $L$. A stabilized and strongly consistent method can be obtained by adding a further term to the Galerkin approximation (13.11), that is by considering the problem

$$
\text { find } u_{h} \in V_{h}: \quad a\left(u_{h}, v_{h}\right)+\mathscr{L}_{h}\left(u_{h}, f ; v_{h}\right)=\left(f, v_{h}\right) \quad \forall v_{h} \in V_{h},
$$

for a suitable form $\mathscr{L}_{h}$ satisfying

$$
\mathscr{L}_{h}\left(u, f ; v_{h}\right)=0 \quad \forall v_{h} \in V_{h}
$$

(This is the case of the generalized Galerkin method (13.46), (13.47), provided we require $\left.b_{h}\left(u, v_{h}\right)=G_{h}\left(v_{h}\right) \quad \forall v_{h} \in V_{h} .\right)$ We observe that in $(13.59)$ the form $\mathscr{L}_{h}$ depends both on the approximate solution $u_{h}$ and on the forcing term $f$. A possible choice that verifies (13.60) is

$$
\mathscr{L}_{h}\left(u_{h}, f ; v_{h}\right)=\mathscr{L}_{h}^{(\rho)}\left(u_{h}, f ; v_{h}\right)=\sum_{K \in \mathscr{T}_{h}}\left(L u_{h}-f, \tau_{K} \mathscr{S}^{(\rho)}\left(v_{h}\right)\right)_{\mathrm{L}^{2}(K)}
$$

where $(u, v)_{\mathrm{L}^{2}(K)}=\int_{K} u v d K, \rho$ and $\tau_{K}$ are parameters to be determined, and

$$
\mathscr{S}^{(\rho)}\left(v_{h}\right)=L_{S S} v_{h}+\rho L_{S} v_{h}
$$

Here, $L_{S}$ and $L_{S S}$ are the symmetric resp. skew-symmetric part of the operator $L$ under exam. A possible choice for $\tau_{K}$ is

$$
\tau_{K}=\delta \frac{h_{K}}{|\mathbf{b}(\mathbf{x})|} \quad \forall \mathbf{x} \in K, \forall K \in \mathscr{T}_{h}
$$

where $\mathbf{b}$ is the convective (or transport) field, $h_{K}$ the diameter of the generic element $K$, and $\delta$ a dimensionless coefficient to be prescribed.

To verify that (13.59) is fully consistent, we note that

$$
\tau_{h}\left(u ; v_{h}\right)=a\left(u, v_{h}\right)+\mathscr{L}_{h}^{(\rho)}\left(u, f ; v_{h}\right)-\left(f, v_{h}\right)
$$

is zero for all $v_{h} \in V_{h}$, thanks to (13.3) and property (13.60). Thus the truncation error (13.57) is null. Let us now see some particular cases associated to three different choices of the parameter $\rho$ :

- if $\rho=1$ we obtain the method called Galerkin Least-Squares (GLS), where

$$
\mathscr{S}^{(1)}\left(v_{h}\right)=L v_{h} .
$$

If we take $v_{h}=u_{h}$ we see that a term proportional to $\int_{K}\left(L u_{h}\right)^{2} d K$ has been added on each triangle to the original bilinear form;

- if $\rho=0$ we obtain the method named Streamline Upwind Petrov-Galerkin (SUPG) where

$$
\mathscr{S}^{(0)}\left(v_{h}\right)=L_{S S} v_{h}
$$

- if $\rho=-1$ we obtain the so-called Douglas-Wang $(\mathrm{DW})$ method where

$$
\mathscr{S}^{(-1)}\left(v_{h}\right)=\left(L_{S S}-L_{S}\right) v_{h}
$$

If $\sigma=0$ and divb $=0$ and we use $\mathbb{P}_{1}$ finite elements, the three methods above coincide, as $-\left.\Delta u_{h}\right|_{K}=0 \quad \forall K \in \mathscr{T}_{h}$.

Let us now limit ourselves to the two most classical procedures, GLS $(\rho=1)$ and SUPG $(\rho=0)$ and to the problem written in conservative form (13.58). We define the " $\rho$ norm"

$$
\|v\|_{(\rho)}=\left\{\mu\|\nabla v\|_{\mathrm{L}^{2}(\Omega)}^{2}+\|\sqrt{\gamma} v\|_{\mathrm{L}^{2}(\Omega)}^{2}+\sum_{K \in \mathscr{T}_{h}}\left(\left(L_{S S}+\rho L_{S}\right) v, \tau_{K} \mathscr{S}^{(\rho)}(v)\right)_{\mathrm{L}^{2}(K)}\right\}^{\frac{1}{2}}
$$

where $\gamma=\frac{1}{2}$ div $\mathbf{b}+\sigma$ when we use the conservative form (13.13) of the operator $L$, otherwise $\gamma=-\frac{1}{2} \operatorname{div} \mathbf{b}+\sigma$ when using the non-conservative form $(13.1)$. In either case we assume that $\gamma$ is a non-negative function. The following (stability) inequality holds: there exists an $\alpha^{*}$ depending on $\gamma$ and on the coercivity constant $\alpha$ of $a(\cdot, \cdot)$ such that

$$
\left\|u_{h}\right\|_{(\rho)} \leq \frac{C}{\alpha^{*}}\|f\|_{\mathrm{L}^{2}(\Omega)}
$$

where $C$ is a suitable constant (see for instance (13.78)). Moreover, under suitable assumptions, as we will se in Sect. 13.8.8, the following error estimate holds

$$
\left\|u-u_{h}\right\|_{(\rho)} \leq C h^{r+1 / 2}|u|_{\mathrm{H}^{r+1}(\Omega)}
$$

hence the order of accuracy of the method increases when the degree $r$ of the polynomials we employ increases, as in the standard Galerkin method. The proofs of $(13.62)$ and (13.63) in the case $\rho=1$ will be provided in Sect. 13.8.8.

The choice of the stabilization parameter $\delta$, measuring the amount of artificial viscosity, is extremely important. To this end, we report in Table $13.1$ the range admitted for such parameter as a function of the chosen stabilized scheme. In the table, $C_{0}$ is the constant of the following inverse inequality

$$
\sum_{K \in \mathscr{T}_{h}} h_{K}^{2} \int_{K}\left|\Delta v_{h}\right|^{2} d K \leq C_{0}\left\|\nabla v_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2} \quad \forall v_{h} \in X_{h}^{r}
$$

Obviously, $C_{0}=C_{0}(r)$. Let us note that for linear finite elements $C_{0}=0$. In such a case, the constant $\delta$ in Table $13.1$ is not subject to any upper bound. On the contrary, if we are interested in polynomials of higher degree, $r \geq 2$, then

$$
C_{0}(r)=\bar{C}_{0} r^{-4}
$$

For a more extensive analysis, we refer to [QV94, Chap. 8], and to [RST96]. We also suggest [Fun97] for the case of an approximation with spectral elements.

Table 13.1. Admissible values for the stabilization parameter $\delta$

\begin{tabular}{ll}
\hline SUPG & $0<\delta<1 / C_{0}$ \\
GLS & $0<\delta$ \\
DW & $0<\delta<1 /\left(2 C_{0}\right)$ \\
\hline
\end{tabular}

\subsubsection{On the choice of the stabilization parameter $\tau_{K}$}

For linear finite elements $(r=1)$ another choice of the stabilization function $\tau_{K}$, alternative to that in $(13.61)$, is

$$
\tau_{K}(\mathbf{x})=\frac{h_{K}}{2|\mathbf{b}(\mathbf{x})|} \xi\left(\mathbb{P e}_{K}\right) \quad \forall \mathbf{x} \in K, \quad \forall K \in \mathscr{T}_{h}
$$

where

$$
\mathbb{P e}_{K}(\mathbf{x})=\frac{|\mathbf{b}(\mathbf{x})| h_{K}}{2 \mu(\mathbf{x})} \quad \forall \mathbf{x} \in K, \quad \forall K \in \mathscr{T}_{h},
$$

is the local Péclet number (in analogy to definition (13.22) for dimension one), and the upwind function $\xi(\cdot)$ can for instance be chosen as follows

$$
\xi(\theta)=\operatorname{coth}(\theta)-1 / \theta, \quad \theta>0
$$

As $\lim _{\theta \rightarrow+\infty} \xi(\theta)=1\left(\right.$ cf. Fig. 13.9), if $\mathbb{P e}_{K}(\mathbf{x}) \gg 1$ then (13.66) reduces, in the limit, to (13.61) with $\delta=1 / 2$. Moreover, since $\theta \rightarrow 0$ implies $\xi(\theta)=\theta / 3+o(\theta)$, we have $\tau_{K}(\mathbf{x}) \rightarrow 0$ when $\mathbb{P e}_{K}(\mathbf{x}) \ll 1$ (in fact no stabilization is necessary if the problem is diffusion-dominated). Other possibilities for the function $\tau_{K}$ are found in the literature. For instance, $h_{K}$ can be replaced in (13.66)-(13.67) by the diameter of the element $K$ along $\mathbf{b}$, or one can choose the upwind function $\xi(\cdot)$ to be $\xi(\theta)=\max \{0,1-1 / \theta\}$, or $\xi(\theta)=\min \{1, \theta / 3\}$ (see [JK07] for more details).

Let us now give a heuristic explanation for the choice (13.66) of the stabilizing function $\tau_{K}$. To this end, take the variational formulation (13.18) of the onedimensional diffusion-transport problem (13.16). Given a uniform partition $\mathscr{T}_{h}$ of $\Omega=(0,1)$ in $N$ intervals of width $h=1 / N$, consider the SUPG method for the discretization of $(13.18)$ :

$$
\text { find } \stackrel{\circ}{u}_{h} \in V_{h} \text { such that } a_{h}\left(\stackrel{\circ}{u}_{h}, v_{h}\right)=F_{h}\left(v_{h}\right) \quad \forall v_{h} \in V_{h},
$$

where $V_{h} \subset H_{0}^{1}(0,1)$ is the space of piecewise-linear continuous polynomials on $\mathscr{T}_{h}$, and

$$
\begin{array}{ll}
a_{h}(u, v)=\int_{0}^{1}\left(\mu u^{\prime} v^{\prime}+b u^{\prime} v\right) d x+\tau \int_{0}^{1}|b|^{2} u^{\prime} v^{\prime} d x & \forall u, v \in V_{h} \\
F_{h}(v)=-\int_{0}^{1} b v d x & \forall v \in V_{h} .
\end{array}
$$

Fig. 13.9. Upwind function $\xi$ defined in (13.68) By defining $\mu_{h}=\mu\left(1+\tau|b|^{2} / \mu\right)$ the bilinear form $a_{h}(\cdot, \cdot)$ may be equivalently written as

$$
a_{h}(u, v)=\int_{0}^{1} \mu_{h} u^{\prime} v^{\prime} d x+\int_{0}^{1} b u^{\prime} v d x \quad \forall u, v \in V_{h}
$$

Choose the parameter $\tau$ as

$$
\tau=\frac{h}{2|b|}\left[\operatorname{coth}(\mathbb{P e})-\frac{1}{\mathbb{P e}}\right]
$$

where $\mathbb{P e}=\frac{b \mid h}{2 \mu}$ is the local Péclet number. By virtue of these definitions we obtain

$$
\begin{aligned}
\tau \frac{|b|^{2}}{\mu} &=\mathbb{P e}\left[\operatorname{coth}(\mathbb{P e})-\frac{1}{\mathbb{P e}}\right] \\
&=\mathbb{P e c o t h}(\mathbb{P e})-1 \\
&=\mathbb{P e}-1+\mathbb{P e}(\operatorname{coth}(\mathbb{P e})-1) \\
&=\mathbb{P e}-1+B(2 \mathbb{P e})
\end{aligned}
$$

The final equality involves the identity:

$$
\begin{aligned}
t(\operatorname{coth}(t)-1) &=t\left[\frac{e^{t}+e^{-t}}{e^{t}-e^{-t}}-1\right]=2 t\left[\frac{e^{-t}}{e^{t}-e^{-t}}\right] \\
&=\frac{2 t}{e^{2 t}-1}=B(2 t), \quad t>0
\end{aligned}
$$

which in turn descends from the definition of $\operatorname{coth}(\cdot)$. Above, $B(\cdot)$ is the Bernoulli function (cf. Sect. 13.6). To sum up, $\mu_{h}$ may be written as

$$
\mu_{h}=\mu\left(1+\tau \frac{|b|^{2}}{\mu}\right)=\mu(1+\phi(\mathbb{P e}))
$$

having chosen $\phi$ as in (13.39). So in this particular case, the SUPG method, with $\tau$ chosen as in (13.69), coincides with the Scharfetter and Gummel method encountered in Sect. 13.6, which is the unique method capable of yielding a numerical solution to a constant-coefficient problem (with constant source) that is nodally exact.

Remark 13.5. If one employs polynomials of degree $r \geq 2$ (as in the $h p$ formulation, or with spectral elements), a more coherent definition of the local Péclet number is this

$$
\mathbb{P e}_{K}^{r}=\frac{|\mathbf{b}(\mathbf{x})| h_{K}}{2 \mu(\mathbf{x}) r}
$$

while the corresponding stabilizing function (13.66) becomes (see [GaAML04])

$$
\tau_{K}(\mathbf{x})=\frac{h_{K}}{2|\mathbf{b}| r} \xi\left(\mathbb{P e}_{K}^{r}\right)
$$



\subsubsection{Analysis of the GLS method}

In this section we want to prove the stability property (13.62) and the convergence property (13.63) in the case of the GLS method (hence for $\rho=1$ ).

We suppose that the differential operator $L$ has the form (13.58), with $\mu>0$ and $\sigma \geq 0$ constant, and $\mathbf{b}$ being a vector function whose components are continuous (e.g. constant), with homogeneous Dirichlet boundary conditions being assigned. The bilinear form $a(\cdot, \cdot): V \times V \rightarrow \mathbb{R}$ associated to the operator $L$ is therefore

$$
a(u, v)=\mu \int_{\Omega} \nabla u \cdot \nabla v d \Omega+\int_{\Omega} \operatorname{div}(\mathbf{b} u) v d \Omega+\int_{\Omega} \sigma u v d \Omega
$$

with $V=\mathrm{H}_{0}^{1}(\Omega)$. For simplicity, we suppose that there exist two constants $\gamma_{0}$ and $\gamma_{1}$ such that

$$
0<\gamma_{0} \leq \gamma(\mathbf{x})=\frac{1}{2} \operatorname{div}(\mathbf{b}(\mathbf{x}))+\sigma \leq \gamma_{1} \quad \forall \mathbf{x} \in \Omega
$$

In this case the form $a(\cdot, \cdot)$ is coercive, as $a(v, v) \geq \mu\|\nabla v\|_{L^{2}(\Omega)}^{2}+\gamma_{0}\|v\|_{L^{2}(\Omega)}^{2}$. Following the procedure developed in Sect. 13.8.5, we can write the symmetric and skewsymmetric parts associated to $L$ as

$$
L_{S} u=-\mu \Delta u+\gamma u, \quad L_{S S} u=\frac{1}{2}(\operatorname{div}(\mathbf{b} u)+\mathbf{b} \cdot \nabla u)
$$

Moreover, we rewrite the stabilized formulation (13.59) by splitting $\mathscr{L}_{h}^{(1)}\left(u_{h}, f ; v_{h}\right)$ in two terms, one containing $u_{h}$, the other $f$ :

$$
\text { find } u_{h} \in V_{h}: \quad a_{h}^{(1)}\left(u_{h}, v_{h}\right)=f_{h}^{(1)}\left(v_{h}\right) \quad \forall v_{h} \in V_{h}
$$

with

$$
a_{h}^{(1)}\left(u_{h}, v_{h}\right)=a\left(u_{h}, v_{h}\right)+\sum_{K \in \mathscr{T}_{h}} \delta\left(L u_{h}, \frac{h_{K}}{|\mathbf{b}|} L v_{h}\right)_{\mathrm{L}^{2}(K)}
$$

and

$$
f_{h}^{(1)}\left(v_{h}\right)=\left(f, v_{h}\right)+\sum_{K \in \mathscr{T}_{h}} \delta\left(f, \frac{h_{K}}{|\mathbf{b}|} L v_{h}\right)_{\mathrm{L}^{2}(K)}
$$

We observe that, using these notations, the strong consistency property $(13.60)$ is expressed via the equality

$$
a_{h}^{(1)}\left(u, v_{h}\right)=f_{h}^{(1)}\left(v_{h}\right) \quad \forall v_{h} \in V_{h}
$$

We can now prove the following preliminary result. This identity follows from definition (13.72) (having chosen $v_{h}=u_{h}$ ) and from (13.70). In the case under exam, the norm $\|\cdot\|_{(1)}$, which we here denote by the symbol $\|\cdot\|_{G L S}$ for convenience, becomes

$$
\left\|v_{h}\right\|_{G L S}^{2}=\mu\left\|\nabla v_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\left\|\sqrt{\gamma} v_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\sum_{K \in \mathscr{T}_{h}} \delta\left(\frac{h_{K}}{|\mathbf{b}|} L v_{h}, L v_{h}\right)_{\mathrm{L}^{2}(K)}
$$

We can prove the following stability result.

Proof. We choose $v_{h}=u_{h}$ in (13.71). By exploiting Lemma $13.1$ and definition (13.76), we can first write that

$$
\left\|u_{h}\right\|_{G L S}^{2}=a_{h}^{(1)}\left(u_{h}, u_{h}\right)=f_{h}^{(1)}\left(u_{h}\right)=\left(f, u_{h}\right)_{\mathrm{L}^{2}(\Omega)}+\sum_{K \in \mathscr{T}_{h}} \delta\left(f, \frac{h_{K}}{|\mathbf{b}|} L u_{h}\right)_{\mathrm{L}^{2}(K)}
$$

We look for an upper bound for the two right-hand side terms of (13.77) separately, by applying suitably the Cauchy-Schwarz and Young inequalities. We thus obtain:

$$
\begin{aligned}
&\left(f, u_{h}\right)_{\mathrm{L}^{2}(\Omega)}=\left(\frac{1}{\sqrt{\gamma}} f, \sqrt{\gamma} u_{h}\right)_{\mathrm{L}^{2}(\Omega)} \leq\left\|\frac{1}{\sqrt{\gamma}} f\right\|_{\mathrm{L}^{2}(\Omega)}\left\|\sqrt{\gamma} u_{h}\right\|_{\mathrm{L}^{2}(\Omega)} \\
&\leq \frac{1}{4}\left\|\sqrt{\gamma} u_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\left\|\frac{1}{\sqrt{\gamma}} f\right\|_{\mathrm{L}^{2}(\Omega)}^{2} \\
&\left.\sum_{K \in \mathscr{T}_{h}} \delta\left(f, \frac{h_{K}}{|\mathbf{b}|} L u_{h}\right)_{\mathrm{L}^{2}(K)}=\sum_{K \in \mathscr{T}_{h}}\left(\sqrt{\delta \frac{h_{K}}{|\mathbf{b}|}} f, \sqrt{\delta \frac{h_{K}}{|\mathbf{b}|}} L u_{h}\right)_{\mathrm{L}^{2}(K)}\right) \\
&\leq \sum_{K \in \mathscr{T}_{h}}\left\|\sqrt{\delta \frac{h_{K}}{|\mathbf{b}|}} f\right\|_{\mathrm{L}^{2}(K)}\left\|\sqrt{\delta} \frac{h_{K}}{|\mathbf{b}|} L u_{h}\right\|_{\mathrm{L}^{2}(K)} \\
&\leq \sum_{K \in \mathscr{T}_{h}} \delta\left(\frac{h_{K}}{|\mathbf{b}|} f, f\right)_{\mathrm{L}^{2}(K)}+\frac{1}{4} \sum_{K \in \mathscr{T}_{h}} \delta\left(\frac{h_{K}}{|\mathbf{b}|} L u_{h}, L u_{h}\right)_{\mathrm{L}^{2}(K)}
\end{aligned}
$$

By summing the two previous upper bounds and by exploiting again definition $(13.76)$, we have

$$
\left\|u_{h}\right\|_{G L S}^{2} \leq\left\|\frac{1}{\sqrt{\gamma}} f\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\sum_{K \in \mathscr{T}_{h}} \delta\left(\frac{h_{K}}{|\mathbf{b}|} f, f\right)_{\mathrm{L}^{2}(K)}+\frac{1}{4}\left\|u_{h}\right\|_{G L S}^{2}
$$

that is, recalling that $h_{K} \leq h$,

$$
\left\|u_{h}\right\|_{G L S}^{2} \leq \frac{4}{3}\left[\left\|\frac{1}{\sqrt{\gamma}} f\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\sum_{K \in \mathscr{T}_{h}} \delta\left(\frac{h_{K}}{|\mathbf{b}|} f, f\right)_{\mathrm{L}^{2}(K)}\right] \leq C^{2}\|f\|_{\mathrm{L}^{2}(\Omega)}^{2},
$$

having set

$$
C=\left(\frac{4}{3} \max _{\mathbf{x} \in \Omega}\left(\frac{1}{\gamma}+\delta \frac{h}{|\mathbf{b}|}\right)\right)^{1 / 2}
$$

We observe that the previous result is valid with the only constraint that the stabilization parameter $\delta$ be positive. In fact, such parameter might also vary for each element $K$. In this case, we would have $\delta_{K}$ instead of $\delta$ in (13.72) and (13.73), while the constant $\delta$ in (13.78) would have the meaning of $\max _{K \in \mathscr{T}_{h}} \delta_{K}$.

We now study the convergence of the GLS method.

Proof. First of all, we rewrite the error as follows

$$
e_{h}=u_{h}-u=\sigma_{h}-\eta,
$$

with $\sigma_{h}=u_{h}-\hat{u}_{h}, \eta=u-\hat{u}_{h}$, where $\hat{u}_{h} \in V_{h}$ is a function that depends on $u$ and that satisfies property (13.79). If, for instance, $V_{h}=X_{h}^{r} \cap H_{0}^{1}(\Omega)$, we can choose $\hat{u}_{h}=\Pi_{h}^{r} u$, that is the finite element interpolant of $u$. We start by estimating the norm $\left\|\sigma_{h}\right\|_{G L S}$. By exploiting the strong consistency of the GLS scheme given by (13.74), we obtain

$$
\left\|\sigma_{h}\right\|_{G L S}^{2}=a_{h}^{(1)}\left(\sigma_{h}, \sigma_{h}\right)=a_{h}^{(1)}\left(u_{h}-u+\eta, \sigma_{h}\right)=a_{h}^{(1)}\left(\eta, \sigma_{h}\right)
$$

thanks to (13.71). Now, by definition (13.72) and thanks to the homogeneous Dirichlet boundary conditions it follows that

$$
\begin{aligned}
&a_{h}^{(1)}\left(\eta, \sigma_{h}\right)=\mu \int_{\Omega} \nabla \eta \cdot \nabla \sigma_{h} d \Omega-\int_{\Omega} \eta \mathbf{b} \cdot \nabla \sigma_{h} d \Omega+\int_{\Omega} \sigma \eta \sigma_{h} d \Omega \\
&+\sum_{K \in \mathscr{T}_{h}} \delta\left(L \eta, \frac{h_{K}}{|\mathbf{b}|} L \sigma_{h}\right)_{\mathrm{L}^{2}(K)}=\underbrace{\mu\left(\nabla \eta, \nabla \sigma_{h}\right)-\sum_{K \in \mathscr{T}_{h}}\left(\eta, L \sigma_{h}\right)_{K}}_{(\mathrm{I})}+\underbrace{2\left(\gamma \eta, \sigma_{h}\right)_{\mathrm{L}^{2}(\Omega)}}_{(\mathrm{III})} \\
&+\underbrace{\sum_{K \in \mathscr{T}_{h}}\left(\eta,-\mu \Delta \sigma_{h}\right)_{\mathrm{L}^{2}(K)}}+\underbrace{\sum_{K \in \mathscr{T}_{h}} \delta\left(L \eta, \frac{h_{K}}{|\mathbf{b}|} L \sigma_{h}\right)_{\mathrm{L}^{2}(K)}}_{(\mathrm{V})} .
\end{aligned}
$$

We now bound the terms $(\mathrm{I})-(\mathrm{V})$ separately. By carefully using the Cauchy-Schwarz and Young inequalities we obtain

$$
\begin{aligned}
(\mathrm{I}) &=\mu\left(\nabla \eta, \nabla \sigma_{h}\right)_{\mathrm{L}^{2}(\Omega)} \leq \frac{\mu}{4}\left\|\nabla \sigma_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\mu\|\nabla \eta\|_{\mathrm{L}^{2}(\Omega)}^{2} \\
(\mathrm{II}) &=-\sum_{K \in \mathscr{T}_{h}}\left(\eta, L \sigma_{h}\right)_{\mathrm{L}^{2}(K)}=-\sum_{K \in \mathscr{T}}\left(\sqrt{\frac{|\mathbf{b}|}{\delta h_{K}}} \eta, \sqrt{\frac{\delta h_{K}}{|\mathbf{b}|}} L \sigma_{h}\right)_{\mathrm{L}^{2}(K)} \\
& \leq \frac{1}{4} \sum_{K \in \mathscr{T}_{h}} \delta\left(\frac{h_{K}}{|\mathbf{b}|} L \sigma_{h}, L \sigma_{h}\right)_{\mathrm{L}^{2}(K)}+\sum_{K \in \mathscr{T}_{h}}\left(\frac{|\mathbf{b}|}{\delta h_{K}} \eta, \eta\right)_{\mathrm{L}^{2}(K)}
\end{aligned}
$$

For the term (IV), thanks again to the Cauchy-Schwarz and Young inequalities, hypothesis (13.80) and the inverse inequality (13.64), we obtain

$$
\begin{aligned}
(\mathrm{IV}) &=\sum_{K \in \mathscr{T}_{h}}\left(\eta,-\mu \Delta \sigma_{h}\right)_{\mathrm{L}^{2}(K)} \\
& \leq \frac{1}{4} \sum_{K \in \mathscr{T}_{h}} \delta \mu^{2}\left(\frac{h_{K}}{|\mathbf{b}|} \Delta \sigma_{h}, \Delta \sigma_{h}\right)_{\mathrm{L}^{2}(K)}+\sum_{K \in \mathscr{T}_{h}}\left(\frac{|\mathbf{b}|}{\delta h_{K}} \eta, \eta\right)_{\mathrm{L}^{2}(K)} \\
& \leq \frac{1}{8} \delta \mu \sum_{K \in \mathscr{T}} h_{K}^{2}\left(\Delta \sigma_{h}, \Delta \sigma_{h}\right)_{\mathrm{L}^{2}(K)}+\sum_{K \in \mathscr{T}_{h}}\left(\frac{|\mathbf{b}|}{\delta h_{K}} \eta, \eta\right)_{\mathrm{L}^{2}(K)} \\
& \leq \frac{\delta C_{0} \mu}{8}\left\|\nabla \sigma_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\sum_{K \in \mathscr{T}_{h}}\left(\frac{|\mathbf{b}|}{\delta h_{K}} \eta, \eta\right)_{\mathrm{L}^{2}(K)}
\end{aligned}
$$

Term (V) can finally be bounded once again thanks to the Cauchy-Schwarz and Young inequalities as follows

$$
\begin{aligned}
(\mathrm{V}) &=\sum_{K \in \mathscr{T}_{h}} \delta\left(L \eta, \frac{h_{K}}{|\mathbf{b}|} L \sigma_{h}\right)_{\mathrm{L}^{2}(K)} \\
& \leq \frac{1}{4} \sum_{K \in \mathscr{T}_{h}} \delta\left(\frac{h_{K}}{|\mathbf{b}|} L \sigma_{h}, L \sigma_{h}\right)_{\mathrm{L}^{2}(K)}+\sum_{K \in \mathscr{T}_{h}} \delta\left(\frac{h_{K}}{|\mathbf{b}|} L \eta, L \eta\right)_{\mathrm{L}^{2}(K)}
\end{aligned}
$$

Thanks to these upper bounds and using once more the definition (13.76), we obtain the following estimate

$$
\begin{aligned}
&\left\|\sigma_{h}\right\|_{G L S}^{2}=a_{h}^{(1)}\left(\eta, \sigma_{h}\right) \leq \frac{1}{4}\left\|\sigma_{h}\right\|_{G L S}^{2} \\
&+\frac{1}{4}\left(\left\|\sqrt{\gamma} \sigma_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\sum_{K \in \mathscr{T}_{h}} \delta\left(\frac{h_{K}}{|\mathbf{b}|} L \sigma_{h}, L \sigma_{h}\right)_{\mathrm{L}^{2}(K)}\right)+\frac{\delta C_{0} \mu}{8}\left\|\nabla \sigma_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2} \\
&+\underbrace{\mu\|\nabla \eta\|_{\mathrm{L}^{2}(\Omega)}^{2}+2 \sum_{K \in \mathscr{T}_{h}}\left(\frac{|\mathbf{b}|}{\delta h_{K}} \eta, \eta\right)_{\mathrm{L}^{2}(K)}+2\|\sqrt{\gamma} \eta\|_{\mathrm{L}^{2}(\Omega)}^{2}+\sum_{K \in \mathscr{T}_{h}} \delta\left(\frac{h_{K}}{|\mathbf{b}|} L \eta, L \eta\right)_{\mathrm{L}^{2}(K)}}_{\mathscr{E}(\eta)} \\
&\leq \frac{1}{2}\left\|\sigma_{h}\right\|_{G L S}^{2}+\mathscr{E}(\eta)
\end{aligned}
$$

having exploited, in the last passage, the assumption that $\delta \leq 2 C_{0}^{-1}$. Then, we can state that

$$
\left\|\sigma_{h}\right\|_{G L S}^{2} \leq 2 \mathscr{E}(\eta)
$$

We now estimate the term $\mathscr{E}(\eta)$, by bounding each of its summands separately. To this end, we will basically use the local approximation property (13.79) and the requirement formulated in (13.80) on the local Péclet number $\mathbb{P e}_{K}$. Moreover, we observe that the constants $C$, introduced in the remainder, depend neither on $h$ nor on $\mathbb{P}_{K}$, but can depend on other quantities such as the constant $\gamma_{1}$ in (13.70), the reaction constant $\sigma$, the norm $\|\mathbf{b}\|_{L^{\infty}(\Omega)}$, the stabilization parameter $\delta$. We then have

$$
\begin{aligned}
&\mu\|\nabla \eta\|_{\mathrm{L}^{2}(\Omega)}^{2} \leq C \mu h^{2 r}|u|_{\mathrm{H}^{r+1}(\Omega)}^{2} \\
&\leq C \frac{\|\mathbf{b}\|_{L^{\infty}(\Omega)} h}{2} h^{2 r}|u|_{\mathrm{H}^{r+1}(\Omega)}^{2} \leq C h^{2 r+1}|u|_{\mathrm{H}^{r+1}(\Omega)}^{2} \\
&\qquad 2 \sum_{K \in \mathscr{T}_{h}}\left(\frac{|\mathbf{b}|}{\delta h_{K}} \eta, \eta\right)_{\mathrm{L}^{2}(K)} \leq C \frac{\|\mathbf{b}\|_{L^{\infty}(\Omega)}}{\delta} \sum_{K \in \mathscr{T}_{h}} \frac{1}{h_{K}} h_{K}^{2(r+1)}|u|_{\mathrm{H}^{r+1}(K)}^{2} \\
&\leq C h^{2 r+1}|u|_{\mathrm{H}^{r+1}(\Omega)}^{2} \\
&2\|\sqrt{\gamma} \eta\|_{\mathrm{L}^{2}(\Omega)}^{2} \leq 2 \gamma_{1}\|\eta\|_{\mathrm{L}^{2}(\Omega)}^{2} \leq C h^{2(r+1)}|u|_{\mathrm{H}^{r+1}(\Omega)}^{2}
\end{aligned}
$$

having exploited, for controlling the third summand, the assumption (13.70). Finding an upper bound for the fourth summand of $\mathscr{E}(\eta)$ is slightly more difficult: first, by elaborating on the term $L \eta$, we have

$$
\begin{aligned}
&\sum_{K \in \mathscr{T}_{h}} \delta\left(\frac{h_{K}}{|\mathbf{b}|} L \eta, L \eta\right)_{\mathrm{L}^{2}(K)}=\sum_{K \in \mathscr{T}_{h}} \delta\left\|\sqrt{\frac{h_{K}}{|\mathbf{b}|}} L \eta\right\|_{L^{2}(K)}^{2} \\
&=\sum_{K \in \mathscr{T}_{h}} \delta\left\|-\mu \sqrt{\frac{h_{K}}{|\mathbf{b}|}} \Delta \eta+\sqrt{\frac{h_{K}}{|\mathbf{b}|}} \operatorname{div}(\mathbf{b} \eta)+\sigma \sqrt{\frac{h_{K}}{|\mathbf{b}|}} \eta\right\|_{\mathrm{L}^{2}(K)}^{2} \\
&\leq C \sum_{K \in \mathscr{T}_{h}} \delta\left(\left\|\mu \sqrt{\frac{h_{K}}{|\mathbf{b}|}} \Delta \eta\right\|_{\mathrm{L}^{2}(K)}^{2}+\left\|\sqrt{\frac{h_{K}}{|\mathbf{b}|}} \operatorname{div}(\mathbf{b} \eta)\right\|_{\mathrm{L}^{2}(K)}^{2}+\left\|\sigma \sqrt{\frac{h_{K}}{|\mathbf{b}|}} \eta\right\|_{\mathrm{L}^{2}(K)}^{2}\right)
\end{aligned}
$$

Now, with a similar computation to the one performed to obtain estimates (13.83) and (13.84), it is easy to prove that the second and third summands of the left-hand side of (13.85) can be bounded using a term of the form $C h^{2 r+1}|u|_{\mathrm{H}^{r+1}(\Omega)}^{2}$, for a suitable choice of the constant $C$. For the first summand, we have

$$
\begin{aligned}
&\sum_{K \in \mathscr{T}_{h}} \delta\left\|\mu \sqrt{\frac{h_{K}}{|\mathbf{b}|}} \Delta \eta\right\|_{\mathrm{L}^{2}(K)}^{2} \leq \sum_{K \in \mathscr{T}_{h}} \delta \frac{h_{K}^{2} \mu}{2}\|\Delta \eta\|_{\mathrm{L}^{2}(K)}^{2} \\
&\leq C \delta\|\mathbf{b}\|_{L^{\infty}(\Omega)} \sum_{K \in \mathscr{T}_{h}} h_{K}^{3}\|\Delta \eta\|_{\mathrm{L}^{2}(K)}^{2} \leq C h^{2 r+1}|u|_{\mathrm{H}^{r+1}(\Omega)}^{2}
\end{aligned}
$$

having again used conditions (13.79) and (13.80). The latter bound allows us to conclude that

$$
\mathscr{E}(\eta) \leq C h^{2 r+1}|u|_{\mathrm{H}^{r+1}(\Omega)}^{2}
$$

that is

$$
\left\|\sigma_{h}\right\|_{G L S} \leq C h^{r+1 / 2}|u|_{\mathrm{H}^{r+1}(\Omega)}
$$

Reverting to (13.82), to obtain the desired estimate for the norm $\left\|u_{h}-u\right\|_{G L S}$ we still have to estimate $\|\eta\|_{G L S}$. This evidently leads to estimating three contributions as in (13.83), (13.84) and (13.85), and eventually produces

$$
\|\eta\|_{G L S} \leq C h^{r+1 / 2}|u|_{\mathrm{H}^{r+1}(\Omega)}
$$

The desired estimate (13.81) follows by combining this result with (13.86).

\subsubsection{Stabilization through bubble functions}

The generalized Galerkin method considered in the previous sections yields a stable numerical solution owing to the enrichment of the bilinear form $a(\cdot, \cdot)$. An alternative strategy consists of adopting a richer subspace than the standard one $V_{h}$. The idea is then to choose both the approximate solution and the test functions in the enriched space, therefore remaining within a classical Galerkin framework. Referring to the usual diffusion-transport-reaction problem of the form (13.1), we introduce the finite dimensional space

$$
V_{h}^{b}=V_{h} \oplus B
$$

where $V_{h}=X_{h}^{r} \cap H_{0}^{1}(\Omega)$ is the usual space and $B$ is a finite-dimensional space of bubble functions, or

$$
B=\left\{v_{B} \in H_{0}^{1}(\Omega):\left.v_{B}\right|_{K}=c_{K} b_{K},\left.b_{K}\right|_{\partial K}=0 \text { and } c_{K} \in \mathbb{R}\right\}
$$

On each element $K$ we then add the correction term $b_{K}$, for which several different choices are possible. As we only wish to deal with the initial grid $\mathscr{T}_{h}$ associated to the space $V_{h}$, a standard choice leads to defining $b_{K}=\lambda_{1} \lambda_{2} \lambda_{3}$ where the $\lambda_{i}$, for $i=0, \ldots, 2$, are the barycentric coordinates on $K$, i.e. linear polynomials defined on $K$, vanishing on one of the sides of the triangle and taking the value 1 at the vertex opposed to such side. (See Sect. $4.4 .3$ for their definition). The function $b_{K}$ coincides in this case with the so-called cubic bubble that takes value 0 on the boundary of $K$ and positive values inside it (see Fig. 13.10 (left)).Hence $c$ is the only degree of freedom associated to the triangle $K$ (it will coincide, for instance, with the largest value taken by $b_{K}$ on $K$, or with the value it takes in the center of gravity). (see Sect. 4.4.3).

Remark 13.6. In order to introduce a computational subgrid on the domain $\Omega$ (obtained as a suitable refinement of the mesh $\mathscr{T}_{h}$ ), we can adopt more complex definitions for the bubble function $b_{K}$. For instance, $b_{K}$ could be a piecewise linear function, again defined on the element $K$ and assuming the value 0 on the boundary of the triangle (like the basis function of linear finite elements associated to some point inside $K$ ) (see Fig. 13.10 (right)) [EG04].

At this point, we can introduce the Galerkin approximation on the space $V_{h}^{b}$ of the problem under exam, which will take the form

$$
\text { find } u_{h}^{b} \in V_{h}^{b}: \quad a\left(u_{h}+u_{b}, v_{h}^{b}\right)=\left(f, v_{h}^{b}\right) \quad \forall v_{h}^{b} \in V_{h}^{b} \text {, }
$$

with $a(\cdot, \cdot)$ being the bilinear form associated to the differential operator $L$.
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-350.jpg?height=270&width=464&top_left_y=928&top_left_x=224)

Fig. 13.10. Example of a cubic (left) and linear (right) bubble We propose to rewrite (13.87) as a stabilized Galerkin scheme in $V_{h}$, by eliminating function $u_{b}$. So far we can only say that, in each element $K,\left.u_{b}\right|_{K}=c_{b, K} b_{K}$ for a suitable (unknown) constant $c_{b, K}$, with $u_{b} \in B$. We decompose both $u_{h}^{b}$ and $v_{h}^{b}$ as sums of a function of $V_{h}$ and a function of $B$, that is

$$
u_{h}^{b}=u_{h}+u_{b}, \quad v_{h}^{b}=v_{h}+v_{b}
$$

For any given $K$, we first select as test function $v_{h}^{b}$ in (13.87) the one identified by $v_{h}=0$ and $v_{b} \in B$ such that

$$
v_{b}= \begin{cases}b_{K} & \text { in } K \\ 0 & \text { elsewhere }\end{cases}
$$

We then have

$$
a\left(u_{h}+u_{b}, v_{b}\right)=a_{K}\left(u_{h}+c_{b, K} b_{K}, b_{K}\right)
$$

having denoted by $a_{K}(\cdot, \cdot)$ the restriction of the bilinear form $a(\cdot, \cdot)$ to the element $K$. We can therefore rewrite (13.87) as

$$
a_{K}\left(u_{h}, b_{K}\right)+c_{b, K} a_{K}\left(b_{K}, b_{K}\right)=\left(f, b_{K}\right)_{\mathrm{L}^{2}(K)}
$$

Exploiting the fact that $b_{K}$ vanishes on the boundary of $K$, we can integrate by parts the first term of (13.88), obtaining $a_{K}\left(u_{h}, b_{K}\right)=\left(L u_{h}, b_{K}\right)_{\mathrm{L}^{2}(K)}$, then the unknown value of the constant $c_{b, K}$, is

$$
c_{b, K}=\frac{\left(f-L u_{h}, b_{K}\right)_{\mathrm{L}^{2}(K)}}{a_{K}\left(b_{K}, b_{K}\right)}
$$

We now choose as test function $v_{h}^{b}$ in (13.87) the one identified by any function $v_{h} \in V_{h}$ and by $v_{b}=0$, thus obtaining

$$
a\left(u_{h}, v_{h}\right)+\sum_{K \in \mathscr{T}_{h}} c_{b, K} a_{K}\left(b_{K}, v_{h}\right)=\left(f, v_{h}\right)_{\mathrm{L}^{2}(\Omega)}
$$

Let us rewrite $a_{K}\left(b_{K}, v_{h}\right)$. By integrating by parts and exploiting the definitions of symmetric and skew-symmetric parts of the differential operator $L$ (see Sect. 13.8.5), we have

$$
\begin{aligned}
a_{K}\left(b_{K}, v_{h}\right)=& \int_{K} \mu \nabla b_{K} \cdot \nabla v_{h} d K+\int_{K} \mathbf{b} \cdot \nabla b_{K} v_{h} d K+\int_{K} \sigma b_{K} v_{h} d K \\
=&-\int_{K} \mu b_{K} \Delta v_{h} d K+\int_{\partial K} \mu b_{K} \nabla v_{h} \cdot \mathbf{n} d \gamma-\int_{K} b_{K} \nabla v_{h} \cdot \mathbf{b} d K \\
&+\int_{\partial K} \mathbf{b} \cdot \mathbf{n} v_{h} b_{K} d \gamma+\int_{K} \sigma b_{K} v_{h} d K=\left(b_{K},\left(L_{S}-L_{S S}\right) v_{h}\right)_{\mathrm{L}^{2}(K)}
\end{aligned}
$$

We have exploited the property that the bubble function $b_{K}$ vanishes on the boundary of the element $K$, and moreover that divb $=0$. In a very similar way we can rewrite the denominator of the constant $c_{b, K}$ in the following way

$$
a_{K}\left(b_{K}, b_{K}\right)=\left(L_{S} b_{K}, b_{K}\right)_{\mathrm{L}^{2}(K)}
$$

Reverting to (13.89), we thus have

$$
a\left(u_{h}, v_{h}\right)+a_{B}\left(u_{h}, f ; v_{h}\right)=\left(f, v_{h}\right)_{\mathrm{L}^{2}(\Omega)} \quad \forall v_{h} \in V_{h}
$$

where

$$
a_{B}\left(u_{h}, f ; v_{h}\right)=\sum_{K \in \mathscr{T}_{h}} \frac{\left(L u_{h}-f, b_{K}\right)_{K}\left(L_{S S} v_{h}-L_{S} v_{h}, b_{K}\right)_{\mathrm{L}^{2}(K)}}{\left(L_{S} b_{K}, b_{K}\right)_{\mathrm{L}^{2}(K)}}
$$

We have therefore found a stabilized Galerkin scheme, which can be formulated in the strongly consistent form (13.59). In the case where $\mathbf{b}$ is constant, we can identify it using a sort of generalized Douglas-Wang method.

By choosing a convenient bubble $b_{K}$ and following an analogous procedure to the one illustrated above, it is also possible to define generalized SUPG and GLS methods (see [BFHR97]). Similar strategies based on the so-called subgrid viscosity can be successfully used. See [EG04] for an extensive analysis.

\subsection{DG methods for diffusion-transport equations}

The Discontinuous Galerkin method introduced in Chap. 12 for the Poisson problem can be extended to the diffusion-transport-reaction problem (13.13) (in conservation form) as follows: find $u_{\delta} \in W_{\delta}^{0}$ s.t.

$$
\begin{gathered}
\left.\left.\sum_{m=1}^{M}\left(\mu \nabla u_{\delta}, \nabla v_{\delta}\right)_{\Omega_{m}}-\sum_{e \in \mathscr{E}_{\delta}} \int_{e}\left[v_{\delta}\right] \cdot\left\{\mu \nabla u_{\delta}\right\}-\tau \sum_{e \in \mathscr{E}_{\delta}} \int_{e}\left[u_{\delta}\right]\{\mu\rangle v_{\delta}\right\}\right\} \\
+\sum_{e \in \mathscr{E}_{\delta}} \int_{e} \gamma|e|^{-1}\left[u_{\delta}\right] \cdot\left[v_{\delta}\right]-\sum_{m=1}^{M}\left(\mathbf{b} u_{\delta}, \nabla v_{\delta}\right)_{\Omega_{m}}+\sum_{e \in \mathscr{E}_{\delta}} \int_{e}\left\{\mathfrak{b} u_{\delta}\right\}_{\mathbf{b}} \cdot\left[v_{\delta}\right] \\
+\sum_{m=1}^{M}\left(\sigma u_{\delta}, v_{\delta}\right)_{\Omega_{m}}=\sum_{m=1}^{M}\left(f, v_{\delta}\right)_{\Omega_{m}}
\end{gathered}
$$

we have used the same notation of Sect. 12.1; moreover,

$$
\left\{\mathbf{b} u_{\delta}\right\}_{\mathbf{b}}= \begin{cases}\mathbf{b} u_{\delta}^{+} & \text {if } \mathbf{b} \cdot \mathbf{n}^{+}>0 \\ \mathbf{b} u_{\delta}^{-} & \text {if } \mathbf{b} \cdot \mathbf{n}^{+}<0 \\ \mathbf{b}\left\{u_{\delta}\right\} & \text { if } \mathbf{b} \cdot \mathbf{n}^{+}=0\end{cases}
$$

Observe that $\left\{\mathbf{b} u_{\delta}\right\}_{\mathbf{b}} \cdot\left[v_{\delta}\right]=0$ if $\mathbf{b} \cdot \mathbf{n}^{+}=0$

If the diffusion-transport-reaction equation is written in non-conservative form as in (13.1), by b $\cdot \nabla u=\operatorname{div}(\mathbf{b} u)-\operatorname{div}(\mathbf{b}) u$ it is sufficient to modify $(13.90)$ by substituting the term

$$
\sum_{m=1}^{M}\left(\sigma u_{\delta}, v_{\delta}\right)_{\Omega_{m}} \text { with } \sum_{m=1}^{M}\left(\eta u_{\delta}, v_{\delta}\right)_{\Omega_{m}}
$$

where $\eta(\mathbf{x})=\sigma(\mathbf{x})-\operatorname{div}(\mathbf{b}(\mathbf{x})) ;$ this time we suppose that there exists a positive constant $\eta_{0}>0$ so that $\eta(\mathbf{x}) \geq \eta_{0}$ for almost every $\mathbf{x} \in \Omega$

The DG method can easily be localized to every subdomain $\Omega_{m}$ (either a finite or a spectral element). Indeed, since test functions do not have to be continuous, for every $m(m=1, \ldots, M)$ we can choose test functions to vanish outside the element $\Omega_{m}$. In this way the sum in (13.90) (or in (12.7)) is reduced to the single index $m$, while the one on the edges is reduced to the edges on the boundary of $\Omega_{m}$. We spot here another peculiarity of the DG method: it fits well with local refinements, element by element, either grid-wise (" $h$-refinement"), or polynomial-wise (" $p$-refinement", $p$ being the local polynomial degree, the same indicated with $r$ for finite elements and with $N$ for spectral elements methods).

In Chaps. $14,15,16$ we will see that in hyperbolic problems the solutions can be discontinuous. This is the case when either the initial and/or boundary data are discontinuous and, more in general, for non-linear problems. In all these cases, approximations based on discontinuous polynomial functions are very appropriate. For further particulars on DG methods for diffusion-transport problems, see, e.g., [Coc99, HSS02, BMS04, BHS06, AM09].

Finally, let us point out that the previous description can be extended to a problem where the spatial operator is written as the divergence of a flux depending on $\nabla u$, $\operatorname{div}(\phi(\nabla u))$. For this we only need to replace in every formula $\nabla u$ with the flux expression $\Phi(\nabla u)$.

We refer to Sect. 13.11 for an analysis of the numerical results obtained by the DG method (13.90).

\subsection{Mortar methods for the diffusion-transport equations}

Mortar methods, described in Chapter 12 for the Poisson problem, can be applied to the discretization of equations (13.1) and of those written in conservation form (13.13). The domain discretization and the choice of master and slave spaces can be carried out as described in Chapter 12 for the Poisson problem.

The bilinear forms $a_{i}\left(u_{\delta}^{(i)}, v_{\delta}^{(i)}\right)$ showing up in equation (12.19) and in system (12.30) now read

$$
a_{i}\left(u_{\delta}^{(i)}, v_{\delta}^{(i)}\right)=\int_{\Omega_{i}} \mu \nabla u_{\delta}^{(i)} \cdot \nabla v_{\delta}^{(i)}+\int_{\Omega_{i}} v_{\delta}^{(i)} \mathbf{b} \cdot \nabla u_{\delta}^{(i)}+\int_{\Omega_{i}} \sigma u_{\delta}^{(i)} v_{\delta}^{(i)}
$$

in case of problem (13.1), and

$$
a_{i}\left(u_{\delta}^{(i)}, v_{\delta}^{(i)}\right)=\int_{\Omega_{i}} \mu \nabla u_{\delta}^{(i)} \cdot \nabla v_{\delta}^{(i)}-\int_{\Omega_{i}} u_{\delta}^{(i)} \mathbf{b} \cdot \nabla v_{\delta}^{(i)}+\int_{\Omega_{i}} \sigma u_{\delta}^{(i)} v_{\delta}^{(i)}+\int_{\partial \Omega_{i}} \mathbf{b} \cdot \mathbf{n} u_{\delta}^{(i)} v_{\delta}^{(i)}
$$

in case of the problem in conservation form (13.13), where the transport term has been integrated by parts.

However, we must point out that when $|\mathbf{b}(\mathbf{x})| \gg \mu$, the mortar solution, as seen in the previous chapter, does not always allow to attain optimal error estimates like (12.20) and (12.21), that is a global error that can be expressed as the sum of local errors, without requiring any compatibility constraints between the discretization of the subdomains sharing the same interface. More precisely, instabilities can arise due to the non-conformity at the interfaces, even if stabilization techniques, like GLS or SUPG, are called into play. To understand why that may occur, we first consider the diffusion-transport-reaction problem in conservation form (13.13) and its weak mortar formulation (as in (12.19)) with $a_{i}\left(u_{\delta}^{(i)}, v_{\delta}^{(i)}\right)$ defined in (13.92). Add now the GLS stabilization by substituting the bilinear forms with the stabilized ones

$$
a_{i, \delta}^{(1)}\left(u_{\delta}^{(i)}, v_{\delta}^{(i)}\right)=a_{i}\left(u_{\delta}^{(i)}, v_{\delta}^{(i)}\right)+\sum_{T_{i, k} \in \mathscr{T}_{i}}\left(L u_{\delta}^{(i)}, \tau_{k} L v_{\delta}^{(i)}\right)_{L^{2}\left(T_{i, k}\right)}
$$

The index (1) in $a_{i . \delta}^{(1)}$ indicates the choice $\rho=1$ in the stabilization, according to the formalism used in the previous sections; the elements $T_{i, k}$ are those of the triangulation $\mathscr{I}_{i}$ of $\Omega_{i}$. Now we write

$$
a_{\delta}^{(1)}\left(u_{\delta}, v_{\delta}\right)=\sum_{i=1}^{2} a_{i, \delta}^{(1)}\left(u_{\delta}^{(i)}, v_{\delta}^{(i)}\right)
$$

(Source terms are stabilized in a similar way.)

Using the classical rules of integration, we have

$$
\begin{aligned}
\int_{\Omega_{i}} \operatorname{div}\left(\mathbf{b} u_{\delta}^{(i)}\right) u_{\delta}^{(i)} &=\frac{1}{2} \int_{\Omega_{i}} \operatorname{div}\left(\mathbf{b} u_{\delta}^{(i)}\right) u_{\delta}^{(i)}-\frac{1}{2} \int_{\Omega_{i}} u_{\delta}^{(i)} \mathbf{b} \cdot \nabla u_{\delta}^{(i)}+\frac{1}{2} \int_{\partial \Omega_{i}} \mathbf{b} \cdot \mathbf{n}_{i}\left(u_{\delta}^{(i)}\right)^{2} \\
&=\frac{1}{2} \int_{\Omega_{i}}(\operatorname{div} \mathbf{b})\left(u_{\delta}^{(i)}\right)^{2}+\frac{1}{2} \int_{\Gamma} \mathbf{b} \cdot \mathbf{n}_{i}\left(u_{\delta}^{(i)}\right)^{2}
\end{aligned}
$$

where $\mathbf{n}_{i}$ indicates the outward unit vector on $\partial \Omega_{i}$ and the last boundary integral is now confined on the internal interface thanks to the homogeneous Dirichlet conditions. Then

$$
\begin{aligned}
a_{i, \delta}^{(1)}\left(u_{\delta}^{(i)}, u_{\delta}^{(i)}\right)=& \mu\left\|\nabla u_{\delta}^{(i)}\right\|_{L^{2}\left(\Omega_{i}\right)}^{2}+\left\|\sqrt{\sigma+\frac{1}{2} \operatorname{divb}} u_{\delta}^{(i)}\right\|_{L^{2}\left(\Omega_{i}\right)}^{2} \\
&+\sum_{T_{i, k} \in \mathscr{T}_{i}}\left(L u_{\delta}^{(i)}, \tau_{k} L u_{\delta}^{(i)}\right)_{L^{2}\left(T_{i, k}\right)}+\frac{1}{2} \int_{\Gamma} \mathbf{b} \cdot \mathbf{n}_{i}\left(u_{\delta}^{(i)}\right)^{2}
\end{aligned}
$$

and

$$
\begin{aligned}
a_{\delta}^{(1)}\left(u_{\delta}, u_{\delta}\right)=& \mu\left\|\nabla u_{\delta}\right\|_{L^{2}(\Omega)}^{2}+\left\|\sqrt{\sigma+\frac{1}{2} \operatorname{div} \mathbf{b}} u_{\delta}\right\|_{L^{2}(\Omega)}^{2} \\
&+\sum_{i=1,2 \atop T_{i, k} \in \mathscr{T}_{i}}\left(L u_{\delta}^{(i)}, \tau_{k} L u_{\delta}^{(i)}\right)_{L^{2}\left(T_{i, k}\right)}+\frac{1}{2} \int_{\Gamma} \mathbf{b} \cdot \mathbf{n}_{\Gamma}\left(\left(u_{\delta}^{(1)}\right)^{2}-\left(u_{\delta}^{(2)}\right)^{2}\right)
\end{aligned}
$$

since $\mathbf{n}_{\Gamma}=\mathbf{n}_{1}=-\mathbf{n}_{2}$ on the interface. Because of the presence of the integral $\frac{1}{2} \int_{\Gamma} \mathbf{b} \cdot \mathbf{n}_{\Gamma}\left(\left(u_{\delta}^{(1)}\right)^{2}-\left(u_{\delta}^{(2)}\right)^{2}\right), a_{\delta}^{(1)}$ can fail to be coercive for to the norm $\|\cdot\|_{G L S}$ or any other discrete norm. To go around this problem, in $\left[\mathrm{AAH}^{+} 98\right]$ it is suggested to add to the bilinear form $a_{\delta}^{(1)}$ one more DGlike stabilization term on $\Gamma$,

$$
\tilde{a}_{\Gamma}\left(u_{\delta}, v_{\delta}\right)=\int_{\Gamma}\left(\mathbf{b} \cdot \mathbf{n}_{1}\right)^{-}\left(u_{\delta}^{(2)}-u_{\delta}^{(1)}\right) v_{\delta}^{(1)}+\int_{\Gamma}\left(\mathbf{b} \cdot \mathbf{n}_{2}\right)^{-}\left(u_{\delta}^{(1)}-u_{\delta}^{(2)}\right) v_{\delta}^{(2)}
$$

Here $x^{-}$indicates the negative part of the number $x$ (i.e. $x^{-}=(|x|-x) / 2$ ). By splitting each integral on $\Gamma$ into two terms according to the sign of $\left(\mathbf{b} \cdot \mathbf{n}_{i}\right)^{-}$, with the aid of simple algebraic operations we obtain

$$
\tilde{a}_{\Gamma}\left(u_{\delta}, u_{\delta}\right)=\frac{1}{2} \int_{\Gamma}\left|\mathbf{b} \cdot \mathbf{n}_{\Gamma}\right|\left(u_{\delta}^{(1)}-u_{\delta}^{(2)}\right)^{2}
$$

The form $a_{\delta}^{(1)}\left(u_{\delta}, u_{\delta}\right)+\tilde{a}_{\Gamma}\left(u_{\delta}, u_{\delta}\right)$ is therefore coercive for the GLS norm (13.77).

\subsection{Some numerical tests}

We now present some numerical solutions obtained using linear finite elements for the following two-dimensional diffusion-transport probem

$$
\left\{\begin{array}{l}
-\mu \Delta u+\mathbf{b} \cdot \nabla u=f \quad \text { in } \Omega=(0,1) \times(0,1) \\
u=g \quad \text { on } \partial \Omega
\end{array}\right.
$$

where $\mathbf{b}=(1,1)^{T}$. To start with let us consider the following constant data: $f \equiv 1$ and
![](https://cdn.mathpix.com/cropped/10058ab4e7f0c5bd6fe5bcd4db204623-355.jpg?height=268&width=722&top_left_y=905&top_left_x=94)

Fig. 13.11. Approximation of problem (13.94) with $\mu=10^{-3}, h=1 / 80$, using the standard (left) and GLS (right) Galerkin method. The corresponding local Péclet number is $\mathbb{P e}=8.84$

$g \equiv 0$. In this case the solution is characterized by a boundary layer near the edges $x=1$ and $y=1$. We have considered two different viscosity values: $\mu=10^{-3}$ and $\mu=10^{-5}$. For both problems we compare the solutions obtained using the standard and GLS Galerkin methods, respectively, by making two different choices for the uniform discretization step $h: 1 / 20$ and $1 / 80$, respectively. In the GLS case we used the value (13.61) of $\tau_{K}$. The combinations of the two values for $\mu$ and $h$ yield four different values for the local Péclet number Pe. As it can be observed by analyzing Figs. 13.11-13.14 (bearing in mind the different vertical scales) for increasing Péclet numbers, the solution provided by the standard Galerkin method denotes stronger and stronger fluctuations. The latter eventually overcome completely the numerical solution (see Fig. 13.14). On the other hand, the GLS method is able to provide an acceptable numerical solution even for extremely high values of $\mathbb{P e}$ (even though it develops an over-shoot at the point $(1,1)$ ).

Let us now choose forcing term $f$ and the boundary data $g$ in such a way that

$$
u(x, y)=x+y(1-x)+\frac{\mathrm{e}^{-1 / \mu}-\mathrm{e}^{-(1-x)(1-y) / \mu}}{1-\mathrm{e}^{-1 / \mu}}
$$

be the exact solution (see e.g. [HSS02]). The corresponding Péclet number is $\mathbb{P} e_{g l}=$ $(\sqrt{2} \mu)^{-1}$. For small values of the viscosity $\mu$, this solution features a boundary layer near the edges $x=1$ and $y=1$. In what follows we consider two different values of the viscosity: $\mu=10^{-1}$ and $\mu=10^{-9}$. The numerical approximation is based on piecewise linear finite elements $(r=1)$. We compare the numerical solutions obtained using the standard Galerkin method, the SUPG method, and the DG method (13.92) (with $\sigma=1$ and $\gamma=10 r^{2}$ ) under the variant DG-N the $(12.7)$, for which the boundary conditions are imposed using the Nitsche penalization technique [Nit71]. In Figs. 13.5 and $13.6$ we plot the numerical solutions using an unstructured triangular grid whose gridsize is $h \approx 1 / 8$. Similar results obtained on a finer grid with $h \approx 1 / 16$ are displayed in Figs. 13.17-13.18; they show that for small Péclet (global) number, the three different methods yield very similar solutions. For large Péclet number, Figs 13.16$13.18$ (be aware of the different vertical scales) show that the solution of the Galerkin
![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-01.jpg?height=210&width=718&top_left_y=963&top_left_x=98)

Fig. 13.12. Approximation of problem (13.94) with $\mu=10^{-3}, h=1 / 20$, using the standard (left) and GLS (right) Galerkin method. The corresponding local Péclet number is $\mathbb{P e}=35.35$ 
![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-02.jpg?height=278&width=726&top_left_y=110&top_left_x=95)

Fig. 13.13. Approximation of problem (13.94) with $\mu=10^{-5}, h=1 / 80$, using the standard (left) and GLS (right) Galerkin method. The corresponding local Péclet number is $\mathbb{P e}=883.88$

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-02.jpg?height=258&width=358&top_left_y=527&top_left_x=98)
![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-02.jpg?height=258&width=718&top_left_y=527&top_left_x=98) (left) and GLS (right) Galerkin method. The corresponding local Péclet number is $\mathbb{P e}=3535.5$

method exhibits very pronounced spurious oscillations. Instead, the SUPG provides acceptable solutions even though they are affected by over-shoot in correspondence to the point $(1,1)$.

The DG method exhibits oscillations on those elements intersecting the outflow boundary, because of the direct (essential) treatment of the boundary condition. Finally the DG-N solution has neither oscillations no over-shoots, thanks to they weak treatment of the boundary condition. However by this (Nitsche) approach the boundary longer solution cannot be properly approximated.

Finally, we consider a pure transport problem, that is $\mathbf{b} \cdot \nabla u=f$ in $\Omega=(0,1)^{2}$ with $u=g \mathrm{su} \Gamma^{-}, \mathbf{b}=(1,1)$, with $f$ and $g$ chosen in such a way that the exact solution is $u(x, y)=1+\sin \left(\pi(x+1)(y+1)^{2} / 8\right)$.

We solve this problem by the DG method with piecewise polynomials of degree $r=1,2,3$ and 4 on a sequence of uniform triangular grids with gridsize $h$. The DG 

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-03.jpg?height=165&width=255&top_left_y=145&top_left_x=196)

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-03.jpg?height=163&width=259&top_left_y=146&top_left_x=459)

(a) Standard Galerkin method

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-03.jpg?height=167&width=254&top_left_y=364&top_left_x=195)

(c) DG method (b) SUPG method

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-03.jpg?height=161&width=258&top_left_y=367&top_left_x=460)

(d) DG-N variant of method DG

Fig. 13.15. The approximate solution of problem (13.94) for $\mu=10^{-1}$ obtained respectively the Galerkin method, the SUPG method, the DG method (13.92) (with $\sigma=1$ and $\gamma=10 r^{2}$ ), and the DG-N variant with weak imposition of boundary conditions. Triangula grid with $h \approx 1 / 8$ and piecewise linear finite elements $(r=1)$

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-03.jpg?height=190&width=254&top_left_y=651&top_left_x=200)

(a) Standard Galerkin method

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-03.jpg?height=195&width=251&top_left_y=875&top_left_x=196)

(c) DG method

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-03.jpg?height=190&width=255&top_left_y=652&top_left_x=463)

(b) SUPG method

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-03.jpg?height=195&width=258&top_left_y=875&top_left_x=460)

(d) DG-N variant of method DG

Fig. 13.16. The approximate solution of problem (13.94) for $\mu=10^{-9}$ obtained respectively the Galerkin method, the SUPG method, the DG method (13.92) (with $\sigma=1$ and $\gamma=10 r^{2}$ ), and the DG-N variant with weak imposition of boundary conditions. Triangula grid with $h \approx 1 / 8$ and piecewise linear finite elements $(r=1)$ 

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-04.jpg?height=162&width=252&top_left_y=147&top_left_x=196)

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-04.jpg?height=163&width=260&top_left_y=146&top_left_x=459)

(a) Standard Galerkin method

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-04.jpg?height=168&width=257&top_left_y=363&top_left_x=195)

(c) DG method (b) SUPG method

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-04.jpg?height=164&width=259&top_left_y=365&top_left_x=459)

(d) DG-N variant of method DG

Fig. 13.17. The approximate solution of problem (13.94) for $\mu=10^{-1}$ obtained respectively by the Galerkin method, the SUPG method, the DG method (13.92) (with $\sigma=1$ and $\gamma=10 r^{2}$ ), and the DG-N variant with weak imposition of boundary conditions. Triangular grid with $h \approx 1 / 16$

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-04.jpg?height=189&width=251&top_left_y=647&top_left_x=200)

(a) Standard Galerkin method

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-04.jpg?height=206&width=256&top_left_y=861&top_left_x=193)

(c) DG method and piecewise linear finite elements $(r=1)$

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-04.jpg?height=189&width=254&top_left_y=647&top_left_x=462)

(b) SUPG method

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-04.jpg?height=200&width=258&top_left_y=865&top_left_x=460)

(d) DG-N variant of method DG

Fig. 13.18. The approximate solution of problem (13.94) for $\mu=10^{-9}$ obtained respectively by the Galerkin method, the SUPG method, the DG method (13.92) (with $\sigma=1$ and $\gamma=10 r^{2}$ ), and the DG-N variant with weak imposition of boundary conditions. Triangular grid with $h \approx 1 / 16$ and piecewise linear finite elements $(r=1)$ 

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-05.jpg?height=342&width=430&top_left_y=94&top_left_x=240)

Fig. 13.19. Approximation error vs number of degrees of freedom for finite elements of degree $r=1,2,3,4$

method provides the following error estimate (see, e.g., [BMS04]).

$$
\left\|u-u_{h}\right\|=\left(\left\|u-u_{h}\right\|_{L^{2}(\Omega)}^{2}+\sum_{e \in \mathscr{E}_{h}}\left\|s_{e}^{1 / 2}\left[u-u_{h}\right]\right\|_{0, e}^{2}\right)^{1 / 2} \leq C h^{r+1 / 2}\|u\|_{H^{r+1}(\Omega)}
$$

$(13.95)$

where $s_{e}=\alpha\left|\mathbf{b} \cdot \mathbf{n}_{\mathbf{e}}\right|$ is a suitable stabilization term, where $\alpha$ is a positive constant independent of $h$ and $e . \mathscr{E}_{h}$ is the set of all the edges of the triangulation and $C$ is a positive constant. In Fig. $13.19$ we report (in logarithmic scale) the errors computed in the energy norm (13.95): the decay as $h^{r+1 / 2}$, as predicted by the error estimate (13.95).
![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-05.jpg?height=246&width=710&top_left_y=870&top_left_x=100)

Fig. 13.20. Finite element solutions obtained on a uniform grid with gridsize $h=1 / 4$ (left) and $h=1 / 8$ (right $)$ 

\subsection{An example of goal-oriented grid adaptivity}

As anticipated in Remark 4.10, the a posteriori analysis presented in Sect. $4.6 .5$ for the control of a suitable functional of the error can be extended to differential problems of various kinds by assuming a suitable redefinition of the local residue (4.98) and of the generalized jump (4.94). A grid adaptation turns out to be particularly useful when dealing with diffusion-transport problems with dominant transport. Here, an accurate placement of the mesh triangles, e.g. at the (internal or boundary) layers, can dramatically reduce the computational cost.

Let us consider problem (13.1) with $\mu=10^{-3}, \mathbf{b}=(y,-x)^{T}, \sigma$ and $f$ identically null, and $\Omega$ coinciding with the $L$-shaped domain (Fig. 13.21) $(0,4)^{2} \backslash(0,2)^{2}$. Let us suppose to assign a homogeneous Neumann condition on $\{x=4\}$ and $\{y=0\}$, a nonhomogeneous Dirichlet condition $(u=1)$ on $\{x=0\}$, and a homogeneous Dirichlet condition on the remaining parts of the boundary. The solution $u$ of $(13.1)$ thus is characterized by two internal layers having a round shape. In order to test the sensitivity of the adapted grid with respect to the specific choice made for the functional $J$, let us consider the two following options:

$$
J(v)=J_{1}(v)=\int_{\Gamma_{1}} \mathbf{b} \cdot \mathbf{n} v d s, \quad \text { with } \quad \Gamma_{1}=\{x=4\} \cup\{y=0\}
$$

for the control of the outgoing normal flow through $\{x=4\}$ and $\{y=0\}$, and

$$
J(v)=J_{2}(v)=\int_{\Gamma_{2}} \mathbf{b} \cdot \mathbf{n} v d s, \quad \text { with } \quad \Gamma_{2}=\{x=4\}
$$

if we are still interested in controlling the flow, but only through $\{x=4\}$. Starting from a quasi-uniform initial grid of 1024 elements, we show in Fig. $13.21$ the (anisotropic) grids obtained for the choice $J=J_{1}$ (left) resp. $J=J_{2}$ (right), at the fourth and second iteration of the adaptive process. Both boundary layers are responsible for the flow through $\Gamma_{1}$, and in fact the grid is refined in correspondence of the two layers. However, only the upper layer is "recognized" as carrying information to the flow along $\Gamma_{2}$. Finally, note the strongly anisotropic nature of the mesh in the figure. In order to follow not only the refinement but also the correct orientation of the grid, triangles are implemented in such a way to follow the directional properties (the boundary layers) of the solution. For further details, refer to [FMP04].

\subsection{Exercises}

1. Decompose in its symmetric and skew-symmetric parts the one-dimensional diffusion-transport-reaction operator

$$
L u=-\mu u^{\prime \prime}+b u^{\prime}+\sigma u
$$


![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-07.jpg?height=330&width=696&top_left_y=118&top_left_x=106)

Fig. 13.21. Fourth adapted grid for the functional $J_{1}$ (left); second adapted grid for the functional $J_{2}$ (right)

2. Split in its symmetric and skew-symmetric parts the diffusion-transport operator written in the non-divergence form

$$
L u=-\mu \Delta u+\mathbf{b} \cdot \nabla u
$$

3. Prove that the one-dimensional linear, quadratic and cubic finite elements yield, in the reference interval $[0,1]$, the following condensed matrices, obtained via the mass-lumping technique:

$$
\begin{array}{ll}
r=1 \quad & \mathbf{M}_{L}=\widehat{\mathbf{M}}=\frac{1}{2} \operatorname{diag}(11) \\
r=2 \quad \mathbf{M}_{L}=\widehat{\mathbf{M}}=\frac{1}{6} \operatorname{diag}(141) \\
r=3 & \left\{\begin{array}{l}
\mathrm{M}_{L}=\frac{1}{8} \operatorname{diag}(1331) \\
\widehat{\mathrm{M}}=\frac{1}{1552} \operatorname{diag}(128648648128)=\operatorname{diag}\left(\frac{8}{97}, \frac{81}{194}, \frac{81}{194}, \frac{8}{97}\right)
\end{array}\right.
\end{array}
$$

4. Consider the problem

$$
\left\{\begin{array}{lr}
-\varepsilon u^{\prime \prime}(x)+b u^{\prime}(x)=1, & 0<x<1 \\
u(0)=\alpha, & u(1)=\beta
\end{array}\right.
$$

where $\varepsilon>0$ and $\alpha, \beta, b \in \mathbb{R}$ are given. Find its finite element formulation with upwind artificial viscosity. Discuss its stability and convergence properties and compare them with that of the Galerkin-linear finite elements formulation.

5. Consider the problem

$$
\left\{\begin{array}{l}
-\varepsilon u^{\prime \prime}(x)+u^{\prime}(x)=1, \quad 0<x<1 \\
u(0)=0, \quad u^{\prime}(1)=1
\end{array}\right.
$$

with $\varepsilon>0$ given. Write its weak formulation and its approximation of Galerkinfinite element type. Verify that the scheme is stable and explain why.

6. Consider the problem

$$
\begin{cases}-\operatorname{div}(\mu \nabla u)+\operatorname{div}(\boldsymbol{\beta} u)+\sigma u=f & \text { in } \Omega \\ -\gamma \cdot \mathbf{n}+\mu \nabla u \cdot \mathbf{n}=0 & \text { on } \Gamma_{N} \\ u=0 & & \text { on } \Gamma_{D}\end{cases}
$$

where $\Omega$ is an open subset of $\mathbb{R}^{2}$ with boundary $\Gamma=\Gamma_{D} \cup \Gamma_{N}, \stackrel{\circ}{\Gamma} \cap \stackrel{\circ}{\Gamma}_{N}=\emptyset, \mathbf{n}$ is the outgoing normal to $\Gamma, \mu=\mu(\mathbf{x})>\mu_{0}>0, \sigma=\sigma(\mathbf{x})>0, f=f(\mathbf{x})$ are given scalar functions, $\boldsymbol{\beta}=\boldsymbol{\beta}(\mathbf{x}), \gamma=\gamma(\mathbf{x})$ are given vector functions.

Approximate it using the Galerkin-linear finite element method. Find under which hypotheses on the coefficients $\mu, \sigma$ and $\beta$ the method is inaccurate, and suggest the relevant remedies in the different cases.

7. Consider the one-dimensional diffusion-transport problem

$$
\left\{\begin{array}{l}
-\left(\mu u^{\prime}-\psi^{\prime} u\right)^{\prime}=1, \quad 0<x<1 \\
u(0)=u(1)=0
\end{array}\right.
$$

where $\mu$ is a positive constant and $\psi$ a given function.

a) Study the existence and uniqueness of problem (13.96) by introducing suitable hypotheses on the function $\psi$, and propose a stable numerical approximation with finite elements.

b) Consider the variable change $u=\rho e^{\psi / \mu}, \rho$ being an auxiliary unknown function. Study the existence and uniqueness of the weak solution of problem $(13.96)$ in the new unknown $\rho$ and provide its numerical approximation using the finite elements method.

c) Compare the two approaches followed in (a) and (b), both from the abstract viewpoint and from the numerical one.

8. Consider the diffusion-transport-reaction problem

$$
\begin{cases}-\Delta u+\operatorname{div}(\mathbf{b} u)+u=0 & \text { in } \Omega \subset \mathbb{R}^{2} \\ u=\varphi \quad \text { on } \Gamma_{D}, \quad \frac{\partial u}{\partial n}=0 & \text { on } \Gamma_{N}\end{cases}
$$

where $\Omega$ is an open bounded domain, $\partial \Omega=\Gamma_{D} \cup \Gamma_{N}, \Gamma_{D} \neq \emptyset$

Prove the existence and uniqueness of the solution by making suitable regularity assumptions on the data $\mathbf{b}=\left(b_{1}(\mathbf{x}), b_{2}(\mathbf{x})\right)^{T}(\mathbf{x} \in \Omega)$ and $\varphi=\varphi(\mathbf{x})\left(\mathbf{x} \in \Gamma_{D}\right)$. In the case where $|\mathbf{b}| \gg 1$, approximate the problem with the artificial diffusionfinite elements and SUPG-finite elements methods, discussing advantages and disadvantages with respect to the Galerkin finite element method. 9. Consider the problem

$$
-\sum_{i, j=1}^{2} \frac{\partial^{2} u}{\partial x_{i} \partial x_{j}}+\beta \frac{\partial^{2} u}{\partial x_{1}^{2}}+\gamma \frac{\partial^{2} u}{\partial x_{1} \partial x_{2}}+\delta \frac{\partial^{2} u}{\partial x_{2}^{2}}+\eta \frac{\partial u}{\partial x_{1}}=f \text { in } \Omega
$$

with $u=0$ on $\partial \Omega$, where $\beta, \gamma, \delta, \eta$ are given coefficients and $f$ is a given function of $\mathbf{x}=\left(x_{1}, x_{2}\right) \in \Omega$

a) Find the conditions on the data that ensure the existence and uniqueness of a weak solution.

b) Provide an approximation using the Galerkin finite element method and analyze its convergence.

c) Under which conditions on the data is the Galerkin problem symmetric?

In such case, provide suitable methods for the solution of the associated algebraic problem. 

\section{Finite differences for hyperbolic equations}

In this chapter we deal with time-dependent problems of hyperbolic type. For their origin and an in-depth analysis see e.g. [Sal08, Chap. 4]. We will limit ourselves to considering the numerical approximation using the finite difference method, which was historically the first one to be applied to this type of equations. To introduce in a simple way the basic concepts of the theory, most of our presentation will concern problems depending on a single space variable. Finite element approximations will be addressed in Chapter 15, the extension to nonlinear problems in Chapter $16$.

\subsection{A scalar transport problem}

Let us consider the following scalar hyperbolic problem

$$
\begin{cases}\frac{\partial u}{\partial t}+a \frac{\partial u}{\partial x}=0, & x \in \mathbb{R}, t>0 \\ u(x, 0)=u_{0}(x), & x \in \mathbb{R}\end{cases}
$$

where $a \in \mathbb{R} \backslash\{0\}$. The solution of such problem is a wave travelling at velocity $a$, in the $(x, t)$ plane, given by

$$
u(x, t)=u_{0}(x-a t), \quad t \geq 0
$$

We consider the curves $x(t)$ in the plane $(x, t)$, solutions of the following ordinary differential equation

$$
\left\{\begin{array}{l}
\frac{d x}{d t}=a, \quad t>0 \\
x(0)=x_{0}
\end{array}\right.
$$

for varying values of $x_{0} \in \mathbb{R}$. They read $x(t)=x_{0}+a t$ and are called characteristic lines (often simply characteristics). The solution of (14.1) along these lines remains constant, for

$$
\frac{d u}{d t}=\frac{\partial u}{\partial t}+\frac{\partial u}{\partial x} \frac{d x}{d t}=0
$$

In the case of the more general problem

$$
\begin{cases}\frac{\partial u}{\partial t}+a \frac{\partial u}{\partial x}+a_{0} u=f, & x \in \mathbb{R}, \quad t>0 \\ u(x, 0)=u_{0}(x), & x \in \mathbb{R}\end{cases}
$$

where $a, a_{0}$, and $f$ are given functions of $(x, t)$, the characteristic lines $x(t)$ are the solutions of the Cauchy problem

$$
\left\{\begin{array}{l}
\frac{d x}{d t}=a(x, t), \quad t>0 \\
x(0)=x_{0}
\end{array}\right.
$$

In such case, the solutions of $(14.2)$ satisfy the following relation

$$
\frac{d}{d t} u(x(t), t)=f(x(t), t)-a_{0}(x(t), t) u(x(t), t)
$$

Therefore it is possible to extract the solution $u$ by solving an ordinary differential equation on each characteristic curve (this approach leads to the so-called characteristic method ).

If, for instance $a_{0}=0$, we find

$$
u(x, t)=u_{0}(x-a t)+\int_{0}^{t} f(x-a(t-s), s) d s, \quad t>0
$$

Let us now consider problem (14.1) in a bounded interval. For instance, let us suppose $x \in[0,1]$ and $a>0$. As $u$ is constant on the characteristics, from Fig. $14.1$ we deduce that the value of the solution at a point $\mathrm{P}$ coincides with the value of $u_{0}$ at the foot $P_{0}$ of the characteristic outgoing from P. Instead, the characteristic outgoing from $\mathrm{Q}$

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-11.jpg?height=244&width=255&top_left_y=954&top_left_x=326)

Fig. 14.1. Examples of characteristic lines (straight lines in this case) issuing from $P$ and $Q$ intersects the straight line $x=0$ for $t>0$. The point $x=0$ is therefore an inflow point at which we must necessarily assign the value of $u$. Note that if $a<0$, the inflow point would be $x=1$.

By referring to problem (14.1) it is useful to observe that if $u_{0}$ were a discontinuous function at $x_{0}$, then such discontinuity would propagate along the characteristic outgoing from $x_{0}$ (this process can be rigorously formalized from a mathematical viewpoint by introducing the concept of weak solution for hyperbolic problems). In order to regularize the discontinuity, one could approximate the initial datum $u_{0}$ with a sequence of regular functions $u_{0}^{\varepsilon}(x), \varepsilon>0$. However, this procedure is only effective if the hyperbolic problem is linear. The solutions of nonlinear hyperbolic problems can indeed develop discontinuities also for regular initial data (as we will see in Chapter 16). In this case the strategy (which also inspires numerical methods) is to regularize the differential equation itself, rather than the initial datum. We can consider the following diffusion-transport equation

$$
\frac{\partial u^{\varepsilon}}{\partial t}+a \frac{\partial u^{\varepsilon}}{\partial x}=\varepsilon \frac{\partial^{2} u^{\varepsilon}}{\partial x^{2}}, \quad x \in \mathbb{R}, t>0
$$

for small values of $\varepsilon>0$, which can be regarded as a parabolic regularization of equation (14.1). If we set $u^{\varepsilon}(x, 0)=u_{0}(x)$, we can prove that

$$
\lim _{\varepsilon \rightarrow 0^{+}} u^{\varepsilon}(x, t)=u_{0}(x-a t), t>0, x \in \mathbb{R}
$$

\subsubsection{An a priori estimate}

Let us now return to the transport-reaction problem (14.2) on a bounded interval

$$
\begin{cases}\frac{\partial u}{\partial t}+a \frac{\partial u}{\partial x}+a_{0} u=f, & x \in(\alpha, \beta), t>0 \\ u(x, 0)=u_{0}(x), & x \in[\alpha, \beta] \\ u(\alpha, t)=\varphi(t), & t>0\end{cases}
$$

where $a(x), f(x, t)$ and $\varphi(t)$ are assigned functions; we have made the assumption that $a(x)>0$, so that $x=\alpha$ is the inflow point (where to impose the boundary condition), while $x=\beta$ is the outflow point.

By multiplying the first equation of (14.3) by $u$, integrating in $x$ and using the formula of integration by parts, we obtain for each $t>0$

$$
\frac{1}{2} \frac{d}{d t} \int_{\alpha}^{\beta} u^{2} d x+\int_{\alpha}^{\beta}\left(a_{0}-\frac{1}{2} a_{x}\right) u^{2} d x+\frac{1}{2}\left(a u^{2}\right)(\beta)-\frac{1}{2}\left(a u^{2}\right)(\alpha)=\int_{\alpha}^{\beta} f u d x
$$

By supposing that there exists a $\mu_{0} \geq 0$ such that

$$
a_{0}-\frac{1}{2} a_{x} \geq \mu_{0} \quad \forall x \in[\alpha, \beta]
$$

we find

$$
\frac{1}{2} \frac{d}{d t}\|u(t)\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2}+\mu_{0}\|u(t)\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2}+\frac{1}{2}\left(a u^{2}\right)(\beta) \leq \int_{\alpha}^{\beta} f u d x+\frac{1}{2} a(\alpha) \varphi^{2}(t)
$$

If $f$ and $\varphi$ are identically zero, then

$$
\|u(t)\|_{L^{2}(\alpha, \beta)} \leq\left\|u_{0}\right\|_{L^{2}(\alpha, \beta)} \quad \forall t>0 .
$$

In the case of the more general problem (14.2), if we suppose that $\mu_{0}>0$, thanks to the Cauchy-Schwarz and Young inequalities we have

$$
\int_{\alpha}^{\beta} f u d x \leq\|f\|_{\mathrm{L}^{2}(\alpha, \beta)}\|u\|_{\mathrm{L}^{2}(\alpha, \beta)} \leq \frac{\mu_{0}}{2}\|u\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2}+\frac{1}{2 \mu_{0}}\|f\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2}
$$

Integrating over time we get the following a priori estimate

$$
\begin{aligned}
&\|u(t)\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2}+\mu_{0} \int_{0}^{t}\|u(s)\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2} d s+a(\beta) \int_{0}^{t} u^{2}(\beta, s) d s \\
&\leq\left\|u_{0}\right\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2}+a(\alpha) \int_{0}^{t} \varphi^{2}(s) d s+\frac{1}{\mu_{0}} \int_{0}^{t}\|f\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2} d s
\end{aligned}
$$

An alternative estimate that does not require the differentiability of $a(x)$, but uses, instead, the hypothesis that $a_{0} \leq a(x) \leq a_{1}$ for two suitable positive constants $a_{0}$ and $a_{1}$, can be obtained by multiplying the equation by $a^{-1}$,

$$
a^{-1} \frac{\partial u}{\partial t}+\frac{\partial u}{\partial x}=a^{-1} f
$$

By multiplying by $u$ and integrating between $\alpha$ and $\beta$ we obtain,

$$
\frac{1}{2} \frac{d}{d t} \int_{\alpha}^{\beta} a^{-1}(x) u^{2}(x, t) d x+\frac{1}{2} u^{2}(\beta, t)=\int_{\alpha}^{\beta} a^{-1}(x) f(x, t) u(x, t) d x+\frac{1}{2} \varphi^{2}(t)
$$

If $f=0$ we immediately obtain

$$
\|u(t)\|_{a}^{2}+\int_{0}^{t} u^{2}(\beta, s) d s=\left\|u_{0}\right\|_{a}^{2}+\int_{0}^{t} \varphi^{2}(s) d s, t>0
$$

We have defined

$$
\|v\|_{a}=\left(\int_{\alpha}^{\beta} a^{-1}(x) v^{2}(x) d x\right)^{\frac{1}{2}}
$$

Thanks to the lower and upper bounds of $a^{-1}$, the latter is equivalent to the norm of $\mathrm{L}^{2}(\alpha, \beta)$. On the other hand, if $f \neq 0$, we can proceed as follows

$$
\|u(t)\|_{a}^{2}+\int_{0}^{t} u^{2}(\beta, s) d s \leq\left\|u_{0}\right\|_{a}^{2}+\int_{0}^{t} \varphi^{2}(s) d s+\int_{0}^{t}\|f\|_{a}^{2} d s+\int_{0}^{t}\|u(s)\|_{a}^{2} d s
$$

having used the Cauchy-Schwarz inequality.

By now applying Gronwall's lemma (see Lemma 2.2) we obtain, for each $t>0$,

$$
\|u(t)\|_{a}^{2}+\int_{0}^{t} u^{2}(\beta, s) d s \leq e^{t}\left(\left\|u_{0}\right\|_{a}^{2}+\int_{0}^{t} \varphi^{2}(s) d s+\int_{0}^{t}\|f\|_{a}^{2} d s\right)
$$

\subsection{Systems of linear hyperbolic equations}

Let us consider a linear system of the form

$$
\begin{cases}\frac{\partial \mathbf{u}}{\partial t}+\mathrm{A} \frac{\partial \mathbf{u}}{\partial x}=\mathbf{0}, & x \in \mathbb{R}, t>0 \\ \mathbf{u}(0, x)=\mathbf{u}_{0}(x), & x \in \mathbb{R}\end{cases}
$$

where $\mathbf{u}:[0, \infty) \times \mathbb{R} \rightarrow \mathbb{R}^{p}, \mathrm{~A}: \mathbb{R} \rightarrow \mathbb{R}^{p \times p}$ is a given matrix, and $\mathbf{u}_{0}: \mathbb{R} \rightarrow \mathbb{R}^{p}$ is the initial datum.

Let us first consider the case where the coefficients of $A$ are constant (i.e. independent of both $x$ and $t$ ). System (14.5) is called hyperbolic if A can be diagonalized and has real eigenvalues, that is

$$
\mathrm{A}=\mathrm{T} \Lambda \mathrm{T}^{-1}
$$

Here $\Lambda=\operatorname{diag}\left(\lambda_{1}, \ldots, \lambda_{p}\right)$, with $\lambda_{i} \in \mathbb{R}$ for $i=1, \ldots, p$, is the diagonal matrix of the eigenvalues of $\mathrm{A}, \mathrm{T}: \mathbb{R} \rightarrow \mathbb{R}^{p \times p}, \mathrm{~T}=\left[\omega^{1}, \omega^{2}, \ldots, \boldsymbol{\omega}^{p}\right]$ is the matrix whose column vectors are the right eigenvectors of $\mathrm{A}$, that is

$$
\mathrm{A\omega}^{k}=\lambda_{k} \boldsymbol{\omega}^{k}, \quad k=1, \ldots, p
$$

Through this similarity transformation system (14.5) becomes

$$
\frac{\partial \mathbf{w}}{\partial t}+\Lambda \frac{\partial \mathbf{w}}{\partial x}=\mathbf{0}
$$

where $\mathbf{w}=\mathrm{T}^{-1} \mathbf{u}$ are called characteristic variables. In this way we obtain $p$ independent equations of the form

$$
\frac{\partial w_{k}}{\partial t}+\lambda_{k} \frac{\partial w_{k}}{\partial x}=0, \quad k=1, \ldots, p
$$

analogous in all to the equation of problem (14.1) (provided that we suppose $a_{0}$ and $f$ null). The solution $w_{k}$ is therefore constant along each characteristic curve $x=x(t)$, solution of the Cauchy problem

$$
\left\{\begin{array}{l}
\frac{d x}{d t}=\lambda_{k}, \quad t>0 \\
x(0)=x_{0} .
\end{array}\right.
$$

Since the $\lambda_{k}$ are constant, the characteristic curves are in fact the lines $x(t)=x_{0}+$ $\lambda_{k} t$ and the solutions read $w_{k}(x, t)=\psi_{k}\left(x-\lambda_{k} t\right)$, where $\psi_{k}$ is a function of a single variable determined by the initial conditions. In the case of problem (14.5), we have that $\psi_{k}(x)=w_{k}(x, 0)$, thus the solution $\mathbf{u}=\mathrm{T} \mathbf{w}$ will have the form

$$
\mathbf{u}(x, t)=\sum_{k=1}^{p} w_{k}\left(x-\lambda_{k} t, 0\right) \omega^{k}
$$

The latter is composed by $p$ travelling, non-interacting waves.

Since a strictly hyperbolic system admits $p$ different characteristic lines issuing from each point $(\bar{x}, \bar{t})$ of the plane $(x, t), u(\bar{x}, \bar{t})$ will only depend on the initial datum at the points $\bar{x}-\lambda_{k} \bar{t}$, for $k=1, \ldots, p$. For this reason, the set of the $p$ points representing the feet of the characteristics outgoing from $(\bar{x}, \bar{t})$, that is

$$
D(\bar{x}, \bar{t})=\left\{x \in \mathbb{R} \mid x=\bar{x}-\lambda_{k} \bar{t}, k=1, \ldots, p\right\}
$$

is called domain of dependence of the solution $\mathbf{u}$ at the point $(\bar{x}, \bar{t})$.

In case we consider a bounded interval $(\alpha, \beta)$ instead of the whole real line, the sign of $\lambda_{k}, k=1, \ldots, p$, denotes the inflow point for each of the characteristic variables. The function $\psi_{k}$ in the case of a problem set on a bounded interval will be determined not only by the initial conditions, but also by the boundary conditions at the inflow of each characteristic variable. Having considered a point $(\bar{x}, \bar{t})$ with $\bar{x} \in(\alpha, \beta)$ and $\bar{t}>0$, if $\bar{x}-\lambda_{k} \bar{t} \in(\alpha, \beta)$ then $w_{k}(\bar{x}, \bar{t})$ is determined by the initial condition, in

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-15.jpg?height=247&width=651&top_left_y=905&top_left_x=129)

Fig. 14.2. The value of $w_{k}$ at a point in the plane $(x, t)$ depends either on the boundary condition or on the initial condition, depending on the value of $x-\lambda_{k} t$. Both signs of $\lambda_{k}$, the positive (right) and negative (left), are shown particular we have $w_{k}(\bar{x}, \bar{t})=w_{k}\left(\bar{x}-\lambda_{k} \bar{t}, 0\right)$. Conversely, if $\bar{x}-\lambda_{k} \bar{t} \notin(\alpha, \beta)$ then the value of $w_{k}(\bar{x}, \bar{t})$ will depend on the boundary condition (see Fig. 14.2):

$$
\begin{aligned}
&\text { if } \lambda_{k}>0, w_{k}(\bar{x}, \bar{t})=w_{k}\left(\alpha, \bar{t}-\frac{\bar{x}-\alpha}{\lambda_{k}}\right) \\
&\text { if } \lambda_{k}<0, w_{k}(\bar{x}, \bar{t})=w_{k}\left(\beta, \bar{t}-\frac{\bar{x}-\beta}{\lambda_{k}}\right)
\end{aligned}
$$

As a consequence, the number of positive eigenvalues determines the number of boundary conditions to be assigned at $x=\alpha$, while at $x=\beta$ we will need to assign as many conditions as the number of negative eigenvalues.

In the case where the coefficients of the matrix A in (14.5) are functions of $x$ and $t$, we denote respectively by

$$
\mathrm{L}=\left[\begin{array}{c}
\mathbf{l}_{1}^{T} \\
\vdots \\
\mathbf{l}_{p}^{T}
\end{array}\right] \quad \text { and } \quad \mathrm{R}=\left[\mathbf{r}_{1} \ldots \mathbf{r}_{p}\right]
$$

the matrices containing the left, resp. right, eigenvectors of $\mathrm{A}$, whose elements satisfy the relations

$$
\mathbf{A} \mathbf{r}_{k}=\lambda_{k} \mathbf{r}_{k}, \quad \mathbf{l}_{k}^{T} \mathbf{A}=\lambda_{k} \mathbf{l}_{k}^{T},
$$

that is

$$
\mathrm{AR}=\mathrm{R} \Lambda, \quad \mathrm{LA}=\Lambda \mathrm{L}
$$

Without loss of generality, we can suppose that $\mathrm{LR}=\mathrm{I}$. Let us now suppose that there exists a vector function $\mathbf{w}$ satisfying the relations

$$
\frac{\partial \mathbf{w}}{\partial \mathbf{u}}=\mathrm{R}^{-1}, \quad \text { that is } \quad \frac{\partial \mathbf{u}_{k}}{\partial \mathbf{w}}=\mathbf{r}_{k}, \quad k=1, \ldots, p
$$

Proceeding as we did initially, we obtain

$$
\mathrm{R}^{-1} \frac{\partial \mathbf{u}}{\partial t}+\Lambda \mathrm{R}^{-1} \frac{\partial \mathbf{u}}{\partial x}=\mathbf{0}
$$

hence the new diagonal system (14.6). By reintroducing the characteristic curves $(14.7)$ (the latter will no longer be straight lines as the eigenvalues $\lambda_{k}$ vary for different values of $x$ and $t$ ), $\mathbf{w}$ is constant along them. Its components are still called characteristic variables; as $\mathrm{R}^{-1}=\mathrm{L}$ (thanks to the normalization relation) we obtain

$$
\frac{\partial w_{k}}{\partial \mathbf{u}} \cdot \mathbf{r}_{m}=\mathbf{l}_{k} \cdot \mathbf{r}_{m}=\delta_{k m}, \quad k, m=1, \ldots, p
$$

The functions $w_{k}, k=1, \ldots, p$ are called Riemann invariants of the hyperbolic system.

\subsubsection{The wave equation}

Let us consider the following second order hyperbolic equation

$$
\frac{\partial^{2} u}{\partial t^{2}}-\gamma^{2} \frac{\partial^{2} u}{\partial x^{2}}=f, \quad x \in(\alpha, \beta), \quad t>0
$$



$$
u(x, 0)=u_{0}(x) \quad \text { and } \quad \frac{\partial u}{\partial t}(x, 0)=v_{0}(x), \quad x \in(\alpha, \beta)
$$

be the initial data and let us suppose, that $u$ is identically null at the boundary

$$
u(\alpha, t)=0 \quad \text { and } \quad u(\beta, t)=0, \quad t>0
$$

In this case, $u$ can represent the vertical displacement of a vibrating elastic chord with lenght $\beta-\alpha$, fixed at the endpoints, and $\gamma$ is a coefficient that depends on the specific mass of the chord and on its tension. The chord is subject to a vertical force whose density is $f$. The functions $u_{0}(x)$ and $v_{0}(x)$ describe the initial displacement and the velocity of the chord.

For simplicity of notation, we denote by $\partial_{t} u$ the derivative $\frac{\partial u}{\partial t}$, by $\partial_{x} u$ the derivative $\frac{\partial u}{\partial x}$ and we use similar notations for the second derivatives.

Let us now suppose that $f$ is null. From equation (14.9) we deduce that the kinetic energy of the system is preserved, that is (see Exercise 1)

$$
\left\|\partial_{t} u(t)\right\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2}+\gamma^{2}\left\|\partial_{x} u(t)\right\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2}=\left\|v_{0}\right\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2}+\gamma^{2}\left\|\partial_{x} u_{0}\right\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2}
$$

With the change of variables

$$
\omega_{1}=\partial_{x} u, \quad \omega_{2}=\partial_{t} u
$$

the wave equation $(14.9)$ becomes the following first-order system

$$
\frac{\partial \omega}{\partial t}+\mathrm{A} \frac{\partial \omega}{\partial x}=\mathbf{f}, \quad x \in(\alpha, \beta), \quad t>0
$$

where

$$
\boldsymbol{\omega}=\left[\begin{array}{l}
\omega_{1} \\
\omega_{2}
\end{array}\right], \quad \mathrm{A}=\left[\begin{array}{cc}
0 & -1 \\
-\gamma^{2} & 0
\end{array}\right], \quad \mathbf{f}=\left[\begin{array}{l}
0 \\
f
\end{array}\right]
$$

whose initial conditions are $\omega_{1}(x, 0)=u_{0}^{\prime}(x)$ and $\omega_{2}(x, 0)=v_{0}(x)$.

Since the eigenvalues of $\mathrm{A}$ are distinct real numbers $\pm \gamma$ (representing the wave propagation rates), system (14.12) is hyperbolic.

Note that, also in this case, regular solutions correspond to regular initial data, while discontinuities in the initial data will propagate along the characteristic lines $\frac{d x}{d t}=\pm \gamma$.

\subsection{The finite difference method}

Out of simplicity we will now consider problem (14.1). To solve the latter numerically, we can use spatio-temporal discretizations based on the finite difference method. In this case, the half-plane $\{t>0\}$ is discretized choosing a temporal step $\Delta t$, a spatial discretization step $h$ and defining the gridpoints $\left(x_{j}, t^{n}\right)$ in the following way

$$
x_{j}=j h, \quad j \in \mathbb{Z}, \quad t^{n}=n \Delta t, \quad n \in \mathbb{N}
$$



$$
\lambda=\Delta t / h
$$

and let us define

$$
x_{j+1 / 2}=x_{j}+h / 2 \text {. }
$$

We seek discrete solutions $u_{i}^{n}$ which approximate $u\left(x_{j}, t^{n}\right)$ for each $j$ and $n$.

Hyperbolic initial value problems are often discretized in time using explicit methods. Of course, this imposes restrictions on the values of $\lambda$ that implicit methods generally do not have. Any explicit finite difference method can be written in the form

$$
u_{j}^{n+1}=u_{j}^{n}-\lambda\left(H_{j+1 / 2}^{n}-H_{j-1 / 2}^{n}\right)
$$

where $H_{j+1 / 2}^{n}=H\left(u_{j}^{n}, u_{j+1}^{n}\right)$ for a suitable function $H(\cdot, \cdot)$ called numerical flux.

The numerical scheme (14.13) is basically the outcome of the following consideration. Suppose that $a$ is constant and let us write equation (14.1) in conservation form

$$
\frac{\partial u}{\partial t}+\frac{\partial(a u)}{\partial x}=0
$$

$a u$ being the $f l u x$ associated to the equation. By integrating in space, we obtain

$$
\int_{x_{j-1 / 2}}^{x_{j+1 / 2}} \frac{\partial u}{\partial t} d x+[a u]_{x_{j-1 / 2}}^{x_{j+1 / 2}}=0, \quad j \in \mathbb{Z}
$$

that is

$$
\frac{\partial}{\partial t} U_{j}+\frac{(a u)\left(x_{j+\frac{1}{2}}\right)-(a u)\left(x_{j-\frac{1}{2}}\right)}{h}=0, \quad \text { where } \quad U_{j}=h^{-1} \int_{x_{j-\frac{1}{2}}}^{x_{j+\frac{1}{2}}} u(x) d x
$$

Equation (14.13) can now be interpreted as an approximation where the temporal derivative is discretized using the forward Euler finite difference scheme, $U_{j}$ is replaced by $u_{j}$ and $H_{j+1 / 2}$ is a suitable approximation of $(a u)\left(x_{j+\frac{1}{2}}\right)$.

\subsubsection{Discretization of the scalar equation}

In the context of explicit methods, numerical methods are distinguished by how the numerical flux $H$ is chosen. In particular, we cite the following methods:

\section{- forward/centered Euler $(\mathrm{FE} / \mathbf{C})$}

$$
u_{j}^{n+1}=u_{j}^{n}-\frac{\lambda}{2} a\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
$$

that takes the form (14.13) provided we define

$$
H_{j+1 / 2}=\frac{1}{2} a\left(u_{j+1}+u_{j}\right)
$$



\section{- Lax-Friedrichs (LF)}

$$
u_{j}^{n+1}=\frac{1}{2}\left(u_{j+1}^{n}+u_{j-1}^{n}\right)-\frac{\lambda}{2} a\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
$$

also of the form (14.13) with

$$
H_{j+1 / 2}=\frac{1}{2}\left[a\left(u_{j+1}+u_{j}\right)-\lambda^{-1}\left(u_{j+1}-u_{j}\right)\right]
$$

\section{- Lax-Wendroff (LW)}

$$
u_{j}^{n+1}=u_{j}^{n}-\frac{\lambda}{2} a\left(u_{j+1}^{n}-u_{j-1}^{n}\right)+\frac{\lambda^{2}}{2} a^{2}\left(u_{j+1}^{n}-2 u_{j}^{n}+u_{j-1}^{n}\right)
$$

that can be rewritten in the form (14.13) provided that we take

$$
H_{j+1 / 2}=\frac{1}{2}\left[a\left(u_{j+1}+u_{j}\right)-\lambda a^{2}\left(u_{j+1}-u_{j}\right)\right]
$$

\section{- Upwind (or forward/decentered Euler) (U)}

$$
u_{j}^{n+1}=u_{j}^{n}-\frac{\lambda}{2} a\left(u_{j+1}^{n}-u_{j-1}^{n}\right)+\frac{\lambda}{2}|a|\left(u_{j+1}^{n}-2 u_{j}^{n}+u_{j-1}^{n}\right)
$$

corresponding to the form (14.13) provided that we choose

$$
H_{j+1 / 2}=\frac{1}{2}\left[a\left(u_{j+1}+u_{j}\right)-|a|\left(u_{j+1}-u_{j}\right)\right]
$$

The $\mathrm{LF}$ method is obtained from the FE/C method by replacing the nodal value $u_{j}^{n}$ in (14.14) with the average of $u_{j-1}^{n}$ and $u_{j+1}^{n}$.

The LW method derives from the Taylor expansion in time

$$
u^{n+1}=u^{n}+\left(\partial_{t} u\right)^{n} \Delta t+\left(\partial_{t t} u\right)^{n} \frac{\Delta t^{2}}{2}+\mathscr{O}\left(\Delta t^{3}\right)
$$

where $\left(\partial_{t} u\right)^{n}$ denotes the partial derivative of $u$ at time $t^{n}$. Then, using equation (14.1), we replace $\partial_{t} u$ by $-a \partial_{x} u$, and $\partial_{t t} u$ by $a^{2} \partial_{x x} u$. Neglecting the remainder $\mathscr{O}\left(\Delta t^{3}\right)$ and approximating the spatial derivatives with centered finite differences, we get (14.18). Finally, the $\mathrm{U}$ method is obtained by discretizing the convective term $a \partial_{x} u$ of the equation with the upwind finite difference, as seen in Chapter 13 , Sect. 13.6.

All of the previously introduced schemes are explicit. An example of implicit method is the following:

\section{- Backward/centered Euler $(\mathbf{B E} / \mathbf{C})$}

$$
u_{j}^{n+1}+\frac{\lambda}{2} a\left(u_{j+1}^{n+1}-u_{j-1}^{n+1}\right)=u_{j}^{n}
$$

Naturally, the implicit schemes can also be rewritten in a general form that is similar to (14.13) where $H^{n}$ is replaced by $H^{n+1}$. In the specific case, the numerical flux will again be defined by $(14.15)$.

The advantage of formulation (14.13) is that it can be extended easily to the case of more general hyperbolic problems.

In particular, we will examine the case of linear systems in Sect. 14.3.2. The extension to the case of nonlinear hyperbolic equations will instead be considered in Sect. 16.2. Finally, we point out the following schemes for approximating the wave equation (14.9), again in the case $f=0$ :

- Leap-Frog

$$
u_{j}^{n+1}-2 u_{j}^{n}+u_{j}^{n-1}=(\gamma \lambda)^{2}\left(u_{j+1}^{n}-2 u_{j}^{n}+u_{j-1}^{n}\right)
$$

\section{- Newmark}

$$
u_{j}^{n+1}-2 u_{j}^{n}+u_{j}^{n-1}=\frac{(\gamma \lambda)^{2}}{4}\left(w_{j}^{n-1}+2 w_{j}^{n}+w_{j}^{n+1}\right)
$$

where $w_{j}^{n}=u_{j+1}^{n}-2 u_{j}^{n}+u_{j-1}^{n}$

\subsubsection{Discretization of linear hyperbolic systems}

Let us consider the linear system (14.5). Generalizing (14.13), a numerical scheme for a finite difference approximation can be written in the form

$$
\mathbf{u}_{j}^{n+1}=\mathbf{u}_{j}^{n}-\lambda\left(\mathbf{H}_{j+1 / 2}^{n}-\mathbf{H}_{j-1 / 2}^{n}\right)
$$

where $\mathbf{u}_{j}^{n}$ is the vector approximating $\mathbf{u}\left(x_{j}, t^{n}\right)$. Now, $\mathbf{H}_{j+1 / 2}$ is a vector numerical flux. Its formal expression can be easily derived by generalizing the scalar case and replacing $a, a^{2}$, and $|a|$ in $(14.15),(14.17),(14.19),(14.21)$ respectively with $\mathrm{A}, \mathrm{A}^{2}$, and $|\mathrm{A}|$, where

$$
|\mathrm{A}|=\mathrm{T}|\Lambda| \mathrm{T}^{-1}
$$

$|\Lambda|=\operatorname{diag}\left(\left|\lambda_{1}\right|, \ldots,\left|\lambda_{p}\right|\right)$ and $\mathrm{T}$ is the matrix of eigenvectors of $\mathrm{A}$

For instance, transforming system (14.5) in $p$ independent transport equations and approximating each of these with an upwind scheme for scalar equations, we obtain the following upwind numerical scheme for the initial system

$$
\mathbf{u}_{j}^{n+1}=\mathbf{u}_{j}^{n}-\frac{\lambda}{2} \mathrm{~A}\left(\mathbf{u}_{j+1}^{n}-\mathbf{u}_{j-1}^{n}\right)+\frac{\lambda}{2}|\mathrm{~A}|\left(\mathbf{u}_{j+1}^{n}-2 \mathbf{u}_{j}^{n}+\mathbf{u}_{j-1}^{n}\right)
$$

The numerical flux of such scheme is

$$
\mathbf{H}_{j+\frac{1}{2}}=\frac{1}{2}\left[\mathrm{~A}\left(\mathbf{u}_{j+1}+\mathbf{u}_{j}\right)-|\mathbf{A}|\left(\mathbf{u}_{j+1}-\mathbf{u}_{j}\right)\right]
$$

The Lax-Wendroff method becomes

$$
\mathbf{u}_{j}^{n+1}=\mathbf{u}_{j}^{n}-\frac{1}{2} \lambda \mathbf{A}\left(\mathbf{u}_{j+1}^{n}-\mathbf{u}_{j-1}^{n}\right)+\frac{1}{2} \lambda^{2} \mathrm{~A}^{2}\left(\mathbf{u}_{j+1}^{n}-2 \mathbf{u}_{j}^{n}+\mathbf{u}_{j-1}^{n}\right)
$$

and its numerical flux is

$$
\mathbf{H}_{j+\frac{1}{2}}=\frac{1}{2}\left[\mathrm{~A}\left(\mathbf{u}_{j+1}+\mathbf{u}_{j}\right)-\lambda \mathrm{A}^{2}\left(\mathbf{u}_{j+1}-\mathbf{u}_{j}\right)\right]
$$



\subsubsection{Boundary treatment}

In case we want to discretize the hyperbolic equation (14.3) on a bounded interval, we will obviously need to use the inflow node $x=\alpha$ to impose the boundary condition, say $u_{0}^{n+1}=\varphi\left(t^{n+1}\right)$, while at all other nodes $x_{j}, 1 \leq j \leq m$ (including the outflow node $x_{m}=\beta$ ) we will write the finite difference scheme.

However, schemes using a centered discretization of the space derivative require a particular treatment at $x_{m}$. Indeed, they would require the value $u_{m+1}$, which is unavailable as it relates to the point with coordinates $\beta+h$, outside the integration interval. The problem can be solved in various ways. An option is to use only the upwind decentered discretization on the last node, as such discretization does not require knowing the datum in $x_{m+1}$; this approach however is only a first-order one. Alternatively, the value $u_{m}^{n+1}$ can be obtained through extrapolation from the values available at the internal nodes. An example could be an extrapolation along the characteristic lines applied to a scheme for which $\lambda a \leq 1 ;$ this provides $u_{m}^{n+1}=u_{m-1}^{n} \lambda a+u_{m}^{n}(1-\lambda a)$.

A further option consists in applying the centered finite difference scheme to the outflow node $x_{m}$ as well, and use, in place of $u_{m+1}^{n}$, an approximation based on a constant extrapolation $\left(u_{m+1}^{n}=u_{m}^{n}\right)$, or on a linear one $\left(u_{m+1}^{n}=2 u_{m}^{n}-u_{m-1}^{n}\right)$.

This matter becomes more problematic in the case of hyperbolic systems, where we must resort to compatibility equations. To shed more light on these aspects and to analyze their possible instabilities deriving from the numerical boundary treatment, the reader can refer to Strickwerda [Str89], [QV94, Chap. 14] and [LeV07].

\subsection{Analysis of the finite difference methods}

We analyze the consistency, stability and convergence properties of the finite difference methods we introduced previously.

\subsubsection{Consistency and convergence}

For a given numerical scheme, the local truncation error is the error generated by expecting the exact solution to verify the numerical scheme itself.

For instance, in the case of scheme (14.14), having denoted by $u$ the solution of the exact problem (14.1), we can define the truncation error at the point $\left(x_{j}, t^{n}\right)$ as follows

$$
\tau_{j}^{n}=\frac{u\left(x_{j}, t^{n+1}\right)-u\left(x_{j}, t^{n}\right)}{\Delta t}+a \frac{u\left(x_{j+1}, t^{n}\right)-u\left(x_{j-1}, t^{n}\right)}{2 h}
$$

If the truncation error

$$
\tau(\Delta t, h)=\max _{j, n}\left|\tau_{j}^{n}\right|
$$

tends to zero when $\Delta t$ and $h$ tend to zero, independently, then the numerical scheme will be said to be consistent. Moreover, we will say that a numerical scheme is accurate to order $p$ in time and to order $q$ in space (for suitable integers $p$ and $q$ ), if for a sufficiently regular solution of the exact problem we have

$$
\tau(\Delta t, h)=\mathscr{O}\left(\Delta t^{p}+h^{q}\right)
$$

Using Taylor expansions suitably, we can then see that the truncation error of the previously introduced methods is:

- Euler (forward or backward) / centered: $\mathscr{O}\left(\Delta t+h^{2}\right)$;

- Upwind: $\mathscr{O}(\Delta t+h)$;

- Lax-Friedrichs: $\mathscr{O}\left(\frac{h^{2}}{\Delta t}+\Delta t+h^{2}\right)$;

- Lax-Wendroff: $\mathscr{O}\left(\Delta t^{2}+h^{2}+h^{2} \Delta t\right)$.

Finally, we will say that a scheme is convergent (in the maximum norm) if

$$
\lim _{\Delta t, h \rightarrow 0}\left(\max _{j, n}\left|u\left(x_{j}, t^{n}\right)-u_{j}^{n}\right|\right)=0
$$

Obviously, we can also consider weaker norms, such as $\|\cdot\|_{\Delta, 1}$ and $\|\cdot\|_{\Delta, 2}$, which we will introduce in $(14.26)$.

\subsubsection{Stability}

We will say that a numerical method for a linear hyperbolic problem is stable if for each instant $T$ there exists a constant $C_{T}>0$ (possibly depending on $T$ ) such that for each $h>0$, there exists $\delta_{0}>0$ (possibly dependent on $\left.h\right)$ such that for each $0<\Delta t<\delta_{0}$ we have

$$
\left\|\mathbf{u}^{n}\right\|_{\Delta} \leq C_{T}\left\|\mathbf{u}^{0}\right\|_{\Delta}
$$

for each $n$ such that $n \Delta t \leq T$, and for each initial datum $\mathbf{u}_{0}$. Note that $C_{T}$ should not depend on $\Delta t$ and $h$. Often (always, in the case of explicit methods) we will have stability only if the temporal step is sufficiently small with respect to the spatial one, that is for $\delta_{0}=\delta_{0}(h)$.

The notation $\|\cdot\|_{\Delta}$ denotes a suitable discrete norm, for instance

$$
\|\mathbf{v}\|_{\Delta, p}=\left(h \sum_{j=-\infty}^{\infty}\left|v_{j}\right|^{p}\right)^{\frac{1}{p}} \text { for } p=1,2, \quad\|\mathbf{v}\|_{\Delta, \infty}=\sup _{j}\left|v_{j}\right|
$$

Note how $\|\mathbf{v}\|_{\Delta, p}$ represents an approximation of the $\mathrm{L}^{p}(\mathbb{R})$ norm, for $p=1,2$ or $+\infty$.

The implicit backward/centered Euler scheme (14.22) is stable in the norm $\|\cdot\|_{\Delta, 2}$ for any choice of the parameters $\Delta t$ and $h$ (see Exercise 2).

A scheme is called strongly stable with respect to the norm $\|\cdot\|_{\Delta}$ if

$$
\left\|\mathbf{u}^{n}\right\|_{\Delta} \leq\left\|\mathbf{u}^{n-1}\right\|_{\Delta}
$$

for each $n$ such that $n \Delta t \leq T$, and for each initial datum $\mathbf{u}_{0}$, which implies that $(14.25)$ is verified with $C_{T}=1$. 

![](https://cdn.mathpix.com/cropped/71627ab35978b0d0d1f5970ad976ca1f-23.jpg?height=188&width=654&top_left_y=110&top_left_x=105)

Fig. 14.3. Geometric interpretation of the CFL condition for a system with $p=2$, where $r_{i}=$ $\bar{x}-\lambda_{i}(t-\bar{t}) i=1,2$. The CFL condition is satisfied on the left, and violated on the right

Remark 14.1. In the context of hyperbolic problems, one often wants long-time solutions (solutions with $T \gg 1$ ). Such cases usually require a strongly stable scheme, as this guarantees that the numerical solution is bounded for each value of $T$.

As we will see, a necessary condition for the stability of an explicit numerical scheme of the form (14.13) is that the temporal and spatial discretization steps satisfy

$$
|a \lambda| \leq 1, \text { or } \Delta t \leq \frac{h}{|a|} .
$$

This is the celebrated $C F L$ condition (from Courant, Friedrichs and Lewy, [CFL28]). The number $a \lambda$ is commonly called $C F L$ number and is a physically dimensionless quantity ( $a$ being a velocity).

The geometrical interpretation of the CFL stability condition is the following. In a finite difference scheme, the value of $u_{j}^{n+1}$ generally depends on the values $u_{j+i}^{n}$ time at $t^{n}$ at the three points $x_{j+i}, i=-1,0,1$. Proceeding backwards, we deduce that the solution $u_{j}^{n+1}$ will only depend on the initial data at the points $x_{j+i}$, for $i=$ $-(n+1), \ldots,(n+1)$ (see Fig. 14.3).

Calling numerical domain of dependence $D_{\Delta t}\left(x_{j}, t^{n}\right)$ the domain of dependence of $u_{j}^{n}$, which will therefore be called numerical dependence domain of $u_{j}^{n}$, the former will verify

$$
D_{\Delta t}\left(x_{j}, t^{n}\right) \subset\left\{x \in \mathbb{R}:\left|x-x_{j}\right| \leq n h=\frac{t^{n}}{\lambda}\right\}
$$

Consequently, for each given point $(\bar{x}, \bar{t})$ we have

$$
D_{\Delta t}(\bar{x}, \bar{t}) \subset\left\{x \in \mathbb{R}:|x-\bar{x}| \leq \frac{\bar{t}}{\lambda}\right\}
$$

In particular, taking the limit for $\Delta t \rightarrow 0$, and fixing $\lambda$, the numerical dependency domain becomes

$$
D_{0}(\bar{x}, \bar{t})=\left\{x \in \mathbb{R}:|x-\bar{x}| \leq \frac{\bar{t}}{\lambda}\right\}
$$

The condition (14.28) is then equivalent to the inclusion

$$
D(\bar{x}, \bar{t}) \subset D_{0}(\bar{x}, \bar{t})
$$

where $D(\bar{x}, \bar{t})$ is the dependency domain of the exact solution defined in (14.8). Note that in the scalar case, $p=1$ and $\lambda_{1}=a$.

Remark 14.2. The CFL condition establishes, in particular, that there is no explicit finite different scheme for hyperbolic initial value problems that is unconditionally stable and consistent. Indeed, suppose the CFL condition is violated. Then there exists at least a point $x^{*}$ in the dependency domain that does not belong to the numerical dependency domain. Then changing the initial datum to $x^{*}$ will only modify the exact solution and not the numerical one. This implies non-convergence of the method and therefore also instability. Indeed, for a consistent method, the Lax-Richtmyer equivalence theorem states that stability is a sufficient condition for its convergence.

Remark 14.3. In the case where $a=a(x, t)$ is no longer constant in (14.1), the CFL condition becomes

$$
\Delta t \leq \frac{h}{\sup _{x \in \mathbb{R}, t>0}|a(x, t)|}
$$

If the spatial discretization step varies, we have

$$
\begin{gathered}
\Delta t \leq \min _{k} \frac{h_{k}}{\sup }|a(x, t)| \\
x \in\left(x_{k}, x_{k+1}\right), t>0
\end{gathered}
$$

with $h_{k}=x_{k+1}-x_{k}$.

Referring to the hyperbolic system (14.5), the CFL stability condition, in analogy to (14.28), will be

$$
\left|\lambda_{k} \frac{\Delta t}{h}\right| \leq 1, \quad k=1, \ldots, p, \quad \text { or, equivalently, } \quad \Delta t \leq \frac{h}{\max _{k}\left|\lambda_{k}\right|}
$$

where $\left\{\lambda_{k}, k=1 \ldots, p\right\}$ are the eigenvalues of $\mathrm{A}$

This condition, as well, can be written in the form (14.29). The latter expresses the requirement that each line of the form $x=\bar{x}-\lambda_{k}(\bar{t}-t), k=1, \ldots, p$, must intersect the horizontal line $t=\bar{t}-\Delta t$ at points $x^{(k)}$ which lie within the numerical dependency domain.

Theorem 14.1. If the CFL condition (14.28) is satisfied, the upwind, Lax-

Friedrichs and Lax-Wendroff schemes are strongly stable in the norm $\|\cdot\|_{\Delta, 1}$

Proof. To prove the stability of the upwind scheme (14.20) we rewrite it in the following form (having supposed $a>0$ )

$$
u_{j}^{n+1}=u_{j}^{n}-\lambda a\left(u_{j}^{n}-u_{j-1}^{n}\right)
$$



$$
\left\|\mathbf{u}^{n+1}\right\|_{\Delta, 1} \leq h \sum_{j}\left|(1-\lambda a) u_{j}^{n}\right|+h \sum_{j}\left|\lambda a u_{j-1}^{n}\right|
$$

Under the hypothesis (14.28) both values $\lambda a$ and $1-\lambda a$ are non-negative. Hence,

$$
\left\|\mathbf{u}^{n+1}\right\|_{\Delta, 1} \leq h(1-\lambda a) \sum_{j}\left|u_{j}^{n}\right|+h \lambda a \sum_{j}\left|u_{j-1}^{n}\right|=\left\|\mathbf{u}^{n}\right\|_{\Delta, 1}
$$

that is, inequality $(14.25)$ holds with $C_{T}=1$. The scheme is therefore strongly stable with respect to the norm $\|\cdot\|_{\Delta}=\|\cdot\|_{\Delta, 1}$.

For the Lax-Friedrichs scheme, always under the CFL condition (14.28), we derive from (14.16) that

$$
u_{j}^{n+1}=\frac{1}{2}(1-\lambda a) u_{j+1}^{n}+\frac{1}{2}(1+\lambda a) u_{j-1}^{n},
$$

so

$$
\begin{aligned}
\left\|\mathbf{u}^{n+1}\right\|_{\Delta, 1} & \leq \frac{1}{2} h\left[\sum_{j}\left|(1-\lambda a) u_{j+1}^{n}\right|+\sum_{j}\left|(1+\lambda a) u_{j-1}^{n}\right|\right] \\
& \leq \frac{1}{2}(1-\lambda a)\left\|\mathbf{u}^{n}\right\|_{\Delta, 1}+\frac{1}{2}(1+\lambda a)\left\|\mathbf{u}^{n}\right\|_{\Delta, 1}=\left\|\mathbf{u}^{n}\right\|_{\Delta, 1}
\end{aligned}
$$

For the Lax-Wendroff scheme, the proof is analogous (see e.g. [QV94, Chap. 14] or $[$ Str89] $)$.

Finally, we can prove that if the CFL condition is verified, the upwind scheme satisfies

$$
\left\|\mathbf{u}^{n}\right\|_{\Delta, \infty} \leq\left\|\mathbf{u}^{0}\right\|_{\Delta, \infty} \quad \forall n \geq 0
$$

i.e. it is strongly stable in the norm $\|\cdot\|_{\Delta, \infty}$. Relation (14.30) is called discrete maximum principle (see Exercise 4).

Theorem 14.2. The backward Euler scheme BE/C is strongly stable in the norm

Theorem 14.2. The backward Euler scheme BE/C is strongly stable in the norm
$\|\cdot\|_{\Delta, 2}$, with no restriction on $\Delta t$. The forward Euler scheme $F E / C$, instead, is
never strongly stable. However, it is stable with constant $C_{T}=e^{T / 2}$ provided that
we assume that $\Delta t$ satisfies the following condition (more restrictive than the

we assume that $\Delta t$ satisfies the following condition (more restrictive than the

Proof. We observe that

$$
(B-A) B=\frac{1}{2}\left(B^{2}-A^{2}+(B-A)^{2}\right) \quad \forall A, B \in \mathbb{R}
$$

As a matter of fact

$$
(B-A) B=(B-A)^{2}+(B-A) A=\frac{1}{2}\left((B-A)^{2}+(B-A)(B+A)\right)
$$

Multiplying (14.22) by $u_{j}^{n+1}$ we find

$$
\left(u_{j}^{n+1}\right)^{2}+\left(u_{j}^{n+1}-u_{j}^{n}\right)^{2}=\left(u_{j}^{n}\right)^{2}-\lambda a\left(u_{j+1}^{n+1}-u_{j-1}^{n+1}\right) u_{j}^{n+1}
$$

Observing that

$$
\sum_{j \in \mathbb{Z}}\left(u_{j+1}^{n+1}-u_{j-1}^{n+1}\right) u_{j}^{n+1}=0
$$

(telescopic sum), we immediately obtain that $\left\|\mathbf{u}^{n+1}\right\|_{\Delta, 2}^{2} \leq\left\|\mathbf{u}^{n}\right\|_{\Delta, 2}^{2}$, which is the result sought for the $\mathrm{BE} / \mathrm{C}$ scheme.

Let us now move to the FE/C scheme and multiply (14.14) by $u_{j}^{n}$. Observing that

$$
(B-A) A=\frac{1}{2}\left(B^{2}-A^{2}-(B-A)^{2}\right) \quad \forall A, B \in \mathbb{R}
$$

we find

$$
\left(u_{j}^{n+1}\right)^{2}=\left(u_{j}^{n}\right)^{2}+\left(u_{j}^{n+1}-u_{j}^{n}\right)^{2}-\lambda a\left(u_{j+1}^{n}-u_{j-1}^{n}\right) u_{j}^{n}
$$

On the other hand, we obtain once again from (14.14) that

$$
u_{j}^{n+1}-u_{j}^{n}=-\frac{\lambda a}{2}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)
$$

and therefore

$$
\left(u_{j}^{n+1}\right)^{2}=\left(u_{j}^{n}\right)^{2}+\left(\frac{\lambda a}{2}\right)^{2}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)^{2}-\lambda a\left(u_{j+1}^{n}-u_{j-1}^{n}\right) u_{j}^{n}
$$

Now summing on $j$ and observing that the last addendum yields a telescopic sum (hence it does not provide any contribution) we obtain, after multiplying by $h$,

$$
\left\|\mathbf{u}^{n+1}\right\|_{\Delta, 2}^{2}=\left\|\mathbf{u}^{n}\right\|_{\Delta, 2}^{2}+\left(\frac{\lambda a}{2}\right)^{2} h \sum_{j \in \mathbb{Z}}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)^{2}
$$

from which we infer that there is no value of $\Delta t$ for which the method is strongly stable. However, as

$$
\left(u_{j+1}^{n}-u_{j-1}^{n}\right)^{2} \leq 2\left[\left(u_{j+1}^{n}\right)^{2}+\left(u_{j-1}^{n}\right)^{2}\right]
$$

we find that, under the hypothesis (14.31),

$$
\left\|\mathbf{u}^{n+1}\right\|_{\Delta, 2}^{2} \leq\left(1+\lambda^{2} a^{2}\right)\left\|\mathbf{u}^{n}\right\|_{\Delta, 2}^{2} \leq(1+\Delta t)\left\|\mathbf{u}^{n}\right\|_{\Delta, 2}^{2}
$$

By recursion, we find

$$
\left\|\mathbf{u}^{n}\right\|_{\Delta, 2}^{2} \leq(1+\Delta t)^{n}\left\|\mathbf{u}^{0}\right\|_{\Delta, 2}^{2} \leq e^{T}\left\|\mathbf{u}^{0}\right\|_{\Delta, 2}^{2}
$$

where we have used the inequality

$$
(1+\Delta t)^{n} \leq e^{n \Delta t} \leq e^{T} \quad \forall n \text { such that } t^{n} \leq T
$$

We conclude that

$$
\left\|\mathbf{u}^{n}\right\|_{\Delta, 2} \leq e^{T / 2}\left\|\mathbf{u}^{0}\right\|_{\Delta, 2}
$$

which is the stability result sought for the $\mathrm{FE} / \mathrm{C}$ scheme.

\subsubsection{Von Neumann analysis and amplification coefficients}

Von Neumann's analysis is useful to investigate the stability of a scheme in the norm $\|\cdot\|_{\Delta, 2}$. To this purpose, we assume that the function $u_{0}(x)$ is $2 \pi$-periodic and thus it can be written as a Fourier series as follows

$$
u_{0}(x)=\sum_{k=-\infty}^{\infty} \alpha_{k} e^{i k x}
$$

where

$$
\alpha_{k}=\frac{1}{2 \pi} \int_{0}^{2 \pi} u_{0}(x) e^{-i k x} d x
$$

is the $k$-th Fourier coefficient. Hence,

$$
u_{j}^{0}=u_{0}\left(x_{j}\right)=\sum_{k=-\infty}^{\infty} \alpha_{k} e^{i k j h}, \quad j=0, \pm 1, \pm 2, \cdots
$$

It can be verified that applying any of the difference schemes seen in Sect. 14.3.1 we get the following relation

$$
u_{j}^{n}=\sum_{k=-\infty}^{\infty} \alpha_{k} e^{i k j h} \gamma_{k}^{n}, \quad j=0, \pm 1, \pm 2, \ldots, \quad n \geq 1
$$

The number $\gamma_{k} \in \mathbb{C}$ is called amplification coefficient of the $k$-th frequency (or harmonic), and characterizes the scheme under exam. For instance, in the case of the forward centered Euler method (FE/C) we find

$$
\begin{aligned}
u_{j}^{1} &=\sum_{k=-\infty}^{\infty} \alpha_{k} e^{i k j h}\left(1-\frac{a \Delta t}{2 h}\left(e^{i k h}-e^{-i k h}\right)\right) \\
&=\sum_{k=-\infty}^{\infty} \alpha_{k} e^{i k j h}\left(1-\frac{a \Delta t}{h} i \sin (k h)\right)
\end{aligned}
$$

Table 14.1. Amplification coefficient for the different numerical schemes in Sect. 14.3.1. We recall that $\lambda=\Delta t / h$

\begin{tabular}{lc}
\hline \multicolumn{1}{c}{ Scheme } & $\gamma_{k}$ \\
\hline Forward/centered Euler & $1-i a \lambda \sin (k h)$ \\
Backward/centered Euler & $(1+i a \lambda \sin (k h))^{-1}$ \\
Upwind & $1-|a| \lambda\left(1-e^{-i k h}\right)$ \\
Lax-Friedrichs & $\cos k h-i a \lambda \sin (k h)$ \\
Lax-Wendroff & $1-i a \lambda \sin (k h)-a^{2} \lambda^{2}(1-\cos (k h))$ \\
\hline
\end{tabular}

Hence,

$$
\gamma_{k}=1-\frac{a \Delta t}{h} i \sin (k h) \quad \text { and thus } \quad\left|\gamma_{k}\right|=\left\{1+\left(\frac{a \Delta t}{h} \sin (k h)\right)^{2}\right\}^{\frac{1}{2}}
$$

As there exist values of $k$ for which $\left|\gamma_{k}\right|>1$, there is no value of $\Delta t$ and $h$ for which the scheme is strongly stable.

Proceeding in a similar way for the other schemes, we find the coefficients reported in Table $14.1$.

We will now see how the von Neumann analysis can be applied to study the stability of a numerical scheme with respect to the $\|\cdot\|_{\Delta, 2}$ norm and to ascertain its dissipation and dispersion properties.

To this purpose, we prove the following result:

$e^{\beta T / m}$. In particular, if we can take $\beta=0$ (and therefore $\left.\left|\gamma_{k}\right| \leq 1 \forall k\right)$ then the

scheme is strongly stable with respect to the same norm.

Proof. We will suppose that problem (14.1) is formulated on the interval $[0,2 \pi]$. In such interval, let us consider $N+1$ equidistant nodes,

$$
x_{j}=j h, \quad j=0, \ldots, N, \quad \text { with } \quad h=\frac{2 \pi}{N}
$$

( $N$ being an even positive integer) where to satisfy the numerical scheme (14.13). Moreover, we will suppose for simplicity that the initial datum $u_{0}$ is periodic. As the numerical scheme only depends on the values of $u_{0}$ at the nodes $x_{j}$, we can replace $u_{0}$ by the Fourier polynomial of order $N / 2$,

$$
\tilde{u}_{0}(x)=\sum_{k=-\frac{N}{2}}^{\frac{N}{2}-1} \alpha_{k} e^{i k x}
$$

which interpolates it at the nodes. Note that $\tilde{u}_{0}$ is a periodic function with period $2 \pi$. We will have, thanks to (14.36),

$$
u_{j}^{0}=u_{0}\left(x_{j}\right)=\sum_{k=-\frac{N}{2}}^{\frac{N}{2}-1} \alpha_{k} e^{i k j h}, \quad u_{j}^{n}=\sum_{k=-\frac{N}{2}}^{\frac{N}{2}-1} \alpha_{k} \gamma_{k}^{n} e^{i k j h}
$$

We note that

$$
\left\|\mathbf{u}^{n}\right\|_{\Delta, 2}^{2}=h \sum_{j=0}^{N-1} \sum_{k, m=-\frac{N}{2}}^{\frac{N}{2}-1} \alpha_{k} \bar{\alpha}_{m}\left(\gamma_{k} \bar{\gamma}_{m}\right)^{n} e^{i(k-m) j h}
$$

As

$$
h \sum_{j=0}^{N-1} e^{i(k-m) j h}=2 \pi \delta_{k m}, \quad-\frac{N}{2} \leq k, m \leq \frac{N}{2}-1
$$

(see e.g. [QSS07, Lemma 10.2]) we find

$$
\left\|\mathbf{u}^{n}\right\|_{\Delta, 2}^{2}=2 \pi \sum_{k=-\frac{N}{2}}^{\frac{N}{2}-1}\left|\alpha_{k}\right|^{2}\left|\gamma_{k}\right|^{2 n}
$$

Thanks to the assumption made on $\left|\gamma_{k}\right|$ we have

$$
\left\|\mathbf{u}^{n}\right\|_{\Delta, 2}^{2} \leq(1+\beta \Delta t)^{\frac{2 n}{m}} 2 \pi \sum_{k=-\frac{N}{2}}^{\frac{N}{2}-1}\left|\alpha_{k}\right|^{2}=(1+\beta \Delta t)^{\frac{2 n}{m}}\left\|\mathbf{u}^{0}\right\|_{\Delta, 2}^{2} \forall n \geq 0
$$

As $1+\beta \Delta t \leq e^{\beta \Delta t}$, we deduce that

$$
\left\|\mathbf{u}^{n}\right\|_{\Delta, 2} \leq e^{\frac{\beta \Delta n}{m}}\left\|\mathbf{u}^{0}\right\|_{\Delta, 2}=e^{\frac{\beta T}{m}}\left\|\mathbf{u}^{0}\right\|_{\Delta, 2} \quad \forall n \quad \text { such that } \quad n \Delta t \leq T
$$

This proves the theorem.

Remark 14.4. Should strong stability be required, the condition $\left|\gamma_{k}\right| \leq 1$ indicated in Theorem $14.3$ is also necessary.

In the case of the upwind scheme (14.20), as

$$
\left|\gamma_{k}\right|^{2}=[1-|a| \lambda(1-\cos k h)]^{2}+a^{2} \lambda^{2} \sin ^{2} k h, \quad k \in \mathbb{Z}
$$

we obtain

$$
\left|\gamma_{k}\right| \leq 1 \text { if } \Delta t \leq \frac{h}{|a|}, \quad k \in \mathbb{Z}
$$

that is, the CFL condition guarantees strong stability in the $\|\cdot\|_{\Delta, 2}$ norm. Proceeding in a similar way, we can verify that (14.38) also holds for the LaxFriedrichs scheme.

The centered backward Euler scheme BE/C instead is unconditionally strongly stable in the norm $\|\cdot\|_{\Delta, 2}$, as $\left|\gamma_{k}\right| \leq 1$ for each $k$ and for each possible choice of $\Delta t$ and $h$, as we previously obtained in Theorem $14.2$ by following a different procedure.

In the case of the centered forward Euler method FE/C we have

$$
\left|\gamma_{k}\right|^{2}=1+\frac{a^{2} \Delta t^{2}}{h^{2}} \sin ^{2}(k h) \leq 1+\frac{a^{2} \Delta t^{2}}{h^{2}}, \quad k \in \mathbb{Z}
$$

If $\beta>0$ is a constant such that

$$
\Delta t \leq \beta \frac{h^{2}}{a^{2}}
$$

then $\left|\gamma_{k}\right| \leq(1+\beta \Delta t)^{1 / 2}$. Hence, applying Theorem $14.3$ (with $\left.m=2\right)$ we deduce that the FE/C scheme is stable, albeit with a more restrictive CFL condition, as previously obtained following a different path in Theorem 14.2.

If we apply the centered forward Euler method to the transport-reaction equation

$$
\frac{\partial u}{\partial t}+a \frac{\partial u}{\partial x}+a_{0} u=0
$$

with $a_{0}>0$, for each $k \in \mathbb{Z}$ we find

$$
\left|\gamma_{k}\right|^{2}=1-2 a_{0} \Delta t+\Delta t^{2} a_{0}^{2}+\lambda^{2} \sin ^{2}(k h) \leq 1-2 a_{0} \Delta t+\Delta t^{2} a_{0}^{2}+\left(\frac{a \Delta t}{h}\right)^{2}
$$

Then the scheme is strongly stable in the $\|.\|_{\Delta, 2}$ norm under the condition

$$
\Delta t<\frac{2 a_{0}}{a_{0}^{2}+h^{-2} a^{2}}
$$

Example 14.1. In order to verify numerically the stability condition (14.41), we have considered equation (14.40) in the interval $(0,1)$ with periodic boundary conditions. We have chosen $a=a_{0}=1$ and the initial datum $u_{0}$ equal to 2 in the interval $(1 / 3,2 / 3)$ and 0 elsewhere. As the initial datum is a square wave, its Fourier expansion has all its $\alpha_{k}$ coefficients not null. On the right of Fig. 14.4, we report $\left\|\mathbf{u}^{n}\right\|_{\Delta, 2}$ in the time interval $(0,2.5)$ for two values of $\Delta t$, one larger and one smaller than the critical value $\Delta t^{*}=2 /\left(1+h^{-2}\right)$, provided by (14.41). Note that for $\Delta t<\Delta t^{*}$ the norm is decreasing, while, in the opposite case, after an initial decrease it grows exponentially. Fig. $14.5$ shows the result for $a_{0}=0$ obtained with $\mathrm{FE} / \mathrm{C}$ using the same initial datum. In the figure on the left, we display the behaviour of $\left\|\mathbf{u}^{n}\right\|_{\Delta, 2}$ for different values of $h$ and using $\Delta t=10 h^{2}$, that is varying the time step based on the restriction provided by inequality (14.39) and taking $\beta=10$. Note how the norm of the solution remains bounded for decreasing values of $h$. On the right-hand side of the same figure, we illustrate the result obtained for the same values of $h$ taking as condition $\Delta t=0.1 h$, which corresponds to a constant CFL number equal to $0.1$. In this case, the discrete norm of the numerical solution blows up as $h$ decreases, as expected. 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-005.jpg?height=238&width=694&top_left_y=112&top_left_x=106)

Fig. 14.4. The figure on the right displays the behaviour of $\left\|\mathbf{u}^{n}\right\|_{\Delta, 2}$, where $\mathbf{u}^{n}$ is the solution of equation (14.40) (with $\left.a=a_{0}=1\right)$ obtained using the $\mathrm{FE} / \mathrm{C}$ method, for two values of $\Delta t$, one smaller and one greater than the critical value $\Delta t^{*}$. On the left, the initial datum used

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-005.jpg?height=239&width=305&top_left_y=460&top_left_x=138)
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-005.jpg?height=238&width=644&top_left_y=460&top_left_x=138) $a_{0}=0$ and for different values of $h$. On the left, the case where $\Delta t$ satisfies the stability condition (14.39). On the right, the results obtained by maintaining the CFL number constant and equal to $0.1$, violating condition $(14.39)$

\subsubsection{Dissipation and dispersion}

Besides allowing to enquire about the stability of a numerical scheme, the analysis of the amplification coefficients is also useful to study its dissipation and dispersion properties.

To clarify the matter, let us consider the exact solution of problem (14.1); then

$$
u\left(x, t^{n}\right)=u_{0}(x-a n \Delta t), \quad n \geq 0, \quad x \in \mathbb{R}
$$

with $t^{n}=n \Delta t$. In particular, using $(14.35)$ we obtain

$$
u\left(x_{j}, t^{n}\right)=\sum_{k=-\infty}^{\infty} \alpha_{k} e^{i k j h}\left(g_{k}\right)^{n} \quad \text { with } g_{k}=e^{-i a k \Delta t}
$$

Comparing (14.42) with (14.36) we can note that the amplification coefficient $\gamma_{k}$ (generated by the specific numerical scheme) is the correspondent of $g_{k}$. We observe that $\left|g_{k}\right|=1$ for each $k \in \mathbb{Z}$, while $\left|\gamma_{k}\right| \leq 1$ in order to guarantee the strong stability of the scheme. Thus, $\gamma_{k}$ is a dissipative coefficient. The smaller $\left|\gamma_{k}\right|$ is, the larger will be the reduction of the amplitude $\alpha_{k}$ and, consequently, the larger will be the dissipation of the numerical scheme.

The ratio $\varepsilon_{a}(k)=\frac{\left|\gamma_{k}\right|}{g_{k} \mid}$ is called amplification error (or dissipation error) of the $k$-th harmonic associated to the numerical scheme (and in our case it coincides with the amplification coefficient).

Having set

$$
\phi_{k}=k h
$$

as $k \Delta t=\lambda \phi_{k}$ we obtain

$$
g_{k}=e^{-i a \lambda \phi_{k}}
$$

The real number $\phi_{k}$, here expressed in radians, is called phase angle of the $k$-th harmonic. We rewrite $\gamma_{k}$ in the following way

$$
\gamma_{k}=\left|\gamma_{k}\right| e^{-i \omega \Delta t}=\left|\gamma_{k}\right| e^{-i \frac{\omega}{k} \lambda \phi_{k}}
$$

and comparing such relation to (14.43), we can deduce that the ratio $\frac{\omega}{k}$ represents the propagation rate of the numerical scheme, relatively to the $k$-th harmonic. The ratio

$$
\varepsilon_{d}(k)=\frac{\omega}{k a}=\frac{\omega h}{\phi_{k} a}
$$

between the numerical propagation and the propagation $a$ of the exact solution is called dispersion error $\varepsilon_{d}$ relative to the $k$-th harmonic.

The amplification (or dissipation) error and the dispersion error for the numerical schemes analyzed up to now are function of the phase angle $\phi_{k}$ and of the CFL number $a \lambda$, as reported in Fig. 14.6. For symmetry reasons we have considered the interval $0 \leq \phi_{k} \leq \pi$ and we have used degrees instead of radians on the $x$-axis to indicate $\phi_{k}$ Note how the forward/centered Euler scheme gives a curve of the amplification factor with values above one for all the CFL schemes we have considered, in accordance with the fact that such scheme is never strongly stable.

Example 14.2. In Fig. $14.7$ we compare the numerical results obtained by solving equation (14.1) with $a=1$ and initial datum $u_{0}$. The solutions are composed by a packet of two sinusoidal waves of equal length $l$ centered at the origin $(x=0)$. In the figures on the left $l=20 h$, while in the right ones we have $l=8 h$. As $k=\frac{2 \pi}{l}$, we have $\phi_{k}=\frac{2 \pi}{l} h$ and therefore the values of the phase angle of the wave packet are $\phi_{k}=\pi / 20$ on the left and $\phi_{k}=\pi / 8$ on the right. The numerical solution has been computed for the value $0.75$ of the CFL number, using the different (stable) schemes illustrated previously. We can note how the dissipative effect is very strong at high frequencies $\left(\phi_{k}=\pi / 4\right)$ and in particular for the first-order upwind, backward/centered Euler and Lax-Friedrichs methods.

In order to appreciate the dispersion effects, the solution for $\phi_{k}=\pi / 4$ after 8 time steps is reported in Fig. 14.8. We can note how the Lax-Wendroff method is the least 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-007.jpg?height=890&width=706&top_left_y=112&top_left_x=104)

Fig. 14.6. Amplification and dispersion errors for different numerical schemes in terms of the phase angle $\phi_{k}=k h$ and for different values of the CFL number

dissipative. Moreover, by observing attentively the position of the numerical wave crests with respect to those of the numerical solution, we can verify that the LaxFriedrichs method features a positive dispersion error. Indeed, the numerical wave anticipates the exact one. The upwind method is also weakly dispersive for a CFL 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-008.jpg?height=902&width=706&top_left_y=115&top_left_x=104)

Fig. 14.7. Numerical solution of the convective transport equation of a sine wave packet with different wavelengths $(l=20 h$ left, $l=8 h$ right $)$ obtained with different numerical schemes. The numerical solution for $t=1$ is displayed by the solid line, while the exact solution at the same time instant is displayed by the dashed line

number equal to $0.75$, while the dispersion of the Lax-Friedrichs and backward Euler methods is evident (even after only 8 time steps!). 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-009.jpg?height=620&width=572&top_left_y=116&top_left_x=172)

Fig. 14.8. Numerical solution of the convective transport of a packet of sinusoidal waves. The solid line represents the solution after 8 time steps. The etched line represents the corresponding exact solution at the same time level

\subsection{Equivalent equations}

To each numerical scheme, we can associate a family of differential equations, called equivalent equations.

\subsubsection{The upwind scheme case}

Let us first focus on the upwind scheme. Suppose there exists a regular function $v(x, t)$ satisfying the difference equation $(14.20)$ at each point $(x, t) \in \mathbb{R} \times \mathbb{R}^{+}$(and not only at the grid nodes $\left.\left(x_{j}, t^{n}\right) !\right)$. We can then write (in the case where $a>0$ )

$$
\frac{v(x, t+\Delta t)-v(x, t)}{\Delta t}+a \frac{v(x, t)-v(x-h, t)}{h}=0
$$

Using the Taylor expansions with respect to $x$ and $t$ relative to the point $(x, t)$ and supposing that $v$ is of class $C^{4}$ with respect to $x$ and $t$, we can write

$$
\begin{aligned}
&\frac{v(x, t+\Delta t)-v(x, t)}{\Delta t}=v_{t}+\frac{\Delta t}{2} v_{t t}+\frac{\Delta t^{2}}{6} v_{t t t}+\mathscr{O}\left(\Delta t^{3}\right) \\
&a \frac{v(x, t)-v(x-h, t)}{h}=a v_{x}-\frac{a h}{2} v_{x x}+\frac{a h^{2}}{6} v_{x x x}+\mathscr{O}\left(h^{3}\right)
\end{aligned}
$$

where the right-hand side derivatives are all evaluated at $(x, t)$. Thanks to $(14.44)$ we deduce that, at each point $(x, t)$, the function $v$ satisfies the relation

$$
v_{t}+a v_{x}=R^{U}+\mathscr{O}\left(\Delta t^{3}+h^{3}\right)
$$

with

$$
R^{U}=\frac{1}{2}\left(a h v_{x x}-\Delta t v_{t t}\right)-\frac{1}{6}\left(a h^{2} v_{x x x}+\Delta t^{2} v_{t t t}\right) .
$$

Formally differentiating such equation in $t$, we find

$$
v_{t t}+a v_{x t}=R_{t}^{U}+\mathscr{O}\left(\Delta t^{3}+h^{3}\right) .
$$

Instead, differentiating it in $x$, we have

$$
v_{x t}+a v_{x x}=R_{x}^{U}+\mathscr{O}\left(\Delta t^{3}+h^{3}\right) .
$$

Hence

$$
v_{t t}=a^{2} v_{x x}+R_{t}^{U}-a R_{x}^{U}+\mathscr{O}\left(\Delta t^{3}+h^{3}\right),
$$

which allows to obtain from $(14.45)$

$$
v_{t}+a v_{x}=\mu v_{x x}-\frac{1}{6}\left(a h^{2} v_{x x x}+\Delta t^{2} v_{t t t}\right)-\frac{\Delta t}{2}\left(R_{t}^{U}-a R_{x}^{U}\right)+\mathscr{O}\left(\Delta t^{3}+h^{3}\right)
$$

having set

$$
\mu=\frac{1}{2} a h(1-(a \lambda))
$$

and, as usual, $\lambda=\Delta t / h$. Now, differentiating (14.47) with respect to $t$ formally, and (14.46) with respect to $x$, we find

$$
\begin{aligned}
v_{t t t} &=a^{2} v_{x x t}+R_{t t}^{U}-a R_{x t}^{U}+\mathscr{O}\left(\Delta t^{3}+h^{3}\right) \\
&=-a^{3} v_{x x x}+a^{2} R_{x x}^{U}+R_{t t}^{U}-a R_{x t}^{U}+\mathscr{O}\left(\Delta t^{3}+h^{3}\right)
\end{aligned}
$$

Moreover, we have that

$$
\begin{aligned}
&R_{t}^{U}=\frac{1}{2} a h v_{x x t}-\frac{\Delta t}{2} v_{t t t}-\frac{a h^{2}}{6} v_{x x x t}-\frac{\Delta t^{2}}{6} v_{t t t t} \\
&R_{x}^{U}=\frac{1}{2} a h v_{x x x}-\frac{\Delta t}{2} v_{t t x}-\frac{a h^{2}}{6} v_{x x x x}-\frac{\Delta t^{2}}{6} v_{t t t x}
\end{aligned}
$$

Using relations $(14.50)$ and $(14.51)$ in (14.48) we obtain

$$
\begin{aligned}
v_{t}+a v_{x}=\mu & v_{x x}-\frac{a h^{2}}{6}\left(1-\frac{a^{2} \Delta t^{2}}{h^{2}}-\frac{3 a \Delta t}{2 h}\right) v_{x x x} \\
&+\underbrace{\frac{\Delta t}{4}\left(\Delta t v_{t t t}-a h v_{x x t}-a \Delta t v_{t t x}\right)}_{(\mathrm{A})} \\
&+\frac{\Delta t}{12}\left(\Delta t^{2} v_{t t t t}-a \Delta t^{2} v_{t t t x}+a h^{2} v_{x x x t}-a^{2} h^{2} v_{x x x x}\right) \\
&-\frac{a^{2} \Delta t^{2}}{6} R_{x x}^{U}-\frac{\Delta t^{2}}{6} R_{t t}^{U}+a \frac{\Delta t^{2}}{6} R_{x t}^{U}+\mathscr{O}\left(\Delta t^{3}+h^{3}\right) .
\end{aligned}
$$

Let us now focus on the third derivatives of $v$ contained in the term (A). Thanks to (14.50), (14.46) and (14.47), respectively, we find:

$$
\begin{aligned}
&v_{t t t}=-a^{3} v_{x x x}+r_{1}, \\
&v_{x x t}=-a v_{x x x}+r_{2}, \\
&v_{t t x}=a^{2} v_{x x x}+r_{3},
\end{aligned}
$$

where $r_{1}, r_{2}$ and $r_{3}$ are terms containing derivatives of $v$ of order no less than four, as well as terms of order $\mathscr{O}\left(\Delta t^{3}+h^{3}\right)$. (Note that it follows from the definition of $R^{U}$ that its derivatives of order two are expressed through derivatives of $v$ of order no less than four.) Regrouping the coefficients that multiply $v_{x x x}$, we therefore deduce from (14.52) that

$$
v_{t}+a v_{x}=\mu v_{x x}+v v_{x x x}+R_{4}(v)+\mathscr{O}\left(\Delta t^{3}+h^{3}\right),
$$

having set

$$
v=-\frac{a h^{2}}{6}\left(1-3 a \lambda+2(a \lambda)^{2}\right)
$$

and having indicated with $R_{4}(v)$ the set of terms containing the derivatives of $v$ of order at least four.

We can conclude that the function $v$ satisfies, respectively, the equations:

$$
v_{t}+a v_{x}=0
$$

if we neglect the terms containing derivatives of order above the first;

$$
v_{t}+a v_{x}=\mu v_{x x}
$$

if we neglect the terms containing derivatives of order above the second;

$$
v_{t}+a v_{x}=\mu v_{x x}+v v_{x x x}
$$

if we neglect the derivatives of order above the third. The coefficients $\mu$ and $v$ are in (14.49) and (14.54). Equations (14.55), (14.56) and (14.57) are called equivalent equations (at the first, second resp. third order) relative to the upwind scheme. 

\subsubsection{The Lax-Friedrichs and Lax-Wendroff case}

Proceeding in a similar way, we can derive the equivalent equations of any numerical scheme. For instance, in the case of the Lax-Friedrichs scheme, having denoted by $v$ a hypothetic function that verifies equation (14.16) at each point $(x, t)$, having observed that

$$
\begin{aligned}
&\frac{1}{2}(v(x+h, t)+v(x-h, t))=v+\frac{h^{2}}{2} v_{x x}+\mathscr{O}\left(h^{4}\right) \\
&\frac{1}{2}(v(x+h, t)-v(x-h, t))=h v_{x}+\frac{h^{3}}{6} v_{x x x}+\mathscr{O}\left(h^{4}\right)
\end{aligned}
$$

we obtain

$$
v_{t}+a v_{x}=R^{L F}+\mathscr{O}\left(\frac{h^{4}}{\Delta t}+\Delta t^{3}\right)
$$

having set

$$
R^{L F}=\frac{h^{2}}{2 \Delta t}\left(v_{x x}-\lambda^{2} v_{t t}\right)-\frac{a h^{2}}{6}\left(v_{x x x}+\frac{\lambda^{2}}{a} v_{t t t}\right)
$$

Proceeding as we did previously, tedious computation allows us to deduce from (14.58) the equivalent equations (14.55)-(14.57), in this case having

$$
\mu=\frac{h^{2}}{2 \Delta t}\left(1-(a \lambda)^{2}\right), \quad v=\frac{a h^{2}}{3}\left(1-(a \lambda)^{2}\right)
$$

In the case of the Lax-Wendroff scheme, the equivalent equations are characterized by the following parameters

$$
\mu=0, \quad v=\frac{a h^{2}}{6}\left((a \lambda)^{2}-1\right)
$$

\subsubsection{On the meaning of coefficients in equivalent equations}

In general, in the equivalent equations the term $\mu v_{x x}$ represents a dissipation, while $v v_{x x x}$ represents a dispersion. We can provide a heuristic proof of this by examining the solution to the problem

$$
\begin{cases}v_{t}+a v_{x}=\mu v_{x x}+v v_{x x x}, & x \in \mathbb{R}, t>0 \\ v(x, 0)=e^{i k x}, & k \in \mathbb{Z}\end{cases}
$$

By applying the Fourier transform we find, if $\mu=v=0$,

$$
v(x, t)=e^{i k(x-a t)}
$$

while for $\mu$ and $v$ arbitrary real numbers (with $\mu>0$ ) we have

$$
v(x, t)=e^{-\mu k^{2} t} e^{i k\left[x-\left(a+v k^{2}\right) t\right]}
$$

Comparing these two relations, we see that for growing values of $\mu$ the modulus of the solution gets smaller. Such effect becomes more remarkable as the frequency $k$ increases (a phenomenon we have already registered in the previous section, albeit with partly different arguments).

The term $\mu v_{x x}$ in (14.59) therefore has a dissipative effect on the solution. In turn, $v$ modifies the propagation rate of the solution, increasing it in the $v>0$ case, and decreasing it if $v<0$. Also in this case, the effect is more notable at high frequencies. Hence, the third derivative term $v v_{x x x}$ introduces a dispersive effect.

In general, in the equivalent equation, even-order spatial derivatives represent diffusive terms, while odd-order derivatives represent dispersive terms. For first-order schemes (such as the upwind scheme) the dispersive effect is often barely visible, as it is disguised by the dissipative one. Taking $\Delta t$ and $h$ of the same order, from $(14.56)$ and (14.57) we evince that $v \ll \mu$ for $h \rightarrow 0$, as $v=O\left(h^{2}\right)$ and $\mu=O(h)$. In particular, if the CFL number is $\frac{1}{2}$, the third-order equivalent equation of the upwind method features a null dispersion, in accordance with the numerical results seen in the previous section.

Conversely, the dispersive effect is evident for the Lax-Friedrichs scheme, as well as for the Lax-Wendroff scheme which, being of second order, does not feature a dissipative term of type $\mu v_{x x}$. However, being stable, the latter cannot avoid being dissipative. Indeed, the fourth-order equivalent equation for the Lax-Wendroff scheme is

$$
v_{t}+a v_{x}=\frac{a h^{2}}{6}\left[(a \lambda)^{2}-1\right] v_{x x x}-\frac{a h^{3}}{6} a \lambda\left[1-(a \lambda)^{2}\right] v_{x x x x}
$$

where the last term is dissipative if $|a \lambda|<1$, as one can easily verify by applying the Fourier transform. We then recover, also for the Lax-Wendroff scheme, the CFL condition.

\subsubsection{Equivalent equations and error analysis}

The technique applied to obtain the equivalent equation denotes a strong analogy with the so-called backward analysis that we encounter during the numerical solution of linear systems, where the computed (not exact) solution is interpreted as the exact solution of a perturbed linear system (see [QSS07, Chap. 3]). As a matter of fact, the perturbed system plays a similar role to that of the equivalent equation.

Moreover, we observe that an error analysis of a numerical scheme can be carried out by using the equivalent equation associated to it. Indeed, by generically denoting by $r=\mu v_{x x}+v v_{x x x}$ the right-hand side of the equivalent equation, by comparison with (14.1) we obtain the error equation

$$
e_{t}+a e_{x}=r
$$

where $e=v-u$. Multiplying such equation by $e$ and integrating in space and time (between 0 and $t$ ) we obtain

$$
\|e(t)\|_{\mathrm{L}^{2}(\mathbb{R})} \leq C(t)\left(\|e(0)\|_{\mathrm{L}^{2}(\mathbb{R})}+\sqrt{\int_{0}^{t}\|r(s)\|_{\mathrm{L}^{2}(\mathbb{R})}^{2} d s}\right), \quad t>0
$$

having used the a priori estimate (14.4). We can assume $e(0)=0$ and therefore observe that $\|e(t)\|_{\mathrm{L}^{2}(\mathbb{R})}$ tends to zero (for $h$ and $\Delta t$ tending to zero) with order 1 for the upwind or Lax-Friedrichs schemes, and with order 2 for the Lax-Wendroff scheme (having supposed $v$ to be sufficiently regular).

\subsection{Exercises}

1. Verify that the solution to the problem (14.9)-(14.10) (with $f=0$ ) satisfies identity (14.11).

[Solution: Multiplying (14.9) by $u_{t}$ and integrating in space we obtain

$$
0=\int_{\alpha}^{\beta} u_{t t} u_{t} d x-\int_{\alpha}^{\beta} \gamma^{2} u_{x x} u_{t} d x=\frac{1}{2} \int_{\alpha}^{\beta}\left[\left(u_{t}\right)^{2}\right]_{t} d x+\int_{\alpha}^{\beta} \gamma^{2} u_{x} u_{x t} d x-\left[\gamma^{2} u_{x} u_{t}\right]_{\alpha}^{\beta}
$$

As

$$
\int_{\alpha}^{\beta} u_{t t} u_{t} d x=\frac{1}{2} \int_{\alpha}^{\beta}\left[\left(u_{t}\right)^{2}\right]_{t} d x \quad \text { and } \quad \int_{\alpha}^{\beta} \gamma^{2} u_{x} u_{x t} d x=\frac{1}{2} \int_{\alpha}^{\beta} \gamma^{2}\left[\left(u_{x}\right)^{2}\right]_{t} d x
$$

integrating $(14.60)$ in time we have

$$
\int_{\alpha}^{\beta} u_{t}^{2}(t) d x+\int_{\alpha}^{\beta} \gamma^{2} u_{x}^{2}(t) d x-\int_{\alpha}^{\beta} v_{0}^{2} d x-\int_{\alpha}^{\beta} \gamma^{2} u_{0 x}^{2} d x=0
$$

Hence (14.11) immediately follows from the latter relation.]

2. Verify that the solution provided by the backward/centered Euler scheme $(14.22)$

is unconditionally stable; more precisely,

$$
\|\mathbf{u}\|_{\Delta, 2} \leq\left\|\mathbf{u}^{0}\right\|_{\Delta, 2} \quad \forall \Delta t, h>0
$$

[Solution: Note that, thanks to (14.32),

$$
\left(u_{j}^{n+1}-u_{j}^{n}\right) u_{j}^{n+1} \geq \frac{1}{2}\left(\left|u_{j}^{n+1}\right|^{2}-\left|u_{j}^{n}\right|^{2}\right) \quad \forall j, n
$$

Then, multiplying (14.22) by $u_{j}^{n+1}$, summing over the index $j$ and using (14.33) we find

$$
\sum_{j}\left|u_{j}^{n+1}\right|^{2} \leq \sum_{j}\left|u_{j}^{n}\right|^{2} \quad \forall n \geq 0
$$

from which the result follows.] 3. Prove (14.30)

[Solution: We note that, in the case where $a>0$, the upwind scheme can be rewritten in the form

$$
u_{j}^{n+1}=(1-a \lambda) u_{j}^{n}+a \lambda u_{j-1}^{n}
$$

Under hypothesis (14.28) both coefficients $a \lambda$ and $1-a \lambda$ are non-negative, hence

$$
\min \left(u_{j}^{n}, u_{j-1}^{n}\right) \leq u_{j}^{n+1} \leq \max \left(u_{j}^{n}, u_{j-1}^{n}\right)
$$

Then

$$
\inf _{l \in \mathbb{Z}}\left\{u_{l}^{0}\right\} \leq u_{j}^{n} \leq \sup _{l \in \mathbb{Z}}\left\{u_{l}^{0}\right\} \quad \forall j \in \mathbb{Z}, \forall n \geq 0
$$

from which (14.30) follows.]

4. Study the accuracy of the Lax-Friedrichs scheme (14.16) for the solution of problem (14.1).

5. Study the accuracy of the Lax-Wendroff scheme (14.18) for the solution of problem (14.1). Chapter 15

\section{Finite elements and spectral methods for hyperbolic equations}

In this chapter we will illustrate how to apply Galerkin methods, and in particular the finite element method and the spectral one, to the spatial and/or temporal discretization of scalar hyperbolic equations. We will treat both continuous as well as discontinuous finite elements.

Let us consider the transport problem (14.3) and let us set for simplicity $(\alpha, \beta)=$ $(0,1), \varphi=0$. Moreover, let us suppose that $a$ is a positive constant and $a_{0}$ a nonnegative constant.

To start with, we proceed with a spatial discretization based on continuous finite elements. We therefore attempt a semidiscretization of the following form:

$\forall t>0$, find $u_{h}=u_{h}(t) \in V_{h}$ such that

$$
\left(\frac{\partial u_{h}}{\partial t}, v_{h}\right)+a\left(\frac{\partial u_{h}}{\partial x}, v_{h}\right)+a_{0}\left(u_{h}, v_{h}\right)=\left(f, v_{h}\right) \forall v_{h} \in V_{h},
$$

$u_{h}^{0}$ being the approximation of the initial datum. We have set

$$
V_{h}=\left\{v_{h} \in X_{h}^{r}: v_{h}(0)=0\right\}, \quad r \geq 1
$$

The space $X_{h}^{r}$ is defined as in (4.14), provided that we replace $(a, b)$ with $(0,1)$.

\subsection{Temporal discretization}

For the temporal discretization of problem (15.1) we will use finite difference schemes such as those introduced in Chapter $14$.

As usual, we will denote by $u_{h}^{n}, n \geq 0$, the approximation of $u_{h}$ at time $t^{n}=n \Delta t$.

\subsubsection{The forward and backward Euler schemes}

In case we use the forward Euler scheme, the discrete problem becomes:

$\forall n \geq 0$, find $u_{h}^{n+1} \in V_{h}$ such that

$$
\frac{1}{\Delta t}\left(u_{h}^{n+1}-u_{h}^{n}, v_{h}\right)+a\left(\frac{\partial u_{h}^{n}}{\partial x}, v_{h}\right)+a_{0}\left(u_{h}^{n}, v_{h}\right)=\left(f^{n}, v_{h}\right) \forall v_{h} \in V_{h}
$$

where $(u, v)=\int_{0}^{1} u(x) v(x) d x$ denotes as usual the scalar product of $\mathrm{L}^{2}(0,1)$

In the case of the backward Euler method, instead of $(15.2)$ we will have

$$
\frac{1}{\Delta t}\left(u_{h}^{n+1}-u_{h}^{n}, v_{h}\right)+a\left(\frac{\partial u_{h}^{n+1}}{\partial x}, v_{h}\right)+a_{0}\left(u_{h}^{n+1}, v_{h}\right)=\left(f^{n+1}, v_{h}\right) \forall v_{h} \in V_{h}
$$

Theorem 15.1. The backward Euler scheme is strongly stable with no restriction on $\Delta t$. Instead, the forward Euler method is strongly stable only for $a_{0}>0$,

Proof. Choosing $v_{h}=u_{h}^{n}$ in (15.2), we obtain (in the case $f=0$ )

$$
\left(u_{h}^{n+1}-u_{h}^{n}, u_{h}^{n}\right)+\Delta t a\left(\frac{\partial u_{h}^{n}}{\partial x}, u_{h}^{n}\right)+\Delta t a_{0}\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)}^{2}=0
$$

For the first term, we use the identity

$$
(v-w, w)=\frac{1}{2}\left(\|v\|_{\mathrm{L}^{2}(0,1)}^{2}-\|w\|_{\mathrm{L}^{2}(0,1)}^{2}-\|v-w\|_{\mathrm{L}^{2}(0,1)}^{2}\right) \forall v, w \in \mathrm{L}^{2}(0,1)
$$

which generalizes (14.34). For the second summand, integrating by parts and using the boundary conditions, we find

$$
\left(\frac{\partial u_{h}^{n}}{\partial x}, u_{h}^{n}\right)=\frac{1}{2}\left(u_{h}^{n}(1)\right)^{2}
$$

Thus, we obtain

$$
\left\|u_{h}^{n+1}\right\|_{\mathrm{L}^{2}(0,1)}^{2}+a \Delta t\left(u_{h}^{n}(1)\right)^{2}+2 a_{0} \Delta t\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)}^{2}=\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)}^{2}+\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)}^{2}
$$

We now seek an estimate for the term $\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)^{2}}^{2}$. To this end, setting in (15.2) 

$$
\begin{aligned}
&v_{h}=u_{h}^{n+1}-u_{h}^{n}, \text { we obtain } \\
&\qquad \begin{aligned}
\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)}^{2} & \leq \Delta t a\left|\left(\frac{\partial u_{h}^{n}}{\partial x}, u_{h}^{n+1}-u_{h}^{n}\right)\right|+\Delta t a_{0}\left|\left(u_{h}^{n}, u_{h}^{n+1}-u_{h}^{n}\right)\right| \\
& \leq \Delta t\left[a\left\|\frac{\partial u_{h}^{n}}{\partial x}\right\|_{L^{2}(0,1)}+a_{0}\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)}\right]\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)}
\end{aligned}
\end{aligned}
$$

By now using the inverse inequality (13.43) (referred to the interval $(0,1)$ ), we obtain

$$
\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{L^{2}(0,1)} \leq \Delta t\left(a C_{I} h^{-1}+a_{0}\right)\left\|u_{h}^{n}\right\|_{L^{2}(0,1)}
$$

Finally, (15.6) becomes

$$
\begin{aligned}
&\left\|u_{h}^{n+1}\right\|_{\mathrm{L}^{2}(0,1)}^{2}+a \Delta t\left(u_{h}^{n}(1)\right)^{2} \\
&\quad+\Delta t\left[2 a_{0}-\Delta t\left(a C_{I} h^{-1}+a_{0}\right)^{2}\right]\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)}^{2} \leq\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)}^{2}
\end{aligned}
$$

If $(15.4)$ is satisfied, then $\left\|u_{h}^{n+1}\right\|_{\mathrm{L}^{2}(0,1)} \leq\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)}$ and we therefore have strong stability in $\mathrm{L}^{2}(0,1)$ norm.

In the case where $a_{0}=0$ the obtained stability condition is meaningless. However, if we suppose that

$$
\Delta t \leq \frac{K h^{2}}{a^{2} C_{I}^{2}}
$$

for a given constant $K>0$, then we can apply the discrete Gronwall lemma (see Lemma 2.3) to (15.7) and we find that the method is stable, with a stability constant which in this case depends on the final time $T$. Precisely,

$$
\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)} \leq \exp \left(K t^{n}\right)\left\|u_{h}^{0}\right\|_{\mathrm{L}^{2}(0,1)} \leq \exp (K T)\left\|u_{h}^{0}\right\|_{\mathrm{L}^{2}(0,1)} \quad \forall n \geq 1
$$

In the case of the backward Euler method (15.3), we choose instead $v_{h}=u_{h}^{n+1}$. By using the relation

$$
(v-w, v)=\frac{1}{2}\left(\|v\|_{\mathrm{L}^{2}(0,1)}^{2}-\|w\|_{\mathrm{L}^{2}(0,1)}^{2}+\|v-w\|_{\mathrm{L}^{2}(0,1)}^{2}\right) \forall v, w \in \mathrm{L}^{2}(0,1)
$$

which generalizes (14.32), we find

$$
\left(1+2 a_{0} \Delta t\right)\left\|u_{h}^{n+1}\right\|_{\mathrm{L}^{2}(0,1)}^{2}+a \Delta t\left(u_{h}^{n+1}(1)\right)^{2} \leq\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)}^{2}
$$

Hence we have strong stability in $\mathrm{L}^{2}(0,1)$, for all $\Delta t$ and for all $a_{0} \geq 0$.

即

\subsubsection{The upwind, Lax-Friedrichs and Lax-Wendroff schemes}

The generalization to the finite elements case of the Lax-Friedrichs (LF), Lax-Wendroff (LW) and upwind (U) finite difference schemes can be attained in different ways. We start by observing that (14.16), (14.18), and (14.20) can be rewritten in the following comprehensive form

$$
\frac{u_{j}^{n+1}-u_{j}^{n}}{\Delta t}+a \frac{u_{j+1}^{n}-u_{j-1}^{n}}{2 h}-\mu \frac{u_{j+1}^{n}-2 u_{j}^{n}+u_{j-1}^{n}}{h^{2}}+a_{0} u_{j}^{n}=0
$$

(Note however that $a_{0}=0$ in (14.16), (14.18) and (14.20).) The second term is the discretization via centered finite differences of the convective term $a u_{x}\left(t^{n}\right)$, while the third one is a numerical diffusion term and corresponds to the discretization via finite differences of $-\mu u_{x x}\left(t^{n}\right)$. The numerical viscosity coefficient $\mu$ is given by

$$
\mu=\left\{\begin{aligned}
h^{2} / 2 \Delta t &(\mathrm{LF}) \\
a^{2} \Delta t / 2 &(\mathrm{LW}) \\
a h / 2 &(\mathrm{U})
\end{aligned}\right.
$$

Equation (15.10) suggests the following finite element version for the approximation of problem (14.3): $\forall n \geq 0$, find $u_{h}^{n+1} \in V_{h}$ such that

$$
\begin{aligned}
&\frac{1}{\Delta t}\left(u_{h}^{n+1}-u_{h}^{n}, v_{h}\right)+a\left(\frac{\partial u_{h}^{n}}{\partial x}, v_{h}\right)+a_{0}\left(u_{h}^{n}, v_{h}\right) \\
&+\mu\left(\frac{\partial u_{h}^{n}}{\partial x}, \frac{\partial v_{h}}{\partial x}\right)-\mu \gamma \frac{\partial u_{h}^{n}}{\partial x}(1) v_{h}(1)=\left(f^{n}, v_{h}\right) \quad \forall v_{h} \in V_{h}
\end{aligned}
$$

where $\gamma=1,0$ depending on whether or not we want to take the boundary contribution into account when integrating by parts the numerical viscosity term.

For the stability analysis, in the case $\gamma=0, a_{0}=0, a>0$, let us set $v_{h}=u_{h}^{n+1}-u_{h}^{n}$, in order to obtain

$$
\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)} \leq \Delta t\left(a+\mu C_{I} h^{-1}\right)\left\|\frac{\partial u_{h}^{n}}{\partial x}\right\|_{\mathrm{L}^{2}(0,1)}
$$

thanks to inequality (4.52). Having now set $v_{h}=u_{h}^{n}$, thanks to $(15.5)$ we obtain

$$
\begin{aligned}
&\left\|u_{h}^{n+1}\right\|_{\mathrm{L}^{2}(0,1)}^{2}-\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)}^{2}+a \Delta t\left(u_{h}^{n}(1)\right)^{2}+2 \Delta t \mu\left\|\frac{\partial u_{h}^{n}}{\partial x}\right\|_{\mathrm{L}^{2}(0,1)}^{2} \\
&\qquad=\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{\mathrm{L}^{2}(0,1)}^{2} \leq \Delta t^{2}\left(a+\mu C_{I} h^{-1}\right)^{2}\left\|\frac{\partial u_{h}^{n}}{\partial x}\right\|_{\mathrm{L}^{2}(0,1)}^{2}
\end{aligned}
$$

A sufficient condition for strong stability (i.e. to obtain an estimate such as $(14.27)$, with respect to $\left.\|\cdot\|_{L^{2}(0,1)}\right)$ is therefore

$$
\Delta t \leq \frac{2 \mu}{\left(a+\mu C_{I} h^{-1}\right)^{2}}
$$

Thanks to (15.11), in the case of the upwind method this is equivalent to

$$
\Delta t \leq \frac{h}{a}\left(\frac{1}{1+C_{I} / 2}\right)^{2}
$$

In the case of linear finite elements, $C_{I} \simeq 2 \sqrt{3}$, whence

$$
\frac{a \Delta t}{h} \lesssim\left(\frac{1}{1+\sqrt{3}}\right)^{2}
$$

The stability analysis we have just developed is based on the energy method and, in this case, leads to sub-optimal results. A better indicator can be obtained by resorting to the von Neumann analysis, as we saw in Sect. 14.4.3. To this end we observe that, in the case of linear finite elements with constant spacing $h,(15.12)$ with $f=0$ can be rewritten in the following way for each internal node $x_{j}$ :

$$
\begin{aligned}
&\frac{1}{6}\left(u_{j+1}^{n+1}+4 u_{j}^{n+1}+u_{j-1}^{n+1}\right)+\frac{\lambda a}{2}\left(u_{j+1}^{n}-u_{j-1}^{n}\right)+\frac{a_{0}}{6} \Delta t\left(u_{j+1}^{n}+4 u_{j}^{n}+u_{j-1}^{n}\right) \\
&-\mu \Delta t \frac{u_{j+1}^{n}-2 u_{j}^{n}+u_{j-1}^{n}}{h^{2}}=\frac{1}{6}\left(u_{j+1}^{n}+4 u_{j}^{n}+u_{j-1}^{n}\right)
\end{aligned}
$$

By comparing such relation to (15.10), we can note that the difference only resides in the term arising from the temporal derivative and from the term of order zero, and has to be attributed to the presence of the mass matrix in the case of finite elements. On the other hand, we saw in Sect. $13.5$ that we can apply the mass-lumping technique to approximate the mass matrix using a diagonal matrix. By proceeding in this way, scheme (15.13) can effectively be reduced to $(15.10)$ (see Exercise 1 ).

Remark 15.1. Note that relations (15.13) refer to the internal nodes. The approach used to handle boundary conditions with the finite element method generally yields different relations than those obtained via the finite difference method.

These observations allow us to extend all the schemes seen in Sect. 14.3.1 to analogous schemes, generated by discretizations in space with continuous linear finite elements. To this end, it will be sufficient to replace the term $u_{j}^{n+1}-u_{j}^{n}$ with

$$
\frac{1}{6}\left[\left(u_{j-1}^{n+1}-u_{j-1}^{n}\right)+4\left(u_{j}^{n+1}-u_{j}^{n}\right)+\left(u_{j+1}^{n+1}-u_{j+1}^{n}\right)\right]
$$

Thus, the general scheme (14.13) is replaced by

$$
\frac{1}{6}\left(u_{j-1}^{n+1}+4 u_{j}^{n+1}+u_{j-1}^{n+1}\right)=\frac{1}{6}\left(u_{j-1}^{n}+4 u_{j}^{n}+u_{j-1}^{n}\right)-\lambda\left(H_{j+1 / 2}^{n *}-H_{j-1 / 2}^{n *}\right)
$$

where

$$
H_{j+1 / 2}^{n *}= \begin{cases}H_{j+1 / 2}^{n} & \text { for explicit time-advancing schemes, } \\ H_{j+1 / 2}^{n+1} & \text { for implicit time-advancing schemes. }\end{cases}
$$

Note that, even if we had adopted a numerical flux corresponding to an explicit time-advancing scheme, the resulting scheme would no longer lead to a diagonal system (indeed, it becomes a tridiagonal one) because of the mass matrix terms. The use of an explicit time-advancing scheme for finite elements might seem inconvenient with respect to a similar full finite difference scheme. However, such a scheme has interesting features. In particular, let us consider its amplification and dispersion coefficients, using the von Neumann analysis illustrated in Sect. 14.4.3. To this end, let us suppose that the differential equation is defined on all of $\mathbb{R}$, or, alternatively, let us consider a bounded interval and assume periodic boundary conditions at its endpoints. In either case, we can assume that relation (15.14) holds for all values of the index $j$. A simple computation leads us to writing the following relation between the amplification coefficient $\gamma_{k}$ of a finite difference scheme (see Table 14.1) and the amplification coefficient $\gamma_{k}^{\mathrm{FEM}}$ of the corresponding finite element scheme

$$
\gamma_{k}^{\mathrm{FEM}}=\frac{3 \gamma_{k}-1+\cos \left(\phi_{k}\right)}{2+\cos \left(\phi_{k}\right)}
$$

where we denote again with $\phi_{k}$ the phase angle relative to the $k$-th harmonic (see Sect. 14.4.3).

We can thus compute the amplification and dispersion errors, which are reported in Fig. 15.1. Comparing them with the analogous errors relating to the corresponding finite difference scheme (reported in Fig. 14.6) we can make the following remarks. The forward Euler scheme is still unconditionally unstable (in the sense of strong stability). The upwind scheme (FEM) is strongly stable if the CFL number is less than $\frac{1}{3}$ (hence, a less restrictive result than the one found using the energy method), while the Lax-Friedrichs (FEM) method never satisfies the condition $\gamma_{k}^{F E M} \leq 1$ (in accordance with the result that we would find using the energy method in this case).

More generally, we can say that in the case of schemes with an explicit temporal treatment, the "finite element" version requires more restrictive stability conditions than the corresponding finite difference one. In particular, for the Lax-Wendroff finite element scheme, that we will denote with LW (FEM), the CFL number must now be less than $\frac{1}{\sqrt{3}}$, instead of 1 as in the finite differences case. However, the LW (FEM) scheme (for the CFL values for which it is stable), is slightly less diffusive and dispersive than the equivalent finite difference scheme, for a wide range of values of the phase angle $\phi_{k}=k h$. The implicit Euler scheme remains unconditionally stable also in the FEM version (coherently with what we obtained using the energy method in Sect. $15.1 .1)$.

Example 15.1. The previous conclusions have been experimentally verified as follows. We have repeated the case of Fig. $14.7$ (right), where we have now considered a CFL value of $0.5$. The numerical solutions obtained via the classical Lax-Wendroff method (LW) and via LW (FEM) for $t=2$ are reported in Fig. 15.2. We can note how the LW (FEM) scheme provides a solution that is more accurate and, especially, featuring a smaller phase error. This result is confirmed by the value of the $\|\cdot\|_{\Delta, 2}$ norm of the error in the two cases. Indeed, by calling $u$ the exact solution and $u_{L W}$ resp. $u_{L W(F E M)}$ the one obtained using the two numerical schemes, $\left\|u_{L W}-u\right\|_{\Delta, 2}=$ $0.78, \quad\left\|u_{L W(F E M)}-u\right\|_{\Delta, 2}=0.49$.

Further tests conducted with non-periodic boundary conditions confirm the stability properties previously derived. 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-022.jpg?height=592&width=704&top_left_y=116&top_left_x=106)

$\mathrm{mr}$

ification error LW (FEM

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-022.jpg?height=152&width=344&top_left_y=713&top_left_x=107)

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-022.jpg?height=150&width=345&top_left_y=847&top_left_x=107)

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-022.jpg?height=132&width=243&top_left_y=854&top_left_x=500)

Fig. 15.1. Amplification and dispersion errors for several finite element schemes obtained from
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-022.jpg?height=284&width=636&top_left_y=713&top_left_x=107) the general scheme $(15.14)$ 

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-023.jpg?height=340&width=398&top_left_y=113&top_left_x=259)

Fig. 15.2. Comparison between the solution obtained via the Lax-Wendroff finite difference scheme (LW) and its finite element version (LW (FEM)) $\left(\phi_{k}=\pi / 4, t=2\right)$

\subsection{Taylor-Galerkin schemes}

We now illustrate a class of finite element schemes named "Taylor-Galerkin"schemes. These are derived in a similar way to the Lax-Wendroff scheme, and we will indeed see that the LW (FEM) version is in this class.

For simplicity, we will refer to the pure transport problem (14.1). The TaylorGalerkin method consists in combining the Taylor formula truncated to the first order

$$
u\left(x, t^{n+1}\right)=u\left(x, t^{n}\right)+\Delta t \frac{\partial u}{\partial t}\left(x, t^{n}\right)+\int_{t^{n}}^{t^{n+1}}\left(s-t^{n}\right) \frac{\partial^{2} u}{\partial t^{2}}(x, s) d s
$$

with equation (14.1). By formally differentiating (14.1) with respect to $t$ we obtain

$$
\frac{\partial^{2} u}{\partial t^{2}}=\frac{\partial}{\partial t}\left(-a \frac{\partial u}{\partial x}\right)=-a \frac{\partial}{\partial x} \frac{\partial u}{\partial t}=a^{2} \frac{\partial^{2} u}{\partial x^{2}}
$$

From (15.16) we then obtain

$$
u\left(x, t^{n+1}\right)=u\left(x, t^{n}\right)-a \Delta t \frac{\partial u}{\partial x}\left(x, t^{n}\right)+a^{2} \int_{t^{n}}^{t^{n+1}}\left(s-t^{n}\right) \frac{\partial^{2} u}{\partial x^{2}}(x, s) d s
$$

We approximate the integral in the following way

$$
\int_{t^{n}}^{t^{n+1}}\left(s-t^{n}\right) \frac{\partial^{2} u}{\partial x^{2}}(x, s) d s \approx \frac{\Delta t^{2}}{2}\left[\theta \frac{\partial^{2} u}{\partial x^{2}}\left(x, t^{n}\right)+(1-\theta) \frac{\partial^{2} u}{\partial x^{2}}\left(x, t^{n+1}\right)\right]
$$

obtained by evaluating the first factor at $s=t^{n}+\frac{\Delta t}{2}$ and the second one through a linear combination (using $\theta \in[0,1]$ as a parameter) of its values in $s=t^{n}$ and $s=t^{n+1}$. We denote by $u^{n}(x)$ the approximating function $u\left(x, t^{n}\right)$. Let us consider two remarkable situations. If $\theta=1$, we obtain a semi-discretized scheme that is explicit in time

$$
u^{n+1}=u^{n}-a \Delta t \frac{\partial u^{n}}{\partial x}+\frac{a^{2} \Delta t^{2}}{2} \frac{\partial^{2} u^{n}}{\partial x^{2}}
$$

If we now discretize in space by finite differences or finite elements, we recover the previously examined LW and LW (FEM) schemes.

Instead, if we take $\theta=\frac{2}{3}$, the approximation error in (15.18) becomes $O\left(\Delta t^{4}\right)$ (supposing that $u$ has the required regularity). De facto, such choice corresponds to approximating $\frac{\partial^{2} u}{\partial x^{2}}$ between $t^{n}$ and $t^{n+1}$ with its linear interpolant. The resulting semidiscretized scheme is written

$$
\left[1-\frac{a^{2} \Delta t^{2}}{6} \frac{\partial^{2}}{\partial x^{2}}\right] u^{n+1}=u^{n}-a \Delta t \frac{\partial u^{n}}{\partial x}+\frac{a^{2} \Delta t^{2}}{3} \frac{\partial^{2} u^{n}}{\partial x^{2}}
$$

its truncation error in time is $\mathscr{O}\left(\Delta t^{3}\right)$.

At this point, a discretization in space using the finite element method leads to the following scheme, called Taylor-Galerkin (TG):

for $n=0,1, \ldots$ find $u_{h}^{n+1} \in V_{h}$ such that

$$
\begin{aligned}
A\left(u_{h}^{n+1}, v_{h}\right)=&\left(u_{h}^{n}, v_{h}\right)-a \Delta t\left(\frac{\partial u_{h}^{n}}{\partial x}, v_{h}\right)-\frac{a^{2} \Delta t^{2}}{3}\left(\frac{\partial u_{h}^{n}}{\partial x}, \frac{\partial v_{h}}{\partial x}\right) \\
&+\gamma \frac{a^{2} \Delta t^{2}}{3} \frac{\partial u_{h}^{n}}{\partial x}(1) v_{h}(1) \quad \forall v_{h} \in V_{h}
\end{aligned}
$$

where

$$
A\left(u_{h}^{n+1}, v_{h}\right)=\left(u_{h}^{n+1}, v_{h}\right)+\frac{a^{2} \Delta t^{2}}{6}\left(\frac{\partial u_{h}^{n+1}}{\partial x}, \frac{\partial v_{h}}{\partial x}\right)-\gamma \frac{a^{2} \Delta t^{2}}{6} \frac{\partial u_{h}^{n+1}}{\partial x}(1) v_{h}(1)
$$

and $\gamma=1,0$ depending on whether or not we want to take into account the boundary contribution when integrating by parts the second derivative.

The latter yields a linear system whose matrix is

$$
\mathrm{A}=\mathrm{M}+\frac{a^{2}(\Delta t)^{2}}{6} \mathrm{~K}
$$

$\mathrm{M}$ is the mass matrix and $\mathrm{K}$ is the stiffness matrix, possibly taking the boundary contribution as well into account (if $\gamma=1$ ).

In the case of linear finite elements, the von Neumann analysis leads to the following $k$-th amplification factor for scheme $(15.20)$

$$
\gamma_{k}=\frac{2+\cos (k h)-2 a^{2} \lambda^{2}(1-\cos (k h))+3 i a \lambda \sin (k h)}{2+\cos (k h)+a^{2} \lambda^{2}(1-\cos (k h))}
$$

It can be proven that the scheme is strongly stable in $\|\cdot\|_{\Delta, 2}$ under the CFL condition $\frac{a \Delta t}{h} \leq 1$. Thus, it has a less restrictive stability condition than the Lax-Wendroff (FEM) 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-025.jpg?height=426&width=534&top_left_y=115&top_left_x=193)

Fig. 15.3. Amplification (top) and dispersion (bottom) error of the Taylor-Galerkin scheme (15.20), as a function of the phase angle $\phi_{k}=k h$ and for different values of the CFL number

scheme.

Fig. $15.3$ shows the behaviour of the amplification and dispersion error for the scheme (15.20), as a function of the phase angle, in analogy to what we have seen for other schemes in Sect. 14.4.4.

In the case of linear finite elements the truncation error of the TG scheme is $\mathscr{O}\left(\Delta t^{3}\right)+\mathscr{O}\left(h^{2}\right)+\mathscr{O}\left(h^{2} \Delta t\right)$.

Example 15.2. To compare the accuracy of the schemes presented in the last two sections, we have considered the problem

$$
\begin{cases}\frac{\partial u}{\partial t}+\frac{\partial u}{\partial x}=0, & x \in(0,0.5), t>0 \\ u(x, 0)=u_{0}(x), & x \in(0,0.5)\end{cases}
$$

with periodic boundary conditions, $u(0, t)=u(0.5, t)$, for $t>0$. The initial datum is $u_{0}(x)=2 \cos (4 \pi x)+\sin (20 \pi x)$, and is illustrated in Fig. $15.4$ (left). The latter superposes two harmonics, one with low frequency one and one with high frequency.

We have considered the Taylor-Galerkin, Lax-Wendroff (FEM), (finite difference) Lax-Wendroff and upwind schemes. In Fig. 15.4 (right) we show the error in discrete norm $\left\|u-u_{h}\right\|_{\Delta, 2}$ obtained at time $t=1$ for different values of $\Delta t$ and with a fixed CFL number of $0.55$. We can note a better convergence of the Taylor-Galerkin scheme, while the two versions of the Lax-Wendroff scheme show the same order of convergence, but with a smaller error for the finite element version. The upwind 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-026.jpg?height=286&width=718&top_left_y=117&top_left_x=98)

Fig. 15.4. Initial condition $u_{0}$ for the simulation of example $15.2$ (left) and error $\left\|u-u_{h}\right\|_{\Delta, 2}$ at $t=1$ for varying $\Delta t$ and fixed CFL for different numerical schemes (right)

scheme is less accurate: it features a larger absolute error and a lower convergence rate. Moreover, it can be verified that for a fixed CFL, the error of the upwind scheme is $\mathscr{O}(\Delta t)$, that of both variants of the Lax-Wendroff scheme is $\mathscr{O}\left(\Delta t^{2}\right)$, while the error of the Taylor-Galerkin scheme is $\mathscr{O}\left(\Delta t^{3}\right)$.

We report in Figs. $15.5$ and $15.6$ the numerical approximations and corresponding errors in maximum norm for the transport problem

$$
\begin{cases}\frac{\partial u}{\partial t}-\frac{\partial u}{\partial x}=0, & x \in(0,2 \pi), t>0 \\ u(x, 0)=\sin (\pi \cos (x)), & x \in(0,2 \pi)\end{cases}
$$

having periodic boundary conditions. Such approximations are obtained using finite differences of order 2 and 4 (ufd2, ufd4), compact finite differences of order 4 and 6 (ucp4, ucp6), and by the Galerkin spectral method with Fourier basis (ugal). For the sake of comparison, we also report the exact solution $u(x, t)=\sin (\pi \cos (x+t))$ (uex).

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-026.jpg?height=233&width=331&top_left_y=917&top_left_x=113)
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-026.jpg?height=234&width=684&top_left_y=916&top_left_x=113) methods (of order 2,4$)$, compact finite difference methods (of order 4 and 6 ) and with the Fourier Galerkin spectral method (from [CHQZ06]) 

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-027.jpg?height=265&width=373&top_left_y=117&top_left_x=276)

Fig. 15.6. Behaviour of the error in maximum norm for the different numerical methods reported in Fig. $15.5$ (from [CHQZ06])

\subsection{The multi-dimensional case}

Let us now move to the multi-dimensional case and consider the following first-order, linear and scalar hyperbolic transport-reaction problem in the domain $\Omega \subset \mathbb{R}^{d}$, with $d=2,3:$

$$
\begin{cases}\frac{\partial u}{\partial t}+\mathbf{a} \cdot \nabla u+a_{0} u=f, & \mathbf{x} \in \Omega, t>0 \\ u=\varphi, & \mathbf{x} \in \partial \Omega^{i n}, t>0 \\ u_{\mid t=0}=u_{0}, & & \mathbf{x} \in \Omega\end{cases}
$$

where $\mathbf{a}=\mathbf{a}(\mathbf{x}), a_{0}=a_{0}(\mathbf{x}, t)$ (possibly zero), $f=f(\mathbf{x}, t), \varphi=\varphi(\mathbf{x}, t)$ and $u_{0}=u_{0}(\mathbf{x})$ are given functions. The inflow boundary $\partial \Omega^{\text {in }}$ is defined by

$$
\partial \Omega^{i n}=\{\mathbf{x} \in \partial \Omega: \mathbf{a}(\mathbf{x}) \cdot \mathbf{n}(\mathbf{x})<0\}
$$

n being the outward unit normal vector to $\partial \Omega$.

For simplicity, we have supposed that a does not depend on $t$; in this way, the inflow boundary $\partial \Omega^{\text {in }}$ does not change with time.

\subsubsection{Semi-discretization: strong and weak treatment of the boundary conditions}

To obtain a semi-discrete approximation of problem (15.22), similar to that used in the one-dimensional case (15.1), we define the spaces

$$
V_{h}=X_{h}^{r}, \quad V_{h}^{i n}=\left\{v_{h} \in V_{h}: v_{h \mid} \partial \Omega^{i n}=0\right\},
$$

where $r$ is an integer $\geq 1$ and $X_{h}^{r}$ was introduced in (4.38). We denote by $u_{0, h}$ and $\varphi_{h}$ two suitable finite element approximations of $u_{0}$ and $\varphi$, respectively, and we consider the problem: for each $t>0$ find $u_{h}(t) \in V_{h}$ such that

$$
\left\{\begin{array}{l}
\int_{\Omega} \frac{\partial u_{h}(t)}{\partial t} v_{h} d \Omega+\int_{\Omega} \mathbf{a} \cdot \nabla u_{h}(t) v_{h} d \Omega+\int_{\Omega} a_{0}(t) u_{h}(t) v_{h} d \Omega \\
=\int_{\Omega} f(t) v_{h} d \Omega \quad \forall v_{h} \in V_{h}^{i n} \\
u_{h}(t)=\varphi_{h}(t) \quad \text { on } \partial \Omega^{i n}
\end{array}\right.
$$

with $u_{h}(0)=u_{0, h} \in V_{h}$.

To obtain a stability estimate, we assume for simplicity that $\varphi$, and therefore $\varphi_{h}$, is identically null. In this case $u_{h}(t) \in V_{h}^{i n}$, and taking, for every $t, v_{h}=u_{h}(t)$, we have the following inequality

$$
\begin{aligned}
\left\|u_{h}(t)\right\|_{\mathrm{L}^{2}(\Omega)}^{2} &+\int_{0}^{t} \mu_{0}\left\|u_{h}(\tau)\right\|_{\mathrm{L}^{2}(\Omega)}^{2} d \tau+\int_{0}^{t} \int_{\partial \Omega \backslash \partial \Omega^{i n}} \mathbf{a} \cdot \mathbf{n} u_{h}^{2}(\tau) d \gamma d \tau \\
& \leq\left\|u_{0, h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\int_{0}^{t} \frac{1}{\mu_{0}}\|f(\tau)\|_{\mathrm{L}^{2}(\Omega)}^{2} d \tau
\end{aligned}
$$

We have assumed that there exists a positive constant $\mu_{0}$ such that, for all $t>0$ and for each $\mathbf{x}$ in $\Omega$,

$$
0<\mu_{0} \leq \mu(\mathbf{x}, t)=a_{0}(\mathbf{x}, t)-\frac{1}{2} \operatorname{diva}(\mathbf{x})
$$

In the case where such hypothesis is not verified (for instance if $\mathbf{a}$ is a constant field and $a_{0}=0$ ), then by using the Gronwall Lemma $2.2$ we obtain

$$
\begin{aligned}
&\left\|u_{h}(t)\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\int_{0}^{t} \int_{\partial \Omega \backslash \partial \Omega^{i n}} \mathbf{a} \cdot \mathbf{n} u_{h}^{2}(\tau) d \gamma d \tau \\
&\leq\left(\left\|u_{0, h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\int_{0}^{t}\|f(\tau)\|_{\mathrm{L}^{2}(\Omega)}^{2} d \tau\right) \exp \int_{0}^{t}\left[1+2 \mu^{*}(\tau)\right] d \tau
\end{aligned}
$$

where we have set $\mu^{*}(t)=\max _{\mathbf{x} \in \bar{\Omega}}|\mu(\mathbf{x}, t)|$.

Supposing for simplicity that $f=0$, if $u_{0} \in H^{r+1}(\Omega)$ we have the following convergence result

$$
\begin{aligned}
\max _{t \in[0, T]}\left\|u(t)-u_{h}(t)\right\|_{L^{2}(\Omega)} &+\left(\int_{0}^{T} \int_{\partial \Omega}|\mathbf{a} \cdot \mathbf{n}|\left|u(t)-u_{h}(t)\right|^{2} d \gamma d t\right)^{1 / 2} \\
& \leq C h^{r}\left\|u_{0}\right\|_{\mathrm{H}^{r+1}(\Omega)}
\end{aligned}
$$

For the proofs, we refer to [QV94, Chap. 14], [Joh87] and to the references cited therein. In problem (15.24) the boundary condition has been imposed in a strong (or essential) way. An alternative option is the weak (or natural) treatment that derives from the integration by parts of the transport term in the first equation in $(15.24)$, where we now consider $v_{h} \in V_{h}$ (i.e. we no longer require that the test function is null on the inflow boundary). We obtain

$$
\begin{aligned}
\left.\int_{\Omega} \frac{\partial u_{h}(t)}{\partial t} v_{h} d \Omega-\int_{\Omega} \operatorname{div}\left(\mathbf{a} v_{h}\right)\right) u_{h}(t) d \Omega \\
&+\int_{\Omega} a_{0} u_{h}(t) v_{h} d \Omega+\int_{\partial \Omega} \mathbf{a} \cdot \mathbf{n} u_{h}(t) v_{h} d \gamma=\int_{\Omega} f(t) v_{h} d \Omega
\end{aligned}
$$

The boundary condition is imposed by replacing $u_{h}$ with $\varphi_{h}$ on the inflow boundary part, obtaining

$$
\begin{gathered}
\int_{\Omega} \frac{\partial u_{h}(t)}{\partial t} v_{h} d \Omega-\int_{\Omega} \operatorname{div}\left(\mathbf{a} v_{h}\right) u_{h}(t) d \Omega+\int_{\Omega} a_{0} u_{h}(t) v_{h} d \Omega+\int_{\partial \Omega \backslash \partial \Omega^{i n}} \mathbf{a} \cdot \mathbf{n} u_{h}(t) v_{h} d \gamma \\
=\int_{\Omega} f(t) v_{h} d \Omega-\int_{\partial \Omega^{i n}} \mathbf{a} \cdot \mathbf{n} \varphi_{h}(t) v_{h} d \gamma \forall v_{h} \in V_{h}
\end{gathered}
$$

Clearly, the solution $u_{h}$ found in this way only satisfies the boundary condition in an approximate way.

A further option consists in counter-integrating (15.28) by parts, thus producing the following formulation: for each $t>0$, find $u_{h}(t) \in V_{h}$ such that

$$
\begin{gathered}
\int_{\Omega} \frac{\partial u_{h}(t)}{\partial t} v_{h} d \Omega+\int_{\Omega} \mathbf{a} \cdot \nabla u_{h}(t) v_{h} d \Omega+\int_{\Omega} a_{0} u_{h}(t) v_{h} d \Omega+\int_{\partial \Omega^{i n}} v_{h}\left(\varphi_{h}(t)-u_{h}(t)\right) \mathbf{a} \cdot \mathbf{n} d \gamma \\
=\int_{\Omega} f(t) v_{h} d \Omega \forall v_{h} \in V_{h}
\end{gathered}
$$

We note that the formulations (15.28) and (15.29) are equivalent: the only difference is the way boundary terms are highlighted. In particular, the boundary integral in formulation (15.29) can be interpreted as a penalization term with which we evaluate how different $u_{h}$ is from the data $\varphi_{h}$ on the inflow boundary. Assuming that hypothesis (15.26) is still true, having chosen $v_{h}=u_{h}(t)$ in (15.29), integrating the convective term by parts and using the Cauchy-Schwarz and Young inequalities, we get the following stability estimate

$$
\begin{aligned}
&\left\|u_{h}(t)\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\int_{0}^{t} \mu_{0}\left\|u_{h}(\tau)\right\|_{\mathrm{L}^{2}(\Omega)}^{2} d \tau+\int_{0}^{t} \int_{\partial \Omega \backslash \partial \Omega^{i n}} \mathbf{a} \cdot \mathbf{n} u_{h}^{2}(\tau) d \gamma d \tau \\
&\leq\left\|u_{0, h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\int_{0}^{t} \int_{\partial \Omega^{i n}}|\mathbf{a} \cdot \mathbf{n}| \varphi_{h}^{2}(\tau) d \gamma d \tau+\int_{0}^{t} \frac{1}{\mu_{0}}\|f(\tau)\|_{\mathrm{L}^{2}(\Omega)}^{2} d \tau
\end{aligned}
$$

In absence of hypothesis (15.26), inequality (15.30) would change in an analogous way to what we have previously seen, provided we use the Gronwall Lemma $2.2$ as we did to derive $(15.27)$.

Remark 15.2. In the case where the boundary condition for problem (15.22) takes the form $\mathbf{a} \cdot \mathbf{n} u=\psi$, we could again impose it weakly by adding a penalization term, that in such case would take the form

$$
\int_{\partial \Omega^{i n}}\left(\psi_{h}(t)-\mathbf{a} \cdot \mathbf{n} u_{h}(t)\right) v_{h} d \gamma
$$

$\psi_{h}$ being a suitable finite element approximation of the datum $\psi$.

Alternatively to the strong and weak imposition of the boundary conditions, i.e. to formulations (15.24) and (15.29), we could adopt a Petrov-Galerkin approach by imposing in a strong way the condition $u_{h}(t)=\varphi_{h}(t)$ on the inflow boundary $\partial \Omega^{\text {in }}$, and requiring $v_{h}=0$ on the outflow boundary $\partial \Omega^{\text {out }}$, yielding the following discrete formulation. Set $V_{h}^{\text {out }}=\left\{v_{h} \in V_{h}: v_{h \mid} \partial \Omega^{\text {out }}=0\right\}$. Then for each $t>0$ find $u_{h}(t) \in V_{h}=$ $X_{h}^{r}$ such that

$$
\left\{\begin{aligned}
\int_{\Omega} \frac{\partial u_{h}(t)}{\partial t} v_{h} d \Omega+\int_{\Omega}\left(\mathbf{a} \cdot \nabla u_{h}(t)\right) v_{h} d \Omega+\int_{\Omega} a_{0}(t) u_{h}(t) v_{h} d \Omega \\
=\int_{\Omega} f(t) v_{h} d \Omega \quad \forall v_{h} \in V_{h}^{\text {out }}
\end{aligned}\right.
$$

We recall that for a Petrov-Galerkin formulation, the well-posedness analysis cannot be based on the Lax-Milgram lemma any longer.

Instead, if the inflow condition were inposed in a weak way, we would have the following formulation:

for each $t>0$, find $u_{h}(t) \in V_{h}=X_{h}^{r}$ such that, for each $v_{h} \in V_{h}^{\text {out }}$,

$$
\begin{aligned}
\int_{\Omega} \frac{\partial u_{h}(t)}{\partial t} v_{h} d \Omega-\int_{\Omega} \operatorname{div}\left(\mathbf{a} v_{h}\right) u_{h}(t) d \Omega+& \int_{\Omega} a_{0}(t) u_{h}(t) v_{h} d \Omega \\
&=\int_{\Omega} f(t) v_{h} d \Omega-\int_{\partial \Omega^{i n}} \mathbf{a} \cdot \mathbf{n} \varphi_{h}(t) v_{h} d \gamma
\end{aligned}
$$

For further details, the reader can refer to [QV94, Chap. 14].

\subsubsection{Temporal discretization}

For an illustrative purpose, let us limit ourselves to considering the Galerkin semidiscrete problem (15.24). Using the backward Euler scheme for the temporal discretization, we obtain the following fully discrete problem: $\forall n \geq 0$ find $u_{h}^{n+1} \in V_{h}$ such that

$$
\left\{\begin{array}{l}
\frac{1}{\Delta t} \int_{\Omega}\left(u_{h}^{n+1}-u_{h}^{n}\right) v_{h} d \Omega+\int_{\Omega} \mathbf{a} \cdot \nabla u_{h}^{n+1} v_{h} d \Omega+\int_{\Omega} a_{0}^{n+1} u_{h}^{n+1} v_{h} d \Omega \\
=\int_{\Omega} f^{n+1} v_{h} d \Omega \quad \forall v_{h} \in V_{h}^{i n} \\
u_{h}^{n+1}=\varphi_{h}^{n+1} \text { on } \partial \Omega^{i n}
\end{array}\right.
$$

where $u_{h}^{0}=u_{0, h} \in V_{h}$ is a suitable approximation in $V_{h}$ of the initial datum $u_{0}$.

Let us limit ourselves to the homogeneous case, where $f=0$ and $\varphi_{h}=0$ (in this case $u_{h}^{n} \in V_{h}^{\text {in }}$ for every $n \geq 0$ ). Having set $v_{h}=u_{h}^{n+1}$ and using identities (15.8) and (15.26), we obtain, for each $n \geq 0$

$$
\begin{aligned}
&\frac{1}{2 \Delta t}\left(\left\|u_{h}^{n+1}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}-\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}\right) \\
&+\frac{1}{2} \int_{\partial \Omega \backslash \partial \Omega^{i n}} \mathbf{a} \cdot \mathbf{n}\left(u_{h}^{n+1}\right)^{2} d \gamma+\mu_{0}\left\|u_{h}^{n+1}\right\|_{\mathrm{L}^{2}(\Omega)}^{2} \leq 0
\end{aligned}
$$

For each $m \geq 1$, summing over $n$ from 0 to $m-1$ we obtain

$$
\begin{aligned}
&\left\|u_{h}^{m}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+2 \Delta t\left(\mu_{0} \sum_{n=0}^{m}\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\frac{1}{2} \sum_{n=0}^{m} \int_{\partial \Omega \backslash \partial \Omega^{i n}} \mathbf{a} \cdot \mathbf{n}\left(u_{h}^{n}\right)^{2} d \gamma\right) \\
&\leq\left\|u_{0, h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}
\end{aligned}
$$

In particular, as $\mathbf{a} \cdot \mathbf{n} \geq 0$ on $\partial \Omega \backslash \partial \Omega^{i n}$, we conclude that

$$
\left\|u_{h}^{m}\right\|_{\mathrm{L}^{2}(\Omega)} \leq\left\|u_{0, h}\right\|_{\mathrm{L}^{2}(\Omega)} \quad \forall m \geq 0
$$

As expected, this method is strongly stable, with no condition on $\Delta t$. We now consider the discretization in time using the forward Euler method

$$
\left\{\begin{array}{l}
\frac{1}{\Delta t} \int_{\Omega}\left(u_{h}^{n+1}-u_{h}^{n}\right) v_{h} d \Omega+\int_{\Omega} \mathbf{a} \cdot \nabla u_{h}^{n} v_{h} d \Omega+\int_{\Omega} a_{0}^{n} u_{h}^{n} v_{h} d \Omega \\
=\int_{\Omega} f^{n} v_{h} d \Omega \quad \forall v_{h} \in V_{h}^{i n} \\
u_{h}^{n+1}=\varphi_{h}^{n+1} \text { on } \partial \Omega^{i n}
\end{array}\right.
$$

We suppose again that $f=0, \varphi=0$ and that the condition $(15.26)$ is verified. Moreover, we suppose that $\|\mathbf{a}\|_{L^{\infty}(\Omega)}<\infty$ and that, for each $t>0,\left\|a_{0}\right\|_{L^{\infty}(\Omega)}<\infty$. Setting $v_{h}=u_{h}^{n}$, exploiting identity (15.5) and integrating the convective term by parts, we obtain

$$
\begin{aligned}
&\frac{1}{2 \Delta t}\left(\left\|u_{h}^{n+1}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}-\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}-\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}\right) \\
&+\frac{1}{2} \int_{\partial \Omega \backslash \partial \Omega^{i n}} \mathbf{a} \cdot \mathbf{n}\left(u_{h}^{n}\right)^{2} d \gamma+\left(-\frac{1}{2} \operatorname{div}(\mathbf{a})+a_{0}^{n},\left(u_{h}^{n}\right)^{2}\right)=0
\end{aligned}
$$

and then, after a few steps,

$$
\begin{aligned}
\left\|u_{h}^{n+1}\right\|_{\mathrm{L}^{2}(\Omega)}^{2} &+\Delta t \int_{\partial \Omega \backslash \partial \Omega^{i n}} \mathbf{a} \cdot \mathbf{n}\left(u_{h}^{n}\right)^{2} d \gamma+2 \Delta t \mu_{0}\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2} \\
& \leq\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}
\end{aligned}
$$

It is now necessary to control the term $\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}$. To this end, we set $v_{h}=$ $u_{h}^{n+1}-u_{h}^{n}$ in $(15.31)$ and obtain

$$
\begin{aligned}
&\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}=-\Delta t\left(\mathbf{a} \nabla u_{h}^{n}, u_{h}^{n+1}-u_{h}^{n}\right)-\Delta t\left(a_{0}^{n} u_{h}^{n}, u_{h}^{n+1}-u_{h}^{n}\right) \\
&\leq \Delta t\|\mathbf{a}\|_{\mathrm{L}^{\infty}(\Omega)}\left|\left(\nabla u_{h}^{n}, u_{h}^{n+1}-u_{h}^{n}\right)\right|+\Delta t\left\|a_{0}^{n}\right\|_{\mathrm{L}^{\infty}(\Omega)}\left|\left(u_{h}^{n}, u_{h}^{n+1}-u_{h}^{n}\right)\right| \\
&\leq \Delta t\|\mathbf{a}\|_{\mathrm{L}^{\infty}(\Omega)}\left\|\nabla u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}+ \\
&\Delta t\left\|a_{0}^{n}\right\|_{\mathrm{L}^{\infty}(\Omega)}\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}
\end{aligned}
$$

Using the inverse inequality (4.52), we obtain

$$
\begin{aligned}
\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}^{2} & \leq \Delta t\left(C_{I} h^{-1}\|\mathbf{a}\|_{\mathrm{L}^{\infty}(\Omega)}+\right.\\
&\left.\left\|a_{0}^{n}\right\|_{\mathrm{L}^{\infty}(\Omega)}\right)\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}
\end{aligned}
$$

and then

$$
\left\|u_{h}^{n+1}-u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)} \leq \Delta t\left(C_{I} h^{-1}\|\mathbf{a}\|_{\mathrm{L}^{\infty}(\Omega)}+\left\|a_{0}^{n}\right\|_{\mathrm{L}^{\infty}(\Omega)}\right)\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}
$$

Using such results to find an upper bound for the term in (15.32), we have

The integral on $\partial \Omega \backslash \partial \Omega^{\text {in }}$ is positive because of the hypotheses on the boundary conditions; hence, if

$$
\Delta t \leq \frac{2 \mu_{0}}{\left(C_{I} h^{-1}\|\mathbf{a}\|_{\mathrm{L}^{\infty}(\Omega)}+\left\|a_{0}^{n}\right\|_{L^{\infty}(\Omega)}\right)^{2}}
$$

we have $\left\|u_{h}^{n+1}\right\|_{\mathrm{L}^{2}(\Omega)} \leq\left\|u_{h}^{n}\right\|_{\mathrm{L}^{2}(\Omega)}$, that is the scheme is strongly stable. Note that the stability condition (15.33) is of parabolic type, similar to the one found in (14.31) for the case of finite difference discretizations.

Remark 15.3. In the case where a is constant and $a_{0}=0$ we have that $\mu_{0}=0$, and the stability condition (15.33) can never be satisfied by a positive $\Delta t$. Thus, the result in (15.33) does not contradict the one we have previously found for the forward Euler scheme. 

\subsection{Discontinuous finite elements}

An alternative approach to the one adopted so far is based on the use of discontinuous finite elements, yielding discontinuous Galerkin method (DG in short) similar to the one already addressed in Chapter 12 and in Sect. $13.9$ for $2 n d$ order problems. This choice is motivated by the fact that, as we previously observed, the solutions of (even linear) hyperbolic problems can be discontinuous.

For a given mesh $\mathscr{T}_{h}$ of $\Omega$, the space of discontinuous finite elements is

$$
W_{h}=Y_{h}^{r}=\left\{v_{h} \in \mathrm{L}^{2}(\Omega) \mid v_{h \mid K} \in \mathbb{P}_{r}, \forall K \in \mathscr{T}_{h}\right\}
$$

that is the space of piecewise polynomial functions of degree less than or equal to $r$, with $r \geq 0$, which are not necessarily continuous across the finite element interfaces.

\subsubsection{The one-dimensional upwind DG method}

In the case of the one-dimensional problem (14.3) in its simplest upwind version, the DG finite element method takes the following form: $\forall t>0$, find a function $u_{h}=$ $u_{h}(t) \in W_{h}$ such that

$$
\begin{aligned}
&\int_{\alpha}^{\beta} \frac{\partial u_{h}(t)}{\partial t} v_{h} d x \\
&+\sum_{i=0}^{m-1}\left[\begin{array}{c}
\left.\int_{x_{i}}^{x_{i+1}}\left(a \frac{\partial u_{h}(t)}{\partial x}+a_{0} u_{h}(t)\right) v_{h} d x+a\left(x_{i}\right)\left(u_{h}^{+}(t)-U_{h}^{-}(t)\right)\left(x_{i}\right) v_{h}^{+}\left(x_{i}\right)\right] \\
=\int_{\alpha} f(t) v_{h} d x \quad \forall v_{h} \in W_{h}
\end{array}\right]
\end{aligned}
$$

where we have supposed that $a(x)$ is a continuous function. We have set, for each $t>0$

$$
U_{h}^{-}(t)\left(x_{i}\right)= \begin{cases}u_{h}^{-}(t)\left(x_{i}\right), & i=1, \ldots, m-1 \\ \varphi_{h}(t)\left(x_{0}\right)\end{cases}
$$

where $\left\{x_{i}, i=0, \cdots, m\right\}$ are the nodes, $x_{0}=\alpha, x_{m}=\beta, h$ is the maximal distance between two consecutive nodes, $v_{h}^{+}\left(x_{i}\right)$ denotes the right limit of $v_{h}$ at $x_{i}, v_{h}^{-}\left(x_{i}\right)$ the left one. For simplicity of notation, the dependence of $u_{h}$ and $f$ on $t$ will often be understood when this does not yield to ambiguities.

We now derive a stability estimate for the solution $u_{h}$ of (15.35), supposing, for simplicity, that the forcing term $f$ is identically null. Having then chosen $v_{h}=u_{h}$ in (15.35), we have (setting $\Omega=(\alpha, \beta)$ )

$$
\begin{aligned}
&\frac{1}{2} \frac{d}{d t}\left\|u_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+ \\
&\sum_{i=0}^{m-1}\left[\int_{x_{i}}^{x_{i+1}}\left(\frac{a}{2} \frac{\partial}{\partial x}\left(u_{h}\right)^{2}+a_{0} u_{h}^{2}\right) d x+a\left(x_{i}\right)\left(u_{h}^{+}-U_{h}^{-}\right)\left(x_{i}\right) u_{h}^{+}\left(x_{i}\right)\right]=0 .
\end{aligned}
$$

Now, integrating the convective term by parts, we have

$$
\begin{aligned}
&\frac{1}{2} \frac{d}{d t}\left\|u_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\sum_{i=0}^{m-1} \int_{x_{i}}^{x_{i+1}}\left(a_{0}-\frac{\partial}{\partial x}\left(\frac{a}{2}\right)\right) u_{h}^{2} d x+ \\
&\sum_{i=0}^{m-1}\left[\frac{a}{2}\left(x_{i+1}\right)\left(u_{h}^{-}\left(x_{i+1}\right)\right)^{2}+\frac{a}{2}\left(x_{i}\right)\left(u_{h}^{+}\left(x_{i}\right)\right)^{2}-a\left(x_{i}\right) U_{h}^{-}\left(x_{i}\right) u_{h}^{+}\left(x_{i}\right)\right]=0
\end{aligned}
$$

Isolating the contribution associated to node $x_{0}$ and exploiting definition (15.36), we can rewrite the second sum in the previous equation as

$$
\begin{aligned}
& \sum_{i=0}^{m-1}\left[\frac{a}{2}\left(x_{i+1}\right)\left(u_{h}^{-}\left(x_{i+1}\right)\right)^{2}+\frac{a}{2}\left(x_{i}\right)\left(u_{h}^{+}\left(x_{i}\right)\right)^{2}-a\left(x_{i}\right) U_{h}^{-}\left(x_{i}\right) u_{h}^{+}\left(x_{i}\right)\right] \\
=& \frac{a}{2}\left(x_{0}\right)\left(u_{h}^{+}\left(x_{0}\right)\right)^{2}-a\left(x_{0}\right) \varphi_{h}\left(x_{0}\right) u_{h}^{+}\left(x_{0}\right)+\frac{a}{2}\left(x_{m}\right)\left(u_{h}^{-}\left(x_{m}\right)\right)^{2}+\\
& \sum_{i=1}^{m-1}\left[\frac{a}{2}\left(x_{i}\right)\left(\left(u_{h}^{-}\left(x_{i}\right)\right)^{2}+\left(u_{h}^{+}\left(x_{i}\right)\right)^{2}\right)-a\left(x_{i}\right) u_{h}^{-}\left(x_{i}\right) u_{h}^{+}\left(x_{i}\right)\right] \\
=& \frac{a}{2}\left(x_{0}\right)\left(u_{h}^{+}\left(x_{0}\right)\right)^{2}-a\left(x_{0}\right) \varphi_{h}\left(x_{0}\right) u_{h}^{+}(\alpha)+\\
& \frac{a}{2}\left(x_{m}\right)\left(u_{h}^{-}\left(x_{m}\right)\right)^{2}+\sum_{i=1}^{m-1} \frac{a}{2}\left(x_{i}\right)\left[u_{h}\left(x_{i}\right)\right]^{2}
\end{aligned}
$$

having denoted by $\left[u_{h}\left(x_{i}\right)\right]=u_{h}^{+}\left(x_{i}\right)-u_{h}^{-}\left(x_{i}\right)$ the jump of function $u_{h}$ at node $x_{i}$. We now suppose, analogously to the multi-dimensional case (see $(15.26))$, that

$$
\exists \gamma \geq 0 \text { suchthat } a_{0}-\frac{\partial}{\partial x}\left(\frac{a}{2}\right) \geq \gamma
$$

Returning to (15.37) and using the relation (15.38) and the Cauchy-Schwarz and Young inequalities, we have

$$
\begin{aligned}
&\frac{1}{2} \frac{d}{d t}\left\|u_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\gamma\left\|u_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\sum_{i=1}^{m-1} \frac{a}{2}\left(x_{i}\right)\left[u_{h}\left(x_{i}\right)\right]^{2}+\frac{a}{2}\left(x_{0}\right)\left(u_{h}^{+}\left(x_{0}\right)\right)^{2}+ \\
&\frac{a}{2}\left(x_{m}\right)\left(u_{h}^{-}\left(x_{m}\right)\right)^{2}=a\left(x_{0}\right) \varphi_{h}\left(x_{0}\right) u_{h}^{+}\left(x_{0}\right) \leq \frac{a}{2}\left(x_{0}\right) \varphi_{h}^{2}\left(x_{0}\right)+\frac{a}{2}\left(x_{0}\right)\left(u_{h}^{+}\left(x_{0}\right)\right)^{2}
\end{aligned}
$$

that is, integrating with respect to time as well, $\forall t>0$,

$$
\begin{gathered}
\left\|u_{h}(t)\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+2 \gamma \int_{0}^{t}\left\|u_{h}(t)\right\|_{\mathrm{L}^{2}(\Omega)}^{2} d t+\sum_{i=1}^{m-1} a\left(x_{i}\right) \int_{0}^{t}\left[u_{h}\left(x_{i}, t\right)\right]^{2} d t+ \\
a\left(x_{m}\right)\left(u_{h}^{-}\left(x_{m}\right)\right)^{2} \leq\left\|u_{0, h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+a\left(x_{0}\right) \int_{0}^{t} \varphi_{h}^{2}\left(x_{0}, t\right) d t
\end{gathered}
$$

Such estimate represents the desired stability result.

Note that, in case the forcing term is no longer null, we can replicate the previous analysis by suitably using the Gronwall Lemma $2.2$ to handle the contribution of $f$. This would lead to an estimate similar to $(15.40)$, however this time the right-hand side of the inequality would become

$$
e^{t}\left(\left\|u_{0, h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+a\left(x_{0}\right) \int_{0}^{t} \varphi_{h}^{2}\left(x_{0}, t\right) d t+\int_{0}^{t}(f(\tau))^{2} d \tau\right)
$$

In the case where the constant $\gamma$ in inequality (15.39) is strictly positive, we could avoid using the Gronwall lemma, and attain an estimate such as $(15.40)$ where in the first term $2 \gamma$ is replaced by $\gamma$, while the second term takes the form $(15.41)$ without the exponential $e^{t}$.

Because of the discontinuity of test functions, (15.35) can be rewritten in an equivalent way as follows, $\forall i=0, \ldots, m-1$,

$$
\begin{aligned}
&\int_{x_{i}}^{x_{i+1}}\left(\frac{\partial u_{h}}{\partial t}+a \frac{\partial u_{h}}{\partial x}+a_{0} u_{h}\right) v_{h} d x+a\left(u_{h}^{+}-U_{h}^{-}\right)\left(x_{i}\right) v_{h}^{+}\left(x_{i}\right) \\
&\quad=\int_{x_{i}} f v_{h} d x \quad \forall v_{h} \in \mathbb{P}_{r}\left(I_{i}\right)
\end{aligned}
$$

with $I_{i}=\left[x_{i}, x_{i+1}\right]$. In other terms, the approximation via discontinuous finite elements yields to element-wise "independent" relations; the only term connecting an element and its neighbours is the jump term $\left(u_{h}^{+}-U_{h}^{-}\right)$that can also be interpreted as the attribution of the boundary datum on the inflow boundary of the element under exam.

We then have a set of small problems to be solved in each element, precisely $r+1$ equations for each interval $\left[x_{i}, x_{i+1}\right]$. Let us write them in compact form as

$$
M_{h} \dot{\mathbf{u}}_{h}(t)+L_{h} \mathbf{u}_{h}(t)=\mathbf{f}_{h}(t) \quad \forall t>0, \quad \mathbf{u}_{h}(0)=\mathbf{u}_{0, h}
$$

$M_{h}$ being the mass matrix, $L_{h}$ the matrix associated to the bilinear form and to the jump relation, $\mathbf{f}_{h}$ the source term:

$$
\begin{aligned}
\left(M_{h}\right)_{p q}=\int_{x_{i}}^{x_{i+1}} \varphi_{p} \varphi_{q} d x, \quad\left(L_{h}\right)_{p q}=\int_{x_{i}}^{x_{i+1}}\left(a \varphi_{q, x}+a_{0} \varphi_{q}\right) \varphi_{p} d x+\left(a \varphi_{q} \varphi_{p}\right)\left(x_{i}\right) \\
&\left(\mathbf{f}_{h}\right)_{p}=\int_{x_{i}}^{x_{i+1}} f \varphi_{p} d x+a U_{h}^{-}\left(x_{i}\right) \varphi_{p}\left(x_{i}\right), \quad p, q=0, \ldots, r
\end{aligned}
$$

We have denoted by $\left\{\varphi_{q}, q=0, \ldots, r\right\}$ a basis for $\mathbb{P}_{r}\left(\left[x_{i}, x_{i+1}\right]\right)$ and by $\mathbf{u}_{h}(t)$ the coefficients of $\left.u_{h}(x, t)\right|_{\left[x_{i}, x_{i+1}\right]}$ in the basis $\left\{\varphi_{q}\right\}$. If we take the Lagrange basis we will have, for instance, the functions reported in Fig. $15.7$ (for $r=0, r=1$ and $r=2$ ) and the values of $\left\{\mathbf{u}_{h}(t)\right\}$ are the ones taken by $u_{h}(t)$ at nodes $\left(x_{i+1 / 2}\right.$ for $r=0, x_{i}$ and $x_{i+1}$ for 

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-036.jpg?height=149&width=715&top_left_y=116&top_left_x=99)

Fig. 15.7. The Lagrange bases for $r=0, r=1$ and $r=2$

$r=1, x_{i}, x_{i+1 / 2}$ and $x_{i+1}$ for $r=2$ ). Note that all previous functions are identically null outside the interval $\left[x_{i}, x_{i+1}\right]$. Moreover, in the case of discontinuous finite elements it is perfectly acceptable to use polynomials of degree $r=0$, in which case the transport term $a \frac{\partial u_{h}}{\partial x}$ will provide a null contribution on each element.

With the aim of diagonalizing the mass matrix, it can be interesting to use as a basis for $\mathbb{P}_{r}\left(\left[x_{i}, x_{i+1}\right]\right)$ the Legendre polynomials $\varphi_{q}(x)=L_{q}\left(2\left(x-x_{i}\right) / h_{i}\right), h_{i}=x_{i+1}-x_{i}$. The family $\left\{L_{q}, q=0,1, \ldots\right\}$ are the orthogonal Legendre polynomials defined over the interval $[-1,1]$, that we have introduced in Sect. 10.2.2. Indeed, in such a way we obtain $\left(M_{h}\right)_{p q}=\frac{h_{i}}{2 p+1} \delta_{p q}, p, q=0, \ldots r$. Obviously, in this case the unknown values $\left\{\mathbf{u}_{h}(t)\right\}$ cannot be interpreted as nodal values of $u_{h}(t)$, but rather as the Legendre coefficients of the expansion of $u_{h}(t)$ in the new basis.

The diagonalization of the mass matrix turns out to be particularly interesting when we use explicit time advancing schemes (such as, e.g., second- and third-order RungeKutta schemes, introduced in Chapter 16). In this case, indeed, we will have a fully explicit problem to solve on each small interval.

For illustrative purposes, we present below some numerical results obtained for problem

$$
\begin{cases}\frac{\partial u}{\partial t}+\frac{\partial u}{\partial x}=0, & x \in(-5,5), t>0 \\ u(-5, t)=0, & t>0\end{cases}
$$

using the initial condition

$$
u(x, 0)= \begin{cases}\sin (\pi x), & x \in(-2,2) \\ 0 & \text { otherwise }\end{cases}
$$

The problem has been discretized using linear finite elements in space, both continuous and discontinuous. For the temporal discretization, we have used the backward Euler scheme in both cases. We have chosen $h=0.25$ and a time step $\Delta t=h$; for such value of $h$ the phase number associated to the sinusoidal wave is $\phi_{k}=\pi / 2$.

In Fig. $15.8$ we report the numerical solution at time $t=1$ together with the corresponding exact solution. The scheme has strong numerical diffusion, but also small oscillations towards the end in the case of continuous elements. Furthermore, we can observe that the numerical solution obtained using discontinuous finite elements, although being discontinuous, no longer features an oscillatory behaviour towards the end. 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-037.jpg?height=266&width=704&top_left_y=128&top_left_x=108)

Fig. 15.8. Solution at time $t=1$ of problem (15.44) with $\phi_{k}=\pi / 2, h=0.25$, obtained using continuous (left) and discontinuous (right) linear finite elements and backward Euler time discretization

Let us now consider the following problem

$$
\begin{cases}\frac{\partial u}{\partial t}+\frac{\partial u}{\partial x}=0, & x \in(0,1), t>0 \\ u(0, t)=1, & t>0 \\ u(x, 0)=0, & x \in[0,1]\end{cases}
$$

which represents the transport of a discontinuity entering the domain. We have considered continuous linear finite elements, with both strong and weak treatment of the boundary conditions, as well as discontinuous linear finite elements. This time, as well, we have used the backward Euler method for the temporal discretization. The grid-size is $h=0.025$ and the time step is $\Delta t=h$.

The results at time $t=0.5$ are represented in Fig. 15.9. We can note how the Dirichlet datum is well represented also by schemes with weak boundary treatment. To this end, for the case of continuous finite elements with weak boundary treatment, we have computed the behaviour of $\left|u_{h}(0)-u(0)\right|$ for $t=0.1$ for several values of $h$, $\Delta t$ being constant. We can note a linear convergence to zero with respect to $h$.

\subsubsection{The multi-dimensional case}

Let us now consider the multi-dimensional case $(15.22)$. Let $W_{h}$ be the space of discontinuous piecewise polynomials of degree $r$ on each element $K \in \mathscr{T}_{h}$, introduced in (15.34). The discontinuous Galerkin (DG) finite element semi-discretization of problem (15.22) becomes: for each $t>0$ find $u_{h}(t) \in W_{h}$ such that

$$
\begin{aligned}
&\int_{\Omega} \frac{\partial u_{h}(t)}{\partial t} v_{h} d \Omega+\sum_{K \in \mathscr{T}_{h}}\left[a_{K}\left(u_{h}(t), v_{h}\right)-\int_{\partial K^{i n}} \mathbf{a} \cdot \mathbf{n}_{K}\left[u_{h}(t)\right] v_{h}^{+} d \gamma\right] \\
&=\int_{\Omega} f(t) v_{h} d \Omega \quad \forall v_{h} \in W_{h}
\end{aligned}
$$


![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-038.jpg?height=530&width=666&top_left_y=126&top_left_x=122)

Fig. 15.9. Solution to problem (15.46) for $t=0.5$ with $h=0.025$ obtained using continuous linear finite elements and strong (top left) and weak (top right) treatment of the boundary Dirichlet condition, while discontinuous elements in space have been used (bottom left). Finally, we show (bottom left) the behavior of $\left|u_{h}(0)-u(0)\right|$ as a function of $h$ for $t=0.1$, in weak treatment of the Dirichlet condition

with $u_{h}(0)=u_{0, h}$, where $\mathbf{n}_{K}$ denotes the outward unit normal vector on $\partial K$, and

$$
\partial K^{i n}=\left\{\mathbf{x} \in \partial K: \mathbf{a}(\mathbf{x}) \cdot \mathbf{n}_{K}(\mathbf{x})<0\right\}
$$

The bilinear form $a_{K}$ is defined in the following way

$$
a_{K}(u, v)=\int_{K}\left(\mathbf{a} \cdot \nabla u v+a_{0} u v\right) d \mathbf{x}
$$

while

$$
\left[u_{h}(\mathbf{x})\right]= \begin{cases}u_{h}^{+}(\mathbf{x})-u_{h}^{-}(\mathbf{x}), & \mathbf{x} \notin \partial \Omega^{i n} \\ u_{h}^{+}(\mathbf{x})-\varphi_{h}(\mathbf{x}), & \mathbf{x} \in \partial \Omega^{i n}\end{cases}
$$

$\partial \Omega^{i n}$ being the inflow boundary (15.23) and with

$$
u_{h}^{\pm}(\mathbf{x})=\lim _{s \rightarrow 0^{\pm}} u_{h}(\mathbf{x}+s \mathbf{a}), \quad \mathbf{x} \in \partial K
$$

For each $t>0$, the stability estimate obtained for problem $(15.47)$ is (thanks to the hypothesis (15.26))

$$
\begin{aligned}
\left\|u_{h}(t)\right\|_{\mathrm{L}^{2}(\Omega)}^{2} &+\int_{0}^{t}\left(\mu_{0}\left\|u_{h}(\tau)\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\sum_{K \in \mathscr{T}_{h}} \int_{\partial K^{i n}}\left|\mathbf{a} \cdot \mathbf{n}_{K}\right|\left[u_{h}(\tau)\right]^{2}\right) d \tau \\
& \leq C\left[\left\|u_{0, h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\int_{0}^{t}\left(\|f(\tau)\|_{\mathrm{L}^{2}(\Omega)}^{2}+\left|\varphi_{h}\right|_{\mathbf{a}, \partial \Omega^{i n}}^{2}\right) d \tau\right],
\end{aligned}
$$

having introduced, for each subset $\Gamma$ of $\partial \Omega$ of positive measure, the seminorm

$$
|v|_{\mathbf{a}, \Gamma}=\left(\int_{\Gamma}|\mathbf{a} \cdot \mathbf{n}| v^{2} d \gamma\right)^{1 / 2}
$$

Supposing for simplicity that $f=0, \varphi=0$, and that $u_{0} \in H^{r+1}(\Omega)$, we can prove the following a priori error estimate

$$
\begin{gathered}
\max _{t \in[0, T]}\left\|u(t)-u_{h}(t)\right\|_{L^{2}(\Omega)}+\left(\int_{0}^{T} \sum_{K \in \mathscr{T}_{h}} \int_{\partial K^{i n}}\left|\mathbf{a} \cdot \mathbf{n}_{K}\right|\left[u(t)-u_{h}(t)\right]^{2} d t\right)^{\frac{1}{2}} \\
\leq C h^{r+1 / 2}\left\|u_{0}\right\|_{\mathrm{H}^{r+1}(\Omega)}
\end{gathered}
$$

For the proofs, we refer to [QV94, Chap. 14], [Joh87], and to the references cited therein.

\subsubsection{DG method with jump stabilization}

Other formulations are possible, based on different forms of stabilization. Let us consider a diffusion and reaction problem such as (15.22) but written in conservation form

$$
\frac{\partial u}{\partial t}+\operatorname{div}(\mathbf{a} u)+a_{0} u=f, \quad \mathbf{x} \in \Omega, t>0
$$

Having now set

$$
a_{K}\left(u_{h}, v_{h}\right)=\int_{K}\left(-u_{h}\left(\mathbf{a} \cdot \nabla v_{h}\right)+a_{0} u_{h} v_{h}\right) d \mathbf{x}
$$

we consider the following approximation based on the DG method (see Sects. $12.1$ and 13.9): for each $t>0$, find $u_{h}(t) \in W_{h}$ such that,

$$
\begin{aligned}
&\int_{\Omega} \frac{\partial u_{h}(t)}{\partial t} v_{h} d \Omega+\sum_{K \in \mathscr{T}_{h}} a_{K}\left(u_{h}(t), v_{h}\right)+\sum_{e \not \subset \partial \Omega^{i n}} \int_{e}\left\{\left\{\mathbf{a} u_{h}(t)\right\}\left[v_{h}\right] d \gamma\right. \\
&+\sum_{e \not \subset \partial \Omega} \int_{e} c_{e}\left[u_{h}(t)\right]\left[v_{h}\right] d \gamma \\
&=\int_{\Omega} f(t) v_{h} d \Omega-\sum_{e \subset \partial \Omega^{i n}} \int(\mathbf{a} \cdot \mathbf{n}) \varphi_{h}(t) v_{h} d \gamma \quad \forall v_{h} \in W_{h}
\end{aligned}
$$

The notations are the following: we denote by $e$ any side of the grid $\mathscr{T}_{h}$ shared by two triangles, say $K_{1}$ and $K_{2}$. For each scalar function $\psi$, piecewise regular on the mesh, with $\psi^{i}=\left.\psi\right|_{K_{i}}$, we have defined its jump on $e$ as follows:

$$
[\psi]=\psi^{1} \mathbf{n}_{1}+\psi^{2} \mathbf{n}_{2}
$$

$\mathbf{n}_{i}$ being the outward unit normal to element $K_{i}$. Instead, if $\sigma$ is a vector function, then its average on $e$ is defined as

$$
\{\sigma\}\}=\frac{1}{2}\left(\sigma^{1}+\sigma^{2}\right) .
$$

Note that the jump $[\psi]$ through $e$ of a scalar function $\psi$ is a vector parallel to the normal to $e$.

These definitions do not depend on the ordering of the elements.

If $e$ is a side belonging to the boundary $\partial \Omega$, then

$$
[\psi]=\psi \mathbf{n}, \quad \begin{cases}{\|} \boldsymbol{\sigma}\}=\boldsymbol{\sigma}\end{cases}
$$

Concerning $c_{e}$, this is a non-negative function which will typically be chosen to be constant on each side. Choosing, for instance, $c_{e}=|\mathbf{a} \cdot \mathbf{n}| / 2$ on each internal side, $c_{e}=-\mathbf{a} \cdot \mathbf{n} / 2$ on $\partial \Omega^{\text {in }}, c_{e}=\mathbf{a} \cdot \mathbf{n} / 2$ on $\partial \Omega^{\text {out }}$, the formulation in $(15.50)$ is reduced to the standard upwind formulation

$$
\begin{aligned}
&\int_{\Omega} \frac{\partial u_{h}(t)}{\partial t} v_{h} d \Omega+\sum_{K \in \mathscr{T} h} a_{K}\left(u_{h}(t), v_{h}\right)+\sum_{e \not \subset \partial \Omega^{i n}} \int_{e}\left\{\mathfrak{a} u_{h}(t)\right\}_{\mathbf{a}}\left[v_{h}\right] d \gamma \\
&=\int_{\Omega} f(t) v_{h} d \Omega-\sum_{e \subset \partial \Omega^{i n}} \int_{e}(\mathbf{a} \cdot \mathbf{n}) \varphi_{h}(t) v_{h} d \gamma \quad \forall v_{h} \in W_{h}
\end{aligned}
$$

Here $\left\{\mathbf{a} u_{h}\right\}_{\mathbf{a}}$ denotes the upwind value of $\mathbf{a} u_{h}$, that coincides with $\mathbf{a} u_{h}^{1}$ if $\mathbf{a} \cdot \mathbf{n}_{1}>0$, with a $u_{h}^{2}$ if $\mathbf{a} \cdot \mathbf{n}_{1}<0$, and finally with $\left\{\mathbf{a} u_{h}\right\}$ if $\mathbf{a} \cdot \mathbf{n}_{1}=0$ (as in definition (13.91)). Finally, if $\mathbf{a}$ is a constant (or divergence-free) vector, then $\operatorname{div}\left(\mathbf{a} u_{h}\right)=\mathbf{a} \cdot \nabla u_{h}$ and (15.51) coincides with (15.47). Formulation (15.50) is called discontinuous Galerkin method with jump stabilization. The latter is stable if $c_{e} \geq \theta_{0}\left|\mathbf{a} \cdot \mathbf{n}_{e}\right|$ (for a suitable $\theta_{0}>0$ ) for each internal side $e$, and also convergent with optimal order. Indeed, in the case of the stationary problem it can be proven that

$$
\left\|u-u_{h}\right\|_{\mathrm{L}^{2}(\Omega)}^{2}+\sum_{e \in \mathscr{T}_{h}}\left\|\sqrt{c_{e}}\left[u-u_{h}\right]\right\|_{\mathrm{L}^{2}(e)}^{2} \leq C h^{2 r+1}\|u\|_{\mathrm{H}^{r+1}(\Omega)}^{2}
$$

For the proof and for other formulations with jump stabilization, including the case of advection-diffusion equations, we refer the reader to [BMS04].

\subsection{Approximation using spectral methods}

In this section we will briefly discuss the approximation of hyperbolic problems with spectral methods. For simplicity, we will limit our discussion to one-dimensional problems. We will first treat the G-NI approximation in a single interval, then the SEM approximation corresponding to a decomposition in sub-intervals where we use discontinuous polynomials when we move from an interval to its neighbors. This provides a generalization of discontinuous finite elements, in the case where we consider polynomials of "high" degree on each element, and the integrals on each element are approximated using the GLL integration formula (10.18).

\subsubsection{The G-NI method in a single interval}

Let us consider the first-order hyperbolic transport-reaction problem (14.3) and let us suppose that $(\alpha, \beta)=(-1,1)$. Then we approximate in space by a spectral collocation method, with strong imposition of the boundary conditions. Having denoted by $\left\{x_{0}=\right.$ $\left.-1, x_{1}, \ldots, x_{N}=1\right\}$ the GLL nodes introduced in Sect. 10.2.3, the semi-discretized problem is:

for each $t>0$, find $u_{N}(t) \in \mathbb{Q}_{N}$ (the space of polynomials $\left.(10.1)\right)$ such that

$$
\begin{cases}\left(\frac{\partial u_{N}}{\partial t}+a \frac{\partial u_{N}}{\partial x}+a_{0} u_{N}\right)\left(x_{j}, t\right)=f\left(x_{j}, t\right), & j=1, \ldots, N \\ u_{N}(-1, t)=\varphi(t) \\ u_{N}\left(x_{j}, 0\right)=u_{0}\left(x_{j}\right), & j=0, \ldots, N\end{cases}
$$

Suitably using the discrete GLL scalar product defined in $(10.25)$, the $\mathrm{G}$-NI approximation of problem $(15.52)$ becomes: for each $t>0$, find $u_{N}(t) \in \mathbb{Q}_{N}$ such that

$$
\left\{\begin{array}{l}
\left(\frac{\partial u_{N}(t)}{\partial t}, v_{N}\right)_{N}+\left(a \frac{\partial u_{N}(t)}{\partial x}, v_{N}\right)_{N}+\left(a_{0} u_{N}(t), v_{N}\right)_{N}=\left(f(t), v_{N}\right)_{N} \quad \forall v_{N} \in \mathbb{Q}_{N}^{-} \\
u_{N}(-1, t)=\varphi(t) \\
u_{N}(x, 0)=u_{0, N}
\end{array}\right.
$$

where $u_{0, N} \in \mathbb{Q}_{N}$ is a suitable approximation of $u_{0}$, and having set $\mathbb{Q}_{N}^{-}=\left\{v_{N} \in \mathbb{Q}_{N}:\right.$ $\left.v_{N}(-1)=0\right\}$. At the inflow, the solution $u_{N}$ satisfies the imposed condition at each time $t>0$, while test functions vanish.

In fact, the solutions of problems (15.52) and (15.53) coincide if $u_{0, N}$ in (15.53) is chosen as the interpolated $\Pi_{N}^{G L L} u_{0}$. To prove this, it is sufficient to choose in (15.53) $v_{N}$ coinciding with the characteristic polynomial $\psi_{j}$ (defined in $\left.(10.12),(10.13)\right)$ associated to the GLL node $x_{j}$, for each $j=1, \ldots, N$.

Let us now derive a stability estimate for formulation (15.53) in the norm $(10.53)$ induced from the discrete scalar product (10.25). For simplicity, we choose a homogeneous inflow datum, that is $\varphi(t)=0$, for each $t$, and $a$ and $a_{0}$ constant. Having chosen, for each $t>0, v_{N}=u_{N}(t)$, we obtain

$$
\frac{1}{2} \frac{\partial}{\partial t}\left\|u_{N}(t)\right\|_{N}^{2}+\frac{a}{2} \int_{-1}^{1} \frac{\partial u_{N}^{2}(t)}{\partial x} d x+a_{0}\left\|u_{N}(t)\right\|_{N}^{2}=\left(f(t), u_{N}(t)\right)_{N}
$$

Suitably rewriting the convective term, integrating with respect to time and using the Young inequality, we have

$$
\begin{aligned}
&\left\|u_{N}(t)\right\|_{N}^{2}+a \int_{0}^{t}\left(u_{N}(1, \tau)\right)^{2} d \tau+2 a_{0} \int_{0}^{t}\left\|u_{N}(\tau)\right\|_{N}^{2} d \tau \\
&=\left\|u_{0, N}\right\|_{N}^{2}+2 \int_{0}^{t}\left(f(\tau), u_{N}(\tau)\right)_{N} d \tau \\
&\leq\left\|u_{0, N}\right\|_{N}^{2}+a_{0} \int_{0}^{t}\left\|u_{N}(\tau)\right\|_{N}^{2} d \tau+\frac{1}{a_{0}} \int_{0}^{t}\|f(\tau)\|_{N}^{2} d \tau
\end{aligned}
$$

that is

$$
\begin{gathered}
\left\|u_{N}(t)\right\|_{N}^{2}+a \int_{0}^{t}\left(u_{N}(1, \tau)\right)^{2} d \tau+a_{0} \int_{0}^{t}\left\|u_{N}(\tau)\right\|_{N}^{2} d \tau \\
\leq\left\|u_{0, N}\right\|_{N}^{2}+\frac{1}{a_{0}} \int_{0}^{t}\|f(\tau)\|_{N}^{2} d \tau
\end{gathered}
$$

The norm of the initial data can be bounded as follows

$$
\left\|u_{0, N}\right\|_{N}^{2} \leq\left\|u_{0, N}\right\|_{\mathrm{L}^{\infty}(-1,1)}^{2}\left(\sum_{i=0}^{N} \alpha_{i}\right)=2\left\|u_{0, N}\right\|_{\mathrm{L}^{\infty}(-1,1)}^{2}
$$

and a similar bound holds for $\|f(\tau)\|_{N}^{2}$ provided that $f$ is a continuous function. Hence, reverting to (15.54) and using inequality (10.54) to bound the norms of the left-hand side, we deduce

$$
\begin{gathered}
\left\|u_{N}(t)\right\|_{\mathrm{L}^{2}(-1,1)}^{2}+a \int_{0}^{t}\left(u_{N}(1, \tau)\right)^{2} d \tau+a_{0} \int_{0}^{t}\left\|u_{N}(\tau)\right\|_{\mathrm{L}^{2}(-1,1)}^{2} d \tau \\
\leq 2\left\|u_{0, N}\right\|_{\mathrm{L}^{\infty}(-1,1)}^{2}+\frac{2}{a_{0}} \int_{0}^{t}\|f(\tau)\|_{\mathrm{L}^{2}(-1,1)}^{2} d \tau .
\end{gathered}
$$

The reinterpretation of the G-NI method as a collocation method is less immediate in the case where the convective term $a$ is not constant and if we start from a conservative formulation of the differential equation in $(15.52)$, that is when the second term on the left-hand side is replaced by $\partial(a u) / \partial x$. In such case, we can show again that the G-NI approximation is equivalent to the collocation approximation where the convective term is replaced by $\partial\left(\Pi_{N}^{G L L}\left(a u_{N}\right)\right) / \partial x$, i.e. by the interpolation derivative $(10.40)$.

Also in the case of a G-NI approximation, we can resort to a weak imposition of the boundary conditions. Such approach is more flexible than the one considered above, and more suitable for the generalization to multi-dimensional problems or systems of equations. As we have seen in the previous section, the starting point for imposing boundary conditions weakly is a suitable integration by parts of the transport terms. Referring to the one-dimensional problem (15.52), we have (if $a$ is constant)

$$
\begin{aligned}
\int_{-1}^{1} a \frac{\partial u(t)}{\partial x} v d x &=-\int_{-1}^{1} a u(t) \frac{\partial v}{\partial x} d x+[a u(t) v]_{-1}^{1} \\
&=-\int_{-1}^{1} a u(t) \frac{\partial v}{\partial x} d x+a u(1, t) v(1)-a \varphi(t) v(-1)
\end{aligned}
$$

Thanks to the above identity, we can immediately formulate the G-NI approximation of problem (15.52) with a weak treatment of boundary conditions: for each $t>0$, find $u_{N}(t) \in \mathbb{Q}_{N}$ such that

$$
\begin{aligned}
&\left(\frac{\partial u_{N}(t)}{\partial t}, v_{N}\right)_{N}-\left(a u_{N}(t), \frac{\partial v_{N}}{\partial x}\right)_{N}+\left(a_{0} u_{N}(t), v_{N}\right)_{N}+ \\
&a u_{N}(1, t) v_{N}(1)=\left(f(t), v_{N}\right)_{N}+a \varphi(t) v_{N}(-1) \forall v_{N} \in \mathbb{Q}_{N}
\end{aligned}
$$

with $u_{N}(x, 0)=u_{0, N}(x)$. We note that both the solution $u_{N}$ and the test function $v_{N}$ are free at the boundary.

An equivalent formulation of $(15.55)$ is obtained by suitably counter-integrating the convective term by parts:

for each $t>0$, find $u_{N}(t) \in \mathbb{Q}_{N}$ such that

$$
\begin{aligned}
&\left(\frac{\partial u_{N}(t)}{\partial t}, v_{N}\right)_{N}+\left(a \frac{\partial u_{N}(t)}{\partial x}, v_{N}\right)_{N}+\left(a_{0} u_{N}(t), v_{N}\right)_{N}+ \\
&a\left(u_{N}(-1, t)-\varphi(t)\right) v_{N}(-1)=\left(f, v_{N}\right)_{N} \forall v_{N} \in \mathbb{Q}_{N}
\end{aligned}
$$

It is now possible to reinterpret such weak formulation as a suitable collocation method. To this end, it is sufficient to choose in $(15.56)$ the test function $v_{N}$ to be the characteristic polynomials (10.12), (10.13) associated to the GLL nodes. Considering first the internal and outflow nodes, and choosing therefore $v_{N}=\psi_{i}$, with $i=1, \ldots, N$, we have

$$
\left(\frac{\partial u_{N}}{\partial t}+a \frac{\partial u_{N}}{\partial x}+a_{0} u_{N}\right)\left(x_{i}, t\right)=f\left(x_{i}, t\right)
$$

having previously simplified the weight $\alpha_{i}$ common to all terms of the equation. On the other hand, by choosing $v_{N}=\psi_{0}$ we obtain the following relation at the inflow node

$$
\begin{aligned}
\left(\frac{\partial u_{N}}{\partial t}\right.&\left.+a \frac{\partial u_{N}}{\partial x}+a_{0} u_{N}\right)(-1, t) \\
&+\frac{1}{\alpha_{0}} a\left(u_{N}(-1, t)-\varphi(t)\right)=f(-1, t)
\end{aligned}
$$

$\alpha_{0}=2 /\left(N^{2}+N\right)$ being the GLL weight associated to node $x_{0}=-1$. From equations (15.57) and (15.58) it then follows that a reformulation in terms of collocation is possible at all the GLL nodes except for the inflow node, for which we find the relation

$$
a\left(u_{N}(-1, t)-\varphi(t)\right)=\alpha_{0}\left(f-\frac{\partial u_{N}}{\partial t}-a \frac{\partial u_{N}}{\partial x}-a_{0} u_{N}\right)(-1, t)
$$

The latter can be interpreted as the fulfillment of the boundary condition of the differential problem (15.52) up to the residue associated to the $u_{N}$ approximation. Such condition is therefore satisfied exactly only in the limit, for $N \longrightarrow \infty$ (i.e. in a natural way).

In accordance with what we noted previously, formulation (15.56) would be complicated, for instance, in case of a non-constant convective field $a$. Indeed,

$$
-\left(a u_{N}(t), \frac{\partial v_{N}}{\partial x}\right)_{N}=\left(a \frac{\partial u_{N}(t)}{\partial x}, v_{N}\right)_{N}-a u_{N}(1, t) v_{N}(1)+a \varphi(t) v_{N}(-1)
$$

would not be true as, in this case, the product $a u_{N}(t) \frac{\partial v_{N}}{\partial x}$ no longer identifies a polynomial of degree $2 N-1$, so the exactness of the numerical integration formula would not hold. It is therefore necessary to apply the interpolation operator $\Pi_{N}^{G L L}$, introduced in Sect. 10.2.3, before counter-integrating by parts, so that

$$
\begin{aligned}
-\left(a u_{N}(t), \frac{\partial v_{N}}{\partial x}\right)_{N} &=-\left(\Pi_{N}^{G L L}\left(a u_{N}(t)\right), \frac{\partial v_{N}}{\partial x}\right)_{N} \\
&=-\left(\Pi_{N}^{G L L}\left(a u_{N}(t)\right), \frac{\partial v_{N}}{\partial x}\right) \\
&=\left(\frac{\partial}{\partial x} \Pi_{N}^{G L L}\left(a u_{N}(t)\right), v_{N}\right)-\left[\left(a u_{N}(t)\right) v_{N}\right]_{-1}^{1}
\end{aligned}
$$

In this case, formulation (15.56) then becomes:

for each $t>0$, find $u_{N}(t) \in \mathbb{Q}_{N}$ such that

$$
\begin{aligned}
&\left(\frac{\partial u_{N}(t)}{\partial t}, v_{N}\right)_{N}+\left(\frac{\partial}{\partial x} \Pi_{N}^{G L L}\left(a u_{N}(t)\right), v_{N}\right)+\left(a_{0} u_{N}(t), v_{N}\right)_{N}+ \\
&a(t)\left(u_{N}(-1, t)-\varphi(t)\right) v_{N}(-1)=\left(f(t), v_{N}\right)_{N} \forall v_{N} \in \mathbb{Q}_{N}
\end{aligned}
$$

with $u_{N}(x, 0)=u_{0, N}(x)$. Also the collocation reinterpretation of formulation (15.56), represented by relations (15.57) and (15.59), will need to be modified with the introduction of the interpolation operator $\Pi_{N}^{G L L}$ (that is by replacing the exact derivative with the interpolation derivative). Precisely, we obtain

$$
\left(\frac{\partial u_{N}}{\partial t}+\frac{\partial}{\partial x} \Pi_{N}^{G L L}\left(a u_{N}(t)\right)+a_{0} u_{N}\right)\left(x_{i}, t\right)=f\left(x_{i}, t\right)
$$

for $i=1, \ldots, N$, and

$$
a(-1)\left(u_{N}(-1, t)-\varphi(t)\right)=\alpha_{0}\left(f-\frac{\partial u_{N}}{\partial t}-\frac{\partial}{\partial x} \Pi_{N}^{G L L}\left(a u_{N}(t)\right)-a_{0} u_{N}\right)(-1, t)
$$

at the inflow node $x=-1$.

\subsubsection{The DG-SEM-NI method}

As anticipated, we will introduce in this section an approximation based on a partition in sub-intervals, in each of which the G-NI method is used. Moreover, the solution will be discontinuous between an interval and its neighbors. This explains the DG (discontinuous Galerkin), SEM (spectral element method), NI (numerical integration) acronym.

Let us reconsider problem (15.52) on the generic interval $(\alpha, \beta)$. On the latter, we introduce a partition in $M$ subintervals $\Omega_{m}=\left(\bar{x}_{m-1}, \bar{x}_{m}\right)$ with $m=1, \ldots, M$. Let

$$
W_{N, M}=\left\{v \in \mathrm{L}^{2}(\alpha, \beta):\left.v\right|_{\Omega_{m}} \in \mathbb{Q}_{N}, \forall m=1, \ldots, M\right\}
$$

be the space of piecewise polynomials of degree $N(\geq 1)$ on each sub-interval. We observe that continuity is not necessarily guaranteed in correspondence of the points $\left\{\bar{x}_{i}\right\}$. Thus, we can formulate the following approximation of problem $(15.52)$ : for each $t>0$, find $u_{N, M}(t) \in W_{N, M}$ such that

$$
\begin{aligned}
&\sum_{m=1}^{M}\left[\left(\frac{\partial u_{N, M}}{\partial t}, v_{N, M}\right)_{N, \Omega_{m}}+\left(a \frac{\partial u_{N, M}}{\partial x}, v_{N, M}\right)_{N, \Omega_{m}}+\left(a_{0} u_{N, M}, v_{N, M}\right)_{N, \Omega_{m}}\right. \\
&\left.+a\left(\bar{x}_{m-1}\right)\left(u_{N, M}^{+}-U_{N, M}^{-}\right)\left(\bar{x}_{m-1}\right) v_{N, M}^{+}\left(\bar{x}_{m-1}\right)\right]=\sum_{m=1}^{M}\left(f, v_{N, M}\right)_{N, \Omega_{m}}
\end{aligned}
$$

for all $v_{N, M} \in W_{N, M}$, with

$$
U_{N, M}^{-}\left(\bar{x}_{i}\right)= \begin{cases}u_{N, M}^{-}\left(\bar{x}_{i}\right), & i=1, \ldots, M-1 \\ \varphi\left(\bar{x}_{0}\right), & \text { for } i=0,\end{cases}
$$

and where $(\cdot, \cdot)_{N, \Omega_{m}}$ denotes the approximation via the GLL formula (10.25) of the scalar product $\mathrm{L}^{2}$ restricted to the element $\Omega_{m}$. To simplify the notations we have omitted to indicate the dependence on $t$ of $u_{N, M}$ and $f$ explicitly. Given the discontinuous nature of the test functions, we can reformulate equation $(15.61)$ on each of the $M$ sub-intervals, by choosing the test function $v_{N, M}$ so that $\left.v_{N, M}\right|_{[\alpha, \beta] \backslash_{m}}=0$. Proceeding in this way, we obtain

$$
\begin{aligned}
&\left(\frac{\partial u_{N, M}}{\partial t}, v_{N, M}\right)_{N, \Omega_{m}}+\left(a \frac{\partial u_{N, M}}{\partial x}, v_{N, M}\right)_{N, \Omega_{m}}+\left(a_{0} u_{N, M}, v_{N, M}\right)_{N, \Omega_{m}} \\
&+a\left(\bar{x}_{m-1}\right)\left(u_{N, M}^{+}-U_{N, M}^{-}\right)\left(\bar{x}_{m-1}\right) v_{N, M}^{+}\left(\bar{x}_{m-1}\right)=\left(f, v_{N, M}\right)_{N, \Omega_{m}}
\end{aligned}
$$

for each $m=1, \ldots, M$. We note that, for $m=1$, the term

$$
a\left(\bar{x}_{0}\right)\left(u_{N, M}^{+}-\varphi\right)\left(\bar{x}_{0}\right) v_{N, M}^{+}\left(\bar{x}_{0}\right)
$$

can be regarded as the imposition in weak form of the inflow boundary condition. On the other hand for $m=2, \ldots, M$, the term

$$
a\left(\bar{x}_{m-1}\right)\left(u_{N, M}^{+}-U_{N, M}^{-}\right)\left(\bar{x}_{m-1}\right) v_{N, M}^{+}\left(\bar{x}_{m-1}\right)
$$

can be interpreted as a penalization term that provides a weak imposition of the continuity of the solution $u_{N, M}$ at the endpoints $\bar{x}_{i}, i=1, \ldots, M-1$.

We now want to interpret formulation (15.61) as a suitable collocation method. To this end, we introduce on each sub-interval $\Omega_{m}$, the $N+1$ GLL nodes $x_{j}^{(m)}$, with $j=$ $0, \ldots, N$, and we denote by $\alpha_{j}^{(m)}$ the corresponding weights (see (10.71)). We now choose the test function $v_{N, M}$ in (15.61) as the characteristic Lagrangian polynomial $\psi_{j}^{(m)} \in \mathbb{P}^{N}\left(\Omega_{m}\right)$ associated to node $x_{j}^{(m)}$ and extended trivially outside the domain $\Omega_{m}$ Given the presence of the jump term, we will have a non-unique rewriting for equation (15.61). We start by considering the characteristic polynomials associated to the nodes $x_{j}^{(m)}$, with $j=1, \ldots, N-1$, and $m=1, \ldots, M$. In this case we will have no contribution from the penalization term, yielding

$$
\left[\frac{\partial u_{N, M}}{\partial t}+a \frac{\partial u_{N, M}}{\partial x}+a_{0} u_{N, M}\right]\left(x_{j}^{(m)}\right)=f\left(x_{j}^{(m)}\right)
$$

For this choice of nodes we thus find exactly the collocation of the differential problem (15.52).

Instead, in the case where the function $\psi_{j}^{(m)}$ is associated to a node of the partition $\left\{\bar{x}_{i}\right\}$, that is $j=0$, with $m=1, \ldots, M$ we have

$$
\begin{array}{r}
\alpha_{0}^{(m)}\left[\frac{\partial u_{N, M}}{\partial t}+a \frac{\partial u_{N, M}}{\partial x}+a_{0} u_{N, M}\right]\left(x_{0}^{(m)}\right) \\
+a\left(x_{0}^{(m)}\right)\left(u_{N, M}^{+}-U_{N, M}^{-}\right)\left(x_{0}^{(m)}\right)=\alpha_{0}^{(m)} f\left(x_{0}^{(m)}\right)
\end{array}
$$

recalling that $U_{N, M}^{-}\left(x_{0}^{(1)}\right)=\varphi\left(\bar{x}_{0}\right)$. We have implicitly adopted the convention that the sub-interval $\Omega_{m}$ should not include $\bar{x}_{m}$, as the discontinuous nature of the adopted method would make us process twice each node $\bar{x}_{i}$, with $i=1, \ldots, M-1$. Equation (15.63) can be rewritten as

$$
\left[\frac{\partial u_{N, M}}{\partial t}+a \frac{\partial u_{N, M}}{\partial x}+a_{0} u_{N, M}-f\right]\left(x_{0}^{(m)}\right)=-\frac{a\left(x_{0}^{(m)}\right)}{\alpha_{0}^{(m)}}\left(u_{N, M}^{+}-U_{N, M}^{-}\right)\left(x_{0}^{(m)}\right)
$$

We observe that while the left-hand side represents the residue of the equation at node $x_{0}^{(m)}$, the right-hand side one is, up to a multiplicative factor, the residue of the weak imposition of the continuity of $u_{N, M}$ at $x_{0}^{(m)}$.

\subsection{Numerical treatment of boundary conditions for hyperbolic systems}

We have seen different strategies to impose the inflow boundary conditions for the scalar transport equation. When considering hyperbolic systems, the numerical treatment of boundary conditions requires more attention. We will explain why the point is using a linear system with constant coefficients in one dimension,

$$
\begin{cases}\frac{\partial \mathbf{u}}{\partial t}+A \frac{\partial \mathbf{u}}{\partial x}=\mathbf{0}, \quad-1<x<1, t>0 \\ \mathbf{u}(x, 0)=\mathbf{u}_{0}(x), \quad-1<x<1\end{cases}
$$

completed with suitable boundary conditions. Following [CHQZ07], we choose the case of a system made of two hyperbolic equations, and take $u$ to be the vector $(u, v)^{T}$, while

$$
A=\left[\begin{array}{cc}
-1 / 2 & -1 \\
-1 & -1 / 2
\end{array}\right]
$$

whose eigenvalues are $-3 / 2$ and $1 / 2$. We make the choice

$$
u(x, 0)=\sin (2 x)+\cos (2 x), \quad v(x, 0)=\sin (2 x)-\cos (2 x)
$$

for the initial conditions and

$$
\begin{aligned}
&u(-1, t)=\sin (-2+3 t)+\cos (-2-t)=\varphi(t) \\
&v(1, t)=\sin (2+3 t)+\cos (2-t)=\psi(t)
\end{aligned}
$$

for the boundary conditions.

Let us now consider the (right) eigenvector matrix

$$
W=\left[\begin{array}{cc}
1 / 2 & 1 / 2 \\
1 / 2 & -1 / 2
\end{array}\right]
$$

whose inverse is

$$
W^{-1}=\left[\begin{array}{cc}
1 & 1 \\
1 & -1
\end{array}\right]
$$

Exploiting the diagonalization

$$
\Lambda=W^{-1} A W=\left[\begin{array}{cc}
-3 / 2 & 0 \\
0 & 1 / 2
\end{array}\right]
$$

we can rewrite the differential equation in (15.64), in terms of the characteristic variables

$$
\mathbf{z}=W^{-1} \mathbf{u}=\left[\begin{array}{l}
u+v \\
u-v
\end{array}\right]=\left[\begin{array}{l}
z_{1} \\
z_{2}
\end{array}\right]
$$

as

$$
\frac{\partial \mathbf{z}}{\partial t}+\Lambda \frac{\partial \mathbf{z}}{\partial x}=\mathbf{0}
$$

The characteristic variable $z_{1}$ propagates towards the left at rate $3 / 2$, while $z_{2}$ propagates towards the right at rate $1 / 2$.

This suggests to assign a condition for $z_{1}$ at $x=1$ and one for $z_{2}$ at $x=-1$. The boundary values of $z_{1}$ and $z_{2}$ can be generated by using the boundary conditions for $u$ and $v$ as follows. From relation (15.66) we have

$$
\mathbf{u}=W \mathbf{z}=\left[\begin{array}{cc}
1 / 2 & 1 / 2 \\
1 / 2 & -1 / 2
\end{array}\right]\left[\begin{array}{l}
z_{1} \\
z_{2}
\end{array}\right]=\left[\begin{array}{c}
1 / 2\left(z_{1}+z_{2}\right) \\
1 / 2\left(z_{1}-z_{2}\right)
\end{array}\right]
$$

that is, exploiting the boundary values (15.65) assigned for $u$ and $v$,

$$
\frac{1}{2}\left(z_{1}+z_{2}\right)(-1, t)=\varphi(t), \quad \frac{1}{2}\left(z_{1}-z_{2}\right)(1, t)=\psi(t)
$$

The conclusion is that, in spite of the diagonal structure of system (15.67), the characteristic variables are in fact coupled by the boundary conditions (15.68).

Hence we have to face the problem of how to handle, from a numerical viewpoint, boundary conditions for systems like (15.64). Indeed, difficulties can arise even from the discretization of the corresponding scalar problem (for $a$ constant $>0$ )

$$
\begin{cases}\frac{\partial z}{\partial t}+a \frac{\partial z}{\partial x}=0, & -1<x<1, t>0 \\ z(-1, t)=\phi(t), & t>0 \\ z(x, 0)=z_{0}(x), & -1<x<1\end{cases}
$$

if we do not use an appropriate discretization scheme. We will illustrate the procedure for a spectral approximation method. As a matter of fact, a correct treatment of the boundary conditions for high-order methods is even more vital than for a finite element or finite difference method, because with spectral methods boundary errors would be propagated inwards with an infinite rate.

Having introduced the partition $x_{0}=-1<x_{1}<\ldots<x_{N-1}<x_{N}=1$ of the interval $[-1,1]$, if we decide to use, say, a finite difference scheme, we encounter problems, essentially in determining the value of $z$ at the outflow node $x_{N}$, unless we use the first order upwind scheme. As a matter of fact, higher order FD schemes such as the centered finite difference scheme would not be able to provide such an approximation unless additional nodes outside the definition interval $(-1,1)$ were introduced.

In contrast, a spectral discretization does not involve any boundary problem. For instance, the collocation scheme corresponding to problem (15.69) can be written as follows:

$\forall n \geq 0$, find $z_{N}^{n} \in \mathbb{Q}_{N}$ such that

$$
\left\{\begin{array}{l}
\frac{z_{N}^{n+1}\left(x_{i}\right)-z_{N}^{n}\left(x_{i}\right)}{\Delta t}+a \frac{\partial z_{N}^{n}}{\partial x}\left(x_{i}\right)=0, \quad i=1, \ldots, N \\
z_{N}^{n+1}\left(x_{0}\right)=\phi\left(t^{n+1}\right)
\end{array}\right.
$$

One equation is associated to each node, whether internal or on the boundary, and the outflow node is treated as any other internal node. However, when moving to system (15.64), two unknowns and two equations are associated with each internal node $x_{i}$, with $i=1, \ldots, N-1$, while at the boundary nodes $x_{0}$ and $x_{N}$ we still have two unknowns but a single equation. Thus, we will need additional conditions for these points: in general, at the endpoint $x=-1$ we will need as many conditions as the positive eigenvalues of $A$ while for $x=1$ we will need to provide as many additional conditions as the negative eigenvalues.

Let us look for a solution to this problem guided by the spectral Galerkin method. Let us suppose we apply a collocation method to system (15.64); then, we want to find $\mathbf{u}_{N}=\left(u_{N, 1}, u_{N, 2}\right)^{T} \in\left(\mathbb{Q}_{N}\right)^{2}$ such that

$$
\frac{\partial \mathbf{u}_{N}}{\partial t}\left(x_{i}\right)+A \frac{\partial \mathbf{u}_{N}}{\partial x}\left(x_{i}\right)=\mathbf{0}, \quad i=1, \ldots, N-1
$$

and with

$$
u_{N, 1}\left(x_{0}, t\right)=\varphi(t), \quad u_{N, 2}\left(x_{N}, t\right)=\psi(t) .
$$

The simplest idea to obtain the two missing equations for $u_{N, 1}$ and $u_{N, 2}$ at $x_{N}$ resp. $x_{0}$, is to exploit the vector equation $(15.70)$ together with the known vectors $\varphi(t)$ and $\psi(t)$ in (15.71). The solution computed in this way is, however, strongly unstable. To seek an alternative approach, the idea is to add to the $2(N-1)$ collocation relations (15.70) and to the "physical" boundary conditions (15.71), the equations of the outgoing characteristics at points $x_{0}$ and $x_{N}$. More in detail, the characteristic outgoing from the domain at point $x_{0}=-1$ is the one associated to the negative eigenvalue of the matrix $A$, and has equation

$$
\frac{\partial z_{1}}{\partial t}\left(x_{0}\right)-\frac{3}{2} \frac{\partial z_{1}}{\partial x}\left(x_{0}\right)=0
$$

while the one associated with the point $x_{N}=1$ is highlighted by the positive eigenvalue $1 / 2$ and is given by

$$
\frac{\partial z_{2}}{\partial t}\left(x_{N}\right)+\frac{1}{2} \frac{\partial z_{2}}{\partial x}\left(x_{N}\right)=0
$$

The choice of the outgoing characteristic is motivated by the fact that the latter carries information from the inside of the domain to the corresponding outflow point, where it makes sense to impose the differential equation.

Equations (15.72) and (15.73) allow us to have a closed system of $2 N+2$ equations in the $2 N+2$ unknowns $u_{N, 1}\left(x_{i}, t\right)=u_{N}\left(x_{i}, t\right), u_{N, 2}\left(x_{i}, t\right)=v_{N}\left(x_{i}, t\right)$, with $i=0, \ldots, N$. For completeness, we can rewrite the characteristic equations (15.72) and (15.73) in terms of the unknowns $u_{N}$ and $v_{N}$, as

$$
\frac{\partial\left(u_{N}+v_{N}\right)}{\partial t}\left(x_{0}\right)-\frac{3}{2} \frac{\partial\left(u_{N}+v_{N}\right)}{\partial x}\left(x_{0}\right)=0
$$

and

$$
\frac{\partial\left(u_{N}-v_{N}\right)}{\partial t}\left(x_{N}\right)+\frac{1}{2} \frac{\partial\left(u_{N}-v_{N}\right)}{\partial x}\left(x_{N}\right)=0
$$

respectively, or in matrix terms as

$$
\begin{aligned}
&{\left[W_{11}^{-1} W_{12}^{-1}\right]\left[\frac{\partial \mathbf{u}_{N}}{\partial t}\left(x_{0}\right)+A \frac{\partial \mathbf{u}_{N}}{\partial x}\left(x_{0}\right)\right]=0} \\
&{\left[W_{21}^{-1} \quad W_{22}^{-1}\right]\left[\frac{\partial \mathbf{u}_{N}}{\partial t}\left(x_{N}\right)+A \frac{\partial \mathbf{u}_{N}}{\partial x}\left(x_{N}\right)\right]=0}
\end{aligned}
$$

Such additional equations are called compatibility equations: they represent a linear combination of the differential equations of the problem at the boundary points with coefficients given by the components of the matrix $W^{-1}$.

Remark 15.4. Due to their global nature, spectral methods (either collocation, Galerkin, or G-NI) propagate immediately, and on the whole domain, every possible numerical perturbation introduced at the boundary. As such, spectral methods represent a good testbed for understanding how suitable numerical strategies are for the boundary treatment of hyperbolic systems.

\subsubsection{Weak treatment of boundary conditions}

Now we want to generalize the approach based on compatibility equations, and move from pointwise relations, such as (15.74), to integral relations, more suitable for numerical approximations such as, e.g., finite elements or G-NI.

Let us, once again, consider the constant coefficient system (15.64) and the notations used in Sect. 15.6. Let $A$ be a real, symmetric and non-singular matrix of order $d$, $\Lambda$ the diagonal matrix whose diagonal entries are the real eigenvalues of $A$, and $W$ the square matrix whose columns are the (right) eigenvectors of $A$. Let us suppose that $W$ is orthogonal, which guarantees that $\Lambda=W^{T} A W$. The characteristic variables, defined as $\mathbf{z}=W^{T} \mathbf{u}$, satisfy the diagonal system (15.67). We introduce the splitting $\Lambda=\operatorname{diag}\left(\Lambda^{+}, \Lambda^{-}\right)$of the eigenvalue matrix, arising by grouping the positive eigenvalues $\left(\Lambda^{+}\right)$and the negative ones $\left(\Lambda^{-}\right)$.Both sub-matrices are diagonal, $\Lambda^{+}$positive definite of order $p, \Lambda^{-}$negative definite of order $n=d-p$. Analogously, we can rewrite $\mathbf{z}$ as $\mathbf{z}=\left(\mathbf{z}^{+}, \mathbf{z}^{-}\right)^{T}$, having denoted by $\mathbf{z}^{+}\left(\mathbf{z}^{-}\right)$the characteristic variables that are constant along the characteristic lines with positive (negative) slope, in other words, lines moving rightwards (leftwards) on the $(x, t)$ reference frame. In correspondence of the right extremum $x=1, \mathbf{z}^{+}$is associated to the outgoing characteristic variables while $\mathbf{z}^{-}$to the incoming ones. Clearly, the roles are switched at the left boundary point $x=-1$.

A simple case occurs when we assign, as boundary conditions, the values of the incoming characteristics at both domain extrema, that is $p$ conditions at $x=-1$ and $n$ conditions at $x=1$. In this case, (15.67) represents a fully-fledged decoupled system. Much more frequently, however, the values of suitable linear combinations of the physical variables are assigned at both boundary points. Reading them in terms of the $z$ variables, these yield linear combinations of the characteristic variables. None of the outgoing characteristics will in principle be determined by these combinations, as the resulting values will generally be incompatible with the ones propagated inwards by the hyperbolic system. In contrast, the boundary conditions should allow to determine the incoming characteristic variables as a function of the outgoing ones and of the problem data.

For the sake of clarity, let us consider the following boundary conditions

$$
B_{L} \mathbf{u}(-1, t)=\mathbf{g}_{L}(t), \quad B_{R} \mathbf{u}(1, t)=\mathbf{g}_{R}(t), \quad t>0
$$

where $\mathbf{g}_{L}$ and $\mathbf{g}_{R}$ are assigned vectors and $B_{L}, B_{R}$ are suitable matrices. At the left extremum $x=-1$, we have $p$ incoming characteristics, and $B_{L}$ will have dimension $p \times d$. Setting $C_{L}=B_{L} W$ and using the splitting $\mathbf{z}=\left(\mathbf{z}^{+}, \mathbf{z}^{-}\right)^{T}$ introduced for $\mathbf{z}$ and the corresponding splitting $W=\left(W^{+}, W^{-}\right)^{T}$ for the eigenvector matrix, we have

$$
C_{L} \mathbf{z}(-1, t)=C_{L}^{+} \mathbf{z}^{+}(-1, t)+C_{L}^{-} \mathbf{z}^{-}(-1, t)=\mathbf{g}_{L}(t)
$$

where $C_{L}^{+}=B_{L} W^{+}$is a $p \times p$ matrix, while $C_{L}^{-}=B_{L} W^{-}$has dimension $p \times n$. We demand that $C_{L}^{+}$is non-singular. Then the incoming characteristic at $x=-1$ can be obtained by

$$
\mathbf{z}^{+}(-1, t)=S_{L} \mathbf{z}^{-}(-1, t)+\mathbf{z}_{L}(t),
$$

$S_{L}=-\left(C_{L}^{+}\right)^{-1} C_{L}^{-}$being a $p \times n$ matrix and $\mathbf{z}_{L}(t)=\left(C_{L}^{+}\right)^{-1} \mathbf{g}_{L}(t)$. In a similar way, we can assign at the right extremum $x=1$ the incoming characteristic variable as

$$
\mathbf{z}^{-}(1, t)=S_{R} \mathbf{z}^{+}(1, t)+\mathbf{z}_{R}(t)
$$

$S_{R}$ being a $n \times p$ matrix.

The matrices $S_{L}$ and $S_{R}$ are called reflection matrices.

The hyperbolic system (15.64) will thus be completed by the boundary conditions (15.75) or, equivalently, by conditions (15.76)-(15.77).

Let us see which advantages can be brought by such a choice for boundary conditions. We start from the weak formulation of problem (15.64), integrating by parts the term containing the space derivative

$$
\int_{-1}^{1} \mathbf{v}^{T} \frac{\partial \mathbf{u}}{\partial t} d x-\int_{-1}^{1}\left(\frac{\partial \mathbf{v}}{\partial x}\right)^{T} A \mathbf{u} d x+\left[\mathbf{v}^{T} A \mathbf{u}\right]_{-1}^{1}=0
$$

for each $t>0, \mathbf{v}$ being an arbitrary, differentiable test function. We want to rewrite the boundary term $\left[\mathbf{v}^{T} A \mathbf{u}\right]_{-1}^{1}$ by exploiting the boundary equations $(15.76)-(15.77)$. Introducing the characteristic variable $W^{T} \mathbf{v}=\mathbf{y}=\left(\mathbf{y}^{+}, \mathbf{y}^{-}\right)^{T}$ associated to the test function $\mathbf{v}$, we will have

$$
\mathbf{v}^{T} A \mathbf{u}=\mathbf{y}^{T} \Lambda \mathbf{z}=\left(\mathbf{y}^{+}\right)^{T} \Lambda^{+} \mathbf{z}^{+}+\left(\mathbf{y}^{-}\right)^{T} \Lambda^{-} \mathbf{z}^{-}
$$

Using relations $(15.76)-(15.77)$, it then follows that

$$
\begin{array}{ll} 
& \int_{-1}^{1} \mathbf{v}^{T} \frac{\partial \mathbf{u}}{\partial t} d x-\int_{-1}^{1}\left(\frac{\partial \mathbf{v}}{\partial x}\right)^{T} A \mathbf{u} d x \\
- & \left(\mathbf{y}^{+}\right)^{T}(-1, t) \Lambda^{+} S_{L} \mathbf{z}^{-}(-1, t)-\left(\mathbf{y}^{-}\right)^{T}(-1, t) \Lambda^{-} \mathbf{z}^{-}(-1, t) \\
+ & \left(\mathbf{y}^{+}\right)^{T}(1, t) \Lambda^{+} \mathbf{z}^{+}(1, t)+\left(\mathbf{y}^{-}\right)^{T}(1, t) \Lambda^{-} S_{R} \mathbf{z}^{+}(1, t) \\
= & \left(\mathbf{y}^{+}\right)^{T}(-1, t) \Lambda^{+} \mathbf{z}_{L}(t)-\left(\mathbf{y}^{-}\right)^{T}(1, t) \Lambda^{-} \mathbf{z}_{R}(t)
\end{array}
$$

We observe that the boundary conditions (15.76)-(15.77) are naturally incorporated in the right-hand side of the system. Moreover, integrating again by parts, it is possible to obtain an equivalent formulation to (15.78) where the boundary conditions are imposed in a weak way

$$
\begin{aligned}
& \int_{-1}^{1} \mathbf{v}^{T} \frac{\partial \mathbf{u}}{\partial t} d x+\int_{-1}^{1} \mathbf{v}^{T} A \frac{\partial \mathbf{u}}{\partial x} d x \\
+&\left(\mathbf{y}^{+}\right)^{T}(-1, t) \Lambda^{+}\left(\mathbf{z}^{+}(-1, t)-S_{L} \mathbf{z}^{-}(-1, t)\right) \\
-&\left(\mathbf{y}^{-}\right)^{T}(1, t) \Lambda^{-}\left(\mathbf{z}^{-}(1, t)-S_{R} \mathbf{z}^{+}(1, t)\right) \\
=&\left(\mathbf{y}^{+}\right)^{T}(-1, t) \Lambda^{+} \mathbf{z}_{L}(t)-\left(\mathbf{y}^{-}\right)^{T}(1, t) \Lambda^{-} \mathbf{z}_{R}(t)
\end{aligned}
$$

Finally, we recall that the following assumption, called dissipation hypothesis, is usually made on the reflection matrices $S_{L}$ and $S_{R}$

$$
\left\|S_{L}\right\|\left\|S_{R}\right\|<1
$$

The matrix norm in (15.80) must be understood as the Euclidean norm of a rectangular matrix, that is the square root of the maximum eigenvalue of $S_{L}^{T} S_{L}$ and $S_{R}^{T} S_{R}$. This assumption is sufficient to guarantee the stability of the previous scheme in the $\mathrm{L}^{2}$ norm. Formulation (15.78) (or (15.79)) is suitable for Galerkin approximations such as Galerkin finite elements, the spectral Galerkin method, the spectral method with Gaussian numerical integration in a single domain (G-NI), the spectral element version, whether continuous (SEM-NI) or discontinuous (DG-SEM-NI) spectral elements.

\subsection{Exercises}

1. Prove that the discretization with continuous linear finite elements (15.13) coincides with the finite difference one (14.22) in case the mass matrix is diagonalized using the mass lumping technique.

[Solution: use the partition of unity property (13.34) as in Sect. 13.5.]

2. Prove the stability inequalities provided in Sect. (15.4) for the semi-discretization based on finite elements.

3. Prove relation (15.13).

4. Discretize system (15.78) using the continuous spectral element method, SEM-NI, and the discontinuous one, DG-SEM-NI. 

\section{Nonlinear hyperbolic problems}

In this chapter we introduce some examples of nonlinear hyperbolic problems. We will point out some characteristic properties of such problems, most notably their ability to generate discontinuous solutions also in the case of continuous initial and boundary data. The numerical approximation of these problems is far from easy. Here we will simply limit ourselves to point out how finite difference and finite element schemes can be applied in the case of one-dimensional equations.

For a more complete discussion, we refer to [LeV07, GR96, Bre00, Tor09, Kro97, LeF02].

\subsection{Scalar equations}

Let us consider the following equation

$$
\frac{\partial u}{\partial t}+\frac{\partial}{\partial x} F(u)=0, \quad x \in \mathbb{R}, t>0
$$

where $F$ is a nonlinear function of $u$ called $f l u x$ of $u$, because on each interval $(\alpha, \beta)$ of $\mathbb{R}$, it satisfies the following relation

$$
\frac{d}{d t} \int_{\alpha}^{\beta} u(x, t) d x=F(u(t, \alpha))-F(u(t, \beta))
$$

Then, if $u=u(x, t)$ represents the density (or concentration) of a physical quantity $q$ and $F$ is its associated flux, the rate of variation of $q$ in $[\alpha, \beta]$ is determined by the net flux through the endpoints $\alpha$ and $\beta$.

For this reason, (16.1) expresses a conservation law. A typical example is Burgers' equation

$$
\frac{\partial u}{\partial t}+u \frac{\partial u}{\partial x}=0
$$

This equation was already considered in Example $1.3$, and corresponds to $(16.1)$ when the flux is $F(u)=u^{2} / 2$. Its characteristic curves are obtained by solving $x^{\prime}(t)=u$.

However, since $u$ is constant on the characteristics, we obtain $x^{\prime}(t)=$ constant, so the characteristics are straight lines. The latter are defined in the plane $(x, t)$ by the map $t \rightarrow\left(x+t u_{0}(x), t\right)$, and the solution to $(16.2)$ is implicitly defined by $u\left(x+t u_{0}(x)\right)=$ $u_{0}(x), t<t_{c}, t_{c}$ being the first instant where such characteristics intersect. For instance, if $u_{0}(x)=\left(1+x^{2}\right)^{-1}$, then $t_{c}=8 / \sqrt{27}$.

Indeed, if $u_{0}^{\prime}(x)$ is negative at some point, having set

$$
t_{c}=-\frac{1}{\min u_{0}^{\prime}(x)}
$$

for $t>t_{c}$ there can be no classical solution (i.e. of class $C^{1}$ ), as

$$
\lim _{t \rightarrow t_{c}^{-}}\left(\inf _{x \in \mathbb{R}} \frac{\partial u}{\partial x}(x, t)\right)=-\infty
$$

Let us consider Fig. 16.1: note how for $t=t_{c}$ the solution has a discontinuity.

To account for this loss of uniqueness, we introduce the notion of weak solution of a hyperbolic equation: we say that $u$ is a weak solution of $(16.1)$ if it satisfies the differential relation (16.1) at all points $x \in \mathbb{R}$ except for those where it is discontinuous. In the latter, we no longer expect (16.1) to hold (it would make no sense to differentiate a discontinuous function). Rather we require the following Rankine-Hugoniot condition to be verified

$$
F\left(u_{r}\right)-F\left(u_{l}\right)=\sigma\left(u_{r}-u_{l}\right)
$$

where $u_{r}$ and $u_{l}$ respectively denote the right and left limit of $u$ at the discontinuity point, and $\sigma$ is the speed of propagation of the discontinuity. Condition (16.3) therefore expresses the fact that the jump of the flux is proportional to the jump of the solution.

Weak solutions are not necessarily unique: among them, the physically correct one is the so-called entropic solution. As we will see at the end of this section, in the case of Burgers' equation, the entropic solution is obtained as the limit, for $\varepsilon \rightarrow 0$, of the

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-055.jpg?height=302&width=340&top_left_y=899&top_left_x=285)

Fig. 16.1. Development of the singularity at the critical time $t_{c}$ solution $u^{\varepsilon}(x, t)$ of the equation having a viscous perturbation term

$$
\frac{\partial u^{\varepsilon}}{\partial t}+\frac{\partial}{\partial x} F\left(u^{\varepsilon}\right)=\varepsilon \frac{\partial^{2} u^{\varepsilon}}{\partial x^{2}}, x \in \mathbb{R}, t>0
$$

with $u^{\varepsilon}(x, 0)=u_{0}(x)$.

In general, we can say that:

- if $F(u)$ is differentiable, a discontinuity that propagates at rate $\sigma$ given by $(16.3)$ satisfies the entropy condition if

$$
F^{\prime}\left(u_{l}\right) \geq \sigma \geq F^{\prime}\left(u_{r}\right)
$$

- if $F(u)$ is not differentiable, a discontinuity that propagates at rate $\sigma$ given by (16.3) satisfies the entropy condition if

$$
\frac{F(u)-F\left(u_{l}\right)}{u-u_{l}} \geq \sigma \geq \frac{F(u)-F\left(u_{r}\right)}{u-u_{r}}
$$

for each u between $u_{l}$ and $u_{r}$.

Example 16.1. Let us consider Burgers' equation with the following initial condition

$$
u_{0}(x)= \begin{cases}u_{l} & \text { if } x<0 \\ u_{r} & \text { if } x>0\end{cases}
$$

where $u_{r}$ and $u_{l}$ are two constants. If $u_{l}>u_{r}$, then there exists a unique weak solution (which is also entropic)

$$
u(x, t)= \begin{cases}u_{l}, & x<\sigma t \\ u_{r}, & x>\sigma t\end{cases}
$$

where $\sigma=\left(u_{l}+u_{r}\right) / 2$ is the propagation rate of the discontinuity (also called shock). In this case the characteristics "enter" the shock (see Fig. 16.2).

If $u_{l}<u_{r}$, there are infinitely many weak solutions: one still has $(16.4)$, but in this case the characteristics exit the discontinuity (see Fig. 16.3). Such solution is unstable, i.e. small perturbations on the data substantially change the solution itself. Another weak solution is

$$
u(x, t)= \begin{cases}u_{l} & \text { if } x<u_{l} t \\ \frac{x}{t} & \text { if } u_{l} t \leq x \leq u_{r} t \\ u_{r} & \text { if } x>u_{r} t\end{cases}
$$
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-056.jpg?height=194&width=512&top_left_y=1005&top_left_x=199)

Fig. 16.2. Entropic solution to Burgers' equation 

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-057.jpg?height=193&width=264&top_left_y=109&top_left_x=451)

Fig. 16.3. Non-entropic solution to Burgers' equation
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-057.jpg?height=200&width=516&top_left_y=346&top_left_x=200)

Fig. 16.4. Rarefaction wave

Such solution, describing a rarefaction wave, is entropic, in contrast to the previous one (see Fig. 16.4).

We say that a hyperbolic problem (16.1) has an entropy function if there exist a strictly convex function $\eta=\eta(u)$ and a function $\Psi=\Psi(u)$ such that

$$
\Psi^{\prime}(u)=\eta^{\prime}(u) F^{\prime}(u)
$$

where the 'prime' denotes the derivative with respect to the argument $u$. The function $\eta$ is called entropy and $\Psi$ is called entropy flux. We recall that a function $\eta$ is said to be convex if for each distinct $u$ and $w$ and for each $\theta \in(0,1)$, we have

$$
\eta(u+\theta(w-u))<(1-\theta) \eta(u)+\theta \eta(w)
$$

If $\eta$ has a continuous second derivative, this is equivalent to requiring that $\eta^{\prime \prime}>0$.

Remark 16.1. The one presented here is a "mathematical" definition of entropy. In the case where (16.1) governs a physical phenomenon, it is often possible to define a "thermodynamic entropy". The latter turns out to be an entropy according to the definition given above.

The quasi-linear form of $(16.1)$ is given by

$$
\frac{\partial u}{\partial t}+F^{\prime}(u) \frac{\partial u}{\partial x}=0
$$

If $u$ is sufficiently regular, it can easily be verified by multiplying (16.6) by $\eta^{\prime}(u)$ that $\eta$ and $\Psi$ satisfy the following conservation law

$$
\frac{\partial \eta}{\partial t}(u)+\frac{\partial \Psi}{\partial x}(u)=0
$$

For a scalar equation it is generally possible to find different pairs of functions $\eta$ and $\Psi$ that satisfy the given conditions.

The operations carried out to derive (16.7) make sense only if $u$ is regular, in particular if there are no discontinuities in the solution. However, we can find the conditions to be verified by the entropy variable at a discontinuity in the solution of (16.1) when such equation represents the limit for $\varepsilon \rightarrow 0^{+}$of the following regularized equation (called viscosity equation)

$$
\frac{\partial u}{\partial t}+\frac{\partial F}{\partial x}(u)=\varepsilon \frac{\partial^{2} u}{\partial x^{2}}
$$

This is and advection diffusion equation (with non-linear advection). Its solution is regular for each $\varepsilon>0 ;$ by performing the same manipulations used previously we can write

$$
\frac{\partial \eta}{\partial t}(u)+\frac{\partial \Psi}{\partial x}(u)=\varepsilon \eta^{\prime}(u) \frac{\partial^{2} u}{\partial x^{2}}=\varepsilon \frac{\partial}{\partial x}\left[\eta^{\prime}(u) \frac{\partial u}{\partial x}\right]-\varepsilon \eta^{\prime \prime}(u)\left(\frac{\partial u}{\partial x}\right)^{2}
$$

By integrating on a generic slab $\left[x_{1}, x_{2}\right] \times\left[t_{1}, t_{2}\right]$ we obtain

$$
\begin{aligned}
&\int_{t_{1}}^{t_{2}} \int_{x_{1}}^{x_{2}}\left[\frac{\partial \eta}{\partial t}(u)+\frac{\partial \Psi(u)}{\partial x}\right] d x d t=\varepsilon \int_{t_{1}}^{t_{2}}\left[\eta^{\prime}\left(u\left(x_{2}, t\right)\right) \frac{\partial u}{\partial x}\left(x_{2}, t\right)\right. \\
&\left.-\eta^{\prime}\left(u\left(x_{1}, t\right)\right) \frac{\partial u}{\partial x}\left(x_{1}, t\right)\right] d t-\varepsilon \int_{t_{1}}^{t_{2}} \int_{x_{1}}^{x_{2}} \eta^{\prime \prime}(u)\left(\frac{\partial u}{\partial x}\right)^{2} d x d t=R_{1}(\varepsilon)+R_{2}(\varepsilon)
\end{aligned}
$$

where we have set

$$
\begin{gathered}
R_{1}(\varepsilon)=\varepsilon \int_{t_{1}}^{t_{2}}\left[\eta^{\prime}\left(u\left(x_{2}, t\right)\right) \frac{\partial u}{\partial x}\left(x_{2}, t\right)-\eta^{\prime}\left(u\left(x_{1}, t\right)\right) \frac{\partial u}{\partial x}\left(x_{1}, t\right)\right] d t \\
R_{2}(\varepsilon)=-\varepsilon \int_{t_{1}}^{t_{2}} \int_{x_{1}}^{x_{2}} \eta^{\prime \prime}(u)\left(\frac{\partial u}{\partial x}\right)^{2} d x d t
\end{gathered}
$$

We have

$$
\lim _{\varepsilon \rightarrow 0^{+}} R_{1}(\varepsilon)=0
$$

while if the solution for $\varepsilon \rightarrow 0^{+}$of the modified problem denotes a discontinuity across a curve of the $(x, t)$ plane, we have

$$
\lim _{\varepsilon \rightarrow 0^{+}} R_{2}(\varepsilon) \neq 0
$$

as the integral containing the term $\left(\frac{\partial u}{\partial x}\right)^{2}$ is, in general, unbounded.

On the other hand $R_{2}(\varepsilon) \leq 0$ for each $\varepsilon>0$, with $\partial^{2} \eta / \partial u^{2}>0$, hence the weak boundary solution for $\varepsilon \rightarrow 0^{+}$satisfies

$$
\int_{t_{1}}^{t_{2}} \int_{x_{1}}^{x_{2}}\left[\frac{\partial \eta}{\partial t}(u)+\frac{\partial \Psi}{\partial x}(u)\right] d x d t \leq 0 \quad \forall x_{1}, x_{2}, t_{1}, t_{2}
$$

In other words

$$
\frac{\partial \eta}{\partial t}(u)+\frac{\partial \Psi}{\partial x}(u) \leq 0, \quad x \in \mathbb{R}, \quad t>0
$$

in a weak sense. There is obviously a relationship between what we have just seen and the notion of entropic solution. If the differential equation admits an entropy function $\eta$, then a weak solution is an entropic solution if and only if $\eta$ satisfies (16.9). In other words, entropic solutions are limits, as $\varepsilon \rightarrow 0^{+}$, of solutions of the regularized problem (16.8).

\subsection{Finite difference approximation}

Let us return to the nonlinear hyperbolic equation (16.1), with initial condition

$$
u(x, 0)=u_{0}(x), x \in \mathbb{R}
$$

We denote by $a(u)=F^{\prime}(u)$ its characteristic rate. Also for this problem, we can use an explicit finite difference scheme of the form (14.13), with $H_{j+1 / 2}^{n}=H\left(u_{j}^{n}, u_{j+1}^{n}\right)$

$$
H_{j+1 / 2}^{n} \simeq \frac{1}{\Delta t} \int_{t^{n}}^{t^{n+1}} F\left(u\left(x_{j+1 / 2}, t\right)\right) d t
$$

approximating the mean flux through $x_{j+1 / 2}$ in the time interval $\left[t^{n}, t^{n+1}\right]$. To have consistency, the numerical flux $H(\cdot, \cdot)$ must verify

$$
H(\bar{u}, \bar{u})=F(\bar{u}),
$$

if $\bar{u}$ is a constant. Then thanks to a classical result by Lax and Wendroff, the functions $u$ such that

$$
u\left(x_{j}, t^{n}\right)=\lim _{\Delta t, h \rightarrow 0} u_{j}^{n}
$$

are weak solutions of the original problem (16.1).

Unfortunately, however, solutions obtained in this manner do not necessarily satisfy the entropy condition (otherwise said, weak solutions may not be entropic).

In order to "recover" the entropic solutions, numerical schemes must introduce a suitable numerical diffusion, as suggested by the analysis of Sect. 16.1. To this end, we rewrite (14.13) in the form

$$
u_{j}^{n+1}=G\left(u_{j-1}^{n}, u_{j}^{n}, u_{j+1}^{n}\right)
$$

and we introduce some definitions. The numerical scheme (16.11) is called:

- monotone if $G$ is a monotonically increasing function in each of its arguments;

- bounded if there exists $C>0$ such that sup $_{j, n}\left|u_{j}^{n}\right| \leq C$

- stable if $\forall h>0, \exists \delta_{0}>0$ (possibly dependent on $\left.h\right)$ such that for each $0<\Delta t<$

$\delta_{0}$, if $\mathbf{u}^{n}$ and $\mathbf{v}^{n}$ are the finite difference solutions obtained starting from the two initial data $\mathbf{u}^{0}$ and $\mathbf{v}^{0}$, then

$$
\left\|\mathbf{u}^{n}-\mathbf{v}^{n}\right\|_{\Delta} \leq C_{T}\left\|\mathbf{u}^{0}-\mathbf{v}^{0}\right\|_{\Delta}
$$

for each $n \geq 0$ such that $n \Delta t \leq T$ and for any choice of the initial data $\mathbf{u}^{0}$ and $\mathbf{v}^{0}$. The constant $C_{T}>0$ is independent of $\Delta t$ and $h$, and $\|\cdot\|_{\Delta}$ is a suitable discrete norm, such as those introduced in (14.26). Note that for linear problems, this definition is equivalent to $(14.25)$. We say that the numerical scheme is strongly stable when in (16.12) we can take $C_{T}=1$ for each $T>0$.

For example, using $F_{j}=F\left(u_{j}\right)$ for simplicity of notation, the Lax-Friedrichs scheme for problem (16.1) is realized through the general scheme (14.13) where we take

$$
H_{j+1 / 2}=\frac{1}{2}\left[F_{j+1}+F_{j}-\frac{1}{\lambda}\left(u_{j+1}-u_{j}\right)\right]
$$

This method is consistent, stable and monotone provided that the following condition (analogous to the CFL condition seen previously in the linear case) holds

$$
\left|F^{\prime}\left(u_{j}^{n}\right)\right| \frac{\Delta t}{h} \leq 1 \quad \forall j \in \mathbb{Z}, \forall n \in \mathbb{N}
$$

A classical result due to N.N. Kuznetsov establishes that monotone schemes of the type (16.11) are bounded, stable, convergent to the entropic solution, and are accurate to order one, at most, with respect to both time and space, that is there exists a constant $C>0$ such that

$$
\max _{j, n}\left|u_{j}^{n}-u\left(x_{j}, t^{n}\right)\right| \leq C(\Delta t+h)
$$

These schemes are generally too dissipative and do not generate accurate solutions except when using very fine grids.

Higher order schemes (called high order shock capturing schemes) can be developed using techniques that allow to calibrate the numerical dissipation as a function of the local regularity of the solution. By doing so one can solve the discontinuities correctly (ensuring the convergence of entropic solution and avoiding spurious oscillations) by using a minimal numerical dissipation. This is a complex topic and cannot be sorted out within a few lines. For an in-depth analysis, we refer to [LeV02b, LeV07, GR96, Hir88].

\subsection{Approximation by discontinuous finite elements}

For the discretization of problem (16.1) we now consider the space approximation based on discontinuous Galerkin (DG) finite elements. Using the same notations introduced in Sect. $15.4$, we seek, for each $t>0, u_{h}(t) \in W_{h}$ such that we have $\forall j=$ $0, \ldots, m-1$ and $\forall v_{h} \in \mathbb{P}_{r}\left(I_{j}\right)$

$$
\int_{I_{j}} \frac{\partial u_{h}}{\partial t} v_{h} d x-\int_{I_{j}} F\left(u_{h}\right) \frac{\partial v_{h}}{\partial x} d x+H_{j+1}\left(u_{h}\right) v_{h}^{-}\left(x_{j+1}\right)-H_{j}\left(u_{h}\right) v_{h}^{+}\left(x_{j}\right)=0
$$

with $I_{j}=\left[x_{j}, x_{j+1}\right]$. The initial datum $u_{h}^{0}$ is provided by the relations

$$
\int_{I_{j}} u_{h}^{0} v_{h} d x=\int_{I_{j}} u_{0} v_{h} d x, \quad j=0, \ldots, m-1
$$

The function $H_{j}$ now denotes the nonlinear flux at node $x_{j}$ and depends on the values of $u_{h}$ at $x_{j}$, that is

$$
H_{j}\left(u_{h}(t)\right)=H\left(u_{h}^{-}\left(x_{j}, t\right), u_{h}^{+}\left(x_{j}, t\right)\right)
$$

for a suitable numerical flux $H(\cdot, \cdot)$. If $j=0$ we will have to set $u_{h}^{-}\left(x_{0}, t\right)=\phi(t)$, which is the boundary datum at the left extremum (assuming of course that this is the inflow point).

We note that there exist various options for the choice of $H$. The first requirement is that the numerical flux $H$ has to be consistent with the flux $F$, i.e. it must satisfy property (16.10) for any constant value $\bar{u}$. Moreover, we want (16.14) to be perturbations of monotone schemes. Indeed, as noted in the previous section, the latter are stable and convergent to the entropic solution albeit being only first-order accurate. More precisely, we require (16.14) to be a monotone scheme when $r=0$. In this case, having denoted by $u_{h}^{(j)}$ the constant value of $u_{h}$ on $I_{j},(16.14)$ becomes

$$
h_{j} \frac{\partial}{\partial t} u_{h}^{(j)}(t)+H\left(u_{h}^{(j)}(t), u_{h}^{(j+1)}(t)\right)-H\left(u_{h}^{(j-1)}(t), u_{h}^{(j)}(t)\right)=0
$$

with initial datum $u_{h}^{0,(j)}=h_{i}^{-1} \int_{x_{j}^{j+1}}^{x_{j+1}} u_{0} d x$ in the interval $I_{j}, j=0, \ldots, m-1$. We have denoted by $h_{j}=x_{j+1}-x_{j}$ the length of $I_{j}$.

In order for scheme (16.16) to be monotone, the flux $H$ must be monotone, which is equivalent to saying that $H(v, w)$ is a Lipschitz function of its arguments, not decreasing in $v$ and not increasing in $w$, that is $v \rightarrow H(v, \cdot)$ is a non-decreasing function while $w \rightarrow H(\cdot, w)$ is non-increasing. In symbols, $H(\uparrow, \downarrow)$.

Three classical examples of monotone fluxes are the following:

\section{Godunov Flux}

$$
H(v, w)= \begin{cases}\min _{v \leq u \leq w} F(u) & \text { if } v \leq w \\ \max _{w \leq u \leq v} F(u) & \text { if } v>w\end{cases}
$$

\section{Engquist-Osher Flux}

$$
H(v, w)=\int_{0}^{v} \max \left(F^{\prime}(u), 0\right) d u+\int_{0}^{w} \min \left(F^{\prime}(u), 0\right) d u+F(0)
$$

\section{Lax-Friedrichs Flux}

$$
H(v, w)=\frac{1}{2}[F(v)+F(w)-\delta(w-v)], \quad \delta=\max _{\inf _{x} u_{0}(x) \leq u \leq \sup _{x} u_{0}(x)}\left|F^{\prime}(u)\right|
$$

The Godunov flux is the one yielding the least amount of numerical dissipation, the Lax-Friedrichs is the cheapest to evaluate. However, numerical experience suggests that if the degree $r$ increases, the choice of the flux $H$ has no significant consequences on the quality of the approximation.

In the linear case, where $F(u)=a u$, all previous fluxes coincide and are equal to the upwind flux

$$
H(v, w)=a \frac{v+w}{2}-\frac{|a|}{2}(w-v) .
$$

In this case we observe that the scheme (16.14) exactly coincides with the one introduced in (15.42) when $a>0$. Indeed, having set $a_{0}=0$ and $f=0$ in (15.42) and integrating by parts we obtain, for each $j=1, \ldots, m-1$

$$
\begin{aligned}
&\int_{I_{j}} \frac{\partial u_{h}}{\partial t} v_{h} d x-\int_{I_{j}}\left(a u_{h}\right) \frac{\partial v_{h}}{\partial x} d x \\
&+\left(a u_{h}\right)^{-}\left(x_{j+1}\right) v_{h}^{-}\left(x_{j+1}\right)-\left(a u_{h}\right)^{-}\left(x_{j}\right) v_{h}^{+}\left(x_{j}\right)=0
\end{aligned}
$$

i.e. (16.14), keeping in mind that in the case under exam $a u_{h}=F\left(u_{h}\right)$ and, $\forall j=$ $1, \ldots, m-1$,

$$
\left(a u_{h}\right)^{-}\left(x_{j}\right)=a \frac{u_{h}^{-}\left(x_{j}\right)+u_{h}^{+}\left(x_{j}\right)}{2}-\frac{a}{2}\left(u_{h}^{+}\left(x_{j}\right)-u_{h}^{-}\left(x_{j}\right)\right)=H_{j}\left(u_{h}\right)
$$

Verifying the case $j=0$ is obvious.

We have the following stability result

$$
\left\|u_{h}(t)\right\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2}+\theta\left(u_{h}(t)\right) \leq\left\|u_{h}^{0}\right\|_{\mathrm{L}^{2}(\alpha, \beta)}^{2}
$$

having set $\left[u_{h}\right]_{j}=u_{h}^{+}\left(x_{j}\right)-u_{h}^{-}\left(x_{j}\right)$, and

$$
\theta\left(u_{h}(t)\right)=|a| \int_{0}^{t} \sum_{j=1}^{m-1}\left[u_{h}(t)\right]_{j}^{2} d t
$$

Note how jumps are also controlled by the initial datum. The convergence analysis provides the following result (under the assumption that $u_{0} \in \mathrm{H}^{r+1}(\alpha, \beta)$ )

$$
\left\|u(t)-u_{h}(t)\right\|_{\mathrm{L}^{2}(\alpha, \beta)} \leq C h^{r+1 / 2}\left|u_{0}\right|_{\mathrm{H}^{r+1}(\alpha, \beta)}
$$

hence a convergence order $(=r+1 / 2)$ larger than the one $(=r)$ we would have using continuous finite elements, as previously encountered in the linear case (see (15.48)). In the nonlinear case and for $r=0$, defining the seminorm

$$
|v|_{T V(\alpha, \beta)}=\sum_{j=0}^{m-1}\left|v_{j+1}-v_{j}\right|, \quad v \in W_{h}
$$

and taking the Engquist-Osher numerical flux in (16.16), we have the following result (due to N.N. Kuznestov)

$$
\left\|u(t)-u_{h}(t)\right\|_{L^{1}(\alpha, \beta)} \leq\left\|u_{0}-u_{h}^{0}\right\|_{L^{1}(\alpha, \beta)}+C\left|u_{0}\right|_{T V(\alpha, \beta)} \sqrt{t h}
$$

Moreover, $\left|u_{h}(t)\right|_{T V(\alpha, \beta)} \leq\left|u_{h}^{0}\right|_{T V(\alpha, \beta)} \leq\left|u_{0}\right|_{T V(\alpha, \beta)}$. 

\subsubsection{Temporal discretization of DG methods}

For the temporal discretization, we first write scheme (16.14) in the algebraic form

$$
\begin{aligned}
&\mathbf{M}_{h} \dot{\mathbf{u}}_{h}(t)=L_{h}\left(\mathbf{u}_{h}(t), t\right), \quad t \in(0, T) \\
&\mathbf{u}_{h}(0)=\mathbf{u}_{h}^{0}
\end{aligned}
$$

$\mathbf{u}_{h}(t)$ being vector of degrees of freedom, $L_{h}\left(\mathbf{u}_{h}(t), t\right)$ the vector resulting from the discretization of the flux term $-\frac{\partial F}{\partial x}$ and $\mathrm{M}_{h}$ the mass matrix. $\mathrm{M}_{h}$ is a block diagonal matrix whose $j$-th block is the mass matrix corresponding to the element $I_{j}$. As previously observed, the latter is diagonal if we resort to the Legendre polynomial basis, which is orthogonal.

For the temporal discretization, in addition to the previously discussed Euler schemes, we can resort to the following second-order Runge-Kutta method:

$$
\begin{aligned}
&\mathrm{M}_{h}\left(\mathbf{u}_{h}^{*}-\mathbf{u}_{h}^{n}\right)=\Delta t L_{h}\left(\mathbf{u}_{h}^{n}, t^{n}\right) \\
&\mathrm{M}_{h}\left(\mathbf{u}_{h}^{* *}-\mathbf{u}_{h}^{*}\right)=\Delta t L_{h}\left(\mathbf{u}_{h}^{*}, t^{n+1}\right) \\
&\mathbf{u}_{h}^{n+1}=\frac{1}{2}\left(\mathbf{u}_{h}^{n}+\mathbf{u}_{h}^{* *}\right)
\end{aligned}
$$

In the case of the linear problem (where $F(u)=a u)$, using $r=1$ this scheme is stable in the norm $\|\cdot\|_{\mathrm{L}^{2}(\alpha, \beta)}$ provided that the condition

$$
\Delta t \leq \frac{1}{3} \frac{h}{|a|}
$$

is satisfied. For an arbitrary $r$, numerical evidence shows that a scheme of order $2 r+1$ must be used, in which case we have stability under the condition

$$
\Delta t \leq \frac{1}{2 r+1} \frac{h}{|a|}
$$

We report the third order Runge-Kutta scheme, to be used preferably when $r=1$ :

$$
\begin{aligned}
&\mathbf{M}_{h}\left(\mathbf{u}_{h}^{*}-\mathbf{u}_{h}^{n}\right)=\Delta t L_{h}\left(\mathbf{u}_{h}^{n}, t^{n}\right) \\
&\mathbf{M}_{h}\left(\mathbf{u}_{h}^{* *}-\left(\frac{3}{4} \mathbf{u}_{h}^{n}+\frac{1}{4} \mathbf{u}_{h}^{*}\right)\right)=\frac{1}{4} \Delta t L_{h}\left(\mathbf{u}_{h}^{*}, t^{n+1}\right) \\
&\mathbf{M}_{h}\left(u_{h}^{n+1}-\left(\frac{1}{3} \mathbf{u}_{h}^{n}+\frac{2}{3} \mathbf{u}_{h}^{* *}\right)\right)=\frac{2}{3} \Delta t L_{h}\left(\mathbf{u}_{h}^{* *}, t^{n+1 / 2}\right)
\end{aligned}
$$

More in general, the following family of Runge-Kutta methods was proposed by $\left[\right.$ Shu88] and [SO88, SO89]. Let us set for notational convenience $K_{h}=M_{h}^{-1} L_{h}$. Then the new value $\mathbf{u}_{h}^{n+1}$ is obtained from $\mathbf{u}_{h}^{n}$ as follows:

1. Set $\mathbf{u}_{h}^{(0)}=\mathbf{u}_{h}^{n}$

2. For $i=1, \ldots, I$ compute the intermediate vectors 2a) $\mathbf{u}_{h}^{i}=\sum_{p=0}^{i-1} \alpha_{i p} \mathbf{w}_{h}^{i p}$,

2b) $\mathbf{w}_{h}^{i p}=\mathbf{u}_{h}^{(p)}+\frac{\beta_{i p}}{\alpha_{i p}} \Delta t^{n} K_{h}\left(\mathbf{u}_{h}^{(p)}\right)$

3. Set $\mathbf{u}_{h}^{n+1}=\mathbf{u}_{h}^{(I)}$

The coefficients are requested to satisfy the following conditions:

i) If $\beta_{i p} \neq 0$ then $\alpha_{i p} \neq 0$;

ii) $\alpha_{i p} \geq 0$

(positivity);

iii) $\sum_{p=0}^{i-1} \alpha_{i p}=1 \quad$ (consistency).

Let us make the stability assumption $\left|\mathbf{w}_{h}^{i p}\right| \leq\left|\mathbf{u}_{h}^{(p)}\right|$ for an arbitrary semi-norm $|\cdot|$ Then ii) and iii) give

$$
\begin{aligned}
\left|\mathbf{u}_{h}^{(i)}\right| &=\left|\sum_{p=0}^{i-1} \alpha_{i p} \mathbf{w}_{h}^{i p}\right| \leq \sum_{p=0}^{i-1} \alpha_{i p}\left|\mathbf{w}_{h}^{i p}\right| \\
& \leq \sum_{p=0}^{i-1} \alpha_{i p}\left|\mathbf{u}_{h}^{(p)}\right| \leq \max _{0 \leq p \leq i-1}\left|\mathbf{u}_{h}^{(p)}\right|
\end{aligned}
$$

whence, in particular, $\left|\mathbf{u}_{h}^{n}\right| \leq\left|\mathbf{u}_{h}^{0}\right| \quad \forall n \geq 0$

The RK-DG schemes are analyzed in [SGT01], where they were named strong stability preserving schemes.

Example 16.2. Let us reconsider the problem of Example $15.2$, which we solve with the discontinuous finite element method, using the third-order Runge-Kutta scheme for the temporal discretization. Our scope is to verify (16.19) experimentally. To this end, we use a very small time step, $\Delta t=5 \times 10^{-4}$, and 5 decreasing values for step $h$ obtained by repeatedly halving the initial value $h=12.5 \times 10^{-3}$. We have compared the error in $\mathrm{L}^{2}(0,1)$ norm at time $t=0.01$ for elements of degree $r$ equal to $0,1,2$ and $3$. The result is reported in logarithmic scale in Fig. 16.5. This is in accordance with the theory by which the error tends to zero as $h^{r+1 / 2}$. Indeed, for $r=1$ in this particular case convergence is faster than what was predicted in theory: the numerical data provides an order of convergence very close to $2$. In the case $r>1$ we have not reported the results for values smaller than $h$, as for such values (and for the selected $\Delta t$ ) the problem is numerically unstable.

Example 16.3. Let us consider the same linear transport problem of the previous example, now using initial datum the square wave illustrated in Fig. $16.6$ left. As the initial datum is discontinuous, the use of high-degree elements does not improve the convergence order, which results to be very close to the theoretical value of $1 / 2$ for all values of $r$ considered. In Fig. $16.7$ we show the oscillations in proximity of the discontinuity of the solution in the case $r=2$, responsible of the convergence degradation, while the solution for $r=0$ denotes no oscillation. 

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-065.jpg?height=328&width=405&top_left_y=113&top_left_x=254)

Fig. 16.5. Error $\left\|u-u_{h}\right\|_{L^{2}(0,1)}$ obtained by solving a linear transport problem with regular initial datum using discontinuous finite elements of degree $r=0,1,2,3$. The error has been computed at time $t=0.01$. The time-advancing scheme is the third-order Runge-Kutta scheme with time step $\Delta t=5 \times 10^{-4}$
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-065.jpg?height=262&width=674&top_left_y=564&top_left_x=120)

Fig. 16.6. Error $\left\|u-u_{h}\right\|_{\mathrm{L}^{2}(0,1)}$ obtained by solving a linear transport problem with initial datum illustrated in the left figure. We have used discontinuous finite elements of degree $r$ equal 0,1 , 2 and 3 . The error has been computed at time $t=0.01$. The temporal progression scheme is the third-order Runge-Kutta scheme with $\Delta t=5 \times 10^{-4}$

In the case of the nonlinear problem, using the second-order Runge-Kutta scheme with $r=0$, under the condition $(16.13)$ we obtain

$$
\left|u_{h}^{n}\right|_{T V(\alpha, \beta)} \leq\left|u_{0}\right|_{T V(\alpha, \beta)},
$$

i.e. strong stability in the norm $|\cdot|_{T V(\alpha, \beta)}$.

When we do not resort to monotone schemes, it is much more difficult to obtain strong stability. In this case, we can limit ourselves to guaranteeing that the total variation of the local averages is uniformly bounded. (See [Coc98].)

Example 16.4. This example illustrates a typical feature of nonlinear problems, that is how discontinuities can show up even if we start from a regular initial datum. To this 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-066.jpg?height=268&width=702&top_left_y=117&top_left_x=104)

Fig. 16.7. Solution at time $t=0.01$ and for $h=3.125 \times 10^{-3}$ for the test case of Fig. 16.6. On the left-hand side, the case $r=3$ : note the presence of oscillations at the discontinuities, while elsewhere the solution is accurate. On the right-hand side, we show the solution obtained when using the same spatial and temporal discretization for $r=0$

end, we consider the Burgers equation (16.2) in the $(0,1)$ interval, with initial datum (see Fig. 16.8)

$$
u_{0}(x)= \begin{cases}1, & 0 \leq x \leq \frac{5}{12} \\ 54\left(2 x-\frac{5}{6}\right)^{3}-27\left(2 x-\frac{5}{6}\right)^{2}+1, & \frac{5}{12}<x<\frac{7}{12} \\ 0, & \frac{7}{12} \leq x \leq 1\end{cases}
$$

It can be easily verified that $u_{0}$, illustrated in Fig. 16.8, is of class $C^{1}(0,1)$.

We have then considered the numerical solution obtained with the discontinuous Galerkin method, using the third-order Runge-Kutta scheme with a time step of $\Delta t=$ $10^{-3}$ and $h=0.01$, for $r=0, r=1$ and $r=2$. Fig. $16.9$ shows the solution at time $t=0.5$ obtained with such schemes. We can note a discontinuity arising, which the numerical scheme solves without oscillations in the case $r=0$, while for larger values of $r$ we have oscillations in proximity of the discontinuity itself

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-066.jpg?height=265&width=340&top_left_y=933&top_left_x=284)

Fig. 16.8. Initial solution $u_{0}$ of the first test case of the Burgers problem 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-067.jpg?height=532&width=696&top_left_y=119&top_left_x=110)

Fig. 16.9. Solution at time $t=0.5$ of the first test case of the Burgers problem. Comparison between the numerical solution for $r=0$ (top left), $r=1$ (top right) and $r=2$ (bottom). For the case $r=0$, the piecewise constant discrete solution has been highlighted by connecting with a dashed line the values at the midpoint of each element

To eliminate the oscillations in proximity of the solution's discontinuities, we can use the technique involving flux limiters, whose description goes beyond the scope of this book. For this we refer the reader to the previously cited bibliography. We limit ourselves to saying that the third-order Runge-Kutta scheme $(16.20)$ is modified as follows

$$
\begin{aligned}
&\mathbf{u}_{h}^{*}=\Lambda_{h}\left(\mathbf{u}_{h}^{n}+\Delta t \mathbf{M}_{h}^{-1} L_{h}\left(\mathbf{u}_{h}^{n}, t^{n}\right)\right) \\
&\mathbf{u}_{h}^{* *}=\Lambda_{h}\left(\frac{3}{4} \mathbf{u}_{h}^{n}+\frac{1}{4} \mathbf{u}_{h}^{*}+\frac{1}{4} \Delta t \mathbf{M}_{h}^{-1} L_{h}\left(\mathbf{u}_{h}^{*}, t^{n+1}\right)\right) \\
&\mathbf{u}_{h}^{n+1}=\Lambda_{h}\left(\frac{1}{3} \mathbf{u}_{h}^{n}+\frac{2}{3} \mathbf{u}_{h}^{* *}+\frac{2}{3} \Delta t \mathbf{M}_{h}^{-1} L_{h}\left(\mathbf{u}_{h}^{* *}, t^{n+1 / 2}\right)\right)
\end{aligned}
$$

$\Lambda_{h}$ being the flux limiter, that is a function depending also on the variations of the computed solutions, i.e. the difference between the values of two adjacent nodes. This is equal to the identity operator where the solution is regular, while it limits its variations if these cause high-frequency oscillations in the numerical solution. Clearly $\Lambda_{h}$ must be constructed in a suitable way, in particular it has to maintain the properties of consistency and conservation of the scheme, and must differ as little as possible from the identity operator so as to prevent accuracy degradation. 

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-068.jpg?height=258&width=332&top_left_y=119&top_left_x=292)

Fig. 16.10. Solution at time $t=0.5$ for the first test case of the Burgers problem. It has been obtained for $r=1$ and by applying the flux limiters technique to regularize the numerical solution near the discontinuities

For the sake of an example, we report in Fig. $16.10$ the result obtained with linear discontinuous finite elements $(r=1)$ for the same test case of Fig. $16.9$ applying flux limiters. The obtained numerical solution is more regular, although slightly more diffusive than that of Fig. 16.9.

Example 16.5. Let us now consider a second problem, where the initial datum is that of Fig. 16.11, obtained by reflecting with respect to the line $x=0.5$ the datum of the previous test case. By keeping all the remaining parameters of the numerical simulation unchanged, we once again examine the solution at $t=0.5$. The latter is illustrated in Fig. 16.12. In this case, the solution remains continuous; in fact, with this initial condition, the characteristic lines (which in the case of the Burgers equation are straight lines in the plane $(x, t)$ with slope $\left.\arctan u^{-1}\right)$ never cross. The zoom allows to appreciate qualitatively the better accuracy of the solution obtained for $r=2$ with respect to the one obtained for $r=1$.

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-068.jpg?height=254&width=333&top_left_y=943&top_left_x=290)

Fig. 16.11. Initial solution $u_{0}$ for the second test case of the Burgers problem 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-069.jpg?height=278&width=700&top_left_y=118&top_left_x=107)

Fig. 16.12. Solution at time $t=0.5$ for the second test case of the Burgers problem. Comparison between the solution obtained for $r=1$ (left) and the one obtained for $r=2$ (right). In the box we illustrate an enlargement of the numerical solution which allows to qualitatively grasp the improved accuracy obtained for $r=2$

\subsection{Nonlinear hyperbolic systems}

In this last section we briefly address the case of nonlinear hyperbolic systems. A classical example is provided by the Euler equations, which are obtained from the following Navier-Stokes equations (for compressible fluids) in $\mathbb{R}^{d}, d=1,2,3$ :

$$
\begin{aligned}
&\frac{\partial \rho}{\partial t}+\sum_{j=1}^{d} \frac{\partial\left(\rho u_{j}\right)}{\partial x_{j}}=0 \\
&\frac{\partial\left(\rho u_{i}\right)}{\partial t}+\sum_{j=1}^{d}\left[\frac{\partial\left(\rho u_{i} u_{j}+\delta_{i j} p\right)}{\partial x_{j}}-\frac{\partial \tau_{i j}}{\partial x_{j}}\right]=0, \quad i=1, \ldots, d \\
&\frac{\partial \rho e}{\partial t}+\sum_{j=1}^{d}\left[\frac{\partial\left(\rho h u_{j}\right)}{\partial x_{j}}-\frac{\partial\left(\sum_{i=1}^{d} u_{i} \tau_{i j}+q_{j}\right)}{\partial x_{j}}\right]=0
\end{aligned}
$$

The variables have the following meaning: $\mathbf{u}=\left(u_{1}, \ldots, u_{d}\right)^{T}$ is the vector of velocities, $\rho$ is the density, $p$ the pressure, $e_{i}+\frac{1}{2}|\mathbf{u}|^{2}$ the total energy per unit of mass, equal to the sum of the internal energy $e_{i}$ and of the kinetic energy of the fluid, $h=e+p / \rho$ the total entalpy per mass unit, $\mathbf{q}$ the thermal flux and finally

$$
\tau_{i j}=\mu\left[\left(\frac{\partial u_{j}}{\partial x_{i}}+\frac{\partial u_{i}}{\partial x_{j}}\right)-\frac{2}{3} \delta_{i j} \operatorname{divu}\right], \quad i, j=1, \ldots, d
$$

the stress tensor ( $\mu$ being the molecular viscosity of the fluid).

The equations in the above system describe the conservation of mass, momentum and energy, respectively. To complete the system, it is necessary to put $e$ in relationship with the variables $\rho, p, \mathbf{u}$, by defining a law

$$
e=\Phi(\rho, p, \mathbf{u})
$$

The latter is normally derived from the state equations of the fluid under exam. In particular, the state equations of the ideal gas

$$
p=\rho R T, \quad e_{i}=C_{v} T
$$

where $R=C_{p}-C_{v}$ is the gas constant and $T$ is its temperature, provide

$$
e=\frac{p}{\rho(\gamma-1)}+\frac{1}{2}|\mathbf{u}|^{2}
$$

where $\gamma=C_{p} / C_{v}$ is the ratio between the specific heats at constant pressure and volume, respectively. The thermal flux $\mathbf{q}$ is usually related to the temperature gradient via the Fick law

$$
\mathbf{q}=-\kappa \nabla T=-\frac{\kappa}{C_{v}} \nabla\left(e-\frac{1}{2}|\mathbf{u}|^{2}\right)
$$

$\kappa$ being the conductibility of the fluid under exam.

If $\mu=0$ and $\kappa=0$, we obtain the Euler equations for non-viscous fluids. The interested reader can find them in specialized fluid dynamics textbooks, or in textbooks on nonlinear hyperbolic systems, such as for instance [Hir88] or [GR96]. Such equations can be written in compact form in the following way

$$
\frac{\partial \mathbf{w}}{\partial t}+\operatorname{Div} F(\mathbf{w})=\mathbf{0}
$$

with $\mathbf{w}=(\rho, \rho \mathbf{u}, \rho e)^{T}$ being the vector of the so-called conservative variables. The flux matrix $F(\mathbf{w})$, a nonlinear function of $\mathbf{w}$, can be obtained from (16.21). For instance, if $d=2$, we have

$$
F(\mathbf{w})=\left[\begin{array}{cc}
\rho u_{1} & \rho u_{2} \\
\rho u_{1}^{2}+p & \rho u_{1} u_{2} \\
\rho u_{1} u_{2} & \rho u_{2}^{2}+p \\
\rho h u_{1} & \rho h u_{2}
\end{array}\right]
$$

Finally, in (16.22) Div denotes the divergence operator of a tensor: if $\tau$ is a tensor with components $\left(\tau_{i j}\right)$, its divergence is a vector with components

$$
(\operatorname{Div}(\tau))_{k}=\sum_{j=1}^{d} \frac{\partial}{\partial x_{j}}\left(\tau_{k j}\right), \quad k=1, \ldots, d
$$

The form (16.22) is called conservation form of the Euler equations. Indeed, by integrating it on any region $\Omega \subset \mathbb{R}^{d}$ and using the Gauss theorem, we obtain

$$
\frac{d}{d t} \int_{\Omega} \mathbf{w} d \Omega+\int_{\partial \Omega} F(\mathbf{w}) \cdot \mathbf{n} d \gamma=0
$$

This is interpreted by saying that the variation in time of $\mathbf{w}$ in $\Omega$ is compensated by the variation of the fluxes through the boundary of $\Omega ;(16.22)$ is thus a conservation law. The Navier-Stokes equations can also be written in conservative form as follows

$$
\frac{\partial \mathbf{w}}{\partial t}+\operatorname{Div} F(\mathbf{w})=\operatorname{Div} G(\mathbf{w})
$$

where $G(\mathbf{w})$ are the so-called viscous fluxes. Remaining in the $d=2$ case, these are given by

$$
G(\mathbf{w})=\left[\begin{array}{cc}
0 & 0 \\
\tau_{11} & \tau_{12} \\
\tau_{21} & \tau_{22} \\
\rho h u_{1}+\mathbf{u} \cdot \tau_{1}+q_{1} & \rho h u_{2}+\mathbf{u} \cdot \tau_{2}+q_{2}
\end{array}\right]
$$

where $\tau_{1}=\left(\tau_{11}, \tau_{21}\right)^{T}$ and $\tau_{2}=\left(\tau_{12}, \tau_{22}\right)^{T}$.

We now rewrite system (16.22) in the form

$$
\frac{\partial \mathbf{w}}{\partial t}+\sum_{i=1}^{d} \frac{\partial F_{i}(\mathbf{w})}{\partial \mathbf{w}} \frac{\partial \mathbf{w}}{\partial x_{i}}=0
$$

This is a particular case of the following quasi-linear form

$$
\frac{\partial \mathbf{w}}{\partial t}+\sum_{i=1}^{d} A_{i}(\mathbf{w}) \frac{\partial \mathbf{w}}{\partial x_{i}}=\mathbf{0}
$$

If the matrix $A^{\alpha}(\mathbf{w})=\sum_{i=1}^{d} \alpha_{i} A_{i}(\mathbf{w})$ can be diagonalized for all real values of $\left\{\alpha_{1}, \ldots, \alpha_{d}\right\}$ and its eigenvalues are real and distinct, then system $(16.25)$ is said to be strictly hyperbolic.

Example 16.6. A simple example of a strictly hyperbolic problem is provided by the so-called $p$-system:

$$
\begin{aligned}
&\frac{\partial v}{\partial t}-\frac{\partial u}{\partial x}=0 \\
&\frac{\partial u}{\partial t}+\frac{\partial}{\partial x} p(v)=0
\end{aligned}
$$

If $p^{\prime}(v)<0$, the two eigenvalues of the Jacobian matrix

$$
A(\mathbf{w})=\left(\begin{array}{cc}
0 & -1 \\
p^{\prime}(v) & 0
\end{array}\right)
$$

are

$$
\lambda_{1}(v)=-\sqrt{-p^{\prime}(v)}<0<\lambda_{2}(v)=+\sqrt{-p^{\prime}(v)}
$$

Example 16.7. For the one-dimensional Euler system (i.e. with $d=1)$ we have: $\mathbf{w}=$ $(\rho, \rho u, e)^{T}$ and $F(\mathbf{w})=\left(\rho u, \rho u^{2}+p, u(e+p)\right)^{T}$. The eigenvalues of the matrix $A_{1}(\mathbf{w})$ of the system are $u-c, u, u+c$ where $c=\sqrt{\gamma \frac{p}{\rho}}$ is the speed of sound. As $u, c \in \mathbb{R}$ the eigenvalues are real and distinct and therefore the one-dimensional Euler system is strictly hyperbolic. The remarks made on the discontinuities of the solution in the scalar case can be extended to the case of systems, by introducing the notion of weak solution also here. The entropy condition can be extended to the case of systems, using for instance the condition proposed by Lax. We observe that in the case of the one-dimensional system (16.22) the Rankine-Hugoniot jump conditions are written in the form

$$
F\left(\mathbf{w}_{+}\right)-F\left(\mathbf{w}_{-}\right)=\sigma\left(\mathbf{w}_{+}-\mathbf{w}_{-}\right)
$$

$\mathbf{w}_{\pm}$being the two constant states that represent the values of the unknowns through the discontinuity, and $\sigma$ representing once again the rate at which the discontinuity propagates. Using the fundamental theorem of calculus, such relation is written in the form

$$
\begin{aligned}
\sigma\left(\mathbf{w}_{+}-\mathbf{w}_{-}\right) &=\int_{0}^{1} D F\left(\theta \mathbf{w}_{+}+(1-\theta) \mathbf{w}_{-}\right) \cdot\left(\mathbf{w}_{+}-\mathbf{w}_{-}\right) d \theta \\
&=A\left(\mathbf{w}_{-}, \mathbf{w}_{+}\right) \cdot\left(\mathbf{w}_{+}-\mathbf{w}_{-}\right)
\end{aligned}
$$

where the matrix

$$
A\left(\mathbf{w}_{-}, \mathbf{w}_{+}\right)=\int_{0}^{1} D F\left(\theta \mathbf{w}_{+}+(1-\theta) \mathbf{w}_{-}\right) d \theta
$$

represents the mean value of the Jacobian of $F$ (denoted by $D F$ ) along the segment connecting $\mathbf{w}_{-}$with $\mathbf{w}_{+}$. Relation (16.26) shows that at each point where a discontinuity occurs the difference between the right and left state $\mathbf{w}_{+}-\mathbf{w}_{-}$is an eigenvector of the matrix $A\left(\mathbf{w}_{-}, \mathbf{w}_{+}\right)$, while the rate of the jump $\sigma$ coincides with the corresponding eigenvalue $\lambda=\lambda\left(\mathbf{w}_{-}, \mathbf{w}_{+}\right)$.Calling $\lambda_{i}(\mathbf{w})$ the $i-$ th eigenvalue of

$$
A(\mathbf{w})=D F(\mathbf{w})
$$

the admissibility condition of Lax requires that

$$
\lambda_{i}\left(\mathbf{w}_{+}\right) \leq \sigma \leq \lambda_{i}\left(\mathbf{w}_{-}\right) \quad \text { for each i. }
$$

Intuitively, this means that the velocity at which a shock of the $i-$ th family travels must exceed the velocity $\lambda_{i}\left(\mathbf{w}_{+}\right)$of the waves that are immediately ahead of the shock, and must be less than the velocity $\lambda_{i}\left(\mathbf{w}_{-}\right)$of the waves behind the shock.

In the case of hyperbolic systems of $m$ equations $(m>1)$, the entropy $\eta$ and its corresponding flux $\Psi$ are still scalar functions, and relation $(16.5)$ becomes

$$
\nabla \Psi(\mathbf{u})=\frac{d \mathbf{F}}{d \mathbf{u}}(\mathbf{u}) \cdot \nabla \eta(\mathbf{u})
$$

which represents a system of $m$ equations and 2 unknowns $(\eta$ and $\Psi)$. If $m>2$, such system may have no solutions.

Remark 16.2. In the case of the Euler equations, the entropy function exists also in the case $m=3$. 

\section{Navier-Stokes equations}

Navier-Stokes equations describe the motion of a fluid with constant density $\rho$ in a domain $\Omega \subset \mathbb{R}^{d}$ (with $\left.d=2,3\right)$. They read as follows

$$
\begin{cases}\frac{\partial \mathbf{u}}{\partial t}-\operatorname{div}\left[v\left(\nabla \mathbf{u}+\nabla \mathbf{u}^{T}\right)\right]+(\mathbf{u} \cdot \nabla) \mathbf{u}+\nabla p=\mathbf{f}, & \mathbf{x} \in \Omega, t>0 \\ \operatorname{div} \mathbf{u}=0, & \mathbf{x} \in \Omega, t>0\end{cases}
$$

$\mathbf{u}$ being the fluid's velocity, $p$ the pressure divided by the density (which will simply be called "pressure"), $v$ the kinematic viscosity, and $\mathbf{f}$ a forcing term per unit of mass that we suppose belongs in $\mathrm{L}^{2}\left(\mathbb{R}^{+} ;\left[\mathrm{L}^{2}(\Omega)\right]^{d}\right)$ (see Sect. 5.2). The first equation is that of conservation of linear momentum, the second one that of conservation of mass, which is also called the continuity equation. The term $(\mathbf{u} \cdot \nabla) \mathbf{u}$ describes the process of convective transport, while $-\operatorname{div}\left[v\left(\nabla \mathbf{u}+\nabla \mathbf{u}^{T}\right)\right]$ the process of molecular diffusion. System (17.1) can be derived by the analogous system for compressible flows introduced in Chapter 16 by assuming $\rho$ constant, using the continuity equation (which, under current assumptions, takes the simplified form $\operatorname{div} \mathbf{u}=0$ ) to simplify the various terms, and finally dividing the equation by $\rho$. Note that in the incompressible case (17.2) the energy equation has disappeared. Indeed, even though such an equation can still be written for incompressible flows, its solution can be found independently once the velocity field is obtained from the solution of $(17.1)$.

When $v$ is constant, from the continuity equation we obtain

$$
\operatorname{div}\left[v\left(\nabla \mathbf{u}+\nabla \mathbf{u}^{T}\right)\right]=v(\Delta \mathbf{u}+\nabla \operatorname{div} \mathbf{u})=v \Delta \mathbf{u}
$$

whence system (17.1) can be written in the equivalent form

$$
\begin{cases}\frac{\partial \mathbf{u}}{\partial t}-v \Delta \mathbf{u}+(\mathbf{u} \cdot \nabla) \mathbf{u}+\nabla p=\mathbf{f}, & \mathbf{x} \in \Omega, t>0 \\ \operatorname{div} \mathbf{u}=0, & \mathbf{x} \in \Omega, t>0\end{cases}
$$

which is the one that we will consider in this chapter.

Equations (17.2) are often called incompressible Navier-Stokes equations. More in general, fluids satisfying the incompressibility condition $\operatorname{div} \mathbf{u}=0$ are said to be incompressible. Constant density fluids necessarily satisfy this condition, however there exist incompressible fluids featuring variable density (e.g., stratified fluids) that are governed by a different system of equations in which the density $\rho$ explicitly shows up. This case will be addressed in Sect. 17.9.

In order for problem (17.2) to be well posed it is necessary to assign the initial condition

$$
\mathbf{u}(\mathbf{x}, 0)=\mathbf{u}_{0}(\mathbf{x}) \quad \forall \mathbf{x} \in \Omega
$$

where $\mathbf{u}_{0}$ is a given divergence-free vector field, together with suitable boundary conditions, such as, e.g., $\forall t>0$,

$$
\left\{\begin{array}{l}
\mathbf{u}(\mathbf{x}, t)=\boldsymbol{\varphi}(\mathbf{x}, t) \quad \forall \mathbf{x} \in \Gamma_{D} \\
\left(v \frac{\partial \mathbf{u}}{\partial \mathbf{n}}-p \mathbf{n}\right)(\mathbf{x}, t)=\boldsymbol{\psi}(\mathbf{x}, t) \quad \forall \mathbf{x} \in \Gamma_{N}
\end{array}\right.
$$

where $\varphi$ and $\psi$ are given vector functions, while $\Gamma_{D}$ and $\Gamma_{N}$ provide a partition of the domain boundary $\partial \Omega$, that is $\Gamma_{D} \cup \Gamma_{N}=\partial \Omega, \dot{\Gamma}_{D} \cap \stackrel{\circ}{\Gamma_{N}}=\emptyset$. Finally, as usual $\mathbf{n}$ is the outward unit normal vector to $\partial \Omega$. If we use the alternative formulation (17.1), the second equation in (17.4) must be replaced by

$$
\left[v\left(\nabla \mathbf{u}+\nabla \mathbf{u}^{T}\right) \mathbf{n}-p \mathbf{n}\right](\mathbf{x}, t)=\psi(\mathbf{x}, t) \quad \forall \mathbf{x} \in \Gamma_{N}
$$

Further considerations on boundary conditions will follow in Sect. 17.9.2.

Denoting with $u_{i}, i=1, \ldots, d$ the components of the vector $\mathbf{u}$ with respect to a Cartesian frame, and with $f_{i}$ the components of $\mathbf{f}$, system (17.2) can be written componentwise as

$$
\left\{\begin{array}{l}
\frac{\partial u_{i}}{\partial t}-v \Delta u_{i}+\sum_{j=1}^{d} u_{j} \frac{\partial u_{i}}{\partial x_{j}}+\frac{\partial p}{\partial x_{i}}=f_{i}, \quad i=1, \ldots, d \\
\sum_{j=1}^{d} \frac{\partial u_{j}}{\partial x_{j}}=0
\end{array}\right.
$$

In the two-dimensional case the Navier-Stokes equations with the boundary conditions previously indicated yield well-posed problems. This means that if all data (initial condition, forcing term, boundary data) are smooth enough, then the solution is continuous together with its derivatives and does not develop singularities in time. Things may go differently in three dimensions, where existence and uniqueness of classical solutions have been proven only locally in time (that is for a sufficiently small time interval). In the following section we will introduce the weak formulation of the Navier-Stokes equations, for which existence of a solution has been proven for all times. However, the issue of uniqueness (which is related to that of regularity) is still open, and is actually the central issue of Navier-Stokes theory. Remark 17.1. The Navier-Stokes equations have been written in terms of the primitive variables $\mathbf{u}$ and $p$, but other sets of variables may be used, too. For instance, in the two-dimensional case it is common to see the vorticity $\omega$ and the streamfunction $\psi$, that are related to the velocity as follows

$$
\omega=\operatorname{rot} \mathbf{u}=\frac{\partial u_{2}}{\partial x_{1}}-\frac{\partial u_{1}}{\partial x_{2}}, \quad \mathbf{u}=\left[\begin{array}{c}
\frac{\partial \psi}{\partial x_{2}} \\
-\frac{\partial \psi}{\partial x_{1}}
\end{array}\right]
$$

The various formulations are in fact equivalent from a mathematical standpoint, although they give rise to different numerical methods. See, e.g., [Qua93].

\subsection{Weak formulation of Navier-Stokes equations}

A weak formulation of problem (17.2)-(17.4) can be obtained by proceeding formally, as follows. Let us multiply the first equation of (17.2) by a test function $\mathbf{v}$ belonging to a suitable space $V$ that will be specified later on, and integrate $\Omega$

$$
\int_{\Omega} \frac{\partial \mathbf{u}}{\partial t} \cdot \mathbf{v} d \Omega-\int_{\Omega} v \Delta \mathbf{u} \cdot \mathbf{v} d \Omega+\int_{\Omega}[(\mathbf{u} \cdot \nabla) \mathbf{u}] \cdot \mathbf{v} d \Omega+\int_{\Omega} \nabla p \cdot \mathbf{v} d \Omega=\int_{\Omega} \mathbf{f} \cdot \mathbf{v} d \Omega
$$

Using Green's formulae (3.16) and (3.17) we find:

$$
\begin{aligned}
-\int_{\Omega} v \Delta \mathbf{u} \cdot \mathbf{v} d \Omega &=\int_{\Omega} v \nabla \mathbf{u} \cdot \nabla \mathbf{v} d \Omega-\int_{\partial \Omega} v \frac{\partial \mathbf{u}}{\partial \mathbf{n}} \cdot \mathbf{v} d \gamma \\
\int_{\Omega} \nabla p \cdot \mathbf{v} d \Omega &=-\int_{\Omega} p \operatorname{div} \mathbf{v} d \Omega+\int_{\partial \Omega} p \mathbf{v} \cdot \mathbf{n} d \gamma
\end{aligned}
$$

Using these relations in the first of (17.2), we obtain

$$
\begin{aligned}
\int_{\Omega} \frac{\partial \mathbf{u}}{\partial t} \cdot \mathbf{v} d \Omega+\int_{\Omega} v \nabla \mathbf{u} \cdot \nabla \mathbf{v} d \Omega+\int_{\Omega}[(\mathbf{u} \cdot \nabla) \mathbf{u}] \cdot \mathbf{v} d \Omega-\int_{\Omega} p \operatorname{div} \mathbf{v} d \Omega \\
&=\int_{\Omega} \mathbf{f} \cdot \mathbf{v} d \Omega+\int_{\partial \Omega}\left(v \frac{\partial \mathbf{u}}{\partial \mathbf{n}}-p \mathbf{n}\right) \cdot \mathbf{v} d \gamma \quad \forall \mathbf{v} \in V
\end{aligned}
$$

(All boundary integrals should indeed be regarded as duality pairings.)

Similarly, by multiplying the second equation of $(17.2)$ by a test function $q$, belonging to a suitable space $Q$ to be specified, then integrating on $\Omega$ it follows

$$
\int_{\Omega} q \operatorname{divu} d \Omega=0 \quad \forall q \in Q
$$

Customarily $V$ is chosen so that the test functions vanish on the boundary portion where a Dirichlet data is prescribed on $\mathbf{u}$, that is

$$
V=\left[\mathrm{H}_{\Gamma_{D}}^{1}(\Omega)\right]^{d}=\left\{\mathbf{v} \in\left[\mathrm{H}^{1}(\Omega)\right]^{d}:\left.\mathbf{v}\right|_{\Gamma_{D}}=\mathbf{0}\right\}
$$

It will coincide with $\left[\mathrm{H}_{0}^{1}(\Omega)\right]^{d}$ if $\Gamma_{D}=\partial \Omega$. If $\Gamma_{N} \neq \emptyset$, we can choose $Q=\mathrm{L}^{2}(\Omega)$. Moreover, if $t>0$, then $\mathbf{u}(t) \in\left[\mathrm{H}^{1}(\Omega)\right]^{d}$, with $\mathbf{u}(t)=\varphi(t)$ on $\Gamma_{D}, \mathbf{u}(0)=\mathbf{u}_{0}$ and $p(t) \in Q$

Having chosen these functional spaces, we can note first of all that

$$
\int_{\partial \Omega}\left(v \frac{\partial \mathbf{u}}{\partial \mathbf{n}}-p \mathbf{n}\right) \cdot \mathbf{v} d \gamma=\int_{\Gamma_{N}} \psi \cdot \mathbf{v} d \gamma \quad \forall \mathbf{v} \in V
$$

Moreover, all the integrals involving bilinear terms are finite. More precisely, by using the vector notation $\mathbf{H}^{k}(\Omega)=\left[\mathrm{H}^{k}(\Omega)\right]^{d}, \mathbf{L}^{p}(\Omega)=\left[\mathrm{L}^{p}(\Omega)\right]^{d}, k \geq 1,1 \leq p<\infty$, we find:

$$
\begin{aligned}
&\left|v \int_{\Omega} \nabla \mathbf{u} \cdot \nabla \mathbf{v} d \Omega\right| \leq v|\mathbf{u}|_{\mathbf{H}^{1}(\Omega)}|\mathbf{v}|_{\mathbf{H}^{1}(\Omega)} \\
&\left|\int_{\Omega} p \operatorname{div} \mathbf{v} d \Omega\right| \leq\|p\|_{\mathrm{L}^{2}(\Omega)}|\mathbf{v}|_{\mathbf{H}^{1}(\Omega)} \\
&\left|\int_{\Omega} q \nabla \mathbf{u} d \Omega\right| \leq\|q\|_{\mathrm{L}^{2}(\Omega)}|\mathbf{u}|_{\mathbf{H}^{1}(\Omega)}
\end{aligned}
$$

For every function $\mathbf{v} \in \mathbf{H}^{1}(\Omega)$, we denote by

$$
\|\mathbf{v}\|_{\mathbf{H}^{1}(\Omega)}=\left(\sum_{k=1}^{d}\left\|v_{k}\right\|_{\mathrm{H}^{1}(\Omega)}^{2}\right)^{1 / 2}
$$

its norm and by

$$
|\mathbf{v}|_{\mathbf{H}^{1}(\Omega)}=\left(\sum_{k=1}^{d}\left|v_{k}\right|_{\mathrm{H}^{1}(\Omega)}^{2}\right)^{1 / 2}
$$

its seminorm. The notation $\|\mathbf{v}\|_{\mathbf{L}^{p}(\Omega)}, 1 \leq p<\infty$, has a similar meaning. The same symbols will be used in case of tensor functions. Thanks to Poincaré's inequality, $|\mathbf{v}|_{\mathbf{H}^{1}(\Omega)}$ is equivalent to the $\operatorname{norm}\|\mathbf{v}\|_{\mathbf{H}^{1}(\Omega)}$ for all functions belonging to $V$.

Also the integral involving the trilinear term is finite. To see how, let us start by recalling the following result (see (2.19); for its proof, see [AF03]): if $d \leq 3$,

$$
\forall \mathbf{v} \in \mathbf{H}^{1}(\Omega), \text { then } \mathbf{v} \in \mathbf{L}^{4}(\Omega) \text { and } \exists C>0 \text { s.t. }\|\mathbf{v}\|_{\mathbf{L}^{4}(\Omega)} \leq C\|\mathbf{v}\|_{\mathbf{H}^{1}(\Omega)}
$$

Using the following three-term Hölder inequality

$$
\left|\int_{\Omega} f g h d \Omega\right| \leq\|f\|_{L^{p}(\Omega)}\|g\|_{\mathrm{L}^{q}(\Omega)}\|h\|_{\mathrm{L}^{r}(\Omega)}
$$

valid for all $p, q, r>1$ such that $p^{-1}+q^{-1}+r^{-1}=1$, we conclude that

$$
\left|\int_{\Omega}[(\mathbf{u} \cdot \nabla) \mathbf{u}] \cdot \mathbf{v} d \Omega\right| \leq\|\nabla \mathbf{u}\|_{\mathbf{L}^{2}(\Omega)}\|\mathbf{u}\|_{\mathbf{L}^{4}(\Omega)}\|\mathbf{v}\|_{\mathbf{L}^{4}(\Omega)} \leq C^{2}\|\mathbf{u}\|_{\mathbf{H}^{1}(\Omega)}^{2}\|\mathbf{v}\|_{\mathbf{H}^{1}(\Omega)}
$$

As for the solution's uniqueness, let us consider again the Navier-Stokes equations in strong form (17.2) (similar considerations can be made on the weak form (17.5), (17.6)). If $\Gamma_{D}=\partial \Omega$, when only boundary conditions of Dirichlet type are imposed, the pressure appears merely in terms of its gradient; in such a case, if we call $(\mathbf{u}, p)$ a solution of $(17.2)$, for any possible constant $c$ the couple $(\mathbf{u}, p+c)$ is a solution too, since $\nabla(p+c)=\nabla p$. To avoid such indeterminacy one can fix a priori the value of $p$ at one given point $\mathbf{x}_{0}$ of the domain $\Omega$, that is set $p\left(\mathbf{x}_{0}\right)=p_{0}$, or, alternatively, require the pressure to have null average, i.e., $\int_{\Omega} p d \Omega=0$. The former condition requires to prescribe a pointwise value for the pressure, but this is inconsistent with our Ansatz that $p \in \mathrm{L}^{2}(\Omega)$. (We anticipate, however, that this is admissible at the numerical level when we look for a continuous finite-dimensional pressure.) For this reason we assume from now on that the pressure is average-free. More specifically, we will consider the following pressure space

$$
\mathrm{Q}=\mathrm{L}_{0}^{2}(\Omega)=\left\{p \in \mathrm{L}^{2}(\Omega): \int_{\Omega} p d \Omega=0\right\}
$$

Further, we observe that if $\Gamma_{D}=\partial \Omega$, the prescribed Dirichlet data $\varphi$ must be compatible with the incompressibility constraint; indeed,

$$
\int_{\partial \Omega} \boldsymbol{\varphi} \cdot \mathbf{n} d \gamma=\int_{\Omega} \operatorname{div} \mathbf{u} \mathrm{d} \Omega=0
$$

If $\Gamma_{N}$ is not empty, i.e. in presence of either Neumann or mixed Dirichlet-Neumann boundary conditions, the problem of pressure indeterminacy (up to an additive constant) no longer exists. In this case we can take $\mathrm{Q}=\mathrm{L}^{2}(\Omega)$. In conclusion, from now on we shall implicitly assume

$$
Q=\mathrm{L}^{2}(\Omega) \quad \text { if } \quad \Gamma_{N} \neq \emptyset, \quad Q=\mathrm{L}_{0}^{2}(\Omega) \quad \text { if } \Gamma_{N}=\emptyset
$$

The weak formulation of the system $(17.2),(17.3),(17.4)$ is therefore:

find $\mathbf{u} \in \mathrm{L}^{2}\left(\mathbb{R}^{+} ;\left[\mathrm{H}^{1}(\Omega)\right]^{d}\right) \cap C^{0}\left(\mathbb{R}^{+} ;\left[\mathrm{L}^{2}(\Omega)\right]^{d}\right), p \in \mathrm{L}^{2}\left(\mathbb{R}^{+} ; Q\right)$ such that

$$
\left\{\begin{aligned}
\int_{\Omega} \frac{\partial \mathbf{u}}{\partial t} \cdot \mathbf{v} d \Omega+v \int_{\Omega} \nabla \mathbf{u} \cdot \nabla \mathbf{v} d \Omega+\int_{\Omega}[(\mathbf{u} \cdot \nabla) \mathbf{u}] \cdot \mathbf{v} d \Omega-\int_{\Omega} p \operatorname{div} \mathbf{v} d \Omega \\
=\int_{\Omega} \mathbf{f} \cdot \mathbf{v} d \Omega+\int_{\Gamma_{N}} \boldsymbol{\psi} \cdot \mathbf{v} d \gamma \quad \forall \mathbf{v} \in V
\end{aligned}\right.
$$

with $\left.\mathbf{u}\right|_{\Gamma_{D}}=\varphi_{D}$ and $\left.\mathbf{u}\right|_{t=0}=\mathbf{u}_{0}$. The space $V$ is the one in $(17.7)$ while $Q$ is the space introduced in (17.8). The spaces of time-dependent functions for $\mathbf{u}$ and $p$ have been introduced in Sect. $2.7$.

As we have already anticipated, existence of solutions can be proven for this problem for both dimensions $d=2$ and $d=3$, whereas uniqueness has been proven only in the case $d=2$ for sufficiently small data (see, e.g., [Tem01] and [Sal08]).

Let us define the Reynolds number

$$
R e=\frac{|\mathbf{U}| L}{v}
$$

where $L$ is a representative length of the domain $\Omega$ (e.g. the length of a channel where the fluid's flow is studied) and $\mathbf{U}$ a representative fluid velocity.

The Reynolds number measures the extent to which convection dominates over diffusion. When $R e \ll 1$ the convective term $(\mathbf{u} \cdot \nabla) \mathbf{u}$ can be omitted, and the Navier-Stokes equations reduce to the so-called Stokes equations, that will be investigated later in this chapter. On the other hand, if $\operatorname{Re}$ is large, problems may arise concerning uniqueness of the solution, the existence of stationary and stable solutions, the possible existence of strange attractors, the transition towards turbulent flows. See [Tem01, FMRT01].

When fluctuations of flow velocity occur at very small spatial and temporal scales, their numerical approximation becomes very difficult if not impossible. In those cases one typically resorts to the so-called turbulence models: the latter allow the approximate description of this flow behaviour through either algebraic or differential equations. This topic will not be addressed in this monograph. The interested readers may consult, e.g., [Wil98] for a description of the physical aspects of turbulent flows, [HYR08] for multiscale analysis of incompressible flows, [Le 05] for modelling aspects of multiscale systems, [MP94] for the analysis of one of the most widely used turbulence models, the so-called $\kappa-\varepsilon$ model. [Sag06] and [BIL06] provide the analysis of the so-called Large $E d d y$ model, which is more computationally expensive but in principle better suited to provide a more realistic description of turbulent flow fields.

Finally, let us mention the Euler equations introduced in (16.21), which are used for both compressible or incompressible flows in those cases in which the viscosity can be neglected. Formally speaking, this corresponds to taking the Reynolds number equal to infinity.

By eliminating the pressure, the Navier-Stokes equations can be rewritten in $r e-$ duced form in the sole variable $\mathbf{u}$. With this aim let us introduce the following subspaces of $\left[\mathrm{H}^{1}(\Omega)\right]^{d}$ :

$$
V_{\text {div }}=\left\{\mathbf{v} \in\left[\mathrm{H}^{1}(\Omega)\right]^{d}: \operatorname{div} \mathbf{v}=0\right\}, \quad \mathrm{V}_{\mathrm{div}}^{0}=\left\{\mathbf{v} \in \mathrm{V}_{\mathrm{div}}: \mathbf{v}=\mathbf{0} \text { on } \Gamma_{\mathrm{D}}\right\}
$$

Upon requiring the test function $\mathbf{v}$ in the momentum equation in $(17.9)$ to belong to the space $V_{\text {div }}$, the term associated to the pressure gradient vanishes, whence we find the following reduced problem for the velocity find $\mathbf{u} \in \mathrm{L}^{2}\left(\mathbb{R}^{+} ; V_{\mathrm{div}}\right) \cap C^{0}\left(\mathbb{R}^{+} ;\left[\mathrm{L}^{2}(\Omega)\right]^{d}\right)$ such that

$$
\begin{aligned}
\int_{\Omega} \frac{\partial \mathbf{u}}{\partial t} \cdot \mathbf{v} d \Omega &+v \int_{\Omega} \nabla \mathbf{u} \cdot \nabla \mathbf{v} d \Omega+\int_{\Omega}[(\mathbf{u} \cdot \nabla) \mathbf{u}] \cdot \mathbf{v} d \Omega \\
&=\int_{\Omega} \mathbf{f} \cdot \mathbf{v} d \Omega+\int_{\Gamma_{N}} \psi \cdot \mathbf{v} d \gamma \quad \forall \mathbf{v} \in V_{\mathrm{div}}^{0}
\end{aligned}
$$

with $\left.\mathbf{u}\right|_{\Gamma_{D}}=\varphi_{D}$ and $\left.\mathbf{u}\right|_{t=0}=\mathbf{u}_{0}$. Since this problem is (nonlinear) parabolic, its analysis can be carried out using techniques similar to those applied in Chapter 5 . (See, e.g., [Sal08].) Obviously, if $\mathbf{u}$ is a solution of (17.9), then it also solves (17.10). Conversely, the following theorem holds. For its proof, see, e.g., [QV94].

Theorem 17.1. Let $\Omega \subset \mathbb{R}^{d}$ be a domain with Lipschitz-continuous boundary

$\partial \Omega$. Let $\mathbf{u}$ be a solution to the reduced problem (17.10). Then there exists a

unique function $p \in \mathrm{L}^{2}\left(\mathbb{R}^{+} ; Q\right)$ such that $(\mathbf{u}, p)$ is a solution to $(17.9)$

Once the reduced problem is solved, there exists a unique way to recover the pressure $p$, and hence the complete solution of the original Navier-Stokes problem (17.9).

In practice, however, this approach can be quite unsuitable from a numerical viewpoint. Indeed, in a Galerkin spatial approximation framework, it would require the construction of finite dimensional subspace, say $V_{\mathrm{div}, h}$, of divergence-free velocity functions. In this regard, see, e.g., [BF91a] and [BBF13] for finite element approximations, and [CHQZ06] for spectral approximations. Moreover, the result of Theorem $17.1$ is not constructive, as it does not provide a way to build the solution pressure $p$. For these reasons one usually prefers to approximate the complete coupled problem (17.9) directly.

\subsection{Stokes equations and their approximation}

In this section we will consider the following generalized Stokes problem with homogeneous Dirichlet boundary conditions

$$
\begin{cases}\alpha \mathbf{u}-v \Delta \mathbf{u}+\nabla p=\mathbf{f} & \text { in } \Omega \\ \operatorname{div} \mathbf{u}=0 & \text { in } \Omega \\ \mathbf{u}=\mathbf{0} & \text { on } \partial \Omega\end{cases}
$$

for a given coefficient $\alpha \geq 0$. This problem describes the motion of an incompressible viscous flow in which the (quadratic) convective term has been neglected, a simplification that is acceptable when $\operatorname{Re} \ll 1$. However, one can generate a problem like (17.11) also while using an implicit temporal discretization of the Navier-Stokes equations, as we will see in Sect. 17.7. From an analytical standpoint, Navier-Stokes equations can be regarded as a compact perturbation of Stokes' equations, as they differ from the latter solely because of the presence of the convective term, which is of first order, whereas the diffusive term is of second order. On the other hand, it is fair to say that this term can have a fundamental impact on the solution's behaviour when the Reynolds number $\operatorname{Re}$ is very large, as already pointed out.

The weak formulation of problem (17.11) reads:

find $\mathbf{u} \in V$ and $p \in Q$ such that

$$
\begin{cases}\int_{\Omega}(\alpha \mathbf{u} \cdot \mathbf{v}+v \nabla \mathbf{u} \cdot \nabla \mathbf{v}) d \Omega-\int_{\Omega} p \operatorname{div} \mathbf{v} d \Omega=\int_{\Omega} \mathbf{f} \cdot \mathbf{v} d \Omega \quad \forall \mathbf{v} \in V \\ \int_{\Omega} q \operatorname{divu} d \Omega=0 & \forall q \in Q\end{cases}
$$

where $V=\left[\mathrm{H}_{0}^{1}(\Omega)\right]^{d}$ and $Q=\mathrm{L}_{0}^{2}(\Omega)$. Now define the bilinear forms $a: V \times V \mapsto \mathbb{R}$ and $b: V \times Q \mapsto \mathbb{R}$ as follows:

$$
\begin{aligned}
&a(\mathbf{u}, \mathbf{v})=\int_{\Omega}(\alpha \mathbf{u} \cdot \mathbf{v}+v \nabla \mathbf{u} \cdot \nabla \mathbf{v}) d \Omega \\
&b(\mathbf{u}, q)=-\int_{\Omega} q \operatorname{divu} d \Omega
\end{aligned}
$$

With these notations, problem (17.12) becomes: find $(\mathbf{u}, p) \in V \times Q$ such that

$$
\begin{cases}a(\mathbf{u}, \mathbf{v})+b(\mathbf{v}, p)=(\mathbf{f}, \mathbf{v}) & \forall \mathbf{v} \in V \\ b(\mathbf{u}, q)=0 & \forall q \in Q\end{cases}
$$

where $(\mathbf{f}, \mathbf{v})=\sum_{i=1}^{d} \int_{\Omega} f_{i} v_{i} d \Omega$

If we consider non-homogeneous boundary conditions, as indicated in $(17.4)$, the weak formulation of the Stokes problem becomes: find $(\dot{\mathbf{u}}, p) \in V \times Q$ such that

$$
\begin{cases}a(\hat{\mathbf{u}}, \mathbf{v})+b(\mathbf{v}, p)=\mathbf{F}(\mathbf{v}) & \forall \mathbf{v} \in V \\ b(\hat{\mathbf{u}}, q)=G(q) & \forall q \in Q\end{cases}
$$

where $V$ and $Q$ are the spaces introduced in (17.7) and (17.8), respectively. Having denoted with $\mathbf{R} \boldsymbol{\varphi} \in\left[\mathrm{H}^{1}(\Omega)\right]^{d}$ a lifting of the boundary datum $\varphi$, we have set $\stackrel{\mathbf{u}}=$ $\mathbf{u}-\mathbf{R} \varphi$, while the new terms on the right-hand side have the following expression:

$$
\mathbf{F}(\mathbf{v})=(\mathbf{f}, \mathbf{v})+\int_{\Gamma_{N}} \psi \mathbf{v} d \gamma-a(\mathbf{R} \varphi, \mathbf{v}), \quad G(q)=-b(\mathbf{R} \varphi, q)
$$

The following result holds:

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-081.jpg?height=239&width=720&top_left_y=159&top_left_x=97)

The pressure $q$ hence plays the role of Lagrange multiplier associated to the divergencefree constraint. As we saw in Sect. 17.1 for the Navier-Stokes equations, it is possible to eliminate, formally, the variable $p$ from the Stokes equations, thus obtaining the following reduced Stokes problem (in weak form)

$$
\text { find } \mathbf{u} \in V_{\mathrm{div}}^{0} \quad \text { such that } \quad a(\mathbf{u}, \mathbf{v})=(\mathbf{f}, \mathbf{v}) \quad \forall \mathbf{v} \in V_{\text {div }}^{0}
$$

This is an elliptic problem for the vector variable $\mathbf{u}$. Existence and uniqueness can be proven using the Lax-Milgram Lemma 3.1. As a matter of fact, $V_{\text {div }}^{0}$ is a Hilbert space with respect to the $\operatorname{norm}\|\nabla \mathbf{v}\|_{\mathrm{L}^{2}(\Omega)}$, because the divergence operator is continuous from $V$ into $\mathrm{L}^{2}(\Omega)$, thus $V_{\mathrm{div}}^{0}$ is a closed subspace of the space $V$. Moreover, the bilinear form $a(\cdot, \cdot)$ is continuous and coercive in $V_{\text {div }}^{0}$, and $\mathbf{f} \in V_{\text {div }}^{\prime}$. Using the CauchySchwarz and Poincaré inequalities, the following estimates can be obtained by taking $\mathbf{v}=\mathbf{u}$ in $(17.17))$

$$
\begin{aligned}
\frac{\alpha}{2}\|\mathbf{u}\|_{\mathrm{L}^{2}(\Omega)}^{2}+v\|\nabla \mathbf{u}\|_{\mathrm{L}^{2}(\Omega)}^{2} & \leq \frac{1}{2 \alpha}\|\mathbf{f}\|_{\mathrm{L}^{2}(\Omega)}^{2}, & \text { if } \alpha \neq 0, \\
\|\nabla \mathbf{u}\|_{L^{2}(\Omega)} & \leq \frac{C_{\Omega}}{v}\|f\|_{L^{2}(\Omega)}, & \text { if } \alpha=0,
\end{aligned}
$$

where $C_{\Omega}$ is the constant of the Poincaré inequality (2.13). Note that the pressure has disappeared from (17.17). However, still from (17.17) we can infer that the vector $\mathbf{w}=\alpha \mathbf{u}-v \Delta \mathbf{u}-\mathbf{f}$, regarded as a linear functional of $\mathrm{H}^{-1}(\Omega)$, vanishes when applied to any vector function of $V_{\text {div }}^{0}$. Thanks to this property, there exists a unique function $p \in Q$ such that $\mathbf{w}=\nabla p$, that is $p$ satisfies the first equation of $(17.11)$ in distributional sense (this entails a suitable use of Thm. $16.7$ that we will introduce later; see, e.g., [QV94]). The couple $(\mathbf{u}, p)$ is therefore the unique solution of the weak problem (17.14).

The Galerkin approximation of problem (17.14) has the following form: find $\left(\mathbf{u}_{h}, p_{h}\right) \in V_{h} \times Q_{h}$ such that

$$
\begin{cases}a\left(\mathbf{u}_{h}, \mathbf{v}_{h}\right)+b\left(\mathbf{v}_{h}, p_{h}\right)=\left(\mathbf{f}, \mathbf{v}_{h}\right) & \forall \mathbf{v}_{h} \in V_{h} \\ b\left(\mathbf{u}_{h}, q_{h}\right)=0 & \forall q_{h} \in Q_{h}\end{cases}
$$

where $\left\{V_{h} \subset V\right\}$ and $\left\{Q_{h} \subset Q\right\}$ represent two families of finite-dimensional subspaces depending on a real discretization parameter $h$. If, instead, we consider problem (17.15)-(17.16) corresponding to non-homogeneous boundary data (17.4), the above formulation needs to be modified by using $\mathbf{F}\left(\mathbf{v}_{h}\right)$ on the right-hand side of the first equation and $G\left(q_{h}\right)$ on that of the second equation. These new functionals can be obtained from (17.16) by replacing $\mathbf{R} \varphi$ with the interpolant of $\varphi$ at the nodes of $\Gamma_{D}$ (and vanishing at all other nodes), and replacing $\psi$ with its interpolant at the nodes sitting on $\Gamma_{N}$. The algebraic formulation of problem (17.18) will be addressed in Sect. 17.4.

The following celebrated theorem is due to F. Brezzi [Bre74], and guarantees uniqueness and existence for problem (17.18):

Theorem 17.3. The Galerkin approximation (17.18) admits one and only one solution if the following conditions hold:

1. The bilinear form a $(\cdot, \cdot)$ is:

a) coercive, that is $\exists \alpha>0$ (possibly depending on h) such that

$$
a\left(\mathbf{v}_{h}, \mathbf{v}_{h}\right) \geq \alpha\left\|\mathbf{v}_{h}\right\|_{V}^{2} \quad \forall \mathbf{v}_{h} \in V_{h}^{*},
$$

where $V_{h}^{*}=\left\{\mathbf{v}_{h} \in V_{h}: b\left(\mathbf{v}_{h}, q_{h}\right)=0 \forall q_{h} \in Q_{h}\right\}$

b) continuous, that is $\exists \gamma>0$ such that

㟍

$$
\left|a\left(\mathbf{u}_{h}, \mathbf{v}_{h}\right)\right| \leq \gamma\left\|\mathbf{u}_{h}\right\|_{V}\left\|\mathbf{v}_{h}\right\|_{V} \quad \forall \mathbf{u}_{h}, \mathbf{v}_{h} \in V_{h}
$$

2. The bilinear form $b(\cdot, \cdot)$ is continuous, that is $\exists \delta>0$ such that

$$
\left|b\left(\mathbf{v}_{h}, q_{h}\right)\right| \leq \delta\left\|\mathbf{v}_{h}\right\|_{V}\left\|q_{h}\right\|_{Q} \quad \forall \mathbf{v}_{h} \in V_{h}, q_{h} \in Q_{h}
$$

3. Finally, there exists a positive constant $\beta$ (possibly depending on $\mathrm{h}$ ) such that

$$
\forall q_{h} \in Q_{h}, \exists \mathbf{v}_{h} \in V_{h}: b\left(\mathbf{v}_{h}, q_{h}\right) \geq \beta\left\|\mathbf{v}_{h}\right\|_{\mathbf{H}^{1}(\Omega)}\left\|q_{h}\right\|_{\mathrm{L}^{2}(\Omega)}
$$

$$
\text { (17.19) }
$$

Under the previous assumptions the discrete solution fulfills the following apriori estimates:

$$
\begin{aligned}
&\left\|\mathbf{u}_{h}\right\|_{V} \leq \frac{1}{\alpha}\|\mathbf{f}\|_{V^{\prime}} \\
&\left\|p_{h}\right\|_{Q} \leq \frac{1}{\beta}\left(1+\frac{\gamma}{\alpha}\right)\|\mathbf{f}\|_{V^{\prime}}
\end{aligned}
$$

where $V^{\prime}$ is the dual space of $V$. Moreover, the following convergence results hold:

$$
\begin{aligned}
&\left\|\mathbf{u}-\mathbf{u}_{h}\right\|_{V} \leq\left(1+\frac{\delta}{\beta}\right)\left(1+\frac{\gamma}{\alpha}\right) \inf _{\mathbf{v}_{h} \in V_{h}}\left\|\mathbf{u}-\mathbf{v}_{h}\right\|_{V}+\frac{\delta}{\alpha} \inf _{q_{h} \in Q_{h}}\left\|p-q_{h}\right\|_{Q} \\
&\left\|p-p_{h}\right\|_{Q} \leq \frac{\gamma}{\beta}\left(1+\frac{\gamma}{\alpha}\right)\left(1+\frac{\delta}{\beta}\right) \inf _{\mathbf{v}_{h} \in V_{h}}\left\|\mathbf{u}-\mathbf{v}_{h}\right\|_{V}
\end{aligned}
$$

It is worth noticing that condition (17.19) is equivalent the existence of a positive constant $\beta$ such that

$$
\inf _{q_{h} \in Q_{h}, q_{h} \neq 0} \sup _{\mathbf{v}_{h} \in V_{h}, \mathbf{v}_{h} \neq 0} \frac{b\left(\mathbf{v}_{h}, q_{h}\right)}{\left\|\mathbf{v}_{h}\right\|_{\mathbf{H}^{1}(\Omega)}\left\|q_{h}\right\|_{\mathrm{L}^{2}(\Omega)}} \geq \beta
$$

For such a reason it is often called the inf-sup condition.

The proof of this theorem requires non-elementary tools of functional analysis. It will be given in Sect. 17.3 for a saddle-point problem that is more general than Stokes' problem. In this perspective, Theorem $17.3$ can be regarded as a special case of Theorems $17.5$ and 17.6. The reader not interested in the theoretical aspects can skip the next section and jump directly to Sect. 17.4.

\subsection{Saddle-point problems}

The scope of this section is to study problems (17.14) and (17.18) and show how the latter's solutions converge to the former's. With this aim we will recast those formulations within a more abstract framework, that will eventually allow the use of the theory proposed in [Bre74, Bab71, BBF13].

\subsubsection{Problem formulation}

Let $X$ and $M$ be two Hilbert spaces endowed with norms $\|\cdot\|_{X}$ and $\|\cdot\|_{M}$. Denoting with $X^{\prime}$ and $M^{\prime}$ the corresponding dual spaces (that is the spaces of linear and bounded functionals defined on $X$ and $M$ ), we introduce the bilinear forms $a(\cdot, \cdot): X \times X \longrightarrow \mathbb{R}$ and $b(\cdot, \cdot): X \times M \longrightarrow \mathbb{R}$ that we suppose to be continuous, meaning there exist two constants $\gamma, \delta>0$ such that for all $w, v \in X$ and $\mu \in M$,

$$
|a(w, v)| \leq \gamma\|w\|_{X}\|v\|_{X}, \quad|b(w, \mu)| \leq \delta\|w\|_{X}\|\mu\|_{M}
$$

Consider now the following constrained problem: find $(u, \eta) \in X \times M$ such that

$$
\begin{cases}a(u, v)+b(v, \eta)=\langle l, v\rangle & \forall v \in X \\ b(u, \mu)=\langle\sigma, \mu\rangle & \forall \mu \in M\end{cases}
$$

where $l \in X^{\prime}$ and $\sigma \in M^{\prime}$ are two assigned linear functionals, while $\langle\cdot, \cdot\rangle$ denotes the pairing between $X$ and $X^{\prime}$ or $M$ and $M^{\prime}$.

Formulation (17.22) is general enough to include the formulation (17.14) of the Stokes problem, that of a generic constrained problem with respect to the bilinear form $a(\cdot, \cdot)$ (with $\eta$ representing the constraint), or again the formulation which is obtained when mixed finite element approximations are used for various kind of boundary-value problems, for instance those of linear elasticity (see, e.g., [BF91a, QV94]).

Problem (17.22) can be conveniently restated in operator form. For this we associate the bilinear forms $a(\cdot, \cdot)$ and $b(\cdot, \cdot)$ with the operators $A \in \mathscr{L}\left(X, X^{\prime}\right)$ and $B \in$ $\mathscr{L}\left(X, M^{\prime}\right)$, defined through the following relations:

$$
\begin{array}{ll}
\langle A w, v\rangle=a(w, v) & \forall w, v \in X \\
\langle B v, \mu\rangle=b(v, \mu) & \forall v \in X, \mu \in M .
\end{array}
$$

The relations pair $X^{\prime}$ to $X$ and $M^{\prime}$ to $M$. In accordance with the notations introduced in Sect. 4.5.3, we denote by $\mathscr{L}(V, W)$ the space of linear and bounded functionals between $V$ and $W$.

Let $B^{T} \in \mathscr{L}\left(M, X^{\prime}\right)$ be the adjoint operator of $B$ defined by

$$
\left\langle B^{T} \mu, v\right\rangle=\langle B v, \mu\rangle=b(v, \mu) \quad \forall v \in X, \mu \in M
$$

The former duality holds between $X^{\prime}$ and $X$, the latter between $M^{\prime}$ and $M$. (This operator was denoted by the symbol $B^{\prime}$ in Sect. $2.6$, see the general definition $(2.20)$. Here, however, it is denoted by $B^{T}$ for consistency with the classical notation used in the framework of saddle-point problems.)

In operator form, the saddle-point problem (17.22) can be restated as follows: find $(u, \eta) \in X \times M$ such that

$$
\begin{cases}A u+B^{T} \eta=l & \text { in } X^{\prime} \\ B u=\sigma & \text { in } M^{\prime}\end{cases}
$$

\subsubsection{Analysis of the problem}

In order to analyze problem (17.24), we introduce the affine manifold

$$
X^{\sigma}=\{v \in X: b(v, \mu)=\langle\sigma, \mu\rangle \forall \mu \in M\}
$$

The space $X^{0}$ denotes the kernel of $B$, that is

$$
X^{0}=\{v \in X: b(v, \mu)=0 \forall \mu \in M\}=\operatorname{ker}(B)
$$

This is a closed subspace of $X$. We can therefore associate (17.22) with the following reduced problem

$$
\text { find } u \in X^{\sigma} \quad \text { such that } \quad a(u, v)=\langle l, v\rangle \quad \forall v \in X^{0} \text {. }
$$

If $(u, \eta)$ is a solution of $(17.22)$, then $u$ is a solution to (17.26). In the following we will introduce suitable conditions that allow the converse to hold, too. Moreover, we would like to prove uniqueness for the solution of (17.26). This would allow us to obtain an existence and uniqueness result for the original saddle-point problem (17.22).

We denote by $X_{\text {polar }}^{0}$ the polar set of $X^{0}$, that is

$$
X_{\text {polar }}^{0}=\left\{g \in X^{\prime}:\langle g, v\rangle=0 \forall v \in X^{0}\right\}
$$

See Fig. 17.1 for a schematic picture. Since $X^{0}=\operatorname{ker}(B)$, we have $X_{\text {polar }}^{0}=(\operatorname{ker}(B))_{\text {polar }}$. The space $X$ is a direct sum of $X^{0}$ and its orthogonal space $\left(X^{0}\right)^{\perp}$,

$$
X=X^{0} \oplus\left(X^{0}\right)^{\perp}
$$



![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-085.jpg?height=315&width=516&top_left_y=110&top_left_x=200)

Fig. 17.1. The spaces $X$ and $M$, the dual spaces $X^{\prime}$ and $M^{\prime}$, the subspaces $X^{0},\left(X^{0}\right)^{\perp}$ and $X_{\text {polar }}^{0}$, and the operators $A, B$ and $B^{T}$. Dashed lines indicate isomorphisms

Since, in general, $\operatorname{ker}(B)$ is not empty, we cannot expect $B$ to be an isomorphism between $X$ and $M^{\prime}$. The aim is to find a condition which guarantees that $B$ is an isomorphism between $\left(X^{0}\right)^{\perp}$ and $M^{\prime}$ (and, similarly, that $B^{T}$ is an isomorphism between $M$ and $\left.X_{\text {polar }}^{0}\right)$.

Proof. First of all we prove the equivalence between a. and b. Owing to definition (17.23) of $B^{T}$, the two inequalities (17.27) and (17.28) do coincide. Let us now prove that $B^{T}$ is an isomorphism between $M$ and $X_{\text {polar }}^{0}$. From (17.28) it follows that $B^{T}$ is an injective operator from $M$ into its range $\mathscr{R}\left(B^{T}\right)$, with continuous inverse. Then $\mathscr{R}\left(B^{T}\right)$ is a closed subspace of $X^{\prime}$. It remains to prove that $\mathscr{R}\left(B^{T}\right)=X_{\text {polar }}^{0}$. From the closed range theorem (see, e.g., [Yos74]), we have

$$
\mathscr{R}\left(B^{T}\right)=(\operatorname{ker}(B))_{\text {polar }}
$$

whence $\mathscr{R}\left(B^{T}\right)=X_{\text {polar }}^{0}$, which is the desired result.

Let us now prove the equivalence between b. and $\mathrm{c}$.. The space $X_{\text {polar }}^{0}$ can be identified with the dual space of $\left(X^{0}\right)^{\perp}$. As a matter of fact, to every $g \in\left(\left(X^{0}\right)^{\perp}\right)^{\prime}$ we can associate a functional $\hat{g} \in X^{\prime}$ which satisfies the relation

$$
\langle\hat{g}, v\rangle=\left\langle g, P^{\perp} v\right\rangle \quad \forall v \in X
$$

where $P^{\perp}$ denotes the orthogonal projection of $X$ onto $\left(X^{0}\right)^{\perp}$, that is

$$
\forall v \in X, \quad P^{\perp} v \in\left(X^{0}\right)^{\perp}:\left(P^{\perp} v-v, w\right)_{X}=0 \quad \forall w \in\left(X^{0}\right)^{\perp}
$$

Clearly, $\hat{g} \in X_{\text {polar }}^{0}$ and it can be verified that $g \longrightarrow \hat{g}$ is an isometric bijection between $\left(\left(X^{0}\right)^{\perp}\right)^{\prime}$ and $X_{\text {polar }}^{0}$. Consequently, $B^{T}$ is an isomorphism from $M$ onto $\left(\left(X^{0}\right)^{\perp}\right)^{\prime}$ satisfying the relation

$$
\left\|\left(B^{T}\right)^{-1}\right\|_{\mathscr{L}\left(X_{\text {polar }}^{0}, M\right)} \leq \frac{1}{\beta^{*}}
$$

if and only if $B$ is an isomorphism from $\left(X^{0}\right)^{\perp}$ onto $M^{\prime}$ satisfying the relation

$$
\left\|B^{-1}\right\|_{\mathscr{L}\left(M^{\prime},\left(X^{0}\right)^{\perp}\right)} \leq \frac{1}{\beta^{*}}
$$

This completes our proof.

At this point we can prove that problem $(17.22)$ is well posed.

Theorem 17.4. Let the bilinear form $a(\cdot, \cdot)$ satisfy the continuity condition (17.21) and be coercive on the space $X^{0}$, that is

$$
\text { in }
$$
1.

1.

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-086.jpg?height=39&width=20&top_left_y=895&top_left_x=774)

$$
\exists \alpha>0: a(v, v) \geq \alpha\|v\|_{X}^{2} \quad \forall v \in X^{0}
$$
(1)

$(17.30)$

Suppose moreover that the bilinear form $b(\cdot, \cdot)$ satisfies the continuity condition (17.21) as well as the compatibility condition $(17.27)$.

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-086.jpg?height=33&width=32&top_left_y=994&top_left_x=766)

Then for every $l \in X^{\prime}$ and $\sigma \in M^{\prime}$, there exists a unique solution $u$ of problem (17.26); furthermore, there exists a unique function $\eta \in M$ such that $(u, \eta)$ is the unique solution to the original saddle-point problem (17.22).

Moreover, the map $(l, \sigma) \longrightarrow(u, \eta)$ is an isomorphism from $X^{\prime} \times M^{\prime}$ onto $X \times M$ and the following a priori estimates hold:

$$
\|u\|_{X} \leq \frac{1}{\alpha}\left[\|l\|_{X^{\prime}}+\frac{\alpha+\gamma}{\beta^{*}}\|\sigma\|_{M^{\prime}}\right]
$$


The symbols $\|\cdot\|_{X^{\prime}}$ and $\|\cdot\|_{M^{\prime}}$ indicate the norms of the dual spaces, and are
defined as in $(17.28)$ and $(17.29)$, respectively.

Proof. The uniqueness of the solution to (17.26) directly follows from the coercivity property (17.30). Let us now prove existence. From assumption (17.27) and the equivalence result stated in c. of Lemma 17.1, we can infer that there exists a unique function $u^{\sigma} \in\left(X^{0}\right)^{\perp}$ such that $B u^{\sigma}=\sigma$, and, moreover,

$$
\left\|u^{\sigma}\right\|_{X} \leq \frac{1}{\beta^{*}}\|\sigma\|_{M^{\prime}}
$$

The saddle-point problem (17.26) can be restated as follows

$$
\text { find } \widetilde{u} \in X^{0} \quad \text { such that } \quad a(\widetilde{u}, v)=\langle l, v\rangle-a\left(u^{\sigma}, v\right) \quad \forall v \in X^{0} \text {. }
$$

The solution $u$ to problem (17.26) is identified by the relation $u=\widetilde{u}+u^{\sigma}$. At this point, existence and uniqueness of the solution $\widetilde{u}$ of problem (17.34) follow by the Lax-Milgram Lemma, together with the a priori estimate

$$
\|\widetilde{u}\|_{X} \leq \frac{1}{\alpha}\left(\|l\|_{X^{\prime}}+\gamma\left\|u^{\sigma}\right\|_{X}\right),
$$

that is, thanks to $(17.33)$,

$$
\|\widetilde{u}\|_{X} \leq \frac{1}{\alpha}\left(\|l\|_{X^{\prime}}+\frac{\gamma}{\beta^{*}}\|\sigma\|_{M^{\prime}}\right)
$$

The uniqueness of the $u$ component of the solution to problem $(17.22)$ is therefore a direct consequence of the uniqueness of $\widetilde{u} \in X^{0}$ and $u^{\sigma} \in\left(X^{0}\right)^{\perp}$, while the stability estimate (17.31) follows again from the combination of (17.35) with (17.33).

We focus now on the component $\eta$ of the solution. Since (17.34) can be restated as

$$
\langle A u-l, v\rangle=0 \quad \forall v \in X^{0}
$$

it follows that $(A u-l) \in X_{\text {polar }}^{0}$, so we can exploit point b. of Lemma $17.1$ and conclude that there exists a unique $\eta \in M$ such that $A u-l=-B^{T} \eta$, that is $(u, \eta)$ is a solution of problem (17.22) and $\eta$ satisfies the inequality

$$
\|\eta\|_{M} \leq \frac{1}{\beta^{*}}\|A u-l\|_{X^{\prime}}
$$

We have already noticed that every solution $(u, \eta)$ to (17.22) yields a solution $u$ to the reduced problem (17.26), whence the uniqueness of the solution of (17.22). Finally, the a priori estimate (17.32) follows from (17.36), by noting that

$$
\|\eta\|_{M} \leq \frac{1}{\beta^{*}}\left[\|A\|_{\mathscr{L}\left(X, X^{\prime}\right)}\|u\|_{X}+\|l\|_{X^{\prime}}\right]
$$

and using the already proven a priori estimate (17.31) on $u$. 

\subsubsection{Galerkin approximation, stability and convergence analysis}

To introduce a Galerkin approximation of the abstract saddle-point problem (17.22), we consider two families of finite-dimensional subspaces $X_{h}$ and $M_{h}$ of the spaces $X$ and $M$, respectively. They can be either finite element piecewise polynomial spaces, or global polynomial (spectral) spaces, or spectral element subspaces. We look for the solution to the following problem:

given $l \in X^{\prime}$ and $\sigma \in M^{\prime}$, find $\left(u_{h}, \eta_{h}\right) \in X_{h} \times M_{h}$ such that:

$$
\begin{cases}a\left(u_{h}, v_{h}\right)+b\left(v_{h}, \eta_{h}\right)=\left\langle l, v_{h}\right\rangle & \forall v_{h} \in X_{h} \\ b\left(u_{h}, \mu_{h}\right)=\left\langle\sigma, \mu_{h}\right\rangle & \forall \mu_{h} \in M_{h}\end{cases}
$$

By following what we did for the continuous problem, we can introduce the subspace

$$
X_{h}^{\sigma}=\left\{v_{h} \in X_{h}: b\left(v_{h}, \mu_{h}\right)=\left\langle\sigma, \mu_{h}\right\rangle \forall \mu_{h} \in M_{h}\right\}
$$

which allows us to introduce the following finite dimensional counterpart of the reduced formulation $(17.26)$

$$
\text { find } u_{h} \in X_{h}^{\sigma} \text { such that } a\left(u_{h}, v_{h}\right)=\left\langle l, v_{h}\right\rangle \quad \forall v_{h} \in X_{h}^{0} \text {. }
$$

Since, in general, $M_{h}$ is different from $M$, the space (17.38) is not necessarily a subspace of $X^{\sigma}$

Clearly, every solution $\left(u_{h}, \eta_{h}\right)$ of (17.37) yields a solution $u_{h}$ for the reduced problem (17.39). In this section we look for conditions that allow us to prove that the converse statement is also true, together with a result of stability and convergence for the solution of problem (17.37).

We start by proving the discrete counterpart of Theorem $17.4$. Proof. The proof can be obtained by repeating that of Theorem $17.4$, considering $X_{h}$ instead of $X, M_{h}$ instead of $M$, and simply noting that

$$
\|l\|_{X_{h}^{\prime}} \leq\|l\|_{X^{\prime}}, \quad\|\sigma\|_{M_{h}^{\prime}} \leq\|\sigma\|_{M^{\prime}}
$$

The coercivity condition (17.30) does not necessarily guarantee (17.40), as $X_{h}^{0} \not \subset$ $X^{0}$, nor does the compatibility condition (17.27) in general imply the discrete compatibility condition (17.41), due to the fact that $X_{h}$ is a proper subspace of $X$. Moreover, in the case in which the constants $\alpha_{h}$ and $\beta_{h}$ in (17.40) and (17.41) are independent of $h$, inequalities (17.42) and (17.43) provide the desired stability result.

Condition (17.41) represents the well known inf-sup or $L B B$ condition (see [BF91a]).

(The condition (17.19) (or (17.20)) is just a special case.)

We move now to the convergence result.

Proof. Consider $v_{h} \in X_{h}, v_{h}^{*} \in X_{h}^{\sigma}$ and $\mu_{h} \in M_{h}$. By subtracting $(17.37)_{1}$ from (17.22) $_{1}$, then adding and subtracting the quantities $a\left(v_{h}^{*}, v_{h}\right)$ and $b\left(v_{h}, \mu_{h}\right)$, we find

$$
a\left(u_{h}-v_{h}^{*}, v_{h}\right)+b\left(v_{h}, \eta_{h}-\mu_{h}\right)=a\left(u-v_{h}^{*}, v_{h}\right)+b\left(v_{h}, \eta-\mu_{h}\right)
$$

Let us now choose $v_{h}=u_{h}-v_{h}^{*} \in X_{h}^{0}$. From the definition of the space $X_{h}^{0}$ and using (17.40) and (17.21), we find the bound

$$
\left\|u_{h}-v_{h}^{*}\right\|_{X} \leq \frac{1}{\alpha_{h}}\left(\gamma\left\|u-v_{h}^{*}\right\|_{X}+\delta\left\|\eta-\mu_{h}\right\|_{M}\right)
$$

from which the estimate (17.44) immediately follows, as

$$
\left\|u-u_{h}\right\|_{X} \leq\left\|u-v_{h}^{*}\right\|_{X}+\left\|u_{h}-v_{h}^{*}\right\|_{X}
$$

Let us prove now the estimate (17.45). Owing to the compatibility condition (17.41), for every $\mu_{h} \in M_{h}$ we can write

$$
\left\|\eta_{h}-\mu_{h}\right\|_{M} \leq \frac{1}{\beta_{h}} \sup _{v_{h} \in X_{h}, v_{h} \neq 0} \frac{b\left(v_{h}, \eta_{h}-\mu_{h}\right)}{\left\|v_{h}\right\|_{X}} .
$$

On the other hand, by subtracting side by side $(17.37)_{1}$ from $(17.22)_{1}$, then adding and subtracting the quantity $b\left(v_{h}, \mu_{h}\right)$, we obtain

$$
b\left(v_{h}, \eta_{h}-\mu_{h}\right)=a\left(u-u_{h}, v_{h}\right)+b\left(v_{h}, \eta-\mu_{h}\right)
$$

Using this identity in (17.47) as well as the continuity inequalities (17.21), it follows that

$$
\left\|\eta_{h}-\mu_{h}\right\|_{M} \leq \frac{1}{\beta_{h}}\left(\gamma\left\|u-u_{h}\right\|_{X}+\delta\left\|\eta-\mu_{h}\right\|_{M}\right)
$$

This yields the desired result, provided we use the error estimate (17.44) that was previously derived for the variable $u$.

Finally, let us prove (17.46). Property (17.41) allows us to use the discrete version of Lemma $17.1$ (now applied in the finite-dimensional subspaces). Then, owing to the discrete counterpart of $(17.29)$, for every $v_{h} \in X_{h}$ we can find a unique function $z_{h} \in\left(X_{h}^{0}\right)^{\perp}$ such that

$$
b\left(z_{h}, \mu_{h}\right)=b\left(u-v_{h}, \mu_{h}\right) \quad \forall \mu_{h} \in M_{h}
$$

and, moreover,

$$
\left\|z_{h}\right\|_{X} \leq \frac{\delta}{\beta_{h}}\left\|u-v_{h}\right\|_{X}
$$

The function $v_{h}^{*}=z_{h}+v_{h}$ belongs to $X_{h}^{\sigma}$, as $b\left(u, \mu_{h}\right)=\left\langle\sigma, \mu_{h}\right\rangle$ for all $\mu_{h} \in M_{h}$. Moreover,

$$
\left\|u-v_{h}^{*}\right\|_{X} \leq\left\|u-v_{h}\right\|_{X}+\left\|z_{h}\right\|_{X} \leq\left(1+\frac{\delta}{\beta_{h}}\right)\left\|u-v_{h}\right\|_{X}
$$

whence the estimate (17.46) follows.

The inequalities (17.44) and (17.45) yield error estimates with optimal convergence rate, provided that the constants $\alpha_{h}$ and $\beta_{h}$ in (17.40) and (17.41) are bounded from below by two constants $\alpha$ and $\beta$ independent of $h$. Let us also remark that inequality (17.44) holds even if the compatibility conditions (17.27) and (17.41) are not satisfied.

Remark 17.2 (Spurious pressure modes). The compatibility condition (17.41) is essential to guarantee the uniqueness of the $\eta_{h}$-component of the solution. Indeed, if (17.41) does not hold, then

$$
\exists \mu_{h}^{*} \in M_{h}, \mu_{h}^{*} \neq 0 \text { s.t. } b\left(v_{h}, \mu_{h}^{*}\right)=0 \quad \forall v_{h} \in X_{h}
$$

Consequently, if $\left(u_{h}, \eta_{h}\right)$ is a solution to problem (17.37), then $\left(u_{h}, \eta_{h}+\tau \mu_{h}^{*}\right)$, for all $\tau \in \mathbb{R}$, is a solution, too. Any such function $\mu_{h}^{*}$ is called spurious mode, or, more specifically, pressure spurious mode when it refers to the Stokes problem (17.18) in which functions $\mu_{h}$ represent discrete pressures. Numerical instabilities can arise since the discrete problem (17.37) is unable to detect such spurious modes.

For a given couple of finite dimensional spaces $X_{h}$ and $M_{h}$, proving that the discrete compatibility condition (17.41) holds with a constant $\beta_{h}$ independent of $h$ is not always easy. Several practical criteria are available, among which we mention those due to Fortin ([For77]), Boland and Nicolaides ([BN83]), and Verfürth ([Ver84]). (See [BF91b].)

\subsection{Algebraic formulation of the Stokes problem}

Let us investigate the structure of the algebraic system associated to the Galerkin approximation (17.18) to the Stokes problem (or, more generally, to a discrete saddlepoint problem like (17.37)). Denote with

$$
\left\{\varphi_{j} \in V_{h}\right\}, \quad\left\{\phi_{k} \in Q_{h}\right\}
$$

the basis functions of the spaces $V_{h}$ and $Q_{h}$, respectively. Le us expand the discrete solutions $\mathbf{u}_{h}$ and $p_{h}$ with respect to such bases,

$$
\mathbf{u}_{h}(\mathbf{x})=\sum_{j=1}^{N} u_{j} \varphi_{j}(\mathbf{x}), \quad p_{h}(\mathbf{x})=\sum_{k=1}^{M} p_{k} \phi_{k}(\mathbf{x})
$$

having set $N=\operatorname{dim} V_{h}$ and $M=\operatorname{dim} Q_{h}$. By choosing as test functions in $(17.18)$ the same basis functions we obtain the following block linear system

$$
\left\{\begin{array}{l}
\mathrm{A} \mathbf{U}+\mathrm{B}^{T} \mathbf{P}=\mathbf{F} \\
\mathrm{B} \mathbf{U}=\mathbf{0}
\end{array}\right.
$$

where $\mathrm{A} \in \mathbb{R}^{N \times N}$ and $\mathrm{B} \in \mathbb{R}^{M \times N}$ are the matrices related respectively to the bilinear forms $a(\cdot, \cdot)$ and $b(\cdot, \cdot)$, whose elements are given by

$$
\mathrm{A}=\left[a_{i j}\right]=\left[a\left(\varphi_{j}, \boldsymbol{\varphi}_{i}\right)\right], \quad \mathrm{B}=\left[b_{k m}\right]=\left[b\left(\varphi_{m}, \phi_{k}\right)\right],
$$

while $\mathbf{U}$ and $\mathbf{P}$ are the vectors of the unknowns,

$$
\mathbf{U}=\left[u_{j}\right], \quad \mathbf{P}=\left[p_{j}\right]
$$

The $(N+M) \times(N+M)$ matrix

$$
\mathrm{S}=\left[\begin{array}{cc}
\mathrm{A} & \mathrm{B}^{T} \\
\mathrm{~B} & 0
\end{array}\right]
$$

is block symmetric (as A is symmetric) and indefinite, featuring real eigenvalues with variable sign (either positive and negative). $\mathrm{S}$ is non-singular iff no eigenvalue is null, a property that follows from the inf-sup condition (17.20). To prove the latter statement we proceed as follows.

Since $\mathrm{A}$ is non-singular $-\mathrm{it}$ is associated to the coercive bilinear form $a(\cdot, \cdot)-$ from the first of (17.49) we can formally obtain $\mathbf{U}$ as

$$
\mathbf{U}=\mathrm{A}^{-1}\left(\mathbf{F}-\mathbf{B}^{T} \mathbf{P}\right)
$$

Using (17.51) in the second equation of (17.49) yields

$$
\mathbf{R} \mathbf{P}=\mathrm{BA}^{-1} \mathbf{F}, \quad \text { where } \quad \mathrm{R}=\mathrm{BA}^{-1} \mathrm{~B}^{T}
$$

This corresponds to having carried out a block Gaussian elimination on system (17.50).

This way we obtain a reduced system for the sole unknown $\mathbf{P}$ (the pressure), which admits a unique solution in case $\mathrm{R}$ is non-singular. Since $\mathrm{A}$ is non-singular and positive definite, the latter condition is satisfied iff $\mathrm{B}^{T}$ has a null kernel, that is

$$
\operatorname{kerB}^{T}=\{\mathbf{0}\}
$$

where $\operatorname{kerB}^{T}=\left\{\mathbf{x} \in \mathbb{R}^{M}: B^{T} \mathbf{x}=\mathbf{0}\right\}$. The latter algebraic condition is in fact equivalent to the inf-sup condition (17.20) (see Exercise 1).

On the other hand, since $\mathrm{A}$ is non-singular, from the existence and uniqueness of $\mathbf{P}$ we infer that there exists a unique vector $\mathbf{U}$ which satisfies (17.51).

In conclusion, system (17.49) admits a unique solution $(\mathbf{U}, \mathbf{P})$ if and only if condition (17.53) holds.

Remark 17.3. Condition (17.53) is equivalent to asking that $B^{T}$ (and consequently $B$ ) has full rank, i.e. that $\operatorname{rank}\left(B^{T}\right)=\min (N, M)$, because rank $\left(B^{T}\right)$ is the maximum number of linearly independent row vectors (or, equivalently, column vectors) of $B^{T}$. In$\operatorname{deed}, \operatorname{rank}\left(B^{T}\right)+\operatorname{dim} \operatorname{ker}\left(B^{T}\right)=M$

Let us consider again Remark $17.2$ concerning the general saddle-point problem and suppose that the inf-sup condition $(17.20)$ does not hold. Then

$$
\exists q_{h}^{*} \in Q_{h}: \quad b\left(\mathbf{v}_{h}, q_{h}^{*}\right)=0 \quad \forall \mathbf{v}_{h} \in V_{h}
$$

Consequently, if $\left(\mathbf{u}_{h}, p_{h}\right)$ is a solution to the Stokes problem (17.18), then $\left(\mathbf{u}_{h}, p_{h}+q_{h}^{*}\right)$ is a solution too, as

$$
\begin{aligned}
a\left(\mathbf{u}_{h}, \mathbf{v}_{h}\right)+b\left(\mathbf{v}_{h}, p_{h}+q_{h}^{*}\right) &=a\left(\mathbf{u}_{h}, \mathbf{v}_{h}\right)+b\left(\mathbf{v}_{h}, p_{h}\right)+b\left(\mathbf{v}_{h}, q_{h}^{*}\right) \\
&=a\left(\mathbf{u}_{h}, \mathbf{v}_{h}\right)+b\left(\mathbf{v}_{h}, p_{h}\right)=\left(\mathbf{f}, \mathbf{v}_{h}\right) \quad \forall \mathbf{v}_{h} \in V_{h}
\end{aligned}
$$

Functions $q_{h}^{*}$ which fail to satisfy the inf-sup condition are invisible to the Galerkin problem(17.18). For this reason, as already observed, they are called spurious pressure modes, or even parasitic modes. Their presence inhibits the pressure solution from 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-093.jpg?height=116&width=500&top_left_y=110&top_left_x=205)

Fig. 17.2. Case of discontinuous pressure: choices that do not satisfy the inf-sup condition, on triangles (left), and on quadrilaterals (right)

being unique, yielding numerical instabilities. For this reason, those finite-dimensional subspaces that violate the compatibility condition (17.20) are said to be unstable, or incompatible.

Two strategies are generally adopted in order to guarantee well-posedness of the numerical problem:

- choose spaces $V_{h}$ and $Q_{h}$ that satisfy the inf-sup condition;

- stabilize (either a priori or a posteriori) the finite dimensional problem by eliminating the spurious modes.

Let us analyze the first type of strategy. To start with, we will consider the case of finite element spaces. To characterize $Q_{h}$ and $V_{h}$ it suffices to choose on every element of the triangulation their degrees of freedom. Since the weak formulation does not require a continuous pressure, we will consider first the case of discontinuous pressures.

As Stokes equations are of order one in $p$ and order two in $\mathbf{u}$, generally speaking it makes sense to use piecewise polynomials of degree $k \geq 1$ for the velocity space $V_{h}$ and of degree $k-1$ for the space $Q_{h}$.

In particular, we might want to use piecewise linear finite elements $\mathbb{P}_{1}$ for each velocity component, and piecewise constant finite elements $\mathbb{P}_{0}$ for the pressure (see Fig. $17.2$ in which, as in all those that will follow, by means of the symbol $\square$ we indicate the degrees of freedom for the pressure, whereas the symbol $\bullet$ identifies those for each velocity component). In fact, this choice, although being quite natural, does not pass the inf-sup test $(17.20)$ (see Exercise 3 ).

When looking for a compatible couple of spaces, the larger the velocity space $V_{h}$, the more likely the inf-sup condition is satisfied. Otherwise said, the space $V_{h}$ should be "rich" enough compared to the space $Q_{h}$. In Fig. $17.3$ we report three different choices of spaces that fulfill the inf-sup condition, still in the case of continuous ve-

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-093.jpg?height=123&width=261&top_left_y=1031&top_left_x=110)
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-093.jpg?height=124&width=690&top_left_y=1030&top_left_x=110angles, (a), and on quadrilaterals, (b). Also the couple (c), known as Crouzeix-Raviart elements, satisfies the inf-sup condition 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-094.jpg?height=264&width=504&top_left_y=110&top_left_x=204)

Fig. 17.4. Case of continuous pressure: the couples (a) and (b) do not satisfy the $\inf -s u p$ condition. The elements used for the velocity components in (c) are known as $\mathbb{P}_{1}$-iso $\mathbb{P}_{2}$ finite elements, whereas couple (d) is called mini-element

locity and discontinuous pressure. Choice (a) is made by $\mathbb{P}_{2}-\mathbb{P}_{0}$ elements, (b) by $\mathbb{Q}_{2}-\mathbb{P}_{0}$ elements, while choice (c) by piecewise linear discontinuous elements for the pressure, while the velocity components are made by piecewise quadratic continuous elements enriched by a cubic bubble function on each triangle $-$ these are the so-called Crouzeix-Raviart elements.

In Fig. 17.4(a), (b) we report two choices of incompatible finite elements in the case of continuous pressure. They consist of piecewise linear elements on triangles (resp. bilinear on quadrilaterals) for both velocity and pressure. More in general, finite elements of the same polynomial degree $k \geq 1$ for both velocity and pressures are unstable. In the same figure, the elements displayed in (c) and (d) are instead stable. In both cases, pressure is a piecewise linear continuous function, whereas velocities are piecewise linear polynomials on each of the four sub-triangles (case (c)), or piecewise linear polynomials enriched by a cubic bubble function (case (d)). The pair $\mathbb{P}_{2}-\mathbb{P}_{1}$ (continuous piecewise quadratic velocities and continuous piecewise linear pressure) is stable. This is the smallest degree representative of the family of the so-called Taylor-Hood elements $\mathbb{P}_{k}-\mathbb{P}_{k-1}, k \geq 2$ (continuous velocities and continuous pressure), that are inf-sup stable. For the proof of the stability results mentioned here, as well for the convergence analysis, the reader can refer to [BF91a].

If we use spectral methods, using equal-order polynomial spaces for both velocity and pressure yields subspaces that violate the inf-sup condition. Compatible spectral spaces can instead be obtained by using, e.g., polynomials of degree $N(\geq 2)$ for each velocity component, and degree $N-2$ for the pressure, yielding the so-called $\mathbb{Q}_{N}-\mathbb{Q}_{N-2}$ approximation. The degrees of freedom for each velocity component are represented by the $(N+1)^{2}$ GLL nodes introduced in Sect. 10.2.3 (see Fig. 17.5). For the pressure, at least two sets of interpolation nodes can be used: either the subset represented by the $(N-1)^{2}$ internal nodes of the set of $(N+1)^{2}$ GLL nodes (Fig. 17.6, left), or the $(N-1)^{2}$ Gauss nodes introduced in Sect. 10.2.2 (Fig. 17.6, right). This choice stands at the base of a spectral-type approximation, such as collocation, G-NI 

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-095.jpg?height=295&width=298&top_left_y=120&top_left_x=309)

Fig. 17.5. The $(N+1)^{2}$ Gauss-Legendre-Lobatto (GLL) nodes (here $N=6$ ), hosting the degrees of freedom of the velocity components
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-095.jpg?height=290&width=622&top_left_y=508&top_left_x=144)

Fig. 17.6. The $(N-1)^{2}$ internal Gauss-Legendre-Lobatto (GLL) nodes (left) and the $(N-1)^{2}$ Gauss-Legendre (GL) nodes (right) (here for $N=6$ ), hosting the degrees of freedom of the pressure

(Galerkin with numerical integration), or SEM-NI (spectral element with numerical integration) (see [CHQZ07]).

\subsection{An example of stabilized problem}

We have seen that finite element or spectral methods that make use of equal-degree polynomials for both velocity and pressure do not fulfill the inf-sup condition and are therefore unstable. However, stabilizing them is possible by SUPG or GLS techniques like those encountered in Chapter 13 in the framework of the numerical approximation of advection-diffusion equations. For a general discussion on stabilization techniques for Stokes equations, the reader can refer e.g. to [BF91a]. Here we limit ourselves to show how the GLS stabilization can be applied to problem (17.18) in case piecewise continuous linear finite elements are used for velocity components as well as for the pressure

$$
V_{h}=\left[\dot{X}_{h}^{1}\right]^{2}, \quad Q_{h}=\left\{q_{h} \in X_{h}^{1}: \int_{\Omega} q_{h} d \Omega=0\right\}
$$

This choice is urged by the need of keeping the global number of degrees of freedom as low as possible, especially when dealing with three-dimensional problems. We set therefore $W_{h}=V_{h} \times Q_{h}$ and, instead of (17.18), consider the following problem (we restrict ourselves to the case where $\alpha=0$ ):

$$
\text { find }\left(\mathbf{u}_{h}, p_{h}\right) \in W_{h}: A_{h}\left(\mathbf{u}_{h}, p_{h} ; \mathbf{v}_{h}, q_{h}\right)=\left(\mathbf{f}_{h}, \mathbf{v}_{h}\right) \quad \forall\left(\mathbf{v}_{h}, q_{h}\right) \in W_{h} .
$$

We have set

$$
\begin{aligned}
&A_{h}: W_{h} \times W_{h} \rightarrow \mathbb{R} \\
&A_{h}\left(\mathbf{u}_{h}, p_{h} ; \mathbf{v}_{h}, q_{h}\right)=a\left(\mathbf{u}_{h}, \mathbf{v}_{h}\right)+b\left(\mathbf{v}_{h}, p_{h}\right)-b\left(\mathbf{u}_{h}, q_{h}\right) \\
&+\delta \sum_{K \in \mathscr{T}_{h}} h_{K}^{2} \int_{K}\left(-v \Delta \mathbf{u}_{h}+\nabla p_{h}-\mathbf{f}\right)\left(-v \Delta \mathbf{v}_{h}+\nabla q_{h}\right) d K
\end{aligned}
$$

and we have denoted with $\delta$ a positive parameter that must be chosen conveniently. This is a strongly consistent approximation of problem (17.11): as a matter of fact, the additional term, which depends on the residual of the discrete momentum equation, is null when calculated on the exact solution as, thanks to $(17.12),-v \Delta \mathbf{u}+\nabla p-\mathbf{f}=\mathbf{0}$. (Note that, in this specific case, $\Delta \mathbf{u}_{h \mid K}=\Delta \mathbf{v}_{h \mid K}=\mathbf{0} \forall K \in \mathscr{T}_{h}$ as we are using piecewise linear finite element functions.).

From the identity

$$
A_{h}\left(\mathbf{u}_{h}, p_{h} ; \mathbf{u}_{h}, p_{h}\right)=v\left\|\nabla \mathbf{u}_{h}\right\|_{\mathbf{L}^{2}(\Omega)}^{2}+\delta \sum_{k \in \mathscr{T}_{h}} h_{K}^{2}\left\|\nabla p_{h}\right\|_{\mathbf{L}^{2}(K)}^{2}
$$

we deduce that the kernel of the bilinear form $A_{h}$ reduces to the null vector, whence problem (17.55) admits one and only one solution. The latter satisfies the stability inequality

$$
v\left\|\nabla \mathbf{u}_{h}\right\|_{\mathbf{L}^{2}(\Omega)}^{2}+\delta \sum_{K \in \mathscr{T}_{h}} h_{K}^{2}\left\|\nabla p_{h}\right\|_{\mathbf{L}^{2}(K)}^{2} \leq C\|\mathbf{f}\|_{\mathbf{L}^{2}(\Omega)}^{2}
$$

$C$ being a constant that depends on $v$ but not on $h$ (see Exercise 7).

By applying Strang's Lemma $10.1$ we can now show that the solution to the generalized Galerkin problem (17.55) satisfies the following error estimate

$$
\left\|\mathbf{u}-\mathbf{u}_{h}\right\|_{\mathbf{H}^{1}(\Omega)}+\left(\delta \sum_{K \in \mathscr{T}_{h}} h_{K}^{2}\left\|\nabla p-\nabla p_{h}\right\|_{\mathbf{L}^{2}(K)}^{2}\right)^{1 / 2} \leq C h
$$

Still using the notations of Sect. 17.2, we can show that (17.55) admits the following matrix form

$$
\left[\begin{array}{cc}
\mathrm{A} & \mathrm{B}^{T} \\
\mathrm{~B} & -\mathrm{C}
\end{array}\right]\left[\begin{array}{l}
\mathbf{U} \\
\mathbf{P}
\end{array}\right]=\left[\begin{array}{l}
\mathbf{F} \\
\mathbf{G}
\end{array}\right]
$$

This system differs from (17.49) without stabilization because of the presence of the non-null block occupying the position $(2,2)$, which is associated to the stabilization term. More precisely,

$$
C=\left(c_{k m}\right), c_{k m}=\delta \sum_{K \in \mathscr{T}_{h}} h_{K}^{2} \int_{K} \nabla \phi_{m} \cdot \nabla \phi_{k} d K, \quad k, m=1, \ldots, M
$$

while the components of the right-hand side $\mathbf{G}$ are

$$
g_{k}=-\delta \sum_{K \in \mathscr{T}_{h}} h_{K}^{2} \int_{K} \mathbf{f} \cdot \nabla \phi_{k} d K, \quad k=1, \ldots, M
$$

In this case, the reduced system for the pressure unknown reads

$$
\mathbf{R} \mathbf{P}=\mathbf{B A}^{-1} \mathbf{F}-\mathbf{G}
$$

In contrast to $(17.52)$, this time $\mathrm{R}=\mathrm{BA}^{-1} \mathrm{~B}^{T}+\mathrm{C}$. The matrix $\mathrm{R}$ is non-singular as $\mathrm{C}$ is a positive definite matrix.

\subsection{A numerical example}

We want to solve the stationary Navier-Stokes equations in the square domain $\Omega=$ $(0,1) \times(0,1)$ with the following Dirichlet conditions

$$
\begin{array}{ll}
\mathbf{u}=\mathbf{0}, & \mathbf{x} \in \partial \Omega \backslash \Gamma \\
\mathbf{u}=(1,0)^{T}, & \mathbf{x} \in \Gamma
\end{array}
$$

where $\Gamma=\left\{\mathbf{x}=\left(x_{1}, x_{2}\right)^{T} \in \partial \Omega: x_{2}=1\right\}$. This problem is known as flow in a liddriven cavity. We will use continuous piecewise bilinear $\mathbb{Q}_{1}-\mathbb{Q}_{1}$ polynomials on rectangular finite elements. As we know, these spaces do not fulfill the compatibility condition approximation; in Fig. 17.7, left, we display the spurious pressure modes that are generated by this Galerkin approximation. In the same figure, right, we have drawn the pressure isolines obtained using a GLS stabilization (addressed in the previous section) on the same kind of finite elements. The pressure is now free of numerical oscillations. Still for the stabilized problem, in Fig. 17.8 we display the streamlines for two different values of the Reynolds number, $\operatorname{Re}=1000$ and $\operatorname{Re}=5000$. The stabilization term amends simultaneously pressure instabilities (by getting rid of the spurious 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-098.jpg?height=324&width=674&top_left_y=122&top_left_x=116)

Fig. 17.7. Pressure isolines for the numerical approximation of the lid-driven cavity problem. Stabilized GLS approximation (on the right); the vertical line corresponds to the null value of the pressure. Non-stabilized approximation (on the left); the presence of a spurious numerical pressure is evident

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-098.jpg?height=327&width=325&top_left_y=582&top_left_x=119)
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-098.jpg?height=326&width=678&top_left_y=582&top_left_x=119) to two different values of the Reynolds number: $\operatorname{Re}=1000$, left, and $\operatorname{Re}=5000$, right

modes) and potential instabilities of the pure Galerkin method that develop when diffusion is dominated by convection, an issue that we have extensively addressed in Chapter $13$.

For the same problem we consider, as well, a spectral G-NI approximation in which the pressure and each velocity component are polynomials of $\mathbb{Q}_{N}$ (with $N=32$ ). As previously observed, this choice of spaces does not fulfill the inf-sup condition, and so it generates spurious pressure modes that are clearly visible in Fig. 17.9, left. A GLS stabilization, similar to that previously used for finite elements, can be set up for the G-NI method, too. The corresponding solution is now stable and free of spurious 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-099.jpg?height=300&width=628&top_left_y=115&top_left_x=143)

Fig. 17.9. Pressure isolines obtained by the pure spectral G-NI method (on the left), and by the GLS stabilized spectral G-NI method (on the right). In either case, polynomials of the same degree, $N=32$, are used for both pressure and velocity. As expected, the pure G-NI method yields spurious pressure solutions. The test case is the same lid-driven cavity problem previously approximated by bilinear finite elements

pressure modes, as the pressure isolines displayed on the right hand of the same figure show.

\subsection{Time discretization of Navier-Stokes equations}

Let us now return to the Navier-Stokes equations (17.2) and focus on the issue of time discretization. To avoid unnecessary cumbersome notation, from now on we will assume that $\Gamma_{D}=\partial \Omega$ and $\varphi=\mathbf{0}$ in (17.4), whence the velocity space becomes $V=$ $\left[\mathrm{H}_{0}^{1}(\Omega)\right]^{d}$. The space discretization of the Navier-Stokes equations yields the following problem: for every $t>0$, find $\left(\mathbf{u}_{h}(t), p_{h}(t)\right) \in V_{h} \times Q_{h}$ such that

$$
\left\{\begin{aligned}
\left(\frac{\partial \mathbf{u}_{h}(t)}{\partial t}, \mathbf{v}_{h}\right) &+a\left(\mathbf{u}_{h}(t), \mathbf{v}_{h}\right)+c\left(\mathbf{u}_{h}(t), \mathbf{u}_{h}(t), \mathbf{v}_{h}\right)+b\left(\mathbf{v}_{h}, p_{h}(t)\right) \\
&=\left(\mathbf{f}_{h}(t), \mathbf{v}_{h}\right) \forall \mathbf{v}_{h} \in V_{h} \\
b\left(\mathbf{u}_{h}(t), q_{h}\right) &=0 \quad \forall q_{h} \in Q_{h}
\end{aligned}\right.
$$

where, as usual, $\left\{V_{h} \subset V\right\}$ and $\left\{Q_{h} \subset Q\right\}$ are two families of finite dimensional subspaces of the velocity and pressure functional spaces, respectively. The trilinear form $c(\cdot, \cdot, \cdot)$, defined by

$$
c(\mathbf{w}, \mathbf{z}, \mathbf{v})=\int_{\Omega}[(\mathbf{w} \cdot \nabla) \mathbf{z}] \cdot \mathbf{v} d \Omega \quad \forall \mathbf{w}, \mathbf{z}, \mathbf{v} \in V
$$

is associated to the nonlinear convective term, while $a(\cdot, \cdot)$ and $b(\cdot, \cdot)$ are the same as in (17.13) (setting however $\alpha=0$ ).

Problem (17.60) is in fact a system of nonlinear differential algebraic equations. By using notations already employed in the previous sections, it can be restated in compact form as follows

$$
\left\{\begin{array}{l}
\mathrm{M} \frac{d \mathbf{u}(t)}{d t}+\mathrm{A} \mathbf{u}(t)+\mathrm{C}(\mathbf{u}(t)) \mathbf{u}(t)+\mathrm{B}^{T} \mathbf{p}(t)=\mathbf{f}(t) \\
\mathrm{B} \mathbf{u}(t)=\mathbf{0}
\end{array}\right.
$$

with $\mathbf{u}(0)=\mathbf{u}_{0} \cdot \mathrm{C}(\mathbf{u}(t))$ is in fact a matrix depending on $\mathbf{u}(t)$, whose generic coefficient is $c_{m i}(t)=c\left(\mathbf{u}(t), \varphi_{i}, \varphi_{m}\right)$. For the temporal discretization of this system let us use, for instance, the $\theta$-method, that was introduced in Sect. $5.1$ for parabolic equations. By setting

$$
\begin{aligned}
&\mathbf{u}_{\theta}^{n+1}=\theta \mathbf{u}^{n+1}+(1-\theta) \mathbf{u}^{n} \\
&\mathbf{p}_{\theta}^{n+1}=\theta \mathbf{p}^{n+1}+(1-\theta) \mathbf{p}^{n} \\
&\mathbf{f}_{\theta}^{n+1}=\theta \mathbf{f}\left(t^{n+1}\right)+(1-\theta) \mathbf{f}\left(t^{n}\right) \\
&\mathrm{C}_{\theta}\left(\mathbf{u}^{n+1, n}\right) \mathbf{u}^{n+1, n}=\theta \mathrm{C}\left(\mathbf{u}^{n+1}\right) \mathbf{u}^{n+1}+(1-\theta) \mathrm{C}\left(\mathbf{u}^{n}\right) \mathbf{u}^{n}
\end{aligned}
$$

we obtain the following system of algebraic equations

$$
\left\{\begin{array}{l}
\mathrm{M} \frac{\mathbf{u}^{n+1}-\mathbf{u}^{n}}{\Delta t}+\mathrm{Au}_{\theta}^{n+1}+\mathrm{C}_{\theta}\left(\mathbf{u}^{n+1, n}\right) \mathbf{u}^{n+1, n}+\mathrm{B}^{T} \mathbf{p}_{\theta}^{n+1}=\mathbf{f}_{\theta}^{n+1} \\
\mathrm{~B} \mathbf{u}^{n+1}=\mathbf{0}
\end{array}\right.
$$

Except for the special case $\theta=0$, which corresponds to the forward Euler method, the solution of this system is quite involved. A possible alternative is to use a semi-implicit scheme, in which the linear part of the equation is advanced implicitly, while nonlinear terms explicitly. By doing so, if $\theta \geq 1 / 2$, the resulting scheme is unconditionally stable, whereas it must obey a stability restriction on the time step $\Delta t$ (depending on $h$ and $v$ ) in all other cases. We further elaborate on this issue in the next section. Later, in Sects. 17.7.2 and 17.7.3 we will address other temporal discretization schemes. For more details, results and bibliographical references, see, e.g., [QV94, Chap. 13].

\subsubsection{Finite difference methods}

We consider at first an explicit temporal discretization of the first equation in $(17.61)$, corresponding to the choice $\theta=0$ in (17.62). If we suppose that all quantities are known at the time $t^{n}$, we can write the associated problem at time $t^{n}+1$ as follows

$$
\left\{\begin{array}{l}
\mathbf{M} \mathbf{u}^{n+1}=H\left(\mathbf{u}^{n}, \mathbf{p}^{n}, \mathbf{f}^{n}\right) \\
\mathbf{B} \mathbf{u}^{n+1}=\mathbf{0}
\end{array}\right.
$$

where $\mathrm{M}$ is the mass matrix whose entries are

$$
m_{i j}=\int_{\Omega} \boldsymbol{\varphi}_{i} \varphi_{j} d \Omega
$$

This system is overdetermined for the unknown vector $\mathbf{u}^{n+1}$, whereas it does not allow the determination of the pressure $\mathbf{p}^{n+1}$. However, if we replace $\mathbf{p}^{n}$ by $\mathbf{p}^{n+1}$ in the momentum equation, we obtain the new linear system

$$
\left\{\begin{array}{l}
\frac{1}{\Delta t} \mathbf{M} \mathbf{u}^{n+1}+\mathbf{B}^{T} \mathbf{p}^{n+1}=\mathbf{G} \\
\mathbf{B} \mathbf{u}^{n+1}=\mathbf{0}
\end{array}\right.
$$

G being a suitable known vector. This system corresponds to a semi-explicit discretization of (17.60). Since $\mathrm{M}$ is symmetric and positive definite, if condition $(17.53)$ is satisfied, then the reduced system $\mathrm{BM}^{-1} \mathrm{~B}^{T} \mathbf{p}^{n+1}=\mathrm{BM}^{-1} \mathbf{G}$ is non-singular. Once solved, the velocity vector $\mathbf{u}^{n+1}$ can be recovered from the first equation of (17.63). This discretization method is temporally stable provided the time step satisfies the following limitation

$$
\Delta t \leq C \min \left(\frac{h^{2}}{v}, \frac{h}{\max _{\mathbf{x} \in \Omega}\left|\mathbf{u}^{n}(\mathbf{x})\right|}\right)
$$

Let us now consider an implicit discretization of $(17.60)$, for instance the backward Euler method, which corresponds to choosing $\theta=1$ in (17.62). As already observed, this scheme is unconditionally stable. It yields a nonlinear algebraic system which can be regarded as the finite element space approximation to the steady Navier-Stokes problem

$$
\left\{\begin{array}{l}
-v \Delta \mathbf{u}^{n+1}+\left(\mathbf{u}^{n+1} \cdot \nabla\right) \mathbf{u}^{n+1}+\nabla p^{n+1}+\frac{\mathbf{u}^{n+1}}{\Delta t}=\tilde{\mathbf{f}} \\
\operatorname{div} \mathbf{u}^{n+1}=0
\end{array}\right.
$$

The solution of such nonlinear algebraic system can be achieved by Newton-Krylov techniques, that is by using a Krylov method (e.g. GMRES or BiCGStab) for the solution of the linear system that is obtained at each Newton iteration step (see, e.g., [Saa96] or [QV94, Chap. 2]). We recall that Newton's method is based on the full linearization of the convective term, $\mathbf{u}_{k}^{n+1} \cdot \nabla \mathbf{u}_{k+1}^{n+1}+\mathbf{u}_{k+1}^{n+1} \cdot \nabla \mathbf{u}_{k}^{n+1}$. A popular approach consists in starting Newton iterations after few Piccard iterations in which the convective term is evaluated as follows: $\mathbf{u}_{k}^{n+1} \cdot \nabla \mathbf{u}_{k+1}^{n+1}$. This approach entails three nested cycles:

- temporal iteration: $t^{n} \rightarrow t^{n+1}$;

- Newton iteration: $\mathbf{x}_{k}^{n+1} \rightarrow \mathbf{x}_{k+1}^{n+1}$;

- Krylov iteration: $\left[\mathbf{x}_{k}^{n+1}\right]_{j} \rightarrow\left[\mathbf{x}_{k}^{n+1}\right]_{j+1}$; for simplicity we have called $\mathbf{x}^{n}$ the couple $\left(\mathbf{u}^{n}, \mathbf{p}^{n}\right)$. Obviously, the goal is the following convergence result:

$$
\lim _{k \rightarrow \infty} \lim _{j \rightarrow \infty}\left[\mathbf{x}_{k}^{n+1}\right]_{j}=\left[\begin{array}{l}
\mathbf{u}^{n+1} \\
\mathbf{p}^{n+1}
\end{array}\right]
$$

Finally, let us operate a semi-implicit, temporal discretization, consisting in treating explicitly the nonlinear convective term. The following algebraic linear system, whose form is similar to (17.49), is obtained in this case

$$
\left\{\begin{array}{l}
\frac{1}{\Delta t} \mathrm{Mu}^{n+1}+\mathrm{A} \mathbf{u}^{n+1}+\mathrm{B}^{T} \mathbf{p}^{n+1}=\mathbf{G} \\
\mathrm{Bu}^{n+1}=\mathbf{0}
\end{array}\right.
$$

where $\mathbf{G}$ is a suitable known vector. In this case the stability restriction on the time step takes the following form

$$
\Delta t \leq C \frac{h}{\max _{\mathbf{x} \in \Omega}\left|\mathbf{u}^{n}(\mathbf{x})\right|}
$$

In all cases, optimal error estimates can be proven.

\subsubsection{Characteristics (or Lagrangian) methods}

The material derivative (also called Lagrangian derivative) of the velocity vector field is defined as

$$
\frac{D \mathbf{u}}{D t}=\frac{\partial \mathbf{u}}{\partial t}+(\mathbf{u} \cdot \nabla) \mathbf{u}
$$

Characteristics methods are based on approximating the material derivative, e.g. by the backward Euler method

$$
\frac{D \mathbf{u}}{D t}(\mathbf{x}) \approx \frac{\mathbf{u}^{n+1}(\mathbf{x})-\mathbf{u}^{n}\left(\mathbf{x}_{p}\right)}{\Delta t}
$$

where $\mathbf{x}_{p}$ is the foot (at time $t^{n}$ ) of the characteristic issuing from $\mathbf{x}$ at time $t^{n+1}$. A system of ordinary differential equations has to be solved to follow backwards the characteristic line $\mathbf{X}$ issuing from the point $\mathbf{x}$

$$
\left\{\begin{array}{l}
\frac{d \mathbf{X}}{d t}(t ; s, \mathbf{x})=\mathbf{u}(t, \mathbf{X}(t ; s, \mathbf{x})), \quad t \in\left(t^{n}, t^{n+1}\right) \\
\mathbf{X}(s ; s, \mathbf{x})=\mathbf{x}
\end{array}\right.
$$

having set $s=t^{n+1}$.

The main difficulty lies in determining the characteristic lines. The first problem is how to suitably approximate the velocity field $\mathbf{u}(\mathbf{t})$ for $t \in\left(t^{n}, t^{n+1}\right)$, as $\mathbf{u}^{n+1}$ is unknown. The simplest way to do so consists in using a forward Euler scheme for the discretization of the material derivative. The second difficulty stems from the fact that a characteristic line may cross several elements of the computational grid. An algorithm is therefore necessary to locate the element in which the characteristic foot falls, or to detect those cases in which the latter hits a boundary edge. With the previous discretization of the material derivative, at every time level $t^{n+1}$ the momentum equation becomes (formally)

$$
\frac{\mathbf{u}^{n+1}(\mathbf{x})-\mathbf{u}^{n}\left(\mathbf{x}_{p}\right)}{\Delta t}-v \Delta \mathbf{u}^{n+1}(\mathbf{x})+\nabla p^{n+1}(\mathbf{x})=\mathbf{f}^{n+1}(\mathbf{x})
$$

If used in the framework of piecewise linear finite elements in space, this scheme is unconditionally stable. Moreover, it satisfies the error estimate

$$
\left\|\mathbf{u}\left(t^{n}\right)-\mathbf{u}^{n}\right\|_{L^{2}(\Omega)} \leq C\left(h+\Delta t+h^{2} / \Delta t\right) \quad \forall n \geq 1
$$

for a positive constant $C$ independent of $v$. Characteristic-based time discretization strategies for spectral methods are reviewed in [CHQZ07, Chap. 3].

\subsubsection{Fractional step methods}

Let us consider an abstract time dependent problem,

$$
\frac{\partial w}{\partial t}+L w=f
$$

where $L$ is a differential operator that splits into the sum of two operators, $L_{1}$ and $L_{2}$, that is

$$
L v=L_{1} v+L_{2} v
$$

Fractional step methods allow the temporal advancement from time $t^{n}$ to $t^{n+1}$ in two steps (or more). At first only the operator $L_{1}$ is advanced in time implicitly, then the solution so obtained is corrected by performing a second step in which only the other operator, $L_{2}$, is in action. This is why these kind of methods are also named operator splitting.

In principle, by separating the two operators $L_{1}$ and $L_{2}$, a complex problem is split into two simpler problems, each one with its own feature. In this respect, the operators $L_{1}$ and $L_{2}$ can be chosen on the ground of physical considerations: diffusion can be split from transport, for instance. In fact, also the solution of Navier-Stokes equations by the characteristic method can be regarded as a fractional step method whose first step operator is expressed by the Lagrangian derivative.

A simple, albeit not optimal fractional step scheme, is the following, known as Yanenko splitting:

1. compute the solution $\tilde{w}$ of the equation

$$
\frac{\tilde{w}-w^{n}}{\Delta t}+L_{1} \tilde{w}=0
$$

2. compute the solution $w^{n+1}$ of the equation

$$
\frac{w^{n+1}-\tilde{w}}{\Delta t}+L_{2} w^{n+1}=f^{n}
$$

By eliminating $\tilde{w}$, the following problem is found for $w^{n+1}$

$$
\frac{w^{n+1}-w^{n}}{\Delta t}+L w^{n+1}=f^{n}+\Delta t L_{1}\left(f^{n}-L_{2} w^{n+1}\right)
$$

If both $L_{1}$ and $L_{2}$ are elliptic operators, this scheme is unconditionally stable with respect to $\Delta t$.

This strategy can be applied to the Navier-Stokes equations (17.2), choosing $L_{1}$ as $L_{1}(\mathbf{w})=-v \Delta \mathbf{w}+(\mathbf{w} \cdot \nabla) \mathbf{w}$ whereas $L_{2}$ is the operator associated to the remaining terms of the Navier-Stokes problem. In this way we have split the main difficulties arising when treating Navier-Stokes equations, the nonlinear part from that imposing the incompressibility constraint. The corresponding fractional step scheme reads:

1. solve the diffusion-transport equation for the velocity $\tilde{\mathbf{u}}^{n+1}$

$$
\begin{cases}\frac{\tilde{\mathbf{u}}^{n+1}-\mathbf{u}^{n}}{\Delta t}-v \Delta \tilde{\mathbf{u}}^{n+1}+\left(\mathbf{u}^{*} \cdot \nabla\right) \mathbf{u}^{* *}=\mathbf{f}^{n+1} & \text { in } \Omega \\ \widetilde{\mathbf{u}}^{n+1}=\mathbf{0} & \text { on } \partial \Omega\end{cases}
$$

2. solve the following coupled problem for the two unknowns $\mathbf{u}^{n+1}$ and $p^{n+1}$

$$
\begin{cases}\frac{\mathbf{u}^{n+1}-\tilde{\mathbf{u}}^{n+1}}{\Delta t}+\nabla p^{n+1}=\mathbf{0} & \text { in } \Omega \\ \operatorname{divu}^{n+1}=0 & \text { in } \Omega \\ \mathbf{u}^{n+1} \cdot \mathbf{n}=0 & \text { on } \partial \Omega\end{cases}
$$

where $\mathbf{u}^{*}$ and $\mathbf{u}^{* *}$ can be either $\tilde{\mathbf{u}}^{n+1}$ or $\mathbf{u}^{n}$ depending on whether the nonlinear convective terms are treated explicitly, implicitly or semi-implicitly. In such a way, in the first step an intermediate velocity $\tilde{\mathbf{u}}^{n+1}$ is calculated, then it is corrected in the second step in order to satisfy the incompressibility constraint. The diffusion-transport problem of the first step can be successfully addressed by using the approximation techniques investigated in Chapter $13$.

More involved is the numerical treatment of the problem associated with the second step. By formally applying the divergence operator to the first equation, we obtain

$$
\operatorname{div} \frac{\mathbf{u}^{n+1}}{\Delta t}-\operatorname{div} \frac{\tilde{\mathbf{u}}^{n+1}}{\Delta t}+\Delta p^{n+1}=0
$$

that is an elliptic boundary-value problem with Neumann boundary conditions

$$
\left\{\begin{array}{l}
-\Delta p^{n+1}=-\operatorname{div} \frac{\tilde{\mathbf{u}}^{n+1}}{\Delta t} \quad \text { in } \Omega \\
\frac{\partial p^{n+1}}{\partial n}=0 \quad \text { on } \partial \Omega
\end{array}\right.
$$

The Neumann condition follows from the condition $\mathbf{u}^{n+1} \cdot \mathbf{n}=0$ on $\partial \Omega$. From the solution of $(17.68)$ we obtain $p^{n+1}$, and thus $\mathbf{u}^{n+1}$ by using the first equation of (17.67),

$$
\mathbf{u}^{n+1}=\tilde{\mathbf{u}}^{n+1}-\Delta t \nabla p^{n+1} \quad \text { in } \Omega
$$

This is precisely the correction to operate on the velocity field in order to fulfill the divergence-free constraint.

In conclusion, at first we solve the elliptic system (17.66) to obtain the intermediate velocity $\tilde{\mathbf{u}}^{n+1}$, then the scalar elliptic problem (17.68) yields the pressure unknown $p^{n+1}$, and finally we obtain the new velocity field $\mathbf{u}^{n+1}$ through the explicit correction equation (17.69).

Let us now investigate the main features of this method.

Assume that we take $\mathbf{u}^{*}=\mathbf{u}^{* *}=\mathbf{u}^{n}$ in the first step; after space discretization, we arrive at a linear system as

$$
\left(\frac{1}{\Delta t} \mathrm{M}+\mathrm{A}\right) \tilde{\mathbf{u}}^{n+1}=\tilde{\mathbf{f}}^{n+1}
$$

Because of the explicit treatment of the convective term, the solution undergoes a stability restriction on the time step like (17.65). On the other hand, this linear system naturally splits into $d$ independent systems of smaller size, one for each spatial component of the velocity field.

If, instead, we use an implicit time advancing scheme, like the one that we would get by setting $\mathbf{u}^{*}=\mathbf{u}^{* *}=\tilde{\mathbf{u}}^{n+1}$, we obtain an unconditionally stable scheme, however with a more involved coupling of all the spatial components due to the nonlinear convective term. This nonlinear algebraic system can be solved by, e.g., a Newton-Krylov method, similar to the one that we have introduced in Sect. 17.7.1. In the second step of the method, we enforce a boundary condition only on the normal component of the velocity field. Yet, we lack any control on the behaviour of the tangential component. This generates a so-called splitting error: although the solution is divergence-free, the failure to satisfy the physical boundary condition on the tangential velocity component yields the onset of a pressure boundary layer of width $\sqrt{v \Delta t}$.

The method just described was originally proposed by Chorin and Temam, and is also called projection method. The reason can be found in the celebrated HelmholtzWeyl decomposition theorem:

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-105.jpg?height=266&width=722&top_left_y=966&top_left_x=97)

Owing to this result, any function $\mathbf{v} \in\left[\mathrm{L}^{2}(\Omega)\right]^{d}$ can be univocally represented as being the sum of a solenoidal (that is, divergence-free) field and of an irrotational field (that is, the gradient of a suitable scalar function).

As a matter of fact, after the first step (17.66) in which the preliminary velocity $\tilde{\mathbf{u}}^{n+1}$ is obtained from $\mathbf{u}^{n}$ by solving the momentum equation, in the course of the second step a solenoidal field $\mathbf{u}^{n+1}$ is constructed in (17.69), with $\mathbf{u}^{n+1} \cdot \mathbf{n}=0$ on $\partial \Omega$. This solenoidal field is the projection of $\widetilde{\mathbf{u}}^{n+1}$, and is obtained by applying the decomposition theorem with the following identifications: $\mathbf{v}=\tilde{\mathbf{w}}^{n+1}, \mathbf{v}=\mathbf{u}^{n+1}, \psi=+\Delta t p^{n+1}$.

The name projection method is due to the fact that

$$
\int_{\Omega} \mathbf{u}^{n+1} \cdot \psi d \Omega=\int_{\Omega} \tilde{\mathbf{u}}^{n+1} \cdot \psi d \Omega \quad \forall \psi \in \mathrm{H}_{\mathrm{div}}^{0}
$$

that is $\mathbf{u}^{n+1}$ is the projection, with respect to the scalar product of $\mathrm{L}^{2}(\Omega)$, of $\tilde{\mathbf{u}}^{n+1}$ on the space $\mathrm{H}_{\text {div }}^{0}$.

Remark 17.4. Several variants of the projection method have been proposed with the aim of reducing the splitting error on the pressure, not only for the finite element method but also for higher order spectral or spectral element space approximations. The interested reader can refer to, e.g., [QV94, Qua93, Pro97, KS05] and [CHQZ07, Chap. 3].

Example 17.1. In Fig. 17.10 we display the isolines of the modulus of velocity corresponding to the solution of Navier-Stokes equations in a two-dimensional domain $\Omega=(0,17) \times(0,10)$ with five round holes. This can be regarded as the orthogonal section of a three dimensional domain with 5 cylinders. A non-homogeneous Dirichlet condition, $\mathbf{u}=[\arctan (20(5-|5-y|)), 0]^{T}$, is assigned at the inflow, a homogeneous Dirichlet condition is prescribed on the horizontal side as well as on the border of the cylinders, while at the outflow the normal component of the stress tensor is set to zero. For the space discretization the stabilized spectral element method was used, with 114 spectral elements, and polynomials of degree 7 for both the pressure and the velocity components on every element, plus a second-order BDF2 scheme for temporal discretization ( see Sect. $8.5$ and also [QSS07]).

\subsection{Algebraic factorization methods and preconditioners for saddle-point systems}

An alternative approach to the solution of systems like (17.49) is the one based on the use of inexact (or incomplete) factorizations of the system matrix (17.50). We remind that these systems can be obtained by the approximate solution of Stokes equations, Navier-Stokes equations (after using one of the linearization approaches described in Sects. 17.7.1, 17.7.2 or 17.7.3), or, more generally, from the approximation of saddlepoint problems, as shown in Sect. 17.3. Let us also point out that, in all these cases, the 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-107.jpg?height=542&width=420&top_left_y=112&top_left_x=248)

Fig. 17.10. Isolines of the modulus of the velocity vector for the test case of Example $17.1$ at the time levels $t=10.5$ (above) and $t=11.4$ (below)

space discretization method can be based on any one of the methods discussed thus far (finite elements, finite differences, finite volumes, spectral methods, etc.). Generally speaking, we will suppose to deal with an algebraic system of the following form

$$
\left[\begin{array}{ll}
\mathrm{C} & \mathrm{B}^{T} \\
\mathrm{~B} & 0
\end{array}\right]\left[\begin{array}{l}
\mathbf{U} \\
\mathbf{P}
\end{array}\right]=\left[\begin{array}{l}
\mathbf{F} \\
\mathbf{0}
\end{array}\right]
$$

where $C$ coincides with $A$ in the case of system (17.49), with $\frac{1}{\Delta t} \mathrm{M}+\mathrm{A}$ in case of system (17.64), while more in general it could be given by $\frac{\alpha}{\Delta t} \mathrm{M}+\mathrm{A}+\delta \mathrm{D}$, with $\mathrm{D}$ being the matrix associated to the pressure gradient operator, in case a linearization or a semi-implicit treatment are applied to the convective term. In the latter case the coefficients $\alpha$ and $\delta$ would depend on the specific linearization or semi-implicit method adopted.

Also in this case we can associate (17.70) with the Schur complement

$$
\mathrm{R} \mathbf{P}=\mathrm{BC}^{-1} \mathbf{F}, \quad \text { with } \mathrm{R}=\mathrm{BC}^{-1} \mathrm{~B}^{T}
$$

which reduces to (17.52) if we start from the Stokes system (17.49) instead of (17.70). We start noticing that the condition number of $\mathrm{R}$ depends on the inf-sup constant $\beta_{h}$, see (17.41), as well as on the continuity constant $\delta$ (see (17.21)). More precisely, in the case of the stationary problem (17.11), the following relations hold

$$
\beta_{h}=\sqrt{\lambda_{\min }}, \quad \delta \geq \sqrt{\lambda_{\max }}
$$

where $\lambda_{\min }$ and $\lambda_{\max }$ are the eigenvalues of $\mathrm{R}$ (see [QV94, Sect. 9.2.1]). Thus, $\operatorname{cond}(\mathrm{R}) \leq \delta^{2} / \beta_{h}^{2}$. In the time-dependent case we get a system like (17.70); in this case the condition number of $\mathrm{R}$ also depends on $\Delta t$ and on the way the convective term has been discretized.

A possible strategy for the solution of (17.52) consists in solving the Schur complement system (17.71) by an iterative method: the conjugate gradient method if $\mathrm{C}=\mathrm{A}$ (as $A$ is symmetric), otherwise the GMRES or the Bi-CGStab method when $\delta \neq 0$. The use of a convenient preconditioner is mandatory. For a more general discussion, see, e.g., [ESW05, BGL05, QV94] for the case of finite element discretizations, and [CHQZ07] for discretization based on spectral methods.

We start by observing that the matrix of system (17.70), that we denote by $S$, can be written as the product LU of two block triangular matrices,

$$
\mathrm{S}=\left[\begin{array}{cc}
\mathrm{I} & 0 \\
\mathrm{BC}^{-1} & \mathrm{I}
\end{array}\right]\left[\begin{array}{cc}
\mathrm{C} & \mathrm{B}^{T} \\
0 & -\mathrm{R}
\end{array}\right]
$$

Each one of the two matrices

$$
P_{D}=\left[\begin{array}{cc}
C & 0 \\
0 & -R
\end{array}\right] \text { or } \quad P_{T}=\left[\begin{array}{cc}
C & B^{T} \\
0 & -R
\end{array}\right]
$$

provides an optimal preconditioner for $S$, a block diagonal preconditioner $\left(\mathrm{P}_{D}\right)$, and a block triangular preconditioner $\left(\mathrm{P}_{T}\right.$ ). Unfortunately, they are both computationally expensive because of the presence on the diagonal of the Schur complement $\mathrm{R}$, which, in turn, contains the inverse of matrix C. Alternatively, we can use their approximants

$$
\widehat{\mathrm{P}}_{\mathrm{D}}=\left[\begin{array}{cc}
\widehat{\mathrm{C}} & 0 \\
0 & -\widehat{\mathrm{R}}
\end{array}\right] \quad \text { or } \quad \widehat{\mathrm{P}}_{\mathrm{T}}=\left[\begin{array}{cc}
\widehat{\mathrm{C}} & \mathrm{B}^{T} \\
0 & -\widehat{\mathrm{R}}
\end{array}\right]
$$

where $\widehat{C}$ and $\widehat{R}$ are two inexpensive approximations of $C$ and $R$, respectively. $\widehat{C}$ can be built from optimal preconditioners of the stiffness matrix, like those that will be introduced in Chapter $19$.

The pressure correction diffusion preconditioner (PCD) makes use of the following approximation of $\mathrm{R}$

$$
\widehat{\mathrm{R}}_{P C D}=\mathrm{A}_{P} \mathrm{C}_{P}^{-1} \mathrm{M}_{P}
$$

where $\mathrm{M}_{P}$ is the pressure mass matrix, $\mathrm{A}_{P}$ the pressure Laplacian matrix, $\mathrm{C}_{P}$ the convection-diffusion pressure matrix. The term "pressure" here means that these matrices are generated by using the basis functions $\left\{\varphi_{k}, k=1, \ldots, M\right\}$ of the finite dimensional pressure subspace $Q_{h}$. This preconditioner is spectrally equivalent to $\mathrm{BM}^{-1} \mathrm{~B}^{T}$, where $\mathrm{M}$ is the velocity mass matrix. See [ESW05]. The application of this preconditioner requires the action of one Poisson pressure solve, a mass matrix solve, and a matrixvector product with $F_{P}$. Boundary conditions should be taken into account while constructing $\mathrm{A}_{P}$ and $\mathrm{C}_{P}$.

The least-squares commutator preconditioner (LSC) is

$$
\widehat{\mathrm{R}}_{L S C}=\left(\mathrm{BM}_{V}^{-1} \mathrm{~B}^{T}\right)\left(\mathrm{BM}_{V}^{-1} \mathrm{CM}_{V}^{-1} \mathrm{~B}^{T}\right)^{-1}\left(\mathrm{~B} \widehat{\mathrm{M}}_{V}^{-1} \mathrm{~B}^{T}\right)
$$

where $\widehat{\mathrm{M}}_{V}$ is the diagonal matrix obtained from the velocity mass matrix $\mathrm{M}$ by disregarding the extra-diagonal terms. Using this preconditioner entails two Poisson solves. The convergence of Krylov iterations with the LSC preconditioner is independent of the grid-size and mildly dependent on the Reynolds number. See [EHS $\left.^{+} 06\right]$.

The augmented Lagrangian preconditioner (AL), introduced in [BO06], reads

$$
\widehat{\mathrm{R}}_{A L}=\left(v \widehat{\mathrm{M}}_{P}^{-1}+\gamma \mathrm{W}^{-1}\right)^{-1}
$$

where $\widehat{\mathrm{M}}_{P}$ is a diagonal matrix that approximates $\mathrm{M}_{P}, \mathrm{~W}$ is a suitably chosen matrix that, in the simplest case, is also given by $\widehat{\mathrm{M}}_{P}, v$ is the flow viscosity and $\gamma$ is a positive parameter (usually taken to be 1). This preconditioner requires the original system (17.70) to be modified by replacing the $(1,1)$ block by $\mathrm{C}+\gamma \mathrm{B}^{T} \mathrm{~W}^{-1} \mathrm{~B}$, which is consistent because $\mathrm{B} \mathbf{u}=\mathbf{0}$. The new term $\gamma \mathrm{B}^{T} \mathrm{~W}^{-1} \mathrm{~B}$ introduces a coupling between the velocity vector components. Convergence, however, is independent of both the grid-size and the Reynolds number.

Finally, let us remark that direct algebraic preconditioners based on incomplete $L U$ factorization (ILU) of the global matrix $\mathrm{S}$ can be used, in combination with suitable reordering of the unknowns. An in-depth discussion is found in [RVS08].

A different LU factorization of $\mathrm{S}$,

$$
\mathrm{S}=\left[\begin{array}{cc}
\mathrm{C} & 0 \\
\mathrm{~B} & -\mathrm{R}
\end{array}\right]\left[\begin{array}{cc}
\mathrm{I} & \mathrm{C}^{-1} \mathrm{~B}^{T} \\
0 & \mathrm{I}
\end{array}\right]
$$

stands at the base of the so-called SIMPLE preconditioner introduced in [Pat80], and obtained by replacing $\mathrm{C}^{-1}$ in both factors $\mathrm{L}$ and $\mathrm{U}$ by a triangular matrix $\mathrm{D}^{-1}$ (for instance, D could be the diagonal of $\mathrm{C}$ ). More precisely,

$$
P_{S I M P L E}=\left[\begin{array}{cc}
C & 0 \\
B & -\widehat{R}
\end{array}\right]\left[\begin{array}{cc}
\mathrm{I} & \mathrm{D}^{-1} \mathrm{~B}^{T} \\
0 & \mathrm{I}
\end{array}\right]=\widehat{\mathrm{L}} \widehat{\mathrm{U}}
$$

with $\widehat{\mathrm{R}}=\mathrm{BD}^{-1} \mathrm{~B}^{T}$.

With this preconditioner, convergence of preconditioned iterative methods deteriorates when the grid-size $h$ decreases and/or the Reynolds number increases. Note that using $\mathrm{P}_{S I M P L E}$ once, say

$$
\mathrm{P}_{S I M P L E} \mathbf{w}=\mathbf{r}
$$

with $\mathbf{r}=\left[\mathbf{r}_{u}, \mathbf{r}_{p}\right]$ and $\mathbf{w}=[\mathbf{u}, \mathbf{p}]$, yields $\widehat{\mathrm{L}} \mathbf{w}^{*}=\mathbf{r}$, and so $\widehat{\mathrm{U}} \mathbf{w}=\mathbf{w}^{*}$, that is, setting $\mathbf{w}^{*}=\left[\mathbf{u}^{*}, \mathbf{p}^{*}\right]:$

$$
\begin{aligned}
&\mathrm{Cu}^{*}=\mathbf{r}_{u} \\
&\widehat{\mathbf{R}}=\mathrm{Bu}^{*}-\mathbf{r}_{p} \\
&\mathbf{u}=\mathbf{u}^{*}-\mathrm{D}^{-1} \mathrm{~B}^{T} \mathbf{p}^{*}
\end{aligned}
$$

This requires a C-solve for the velocity and a pressure Poisson solve (for $\mathrm{BD}^{-1} \mathrm{~B}^{T}$ ).

Several generalizations of the SIMPLE preconditioner have been proposed, going under the name of SIMPLER, $h$-SIMPLE and MSIMPLER. Using $\mathrm{P}_{\text {SIMPLER }}$ instead of $P_{\text {SIMPLE }}$ in (17.73) involves the following steps:

$$
\begin{aligned}
&\widehat{\mathrm{R}} \mathbf{p}^{0}=\mathrm{BD}^{-1} \mathbf{r}_{u}-\mathbf{r}_{p}, \\
&\mathbf{C u}^{*}=\mathbf{r}_{u}-\mathrm{B}^{T} \mathbf{p}^{0} \\
&\widehat{\mathrm{R}} \mathbf{p}^{*}=\mathrm{Bu}^{*}-\mathbf{r}_{p} \\
&\mathbf{u}=\mathbf{u}^{*}-\mathrm{D}^{-1} \mathrm{~B}^{T} \mathbf{p}^{*} \\
&\mathbf{p}=\mathbf{p}^{*}+\omega \mathbf{p}^{0},
\end{aligned}
$$

with $\omega \in] 0,1]$ being a possible relaxation parameter $(\omega=1$ in SIMPLER, $\omega \neq 1$ in $\operatorname{SIMPLER}(\omega)$ ). It therefore involves two pressure Poisson solves and one C-velocity solve; however, in general it enjoys faster convergence than SIMPLE.

The preconditioner $h S I M P L E$ ( $h=$ hybrid) is based on a combined application of SIMPLE and SIMPLER. Finally, the preconditioner MSIMPLER makes use of the same steps (17.77)-(17.81) as SIMPLER, but the approximate Schur complement $\mathrm{R}=$ $\mathrm{BD}^{-1} \mathrm{~B}^{T}$ is replaced by the least-squares commutator $\widehat{\mathrm{R}}_{L S C}$. The convergence is better than with other variants of SIMPLE.

For more discussion and a comparative analysis see [RVS09] and also [Wes01].

The $\widehat{L} \widehat{U}$ factorization used in $\mathrm{P}_{S I M P L E}$ can be regarded as a special case of a more general family of inexact or algebraic factorizations that read as follows

$$
\widehat{\mathrm{S}}=\widehat{\mathrm{L}} \widehat{\mathrm{U}}=\left[\begin{array}{cc}
\mathrm{C} & 0 \\
\mathrm{~B} & -\mathrm{B} \mathscr{L} \mathrm{B}^{T}
\end{array}\right]\left[\begin{array}{cc}
\mathrm{I} & \mathscr{U} \mathrm{B}^{T} \\
0 & \mathrm{I}
\end{array}\right]
$$

Here $\mathscr{L}$ and $\mathscr{U}$ represent two (not necessarily coincident) approximations of $\mathrm{C}^{-1}$. Using this inexact factorization, the solution of the linear system

$$
\widehat{\mathrm{S}}\left[\begin{array}{l}
\widehat{\mathbf{u}} \\
\widehat{\mathbf{p}}
\end{array}\right]=\left[\begin{array}{l}
\mathbf{F} \\
\mathbf{0}
\end{array}\right]
$$

can be found through the following steps:

$$
\begin{array}{lll}
\text { step } \widehat{\mathrm{L}}: & \left\{\begin{array}{lll}
\mathrm{Cu}^{*}=\mathbf{F} & & \text { (intermediate velocity) } \\
-\mathrm{B} \mathscr{L} \mathrm{B}^{T} \widehat{\mathbf{p}}=-\mathrm{Bu}^{*}
\end{array}\right. & \begin{array}{l}
\text { (pressure })
\end{array} \\
\text { step } \widehat{\mathrm{U}}: \quad \widehat{\mathbf{u}}=\mathbf{u}^{*}-\mathscr{U} \mathrm{B}^{T} \widehat{\mathbf{p}} & \text { (final velocity) }
\end{array}
$$

When used in connection with time-dependent (either Stokes or Navier-Stokes) problems, e.g. (17.64), two different possibilities stand out [QSV00]:

$$
\begin{gathered}
\mathscr{L}=\mathscr{U}=\left(\frac{1}{\Delta t} \mathrm{M}\right)^{-1} \\
\mathscr{L}=\left(\frac{1}{\Delta t} \mathrm{M}\right)^{-1} \quad \text { and } \quad \mathscr{U}=\mathrm{C}^{-1}
\end{gathered}
$$

The former (17.83) is named Chorin-Temam algebraic approximation because the steps $\hat{L}$ and $\hat{U}$ can be regarded as the algebraic counterpart of the Chorin-Temam fractional step method described previously (see Sect. 17.7.3).

The second choice, (17.84), is called a Yosida approximation as it can be interpreted as a Yosida regularization of the Schur complement ([Ven98]). The potential advantage of this strategy with respect to the one based on differential fractional step methods is that it does not require any special care about boundary conditions. The latter are implicitly accounted for in the algebraic formulation (17.70) and no further requirement is needed in the course of the $\widehat{\mathrm{L}}$ and $\widehat{\mathrm{U}}$ steps.

Several generalizations of the inexact factorization technique (17.82) are possible, based on different choices of the factors $\mathscr{L}$ and $\mathscr{U}$. In case the time dependent Navier-Stokes equations are discretized in time by high-order $(\geq 2)$ temporal schemes, inexact factors are chosen so that the time discretization order is maintained. See [GSV06, SV05, Ger08].

In Fig. 17.11 we display the error behaviour corresponding to the approximation of the time dependent Navier-Stokes equations on the domain $\Omega=(0,1)^{2}$ using the spectral element method (SEM) with $4 \times 4$ square elements with side-length $H=0.25$, and polynomials of degree $N=8$ for the velocity components and $N=6$ for the pressure. The exact solution is $\mathbf{u}(x, y, t)=(\sin (x) \sin (y+t), \cos (x) \cos (y+t))^{T}, p(x, y, t)=$ $\cos (x) \sin (y+t)$. The temporal discretization is based on implicit backward differentiation formulae of order $2(\mathrm{BDF} 2), 3$ (BDF3), and 4 (BDF4) (see [QSS07]), then on inexact (Yosida) algebraic factorizations of order 2,3, and 4 , respectively. Denoting by $\left(\mathbf{u}_{N}^{n}, p_{N}^{n}\right)$ the numerical solution at the time level $t^{n}$, the errors on velocity and pressure
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-111.jpg?height=236&width=696&top_left_y=958&top_left_x=104)

Fig. 17.11. Velocity errors $E_{\mathbf{u}}$ on the left; pressure errors $E_{p}$ on the right are defined as

$$
E_{\mathbf{u}}=\left(\Delta t \sum_{n=0}^{N_{T}}\left\|\mathbf{u}\left(t^{n}\right)-\mathbf{u}_{N}^{n}\right\|_{H^{1}(\Omega)}^{2}\right)^{1 / 2} \text { and } E_{p}=\left(\Delta t \sum_{n=0}^{N_{T}}\left\|p\left(t^{n}\right)-p_{N}^{n}\right\|_{L^{2}(\Omega)}^{2}\right)^{1 / 2}
$$

Errors on velocity are infinitesimal with respect to $\Delta t$ of order 2,3, and 4 , respectively, whereas errors on pressure are of order $3 / 2,5 / 2$ and $7 / 2$, respectively.

\subsection{Free surface flow problems}

Free surface flows can manifest under various situations and different shapes. A free surface is generated every time that two immiscible fluids get in contact. They can give rise to jets, [LR98], bubbles [HB76, TF88], droplets [Max76] and films. This kind of fluids are encountered in a variety of different applications, such as waves in rivers, lakes and oceans [Bla02, Qu02], the interaction between waves with solid media (boats, coasts, etc.) [Wya00, KMI $\left.^{+} 83\right]$, injection, moulding and extrusion of polymers and liquid metals [Cab03], chemical reactors or bioreactors, etc. Depending upon the spatial and temporal scales involved, processes like heat transfer, surface tension, laminar to turbulent transition, compressibility and chemical reactions, interaction with solids, might have a relevant impact on the flow behaviour. In what follows we will focus on laminar flows for viscous Newtonian fluids subject to surface tension; in these circumstances, the flow can be described by the incompressible Navier-Stokes equations.

When modeling this kind of fluids, two different approaches can be adopted:

- Front-tracking methods. These methods consider the free surface as being the boundary of a moving domain on which suitable boundary conditions are specified. At the interior of the domain, a conventional fluid model is used; special attention however should be paid to the fact that the domain is not fixed. On the other side of the domain, the fluid, e.g. air, is usually neglected, or otherwise modelled in a simplified fashion without explicitly solving it (see, e.g., [MP97]).

- Front-capturing methods. The two fluids are in fact considered as a single fluid in a domain with fixed boundaries, whose properties like density and viscosity vary as piecewise constant functions. The discontinuity line is in fact the free surface (see, e.g., [HW65, HN81]).

For a review on numerical methods for free-boundary problems, see [Hou95].

In what follows we will consider front-capturing methods. More precisely, we will derive a mathematical model for the case of a general fluid with variable density and viscosity, which would therefore be appropriate to model the flow of two fluids separated by a free surface.

\subsubsection{Navier-Stokes equations with variable density and viscosity}

We consider the general case of a viscous incompressible flow whose density $\rho$ and dynamical viscosity $\mu$ vary both in space and in time. Within a given spatial domain 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-113.jpg?height=288&width=612&top_left_y=114&top_left_x=149)

Fig. 17.12. Two typical grids in two dimensions for front-tracking methods (left) and frontcapturing methods (right). The thick line represents the free surface

$\Omega \subset \mathbb{R}^{d}$, the evolution of the fluid's velocity $\mathbf{u}=\mathbf{u}(\mathbf{x}, t)$ and pressure $p=p(\mathbf{x}, t)$ are modelled by the following equations:

$$
\begin{cases}\rho \partial_{t} \mathbf{u}+\rho(\mathbf{u} \cdot \nabla) \mathbf{u}-\operatorname{div}(2 \mu \mathbf{D}(\mathbf{u}))+\nabla p=\mathbf{f}, & \mathbf{x} \in \Omega, t>0 \\ \operatorname{div} \mathbf{u}=0, & \mathbf{x} \in \Omega, t>0\end{cases}
$$

in which $(\mathbf{D}(\mathbf{v}))=\frac{\nabla \mathbf{v}+\nabla \mathbf{v}^{T}}{2}$ is the symmetric gradient of $\mathbf{v}$, a tensor that is also called rate of deformation, while f denotes a volumetric force, for instance gravity. (Here $\partial_{t}$ stands for $\partial / \partial t$. The divergence of a tensor was introduced in (16.23).)

These equations must be supplemented by suitable initial and boundary conditions. In case $\rho$ is constant we retrieve the form (17.1). Note that incompressibility is not in contradiction with variable density. Incompressibility means that one single fluid parcel does not change volume and thus density, whereas variable density means that different fluid parcels may have different densities. The last two terms of the lefthand side (17.85) can be rewritten as $-\operatorname{div} \mathbf{T}(\mathbf{u}, p)$, where

$$
\mathbf{T}(\mathbf{u}, p)=2 \mu \mathbf{D}(\mathbf{u})-\mathbf{I} p
$$

is the stress tensor while $\mathbf{I}$ is the $d \times d$ identity tensor. A complete derivation of this model can be found, e.g., in [LL59].

An equation for the density function $\rho$ can be obtained from the mass balance equation

$$
\begin{array}{lr}
\mathrm{D}_{t} \rho=\partial_{t} \rho+\mathbf{u} \cdot \nabla \rho=0, & \mathbf{x} \in \Omega, t>0 \\
\left.\rho\right|_{t=0}=\rho_{0}, & \mathbf{x} \in \Omega
\end{array}
$$

where $\mathrm{D}_{t}$ indicates the material, or Lagrangian, derivative, see Sect. 17.7.2. In those cases in which viscosity $\mu$ can be expressed in terms of the density, that is $\mu=\mu(\rho)$, this relation, together with (17.87), provides the model for the evolution of $\rho$ and $\mu$. Models adapted to the special case of a flow of two fluids are described in Sect. 17.9.3. The analysis of the coupled problem (17.85)-(17.86)-(17.87) is a challenging task. We refer the reader to [Lio96]. A global existence result can be proved if $\mathbf{f}=\rho \mathbf{g}$ and $\sigma=0$. This proof requires $\Omega$ to be a smooth, bounded, connected open subset of $\mathbb{R}^{d}$, and that homogeneous Dirichlet boundary conditions (i.e., with $\left.\mathbf{g}_{D}=\mathbf{0}\right)$ are imposed on the whole boundary. If the initial and source data satisfy

$$
\begin{aligned}
&\rho_{0} \geq 0 \text { a.e. in } \Omega, \quad \rho_{0} \in L^{\infty}(\Omega), \quad \rho_{0} \mathbf{u}_{0} \in L^{2}(\Omega)^{d}, \quad \rho_{0}\left|\mathbf{u}_{0}\right|^{2} \in L^{1}(\Omega), \\
&\text { and } \quad \mathbf{g} \in L^{2}(\Omega \times(0, T))^{d}
\end{aligned}
$$

then there exist global weak solutions which satisfy

$$
\begin{aligned}
&\rho \in L^{\infty}(\Omega \times(0, T)), \quad \rho \in C\left([0, \infty) ; L^{p}(\Omega)\right) \quad \forall p \in[1, \infty) \\
&\mathbf{u} \in\left[L^{2}\left(0, T ; H_{0}^{1}(\Omega)\right)\right]^{d}, \quad \nabla \mathbf{u} \in\left[L^{2}(\Omega \times(0, T))\right]^{d \times d} \\
&\rho|\mathbf{u}|^{2} \in L^{\infty}\left(0, T ; L^{1}(\Omega)\right)
\end{aligned}
$$

Another result by Tanaka [Tan93] treats the case where the surface tension coefficient $\sigma$ is different from zero but constant. Under some (stronger) regularity assumptions on the initial data, it has been proved that a global solution exists for sufficiently small initial data and external forces. Moreover, local uniqueness (in time) is proved.

\subsubsection{Boundary conditions}

Let us generalize the discussion on boundary conditions of the beginning of this chapter to the case of the more general formulation (17.85), (17.86) of the Navier-Stokes equations. We still consider a splitting of the boundary $\partial \Omega$ of the domain $\Omega$ into a finite number of components and impose on them appropriate boundary conditions. Several kind of conditions are admissible: for a general discussion see, e.g., [QV94] and the references therein. In the following we just describe the most commonly used conditions for free surface flows.

The Dirichlet boundary conditions prescribe the value of the velocity vector on a boundary subset $\Gamma_{D}$

$$
\mathbf{u}=\varphi \quad \text { on } \quad \Gamma_{D} \subset \partial \Omega
$$

They are used either for imposing a velocity profile on the inflow boundary, or to model a solid boundary moving with a prescribed velocity. In the latter case they are said to be no-slip boundary condition, as they force the fluid not to slip but to stick to the wall.

As we have already noted, when Dirichlet boundary conditions are specified on the entire boundary $\partial \Omega$, the pressure is not uniquely defined. In this case, if $(\mathbf{u}, p)$ is a solution of (17.85), (17.86) and (17.88), then $(\mathbf{u}, p+c), c \in \mathbb{R}$ is also a solution of the same set of equations. Using the Gauss theorem, from equation (17.86) it follows that $\mathbf{g}_{D}$ has to satisfy the compatibility condition

$$
\int_{\partial \Omega} \mathbf{g}_{D} \cdot \mathbf{n} \mathrm{d} \gamma=0
$$

Neumann boundary conditions prescribe a force $\psi_{N}$ per unit area as the normal component of the stress tensor

$$
\mathbf{T}(\mathbf{u}, p) \mathbf{n}=2 \mu \mathbf{D}(\mathbf{u}) \mathbf{n}-p \mathbf{n}=\psi \quad \text { on } \Gamma_{N} \subset \partial \Omega
$$

where $\mathbf{n}$ is the outer unit normal on $\Gamma_{N}$. When $\psi_{N}=\mathbf{0}$ the subset $\Gamma_{N}$ is called a free outflow. For vanishing velocity gradients, the force $\psi_{N}$ corresponds to the pressure on the boundary. See also [HRT96] for more details about the interpretation and implications of this type of boundary conditions. Neumann boundary conditions are used to model a given force per unit area $\mathbf{g}_{N}$ on the boundary.

Mixed boundary conditions prescribe values of the normal component of the velocity field, as well as on the tangential component of the normal stresses, that is:

$$
\begin{array}{ll}
\mathbf{u} \cdot \mathbf{n}=\varphi \cdot \mathbf{n} & \text { on } \Gamma_{D}, \\
(\mathbf{T}(\mathbf{u}, p) \mathbf{n}) \cdot \tau=(2 \mu \mathbf{D}(\mathbf{u}) \mathbf{n}) \cdot \tau=0 & \text { on } \Gamma_{N}, \quad \forall \tau: \tau \cdot \mathbf{n}=0 .
\end{array}
$$

The choice $\varphi=\mathbf{0}$ models the symmetry of the solution along $\Gamma_{D}$, but also a free slip on $\Gamma_{D}$ without penetration. In this case we talk about free-slip boundary conditions.

In some situations, a smooth transition from slip to no-slip boundary conditions is desired. This can be realized by imposing Dirichlet boundary conditions in the normal direction, in analogy to the free slip boundary conditions, and to replace the boundary condition in the tangential direction by Robin boundary conditions, a linear combination of Dirichlet and Neumann boundary conditions:

$$
\begin{array}{ll}
\mathbf{u} \cdot \mathbf{n}=\varphi \cdot \mathbf{n} & \text { on } \Gamma_{D} \\
\left(\omega C_{\tau} \mathbf{u}+(1-\omega)(\mathbf{T}(\mathbf{u}, p) \mathbf{n})\right) \cdot \tau= \\
\left(\omega C_{\tau} \mathbf{u}+(1-\omega)(2 \mu \mathbf{D}(\mathbf{u}) \mathbf{n})\right) \cdot \tau=\omega C_{\tau} \mathbf{g}_{D} \cdot \tau & \text { on } \Gamma_{N}, \quad \forall \tau: \tau \cdot \mathbf{n}=0
\end{array}
$$

The parameter $\omega \in[0,1]$ determines the regime. For $\omega=0$ we have free-slip boundary conditions, whereas for $\omega=1$ we have no-slip boundary conditions. In practice, $\omega$ can be a smooth function of space and time, with values in $[0,1]$, allowing thus a smooth transition between the two cases. This holds for $\varphi=\mathbf{0}$, but transition boundary conditions cover also the general Dirichlet case for $\varphi \neq 0$ and $\omega=1$. The weight $C_{\tau}$ can be seen as a conversion factor between velocities and force per unit area. This type of boundary conditions has been studied in detail in [Joe05].

\subsubsection{Application to free surface flows}

A free surface flow can be modeled by (17.85)-(17.86). In this perspective the free surface is an interface, denoted by $\Gamma(t)$, cutting the domain $\Omega$ into two open subdomains $\Omega^{+}(t)$ and $\Omega^{-}(t)$. The initial position of the interface is known, $\Gamma(0)=\Gamma_{0}$, and the interface moves with fluid velocity $\mathbf{u}$. On each subdomain, we have constant densities and viscosities denoted by $\rho^{+}, \rho^{-}, \mu^{+}$and $\mu^{-}$. We require $\rho^{\pm}>0$ and $\mu^{\pm}>0$.

Density and viscosity are then globally defined as follows:

$$
\rho(\mathbf{x}, t)=\left\{\begin{array}{ll}
\rho^{-} & \mathbf{x} \in \Omega^{-}(t) \\
\rho^{+} & \mathbf{x} \in \Omega^{+}(t)
\end{array} \quad \mu(\mathbf{x}, t)= \begin{cases}\mu^{-} & \mathbf{x} \in \Omega^{-}(t) \\
\mu^{+} & \mathbf{x} \in \Omega^{+}(t)\end{cases}\right.
$$

In order to model buoyancy effects, the gravitational force $\mathbf{f}=\rho \mathbf{g}$, where $\mathbf{g}$ is the vector of gravity acceleration, has to be inserted in the right-hand side.

As the viscosity is discontinuous across the interface, equation (17.85) can hold strongly only on the interior of the two subdomains. The latter must therefore be coupled with suitable interface conditions (see, e.g., [Smo01]).

We denote by $\mathbf{n}_{\Gamma}$ the interface unit normal pointing from $\Omega^{-}$into $\Omega^{+}$and by $\kappa$ the interface curvature, defined as

$$
\kappa=\sum_{i=1}^{d-1} \frac{1}{R_{\tau_{i}}}
$$

where $R_{\tau_{i}}$ are the radii of curvature along the principal vectors $\tau_{i}$ which span the tangent space to the interface $\Gamma$. The sign of $R_{\tau_{i}}$ is such that $R_{\tau_{i}} \mathbf{n}_{\Gamma}$ points from $\Gamma$ to the center of the circle approximating $\Gamma$ locally.

The jump of a quantity $v$ across the interface is denoted by $[v]_{\Gamma}$ and defined as

$$
\begin{aligned}
[v]_{\Gamma}(\mathbf{x}, t) &=\lim _{\varepsilon \rightarrow 0^{+}}\left(v\left(\mathbf{x}+\varepsilon \mathbf{n}_{\Gamma}, t\right)-v\left(\mathbf{x}-\varepsilon \mathbf{n}_{\Gamma}, t\right)\right) \\
&=\left.v\right|_{\Omega^{+}(t)}(\mathbf{x}, t)-\left.v\right|_{\Omega^{-}(t)}(\mathbf{x}, t) \quad \forall \mathbf{x} \in \Gamma(t)
\end{aligned}
$$

The interface conditions then read:

$$
\begin{aligned}
&{[\mathbf{u}]_{\Gamma}=\mathbf{0}} \\
&{\left[\mathbf{T}(\mathbf{u}, p) \mathbf{n}_{\Gamma}\right]_{\Gamma}=\left[2 \mu \mathbf{D}(\mathbf{u}) \mathbf{n}_{\Gamma}-p \mathbf{n}_{\Gamma}\right]_{\Gamma}=\sigma \kappa \mathbf{n}_{\Gamma} .}
\end{aligned}
$$

Equation (17.91) is called the kinematic interface condition. It expresses the property that all components of the velocity are continuous. In fact the normal component has to be continuous because there is no flow through the interface, whereas the tangential component(s) have to be continuous because both fluids are assumed viscous $\left(\mu^{+}>0\right.$ and $\left.\mu^{-}>0\right)$.

Equation (17.92) is refered to as the dynamic interface condition. It expresses the property that the normal stress jumps by the same amount given by the surface tension force. This force is proportional to the interface curvature and points in the same direction of the interface normal. The surface tension coefficient $\sigma$ depends on the fluid pairing, and in general also on temperature. We will assume it to be constant, as all heat transfer effects are neglected.

Note that the evolution of the interface has to be compatible with the mass conservation equation (17.87). Mathematically, this equation has to be understood in the weak sense, i.e, in the sense of distributions, as the density is discontinuous across the interface and, consequently, its derivatives can only be interpreted weakly.

As this form of the mass conservation equation is often not convenient for numerical simulations, other equivalent models that describe the evolution of the interface $\Gamma(t)$ have been introduced. A short overview is presented in Sect. 17.10. 

\subsection{Interface evolution modelling}

We give here a short overview of different approaches for modelling the evolution of an interface $\Gamma(t)$ in a fixed domain $\Omega$.

\subsubsection{Explicit interface descriptions}

An interface can be represented explicitly by a set of marker points or line segments (in $2 \mathrm{D}$, surface segments in $3 \mathrm{D}$ ) on the interface, that are transported by the fluid velocity.

In the case of marker points, introduced in [HW65], the connectivity of the interface between the points is not known and has to be reconstructed whenever needed. In order to simplify this task, additional markers are usually placed near the interface, marking $\Omega^{+}$or $\Omega^{-}$. The advection of the markers is simple, and connectivity can change easily. However it is still somewhat cumbersome to reconstruct the interface from the marker distribution. Typically, it is also necessary to redistribute the markers, introduce new ones or discard existing ones.

Several markers can be connected to define a line or surface, either straight (plane) or curved, e.g. by NURBS. A set of such geometrical objects can now define the surface. Its evolution is modeled by the evolution of the constituting objects, and thus by the markers defining them. The connectivity of the interface is thereby conserved. This solves the difficulty of pure marker methods, and brings a new drawback in turn: topological changes of the interface are allowed by the underlying physics but not by this description. Sophisticated procedures have to be applied to detect and handle interface breakup correctly.

\subsubsection{Implicit interface descriptions}

In front-capturing methods, the interface is represented implicitly by the value of a scalar function $\phi: \Omega \times(0, T) \rightarrow \mathbb{R}$ that tells to which subset any point $\mathbf{x}$ belongs: $\Omega^{+}(t)$ or $\Omega^{-}(t)$. A transport equation solved for $\phi$ then describes the evolution of the interface. By this feature, all implicit interface models share the advantage that topology changes of the interface are possible in the model, and that these happen without special intervention.

\section{Volume-of-fluid methods}

The volume of fluid methods (VOF) were originally introduced by Hirt and Nichols [HN81]. Let $\phi$ be a piecewise constant function such that

$$
\phi(\mathbf{x}, t)= \begin{cases}1, & \mathbf{x} \in \Omega^{+}(t) \\ 0, & \mathbf{x} \in \Omega^{-}(t)\end{cases}
$$

the interface $\Gamma(t)$ is thus located at the discontinuity of the function $\phi$, while density and viscosity are simply defined as

$$
\rho=\rho^{-}+\left(\rho^{+}-\rho^{-}\right) \phi, \quad \mu=\mu^{-}+\left(\mu^{+}-\mu^{-}\right) \phi
$$

The transport equation is usually discretized with cell-centered finite volume methods, approximating $\phi$ by a constant value in each grid cell (see Sects. $9.1$ and 17.12). Due to discretization errors and diffusive transport schemes, the approximation $\phi$ will take values between 0 and 1 , which by virtue of equation (17.93) can be (and usually are) interpreted as the volume fraction of the fluid occupying $\Omega^{+}$. This explains the name volume of fluid. Volume fractions between 0 and 1 actually represent a mixture of the two fluids. As the fluids are assumed immiscible, this behaviour is not desired, especially because mixing effects may not stay concentrated near the interface but spread over the whole domain $\Omega$. When this happens, the supposedly sharp interface becomes more and more diffuse. Several techniques exist to limit this problem. Elaborate procedures have been developed for the reconstruction of normals and curvature of a diffuse interface.

Volume of fluid methods have the advantage that applying a conservative discretization of the transport equation ensures mass conservation of the fluid, because the relation (17.93) between $\phi$ and $\rho$ is linear.

\section{Level-set methods}

In order to circumvent the problems with volume of fluid methods, Dervieux and Thomasset [DT80] proposed in 1980 to define the interface as the zero level set of a continuous pseudo-density function and to apply this method to flow problems. Their approach was then studied more systematically in [OS88] and subsequent publications, where the term level-set method was coined. The first application to flow problems was by Mulder, Osher and Sethian in 1992 [MOS92]. In constrast with volume of fluid approaches, these methods allow to keep the interface sharp, as $\phi$ is defined as a continuous function such that:

$$
\begin{array}{ll}
\phi(\mathbf{x}, t)>0, & \mathbf{x} \in \Omega^{+}(t) \\
\phi(\mathbf{x}, t)<0, & \mathbf{x} \in \Omega^{-}(t) \\
\phi(\mathbf{x}, t)=0, & \mathbf{x} \in \Gamma(t)
\end{array}
$$

The function $\phi$ is called level-set function, because the interface $\Gamma(t)$ is its zero level set, its isoline or isosurface associated to the value zero

$$
\Gamma(t)=\{\mathbf{x} \in \Omega: \phi(\mathbf{x}, t)=0\}
$$

The density and the viscosity can now be expressed in function of $\phi$ as

$$
\rho=\rho^{-}+\left(\rho^{+}-\rho^{-}\right) H(\phi), \quad \mu=\mu^{-}+\left(\mu^{+}-\mu^{-}\right) H(\phi)
$$

where $H(\cdot)$ is the Heaviside function

$$
H(\xi)= \begin{cases}0, & \xi<0 \\ 1, & \xi>0\end{cases}
$$

By construction, the interface stays sharp in a level-set model, and the immiscible fluids do not start to mix. In addition, the determination of the normals and the curvature of the interface are more straightforward and very natural. In turn, as the relation (17.95) is not linear, applying a conservative discretization of the transport equation for $\phi$ does not ensure mass conservation of the fluid after discretization. This is not a big problem, however, as the mass error still disappears with grid refinement and is outweighed by advantages of the level-set formulation.

The evolution of the free surface is described by an advection equation for the level-set function

$$
\begin{cases}\partial_{t} \phi+\mathbf{u} \cdot \nabla \phi=0 & \text { in } \Omega \times(0, T) \\ \phi=\phi_{0} & \text { in } \Omega \text { at } t=0 \\ \phi=\phi_{i n} & \text { on } \partial \Sigma_{i n} \times(0, T)\end{cases}
$$

where $\Sigma_{i n}$ is the inflow boundary

$$
\Sigma_{i n}=\{(\mathbf{x}, t) \in \partial \Omega \times(0, T): \mathbf{u}(\mathbf{x}, t) \cdot \mathbf{n}<0\}
$$

The flow equations (17.85)-(17.86) and the level-set equation (17.96) are therefore coupled. Equation (17.96) can be derived as follows [MOS92]: let $\overline{\mathbf{x}}(t)$ be the path of a point on the interface $\Gamma(t)$. This point moves with the fluid, thus $\mathrm{D}_{t} \overline{\mathbf{x}}(t)=\mathbf{u}(\overline{\mathbf{x}}(t), t)$. Since the function $\phi$ is always zero on the moving interface, we must have

$$
\phi(\overline{\mathbf{x}}(t), t)=0
$$

Deriving with respect to time and applying the chain rule, we obtain

$$
\partial_{t} \phi+\nabla \phi \cdot \mathbf{u}=0 \quad \text { on } \Gamma(t) \quad \forall t \in(0, T) .
$$

If we consider instead a path of a point in $\Omega^{\pm}$, we may require $\phi(\overline{\mathbf{x}}(t), t)=\pm c, c>0$, in order to ensure that the sign of $\phi(\overline{\mathbf{x}}, t)$ does not change and that $\overline{\mathbf{x}}(t) \in \Omega^{\pm}(t)$ for all $t$ hereby.

In this way, equation (17.97) generalizes to the whole domain $\Omega$, which gives us equation (17.96).

We can now verify that mass conservation is satisfied: using (17.95), we obtain formally

$$
\begin{aligned}
\partial_{t} \rho+\mathbf{u} \cdot \nabla \rho &=\left(\rho^{+}-\rho^{-}\right)\left(\partial_{t} H(\phi)+\mathbf{u} \cdot \nabla H(\phi)\right) \\
&=\left(\rho^{+}-\rho^{-}\right) \delta(\phi)\left(\partial_{t} \phi+\mathbf{u} \cdot \nabla \phi\right)
\end{aligned}
$$

where $\delta(\cdot)$ denotes the Dirac delta function. By equation (17.96), the third factor in (17.98) is zero. Hence equation (17.87) holds and the mass conservation is satisfied by the level-set interface evolution model.

\section{Interface-related quantities}

In the context of two fluid flow, the interface normal and curvature are of particular interest. Namely the surface tension is proportional to the curvature and acting in the normal direction.

We shall now explain how the quantitites depend on $\phi$, without going into the details of the differential geometry involved. See, e.g., [Spi99] for a detailed and rigorous derivation. The unit normal $\mathbf{n}_{\Gamma}$ is orthogonal to all tangent directions $\tau$, which in turn are characterized by the fact that the directional derivative of $\phi$ in any tangent direction must vanish:

$$
0=\partial_{\tau} \phi=\nabla \phi \cdot \tau \quad \text { on } \Gamma
$$

The gradient of $\phi$ is thus orthogonal to all tangent directions, and we can define the interface unit normal by normalizing it

$$
\mathbf{n}_{\Gamma}=\frac{\nabla \phi}{|\nabla \phi|}
$$

Note that by this definition, $\mathbf{n}_{\Gamma}$ points from $\Omega^{-}$into $\Omega^{+}$. Moreover, as $\phi$ is defined not only on the interface but in the whole domain, the expression for the normal generalizes naturally to the entire domain, too.

In order to derive the expression for the curvature, we need to consider the principal tangent direction(s) $\tau_{i}, i=1 \ldots d-1$. These are the directions in which the interface is approximated by a circle (cylinder), i.e., the directional derivative of $\mathbf{n}_{\Gamma}$ in the direction $\tau_{i}$ is parallel to $\tau_{i}$

$$
\partial_{\tau_{i}} \mathbf{n}_{\Gamma}=\nabla \mathbf{n}_{\Gamma} \tau_{i}=-\kappa_{i} \tau_{i}, \quad \kappa_{i} \in \mathbb{R}, \quad i=1 \ldots d-1
$$

The bigger $\left|\kappa_{i}\right|$, the more curved the surface in this direction, and the coefficients $\kappa_{i}$ are in fact called principal curvatures. It follows from straightforward computations that $\kappa_{i}=\left(R_{\tau_{i}}\right)^{-1}$, where the values $R_{\tau_{i}}$ are the radii of the approximating circles (cylinders) as of equation (17.90).

We can see from equation (17.100) that the $d-1$ values $-\kappa_{i}$ are eigenvalues of the $d \times d$-tensor $\nabla \mathbf{n}_{\Gamma}$. By (17.99), $\mathbf{n}_{\Gamma}$ is (essentially) a gradient field which is smooth near the interface. The rank-two tensor $\nabla \mathbf{n}_{\Gamma}$ is thus (essentially) a tensor of second derivatives of a smooth function, and hence symmetric. So it has one more real eigenvalue, whose associated eigenvector must be $\mathbf{n}_{\Gamma}$, because the eigenvectors of a symmetric tensor are orthogonal. It is easy to see that the respective eigenvalue is zero

$$
\left(\nabla \mathbf{n}_{\Gamma} \mathbf{n}_{\Gamma}\right)_{i}=\sum_{j=1}^{d}\left(\partial_{x_{i}} n_{j}\right) n_{j}=\sum_{j=1}^{d} \frac{1}{2} \partial_{x_{i}}\left(n_{j}^{2}\right)=\frac{1}{2} \partial_{x_{i}}\left|\mathbf{n}_{\Gamma}\right|^{2}=0
$$

as $\left|\mathbf{n}_{\Gamma}\right|=1$ by construction (17.99).

Starting from equation (17.90), we obtain for the curvature

$$
\kappa=\sum_{i=1}^{d-1} \frac{1}{R_{\tau_{i}}}=\sum_{i=1}^{d-1} \kappa_{i}=-\operatorname{tr}\left(\nabla \mathbf{n}_{\Gamma}\right)=-\nabla \cdot \mathbf{n}_{\Gamma}
$$

and using equation (17.99), we get

$$
\kappa=-\nabla \cdot\left(\frac{\nabla \phi}{|\nabla \phi|}\right)
$$



\section{Initial Condition}

Although we know the position $\Gamma_{0}$ of the interface at $t=0$, the associated level-set function $\phi_{0}$ is not uniquely defined. The freedom of choice can be used to simplify further subsequent tasks. We notice that steep gradients of $\phi$ make the numerical solution of equation (17.96) more difficult (see e.g. [QV94]), whereas flat gradients decrease the numerical stability when determining $\Gamma$ from $\phi$. A good compromise is thus the further constraint $|\nabla \phi|=1$.

A function which fulfills this constraint is the distance function

$$
\operatorname{dist}(\mathbf{x} ; \Gamma)=\min _{\mathbf{y} \in \Gamma}|\mathbf{x}-\mathbf{y}|
$$

which at each point $\mathbf{x}$ takes the value of the minimum Euclidean distance from $\mathbf{x}$ to $\Gamma$. Multiplying this function by $-1$ on $\Omega^{-}$, we obtain the signed distance function

$$
\operatorname{sdist}(\mathbf{x} ; \Gamma)= \begin{cases}\operatorname{dist}(\mathbf{x} ; \Gamma), & \mathbf{x} \in \Omega^{+} \\ 0, & \mathbf{x} \in \Gamma \\ -\operatorname{dist}(\mathbf{x} ; \Gamma), & \mathbf{x} \in \Omega^{-}\end{cases}
$$

It is thus usual and reasonable to choose $\phi_{0}$, representing an initial interface $\Gamma_{0}$, as $\phi_{0}(\mathbf{x})=\operatorname{sdist}\left(\mathbf{x} ; \Gamma_{0}\right)$

Since $|\nabla \phi|=1$, the expressions of the interface normal and curvature simplify further to

$$
\mathbf{n}_{\Gamma}=\nabla \phi \quad \text { and } \quad \kappa=-\nabla \cdot \nabla \phi=-\Delta \phi
$$

\section{Reinitialization}

Unfortunately, the property $|\nabla \phi|=1$ is not preserved under advection of $\phi$ with the fluid velocity $\mathbf{u}$. This is not a problem as long as $|\nabla \phi|$ does not stay too far from 1 , which however cannot be guaranteed in general. Two different strategies can be followed to cope with this issue.

One approach is to determine an advection velocity field that gives the same interface motion as the fluid velocity field, while preserving the distance property. Actually such a velocity field exists and is called extension velocity, as it is constructed by extending the velocity prescribed on the interface to the whole domain; efficient algorithms are described in [AS99].

Alternatively, we can still use the fluid velocity $\mathbf{u}$ for advecting the level-set function $\phi$, and intervene when $|\nabla \phi|$ becomes too large or too small. The action to be taken in this case is known as reinitialization, as the procedure is partially the same as for initialization with the initial condition. Suppose we decide to reinitialize at time $t=t_{r}$ :

1. Given $\phi\left(\cdot, t_{r}\right)$, find $\Gamma\left(t_{r}\right)=\left\{\mathbf{x}: \phi\left(\mathbf{x}, t_{r}\right)=0\right\}$;

2. Replace $\phi\left(\cdot, t_{r}\right)$ by $\operatorname{sdist}\left(\cdot, \Gamma\left(t_{r}\right)\right)$.

Interestingly, it turns out that the problem of finding the extension velocity is closely related to the problem of reinitializing $\phi$ with a signed distance function. The same algorithms can be used and the same computational cost has to be expected. Two conceptual differences favour the reinitialization approach, though: firstly, the extension velocities have to be computed at every time step, whereas reinitialization can be performed only when necessary, which results in a global reduction of the computational costs. Secondly, the approximated extension velocities will only approximately preserve the distance property and may not guarantee that reinitialization is unnecessary.

Algorithmic details about the efficient construction of an approximation to the signed distance function, especially for the three-dimensional case, can be found in [Win07].

\subsection{Finite volume approximation}

The finite volume approach described in Chapter 9 is widely used for the solution of problems described by differential equations, with applications in different engineering fields. In particular, the most frequently used commercial codes in the field of fluid dynamics adopt finite volume schemes for the solution of the Navier-Stokes equations. The latter are often coupled with models of turbulence, transition, combustion, transport and reaction of chemical species.

When applied to incompressible Navier-Stokes equations, the saddle-point nature of the problem makes the choice of control volumes critical. The most natural choice, with coinciding velocity and pressure nodes, can generate spurious pressure modes. The reason is similar to what was previously noticed for Galerkin finite element approximations: discrete spaces which implicitly underlie the choice of control volumes must satisfy a compatibility condition if we want the problem to be well-posed.

For this reason, it is commonplace to adopt different control volumes, and henceforth nodes, for velocity and pressure. An example is illustrated in Fig. 17.13, where we display a possible choice of nodes for the velocity components and for pressure (on the staggered grid), as well as the corresponding control volumes. The control volumes corresponding to the velocity are used for the discretization of momentum equations, while the pressure ones are used for the continuity equation. We recall that the latter does not contain the temporal derivative term.

Alternatively, we can adopt stabilization techniques similar to the ones seen in Sect. 17.5, that allow to place the velocity and pressure nodes on the same grid. The interested reader can consult the monographs [FP02], [Kro97], [Pat80] and [VM96] for further details.

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-122.jpg?height=163&width=202&top_left_y=988&top_left_x=474)

Fig. 17.13. A staggered finite volume grid for velocity and pressure. On the left-hand side we sketch the control volumes for the continuity equation, on the right-hand side the ones used for momentum equations 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-123.jpg?height=222&width=622&top_left_y=112&top_left_x=146)

Fig. 17.14. Vorticity field of an incompressible flow around 5 cylinders at time instants $t=100$ (left) and $t=102$ (right), $\operatorname{Re}=200$

In Fig. 17.14 we display the vorticity field of an incompressible flow around 5 cylinders at two time instants (this is the same problem described in Example 17.1, see Fig. 17.10) with a Reynolds number of $200$. In this case the Navier-Stokes equations are solved by a cell-centered finite volume discretization. The computational grid used here features 103932 elements and a time step $\Delta t=0.001$.

Let us also report the simulation of the hydrodynamic flow around an America's Cup sailing boat in upwind regime, targeted at studying the efficiency of its appendages (bulb, keel and winglets) (see Fig. 17.15, left). The computational grid used in this case is hybrid, with surface elements of triangular and quadrangular shape, and volume elements of tetrahedral, hexahedral, prismatic and pyramidal shape (see Fig. 17.15, right).

The mathematical model is the one illustrated in Sect. $17.9$ for free-surface fluids. The Navier-Stokes equations, however, are coupled with a $k-\varepsilon$ turbulence model [MP94], through an approach of type RANS (Reynolds Averaged Navier-Stokes). The problem's unknowns are the values of the variables (velocity, pressure and turbulent quantities) at the center of control volumes, which in this case correspond to the volume elements of the grid.

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-123.jpg?height=267&width=353&top_left_y=909&top_left_x=97)
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-123.jpg?height=268&width=720&top_left_y=908&top_left_x=97) bulb-keel intersection (right) The time-dependent Navier-Stokes equations are advanced in time using a fractional-step scheme, as described in Sect. 17.7.3. As previously pointed out, the choice of placing velocity and pressure at the same points makes it necessary to adopt a suitable stabilization of the equations [RC83]. For the computation of the free surface, we have used both the volume-of-fluid method and the one based on the level-set technique, described in Sect. 17.10.2, the latter being more costly from a computational viewpoint but in general less dissipative.

Simulations of this kind can require grids with very many elements, in the cases where one wants to reproduce complex fluid dynamics phenomena such as the turbulent flow around complex geometries, or the presence of regions of flow separation. The grid used in this case is composed of 5 million cells and generates an algebraic system with more than 30 million unknowns. Problems of this size are generally solved by resorting to parallel computation techniques based on domain decomposition methods (see Chap. 19) in order to distribute the computation over several processors.

The analysis of pressure distributions and of wall shear stresses, as well as the visualization of the 3 D flow through streamlines (see Figs. 17.16 and 17.17) are indeed very useful during the hydrodynamic-project phase, whose aim is the optimization of the boat's performances (see e.g. [PQ05, PQ07, DPQ08]).
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-124.jpg?height=258&width=720&top_left_y=645&top_left_x=96)

Fig. 17.16. Surface pressure distribution (left) and streamlines around the hull appendages (right)

\subsection{Exercises}

1. Prove that condition (17.53) is equivalent to the inf-sup condition (17.20).

[Solution: note that condition (17.53) is violated iff $\exists \mathbf{p}^{*} \neq \mathbf{0}$ with $\mathbf{p}^{*} \in \mathbb{R}^{M}$ such that $\mathrm{B}^{T} \mathbf{p}^{*}=\mathbf{0}$ or, equivalently, if $\exists p_{h}^{*} \in \mathbb{Q}_{h}$ such that $b\left(\varphi_{n}, p_{h}^{*}\right)=0 \forall n=1, \ldots, N$. This is equivalent to $b\left(\mathbf{v}_{h}, p_{h}^{*}\right)=0 \forall \mathbf{v}_{h} \in V_{h}$, which in turn is equivalent to violating $(17.20) .]$ 2. Prove that a necessary condition in order that $(17.53)$ be satisfied is that $2 N \geq M$. $\left[\right.$ Solution: we have $N=\operatorname{rank}(\mathrm{B})+\operatorname{dim}(\operatorname{kerB})$, while $M=\operatorname{rank}\left(\mathrm{B}^{T}\right)+\operatorname{dim}\left(\operatorname{kerB}^{T}\right)=$ $\operatorname{rank}\left(\mathrm{B}^{T}\right)=\operatorname{rank}(\mathrm{B})$. Consequently, we have $N-M=\operatorname{dim}(\operatorname{kerB}) \geq 0$, thus the condition $N \geq M$ is necessary for the solution to be unique.]

3. Show that the finite element couple $\mathbb{P}_{1}-\mathbb{P}_{0}$ for velocity and pressure does not satisfy the $i n f-s u p$ condition.

[Solution: we restrict ourselves to a two-dimensional Dirichlet problem, and consider a simple uniform triangulation made of $2 n^{2}$ triangles, $n \geq 2$, like the one displayed in Fig. 17.18, left. This triangulation carries $M=2 n^{2}-1$ degrees of freedom for the pressure (one value for every triangle except for one, as our pressure we must have null average), $N=2(n-1)^{2}$ for the velocity field (which correspond to the values of two components at each internal vertex). Thus the necessary condition $N \geq M$ proven in Exercise 2 is not fulfilled in the current case.]

4. Show that on a grid made by rectangles, the finite element couple $\mathbb{Q}_{1}-\mathbb{Q}_{0}$ of bilinear polynomials for the velocity components and constant pressure on each rectangle does not satisfy the inf-sup condition.

[Solution: consider a square computational domain and a uniform Cartesian grid made of $n \times n$ squares as in Fig. $17.18$ right $)$. There are $(n-1)^{2}$ internal nodes
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-125.jpg?height=258&width=720&top_left_y=603&top_left_x=96)

Fig. 17.17. Current lines around the sails during downwind navigation (left) and streamlines around the hull appendages (right)

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-125.jpg?height=231&width=237&top_left_y=945&top_left_x=167)
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-125.jpg?height=232&width=582&top_left_y=944&top_left_x=166) (right) finite elements with spurious pressure modes carrying $N=2(n-1)^{2}$ degrees of freedom for the velocity and $M=n^{2}-1$ for the pressure. The necessary condition is therefore satisfied provided $n \geq 3$. We therefore verify directly that the $\inf -\sup$ condition does not hold. Let $h$ be the uniform size of the element edges and denote by $q_{i \pm 1 / 2, j \pm 1 / 2}$ the value at the midpoint $\left(x_{i \pm 1 / 2}, y_{j \pm 1 / 2}\right)=\left(x_{i} \pm h / 2, y_{i} \pm h / 2\right)$ of a given function $q$. Let $K_{i j}$ be a square of the grid. A simple calculation shows that

$$
\begin{aligned}
&\int_{\Omega} q_{h} \operatorname{div} \mathbf{u}_{h} d \Omega=\frac{h}{2} \sum_{i, j=1}^{n-1} u_{i j}\left(q_{i-1 / 2, j-1 / 2}+q_{i-1 / 2, j+1 / 2}\right. \\
&\left.-q_{i+1 / 2, j-1 / 2}-q_{i+1 / 2, j+1 / 2}\right) \\
&+v_{i j}\left(q_{i-1 / 2, j-1 / 2}-q_{i-1 / 2, j+1 / 2}+q_{i+1 / 2, j-1 / 2}-q_{i+1 / 2, j+1 / 2}\right)
\end{aligned}
$$

Clearly, any element-wise constant function $p^{*}$ whose value is 1 on the black elements and $-1$ on the white ones of Fig. 17.18, right, is a spurious pressure.]

5. Consider the steady Stokes problem with non-homogeneous Dirichlet boundary conditions

$$
\begin{cases}-v \Delta \mathbf{u}+\nabla p=\mathbf{f} & \text { in } \Omega \subset \mathbb{R}^{2} \\ \operatorname{div} \mathbf{u}=0 & & \text { in } \Omega \\ \mathbf{u}=\mathbf{g} & & \text { on } \Gamma=\partial \Omega\end{cases}
$$

where $\mathbf{g}$ is a given vector function. Show that $\int_{\Gamma} \mathbf{g} \cdot \mathbf{n}=0$ is a necessary condition for the existence of a weak solution. Show that the right-hand side of the weak form of the momentum equation identifies an element of $V^{\prime}$, the dual of the space $V=\left[\mathrm{H}_{0}^{1}(\Omega)\right]^{d}$.

6. Do the same (as in Exercise 5) for the non-homogeneous Navier-Stokes problem

$$
\begin{cases}(\mathbf{u} \cdot \nabla) \mathbf{u}-v \Delta \mathbf{u}+\nabla p=\mathbf{f} & \text { in } \Omega \subset \mathbb{R}^{2} \\ \operatorname{div} \mathbf{u}=0 & \text { in } \Omega \\ \mathbf{u}=\mathbf{g} & \text { on } \Gamma=\partial \Omega\end{cases}
$$

7. Prove the a priori estimate (17.57).

$\left[\right.$ Solution: choose $\mathbf{v}_{h}=\mathbf{u}_{h}$ and $q_{h}=p_{h}$ as test functions in (17.55). Then apply the Cauchy-Schwarz, Young and Poincaré inequalities to bound the right-hand side.] Chapter 18

\section{Optimal control of partial differential equations}

In this chapter we will introduce the basic concepts of optimal control for linear elliptic partial differential equations. At first we present the classical theory in functional spaces "à la J.L.Lions", see [Lio71, Lio72]; then we will address the methodology based on the use of the Lagrangian functional (see, e.g., [Mau81, BKR00, Jam88]). Finally, we will show two different numerical approaches for control problems, based on the Galerkin finite element method.

This in intended to be an elementary introduction to this fascinating and complex subject. The interested reader is advised to consult more specialized monographs such as, e.g., [Lio71, AWB71, ATF87, Ago03, BKR00, Gun03, Jam88, APV98, MP01, FCZ04, DZ06, Zua05, IK08, HPUU09, Tro10, BS12]. For the basic concepts of functional analysis here used, see Chapter 2 and also [Ada75, BG87, Bre86, Rud91, Sal08, TL58].

\subsection{Definition of optimal control problems}

In abstract terms, a control problem can be expressed by the paradigm illustrated in Fig. 18.1. There is a system expressed by a state problem that can be either an algebraic problem, an initial-value problem for ordinary differential equations, or a boundaryvalue problem for partial differential equations. Its solution, that will generically be denoted by $y$, depends on a variable $u$ representing the control that can be exerted on the system. The goal of a control problem is to find the control $u$ in such a way that a suitable output variable, denoted by $z$ and called observed variable (which is a function of $u$ through $y$ ), takes a desired "value" $z_{d}$, the so-called observation, or target.

The problem is said to be controllable if a control $u$ exists such that the observed variable $z$ matches exactly the desired value $z_{d}$. Not all systems are controllable (see, in this respect, the review paper [Zua06]): take for instance the simple case in which the state problem is the linear algebraic system $A \mathbf{y}=\mathbf{b}$, where $A$ is a given $n \times n$ nonsingular matrix and $\mathbf{b}$ a given vector of $\mathbb{R}^{n}$. 



$\left(\begin{array}{c}\text { Input } \\ \text { Control }\end{array} \longrightarrow(\begin{array}{c}{\text { System }}{\text { State }}\end{array} \longrightarrow \underbrace{\text { Output }}{\text { Observation }}\right.$

Fig. 18.1. The essential ingredients of a control problem

Assume moreover that the observation is represented by one component $\sigma$, say the first one, and suppose that the control is one of the components of the right-hand side, say the last one. The question therefore reads: "Find $u \in \mathbb{R}$ such that the solution of the linear system $A \mathbf{y}=\mathbf{b}+[0, \ldots, 0, u]^{T}$ satisfies $y_{1}=y_{1}^{*} \cdot, y_{1}^{*}$ being a given value. In general, this problem admits no solution. For this reason it is often preferred to replace the problem of controllability by one of optimization: by doing so one does not expect the output variable $z$ to be exactly equal to the observation $z_{d}$, but that the difference between $z$ and $z_{d}$ (in a suitable sense) be the smallest possible. Therefore, control and optimization are two intimately related concepts, as we will see later on in this chapter.

As already noted, we will only consider systems governed by elliptic PDEs. With this aim, we start by introducing the mathematical entities that enter in the control problem.

- The control function $u$. It belongs to a functional space $\mathscr{U}_{a d}$, called the space of admissible controls. In general, $\mathscr{U}_{a d} \subseteq \mathscr{U}$, where $\mathscr{U}$ is a functional space apt to describe the role assumed by $u$ in the given state equation. If $\mathscr{U}_{a d}=\mathscr{U}$ the control problem is unconstrained; if $\mathscr{U}_{a d} \subset \mathscr{U}$ the control problem is said to be $\mathrm{con}$ strained.

- The state of the system $y(u) \in \mathscr{V}$ (a suitable functional space), a function depending on the control $u$ that satisfies the equation of state

$$
A y(u)=f,
$$

where $A: \mathscr{V} \mapsto \mathscr{V}^{\prime}$ is a differential operator (linear or not). This problem describes a physical problem subject to suitable boundary conditions. As we will see, the control function can enter in the right-hand side, in the boundary data, or in the coefficients of the differential operator.

- The observation function in $z(u)$, also depending on the control $u$ through $y$ and on a suitable operator $C: \mathscr{V} \rightarrow \mathscr{Z}$,

$$
z(u)=C y(u)
$$

This function belongs to the space $\mathscr{Z}$ of the observed functions and must "approach" the observation function $z_{d}$. As a matter of fact, optimizing system (18.1) means finding the control function $u$ such that the function $z(u)$ is "as close as possible" to the observation function $z_{d}$. This goal will be achieved through a minimization process that we are going to describe.

- A cost functional $J(u)$, defined on the space $\mathscr{U}_{a d}$

$$
u \in \mathscr{U}_{a d} \mapsto J(u) \in \mathbb{R} \quad \text { with } J(u) \geq 0
$$

In general, $J$ will depend on $u$ (also) through $z(u)$, that is $J(u)=\tilde{J}(u, z(u))$, for a suitable functional $\tilde{J}: \mathscr{U}_{a d} \times \mathscr{Z} \rightarrow \mathbb{R}$.

The optimal control problem can be formulated in either following way:

i) find $u \in \mathscr{U}_{a d}$ such that

$$
J(u)=\inf J(v) \quad \forall v \in \mathscr{U}_{a d}
$$

ii) find $u \in \mathscr{U}_{a d}$ such that the following inequality holds

$$
J(u) \leq J(v) \quad \forall v \in \mathscr{U}_{a d}
$$

The function $u$ that satisfies (18.2) (or (18.3)) is called optimal control of system (18.1).

Before analyzing the existence and uniqueness properties of the control problem and characterizing the condition of optimality, let us consider a simple finite-dimensional example.

\subsection{A control problem for linear systems}

Let $A$ be a $n \times n$ non-singular matrix and $B$ a $n \times q$ matrix. Moreover, let $\mathbf{f}$ be a vector of $\mathbb{R}^{n}, \mathbf{u}$ a vector of $\mathbb{R}^{q}$ representing the control. The vector $\mathbf{y}=\mathbf{y}(\mathbf{u}) \in \mathbb{R}^{n}$ which represents the state satisfies the following linear system

$$
A \mathbf{y}=\mathbf{f}+B \mathbf{u}
$$

We look for a control $\mathbf{u}$ that minimizes the following linear functional:

$$
J(\mathbf{u})=\left\|\mathbf{z}(\mathbf{u})-\mathbf{z}_{d}\right\|_{\mathbb{R}^{m}}^{2}+\|\mathbf{u}\|_{N}^{2}
$$

In this equation, $\mathbf{z}_{d}$ is a given vector (the target) of $\mathbb{R}^{m}, \mathbf{z}(\mathbf{u})=C \mathbf{y}(\mathbf{u})$ is the vector to be observed, where $C$ is a $m \times n$ matrix, $\|\mathbf{u}\|_{N}=(N \mathbf{u}, \mathbf{u})_{\mathbb{R}^{q}}^{1 / 2}$ is the $N$-norm of $\mathbf{u}, N$ being a given symmetric and positive definite matrix of dimension $q \times q$.

Upon interpreting the term $\|\mathbf{u}\|_{N}^{2}$ as the energy associated to the control, the problem is therefore: choose the control so that the observation $\mathbf{z}(\mathbf{u})$ is close to the target $\mathbf{z}_{d}$ and its energy is small.

Note that

$$
J(\mathbf{u})=\left(C A^{-1}(\mathbf{f}+B \mathbf{u})-\mathbf{z}_{d}, C A^{-1}(\mathbf{f}+B \mathbf{u})-\mathbf{z}_{d}\right)_{\mathbb{R}^{m}}+(N \mathbf{u}, \mathbf{u})_{\mathbb{R}^{q}}
$$

The cost functional $J(\mathbf{u})$ is therefore a quadratic function of $\mathbf{u}$ which has on $\mathbb{R}^{q}$ a global minimum. The latter is characterized by the condition

$$
J^{\prime}(\mathbf{u}) \mathbf{h}=0 \quad \forall \mathbf{h} \in \mathbb{R}^{q}
$$

where $J^{\prime}(\mathbf{u}) \mathbf{h}$ is the (Gâteaux) directional derivative along the direction $\mathbf{h}$ computed at the "point" $\mathbf{u}$, that is (see Definition $2.6$ of Chap. 2)

$$
J^{\prime}(\mathbf{u}) \mathbf{h}=\lim _{t \rightarrow 0} \frac{J(\mathbf{u}+t \mathbf{h})-J(\mathbf{u})}{t}
$$

Since

$$
A \mathbf{y}^{\prime}(\mathbf{u}) \mathbf{h}=B \mathbf{h} \text { and } \mathbf{z}^{\prime}(\mathbf{u}) \mathbf{h}=C \mathbf{y}^{\prime}(\mathbf{u}) \mathbf{h}
$$

for all $\mathbf{u}$ and $\mathbf{h}$, from $(18.6)$ we obtain

$$
\begin{aligned}
J^{\prime}(\mathbf{u}) \mathbf{h} &=2\left[\left(\mathbf{z}^{\prime}(\mathbf{u}) \mathbf{h}, \mathbf{z}(\mathbf{u})-\mathbf{z}_{d}\right)_{\mathbb{R}^{m}}+(N \mathbf{u}, \mathbf{h})_{\mathbb{R}} q\right] \\
&=2\left[\left(C A^{-1} B \mathbf{h}, C \mathbf{y}(\mathbf{u})-\mathbf{z}_{d}\right)_{\mathbb{R}^{m}}+(N \mathbf{u}, \mathbf{h})_{\mathbb{R}^{q}}\right]
\end{aligned}
$$

Let us introduce the solution $\mathbf{p}=\mathbf{p}(\mathbf{u}) \in \mathbb{R}^{n}$ of the following system, that is called the adjoint state of $(18.4)$

$$
A^{T} \mathbf{p}(\mathbf{u})=C^{T}\left(C \mathbf{y}(\mathbf{u})-\mathbf{z}_{d}\right)
$$

From (18.8) we deduce

$$
J^{\prime}(\mathbf{u}) \mathbf{h}=2\left[(B \mathbf{h}, \mathbf{p}(\mathbf{u}))_{\mathbb{R}^{n}}+(N \mathbf{u}, \mathbf{h})_{\mathbb{R} q}\right]
$$

that is

$$
J^{\prime}(\mathbf{u})=2\left[B^{T} \mathbf{p}(\mathbf{u})+N \mathbf{u}\right]
$$

Since $J$ attains its minimum at the point $\mathbf{u}$ for which $J^{\prime}(\mathbf{u})=\mathbf{0}$, we can conclude that the three-field system

$$
\left\{\begin{array}{l}
A \mathbf{y}=\mathbf{f}+B \mathbf{u} \\
A^{T} \mathbf{p}=C^{T}\left(C \mathbf{y}-\mathbf{z}_{d}\right) \\
B^{T} \mathbf{p}+N \mathbf{u}=\mathbf{0}
\end{array}\right.
$$

admits a unique solution $(\mathbf{u}, \mathbf{y}, \mathbf{p}) \in \mathbb{R}^{q} \times \mathbb{R}^{n} \times \mathbb{R}^{n}$ and that $\mathbf{u}$ is the unique optimal control of the original system.

In the next section we will introduce several examples of optimal control problems for the Laplace equation.

\subsection{Some examples of optimal control problems for the Laplace equation}

Consider for simplicity the case where the elliptic operator $A$ is the Laplacian. We define two different families of optimal control problems: the distributed control and the boundary control.

- Distributed control. Let us introduce the state problem

$$
\begin{cases}-\Delta y=f+u & \text { in } \Omega \\ y=0 & \text { on } \Gamma=\partial \Omega\end{cases}
$$

where $\Omega$ is a domain in $\mathbb{R}^{n}, y \in \mathscr{V}=H_{0}^{1}(\Omega)$ is the state variable, $f \in L^{2}(\Omega)$ is a given source term, and $u \in \mathscr{U}_{a d}=L^{2}(\Omega)$ is the control function. We can consider two different kinds of cost functional: - on the domain, for instance

$$
J(u)=\int_{\Omega}\left(y(u)-z_{d}\right)^{2} d \Omega
$$

- on the boundary, for instance (provided $y(u)$ is sufficiently regular)

$$
J(u)=\int_{\Gamma}\left(\frac{\partial y(u)}{\partial n}-z_{d_{\Gamma}}\right)^{2} d \gamma
$$

The functions $z_{d}$ and $z_{d_{\Gamma}}$ are two prescribed observation (or target) functions.

- Boundary control. Consider now the following state problem

$$
\begin{cases}-\Delta y=f & \text { in } \Omega \\ y=u & \text { on } \Gamma_{D} \\ \frac{\partial y}{\partial n}=0 & \text { on } \Gamma_{N}\end{cases}
$$

with $\Gamma_{D} \cup \Gamma_{N}=\partial \Omega$ and $\stackrel{\circ}{\Gamma}_{D} \cap \stackrel{\circ}{\Gamma}_{N}=\emptyset$. The control $u \in H^{\frac{1}{2}}\left(\Gamma_{D}\right)$ is defined on the Dirichlet boundary. Two different kinds of cost functional can be considered:

- on the domain, as in (18.13);

- on the boundary, for instance

$$
J(u)=\int_{\Gamma_{N}}\left(y(u)-z_{d_{\Gamma_{N}}}\right)^{2} d \gamma
$$

Here, too, $z_{d_{\Gamma_{N}}}$ represents a given observation function.

\section{$18.4$ On the minimization of linear functionals}

In this section we recall some results about the existence and uniqueness of extrema of linear functionals, with focus on those associated to control problems addressed in this chapter. For more results see, e.g., [Lio71, BG87, Bre86, TL58]. We consider a Hilbert space $\mathscr{U}$, endowed with a scalar product $(\cdot, \cdot)$, and a bilinear form $\pi$

$$
u, v \mapsto \pi(u, v) \quad \forall u, v \in \mathscr{U}
$$

that we assume to be symmetric, continuous and coercive. The norm induced on $\mathscr{U}$ by the scalar product will be denoted by $\|w\|=\sqrt{(w, w)}$. Let

$$
v \mapsto F(v) \quad \forall v \in \mathscr{U}
$$

be a linear and bounded functional on $\mathscr{U}$. Finally, let $\mathscr{U}_{a d}$ be the closed subspace of U of admissible control functions, and consider the following cost functional

$$
J(v)=\pi(v, v)-2 F(v) \quad \forall v \in \mathscr{U}_{a d}
$$

The following result holds:

Theorem 18.1. Under the previous assumptions, there exists a unique $u \in \mathscr{U}_{a d}$ such that

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-132.jpg?height=16&width=24&top_left_y=208&top_left_x=594)

$$
J(u)=\inf J(v) \quad \forall v \in \mathscr{U}_{a d},
$$

where $J(v)$ is defined in (18.17); $u$ is called optimal control. Moreover:

(i) The function $u \in \mathscr{U}_{\text {ad }}$ satisfies the variational inequality

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-132.jpg?height=65&width=167&top_left_y=368&top_left_x=132)

$$
\pi(u, v-u) \geq F(v-u) \quad \forall v \in \mathscr{U}_{a d} .
$$

$(18.19)$

(ii) If $\mathscr{U}_{a d}=\mathscr{U}$ (that is we consider $a$ non-constrained optimization problem), owing to the Lax-Milgram Lemma 3.1, u satisfies the following Euler equation associated to $(18.18)$

$$
\pi(u, w)=F(w) \quad \forall w \in \mathscr{U}
$$

$(18.20)$

(iii) If $\mathscr{U}_{a d}$ is a closed convex cone with vertex at the origin $0^{1}, u$ satisfies

-

$$
\pi(u, v) \geq F(v) \quad \forall v \in \mathscr{U}_{a d} \quad \text { and } \quad \pi(u, u)=F(u)
$$

(iv) Suppose the map $v \mapsto F(v)$ is strictly convex and differentiable, $J$ (not necessarily quadratic) satisfies: $J(v) \rightarrow \infty$ when $\|v\|_{\mathscr{W}} \rightarrow \infty \forall v \in \mathscr{U}_{a d}$. Then the unique function $u \in \mathscr{U}_{a d}$ which satisfies condition (18.18) is characterized by the variational inequality

Proof. For a complete proof see, e.g., [Lio71, Chap. 1, Theorem 1.1]. Here we prove (18.19). If $u$ minimizes (18.18), then for all $v \in \mathscr{U}_{a d}$ and every $0<\vartheta<1, J(u) \leq$ $J((1-\vartheta) u+\vartheta v)$, thus $\frac{1}{\vartheta}[J(u+\vartheta(v-u))-J(u)] \geq 0$. This inequality still holds when $\vartheta \rightarrow 0$ (provided this limit exists), whence

$$
J^{\prime}(u)(v-u) \geq 0 \quad \forall v \in \mathscr{U}_{a d} .
$$

Inequality (18.19) follows by recalling the definition (18.17) of $J$.

${ }^{1}$ A linear metric space $W$ is a closed convex cone with vertex in the origin 0 if: (1) $0 \in W$,

(2) $x \in W \Rightarrow k x \in W \forall k \geq 0$, (3) $x, y \in W \Rightarrow x+y \in W$, (4) $W$ is closed. The converse holds as well (whence (18.18) and (18.19) are in fact equivalent). Indeed, should $u$ satisfy (18.19), and so $(18.24)$, thanks to the convexity of the map $v \mapsto J(v)$ for every $0<\vartheta<1$ one has

$$
J(v)-J(w) \geq \frac{1}{\vartheta}[J((1-\vartheta) w+v)-J(w)] \quad \forall v, w \in \mathscr{U}_{a d}
$$

Taking the limit as $\vartheta \rightarrow 0$ we obtain

$$
J(v)-J(w) \geq J^{\prime}(w)(v-w)
$$

Taking $w=u$ and using $(18.24)$ we obtain that $J(v) \geq J(u)$, that is (18.18).

To prove $(18.20)$ it is sufficient to choose $v=u \pm w \in \mathscr{U}$ in (18.19).

Let us now prove $(18.21)$. The first inequality can be obtained by replacing $v$ with $v+u$ in (18.19). Setting now $v=0$ in (18.19) we obtain $\pi(u, u) \leq F(u)$. By combining the latter with the first inequality in (18.21) we obtain the second equation in $(18.21)$. The converse (that is, $(18.21) \Rightarrow(18.19)$ ) is obvious.

For the proof of (18.22) and (18.23), see [Lio71, Chap. 1, Theorem 1.4].

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-133.jpg?height=20&width=23&top_left_y=489&top_left_x=742)

Remark 18.1. If $J(v)$ is differentiable in $v, \forall v \in \mathscr{U}$, then for every minimizing function $u \in \mathscr{U}$ of $J$ (provided it does exist) we have $J^{\prime}(u)=0$. Moreover, under the assumptions of Theorem $18.1$ (step (iv)), there exists at least one minimizing element $u \in \mathscr{U}$

We can summarize by saying that the solution $u \in \mathscr{U}_{a d}$ of the minimization problem satisfies the following (equivalent) conditions:

$\begin{array}{lll}\text { i) } & J(u)=\inf J(v) & \forall v \in \mathscr{U}_{a d} \\ \text { ii) } & J(u) \leq J(v) & \forall v \in \mathscr{U}_{a d} \\ \text { iii) } & J^{\prime}(u)(v-u) \geq 0 & \forall v \in \mathscr{U}_{a d} \\ \text { iv) } & J^{\prime}(v)(v-u) \geq 0 & \forall u \in \mathscr{U}_{a d}\end{array}$

Before closing this section, consider the abstract problem of finding $u \in \mathscr{U}_{a d}$ satisfying the variational inequality (18.19) (when $\pi(\cdot, \cdot)$ is not symmetric, this problem does not correspond to a problem of minimization in the calculus of variations).

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-133.jpg?height=143&width=721&top_left_y=899&top_left_x=97)

For the proof, see [Lio71, Chap. 1, Theorem 2.1].

\subsection{The theory of optimal control for elliptic problems}

In this section we illustrate some existence and uniqueness results for the solution of a control problem governed by a linear elliptic equation (the equation of state). For the sake of simplicity we restrict to distributed control (see Sect. 18.3); however, similar results hold for boundary control problems as well, see [Lio71].

Let $\mathscr{V}$ and $\mathscr{H}$ be two Hilbert spaces, $\mathscr{V}^{\prime}$ the dual of $\mathscr{V}$ and $\mathscr{H}^{\prime}$ that of $\mathscr{H}$, and assume that $\mathscr{V}$ is dense in $\mathscr{H}$ with continuous injection. We recall that in this case, property $(2.10)$ of Chapter 2 holds. In addition, denote by $(\cdot, \cdot)$ the scalar product of $\mathscr{H}$ and suppose that $a(u, v)$ is a bilinear, continuous and coercive form on $\mathscr{V}$ (but not necessarily symmetric). Under these assumptions, the Lax-Milgram lemma guarantees that there exists a unique solution $y \in \mathscr{V}$ of problem

$$
a(y, \varphi)=(f, \varphi) \quad \forall \varphi \in \mathscr{V}
$$

By introducing the operator $A$ associated with the bilinear form $a(\cdot, \cdot)$ (see (3.39))

$$
A \in \mathscr{L}\left(\mathscr{V}, \mathscr{V}^{\prime}\right): \mathscr{v}^{\prime}\langle A \varphi, \psi\rangle_{\mathscr{V}}=a(\varphi, \psi) \quad \forall \varphi, \psi \in \mathscr{V}
$$

problem (18.26) becomes (in operator form)

$$
A y=f \quad \text { in } \mathscr{V}^{\prime}
$$

This equation will be supplemented by a distributed control term.

Let $\mathscr{U}$ be a Hilbert space of control functions, and $B$ an operator belonging to the space $\mathscr{L}\left(\mathscr{U}, \mathscr{V}^{\prime}\right)$. For every control function $u$ the equation of state of the system is

$$
A y(u)=f+B u \quad \text { in } \mathscr{V}^{\prime}
$$

or, in weak form,

$$
y(u) \in \mathscr{V}: a(y(u), \varphi)=(f, \varphi)+b(u, \varphi) \quad \forall \varphi \in \mathscr{V}
$$

where $b(\cdot, \cdot)$ is the bilinear form associated with the operator $B$, that is

$$
b(u, \varphi)=\mathscr{V}^{\prime}\langle B u, \varphi\rangle_{\mathscr{V}} \quad \forall u \in \mathscr{U}, \forall \varphi \in \mathscr{V}
$$

Let us denote with $\mathscr{Z}$ the Hilbert space of observation functions, and introduce the equation of observation

$$
z(u)=C y(u)
$$

for a suitable operator $C \in \mathscr{L}(\mathscr{V}, \mathscr{Z})$. At last, let us define the cost functional

$$
J(y(u), u)=\left\|C y(u)-z_{d}\right\|_{\mathscr{Z}}^{2}+(N u, u)_{\mathscr{W}},
$$

that we will indicate with the shorthand notation $J(u)$. Here $N \in \mathscr{L}(\mathscr{U}, \mathscr{U})$ is a symmetric positive definite form such that

$$
(N u, u)_{\mathscr{U}} \geq v\|u\|_{\mathscr{U}}^{2} \quad \forall u \in \mathscr{U},
$$

where $v>0$ and $z_{d} \in \mathscr{Z}$ is the desired (target) observation.

The optimal control problem consists in finding $u \in \mathscr{U}_{a d} \subseteq \mathscr{U}$ such that

$$
J(u)=\inf J(v) \quad \forall v \in \mathscr{U}_{a d}
$$

A reminder of the spaces and operators involved in the above definition of optimal control problem is depicted in Fig. 18.2. 

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-135.jpg?height=244&width=502&top_left_y=123&top_left_x=198)

Fig. 18.2. Functional spaces and operators involved in the statement of the control problem

Remark 18.2. When minimizing (18.32), one actually minimizes a combination between two terms. The former enforces that the observation $z(u)$ is closed to the desired value (the target) $z_{d}$. The latter penalizes the use of a control $u$ that is "too expensive". Heuristically speaking, we are trying to make $z(u)$ go towards $z_{d}$ by a reduced effort. Note that this theory applies also if the form $N$ is null, but in this case one can only prove the existence of an optimal control, not its uniqueness.

In order to apply the abstract theoretical results stated in Sect. 18.4, by noticing that the map $u \mapsto y(u)$ from $\mathscr{U}$ in $\mathscr{V}$ is affine we can rewrite $(18.32)$ as follows

$$
J(u)=\left\|C[y(u)-y(0)]+C y(0)-z_{d}\right\|_{\mathscr{Z}}^{2}+(N u, u)_{\mathscr{U}}
$$

Let us now define a bilinear form $\pi$ that is continuous in $\mathscr{U}$ and a functional $F$ as follows, for all $u, v \in \mathscr{U}$ :

$$
\begin{gathered}
\pi(u, v)=(C[y(u)-y(0)], C[y(v)-y(0)]) \mathscr{Z}+(N u, v)_{\mathscr{U}}, \\
F(v)=\left(z_{d}-C y(0), C[y(v)-y(0)]\right) \mathscr{\mathscr { Z }}
\end{gathered}
$$

Then

$$
J(v)=\pi(v, v)-2 F(v)+\left\|z_{d}-C y(0)\right\|_{\mathscr{Z}}^{2}
$$

Since $\|C[y(v)-y(0)]\|_{\mathscr{Z}}^{2} \geq 0$, owing to $(18.33)$ we obtain

$$
\pi(v, v) \geq v\|v\|_{\mathscr{U}}^{2} \quad \forall v \in \mathscr{U}
$$

By doing so we have cast the control problem in the general formulation addressed in Sect. 18.4. Then Theorem $18.1$ guarantees existence and uniqueness of the control function $u \in \mathscr{U}_{a d}$.

At this stage we would like to study the structure of the equations useful to the solution of the control problem. Thanks to Theorem 18.1, and since $A$ is an isomorphism between $\mathscr{V}$ and $\mathscr{V}^{\prime}$ (see Definition $2.4$ ), we have

$$
y(u)=A^{-1}(f+B u)
$$

whence $y^{\prime}(u) \psi=A^{-1} B \psi$ and therefore

$$
y^{\prime}(u)(v-u)=A^{-1} B(v-u)=y(v)-y(u)
$$

Since the optimal control must satisfy (18.22), dividing by 2 inequality (18.22) we obtain, thanks to $(18.35)$

$$
\left(C y(u)-z_{d}, C[y(v)-y(u)]\right)_{\mathscr{Z}}+(N u, v-u)_{\mathscr{U}} \geq 0 \quad \forall v \in \mathscr{U}_{a d}
$$

Let now $C^{\prime} \in \mathscr{L}\left(\mathscr{Z}^{\prime}, \mathscr{V}^{\prime}\right)$ be the adjoint of the operator $C \in \mathscr{L}(\mathscr{V}, \mathscr{Z})(\operatorname{see}(2.20))$. Then

$$
\mathscr{Z}\langle C y, v\rangle_{\mathscr{Z}^{\prime}}=\mathscr{v}\left\langle y, C^{\prime} v\right\rangle_{\mathscr{V}^{\prime}} \quad \forall y \in \mathscr{V}, \forall v \in \mathscr{Z}^{\prime}
$$

hence (18.36) becomes

$$
\mathscr{V}^{\prime}\left\langle C^{\prime} \Lambda_{\mathscr{Z}}\left(C y(u)-z_{d}\right), y(v)-y(u)\right\rangle_{\mathscr{V}}+(N u, v-u)_{\mathscr{U}} \geq 0 \quad \forall v \in \mathscr{U}_{a d}
$$

where $\Lambda_{\mathscr{Z}}$ denotes the canonical Riesz isomorphism from $\mathscr{Z}$ to $\mathscr{Z}^{\prime}($ see $(2.5))$. Let us now introduce the adjoint operator $A^{\prime} \in \mathscr{L}\left(\mathscr{V}, \mathscr{V}^{\prime}\right)$ of $A$

$$
\mathscr{v}\left\langle A^{\prime} \varphi, \psi\right\rangle_{\mathscr{V}}=\mathscr{v}\langle\varphi, A \psi\rangle_{\mathscr{V}} \quad \forall \varphi, \psi \in \mathscr{V}
$$

This operator was denoted with the symbol $A^{*}$ in Sect. $3.5$ (see the Lagrange identity (3.42)). Owing to $(3.40)$ and $(3.41)$ we obtain

$$
\mathscr{V}^{\prime}\left\langle A^{\prime} \varphi, \psi\right\rangle_{\mathscr{V}}=a(\psi, \varphi) \quad \forall \varphi, \psi \in \mathscr{V}
$$

We define adjoint state (or adjoint variable) $p(u) \in \mathscr{V}$ the solution of the adjoint equation

$$
A^{\prime} p(u)=C^{\prime} \Lambda_{\mathscr{Z}}\left(C y(u)-z_{d}\right)
$$

with $u \in \mathscr{U}$. Thanks to (18.39), we obtain

$$
\begin{gathered}
\mathscr{V}^{\prime}\left\langle C^{\prime} \Lambda_{\mathscr{Z}}\left(C y(u)-z_{d}\right), y(v)-y(u)\right\rangle_{\mathscr{V}}= \\
\mathscr{y}^{\prime}\left\langle A^{\prime} p(u), y(v)-y(u)\right\rangle_{\mathscr{V}}=
\end{gathered}
$$

(thanks to the definition of $\left.A^{\prime}\right) \quad \mathscr{V}\langle p(u), A(y(v)-y(u))\rangle_{\mathscr{V}}=$

$$
\text { (thanks to (18.28)) } \quad \mathscr{V}\langle p(u), B(v-u)\rangle_{\mathscr{V}}=\mathscr{\mathscr { U } ^ { \prime }}\left\langle B^{\prime} p(u), v-u\right\rangle_{\mathscr{U}},
$$

where $B^{\prime} \in \mathscr{L}\left(\mathscr{V}, \mathscr{U}^{\prime}\right)$ is the adjoint operator of $B$ (see $\left.(2.20)\right)$. It follows that, with the help of the Riesz canonical isomorphism $\Lambda_{\mathscr{W}}$ of $\mathscr{U}$ and $\mathscr{U}^{\prime}$ (see again $\left.(2.5)\right)$, inequality (18.22) can be rewritten as

$$
\frac{1}{2} J^{\prime}(u)(v-u)=\left(\Lambda_{\mathscr{U}}^{-1} B^{\prime} p(u)+N u, v-u\right) \mathscr{U} \geq 0 \quad \forall v \in \mathscr{U}_{a d}
$$

In the case of unconstrained control, that is when $\mathscr{U}_{a d}=\mathscr{U}$, the last inequality becomes in fact an equality, that is

$$
B^{\prime} p(u)+\Lambda_{\mathscr{U}} N u=0 .
$$

This follows by taking $v=u-\left(\Lambda_{\mathscr{W}}^{-1} B^{\prime} p(u)+N u\right)$ in (18.40). In the case where $\mathscr{V} \subset \mathscr{U}$ the previous equation implies that

$$
b(v, p)+n(u, v)=0 \quad \forall v \in \mathscr{V}
$$

The final result is recapped in the following Theorem ([Lio71, Chap. 2, Thm. 1.4]).

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-137.jpg?height=702&width=723&top_left_y=255&top_left_x=95)

Remark 18.3. Apart from the term depending on the form $N, J^{\prime}$ can be obtained from the adjoint variable $p$ through the operator $B^{\prime}$. This result will stand at the base of the numerical methods which are useful to determine an approximate control function. If $\mathscr{U}_{a d}=\mathscr{U}$, then the optimal control satisfies

$$
N u=-\Lambda_{\mathscr{U}}^{-1} B^{\prime} p(u)
$$

Thanks to identity $(2.24)$ we have

$$
\Lambda_{\mathscr{U}}^{-1} B^{\prime}=B^{T} \Lambda_{\mathcal{V}}
$$

where $\Lambda_{\mathscr{V}}$ is the Riesz canonical isomorphism from $\mathscr{V}$ to $\mathscr{V}^{\prime}$ and $B^{T}: \mathscr{V}^{\prime} \rightarrow \mathscr{U}$ is the transpose operator of $B$ introduced in $(2.22)$. 

\subsection{Some examples of optimal control problems}

In this section we introduce three examples of optimal control problems.

\subsubsection{A Dirichlet problem with distributed control}

Let us recover the example of distributed control (18.12) and consider the following cost functional (to be minimized)

$$
J(v)=\int_{\Omega}\left(y(v)-z_{d}\right)^{2} d \Omega+(N v, v)
$$

in which, for instance, we can set $N=v I, v>0$. In this case $\mathscr{V}=H_{0}^{1}(\Omega), \mathscr{H}=$ $L^{2}(\Omega), \mathscr{U}=\mathscr{H}\left(\right.$ then $\left.(N v, v)=(N v, v)_{\mathscr{U}}\right)$ therefore $\Lambda_{\mathscr{W}}$ is the identity operator. Moreover, $B$ is the identity operator, $C$ is the injection operator of $\mathscr{V}$ in $\mathscr{H}, \mathscr{Z}=\mathscr{H}$ and therefore $\Lambda_{\mathscr{Z}}$ is the identity operator. Finally, $a(u, v)=\int_{\Omega} \nabla u \cdot \nabla v d \Omega$. Owing to Theorem $18.3$ we obtain, with $A=-\Delta$, the following optimality system:

$$
\begin{cases}y(u) \in H_{0}^{1}(\Omega) \quad: \quad A y(u)=f+u \quad \text { in } \Omega \\ p(u) \in H_{0}^{1}(\Omega) & : \quad A^{\prime} p(u)=y(u)-z_{d} \quad \text { in } \Omega \\ u \in \mathscr{U}_{a d} \quad & : \int_{\Omega}(p(u)+N u)(v-u) d \Omega \geq 0 \quad \forall v \in \mathscr{U}_{a d}\end{cases}
$$

In the unconstrained case in which $\mathscr{U}_{a d}=\mathscr{U}\left(=L^{2}(\Omega)\right)$, the last inequality reduces to the equation

$$
p(u)+N u=0,
$$

as we can see by taking $v=u-(p(u)+N u)$.

The first two equations of (18.49) provide a system for the two variables $y$ and $p$

$$
\left\{\begin{array}{lll}
A y+N^{-1} p=f \quad \text { in } \Omega, & y=0 \quad \text { on } \partial \Omega \\
A^{\prime} p-y=-z_{d} \quad \text { in } \Omega, & p=0 \quad \text { on } \partial \Omega
\end{array}\right.
$$

whose solution provides the optimal control $u=-N^{-1} p$.

If $\Omega$ has a smooth boundary, by the elliptic regularity property both $y$ and $p$ belong to the space $H^{2}(\Omega)$. Since $N^{-1}$ maps $H^{2}(\Omega)$ into itself, the optimal control $u$ belongs to $H^{2}(\Omega)$, too.

\subsubsection{A Neumann problem with distributed control}

Consider now the problem

$$
\begin{cases}A y(u)=f+u & \text { in } \Omega \\ \frac{\partial y(u)}{\partial n_{A}}=g & & \text { on } \partial \Omega\end{cases}
$$

for given $f \in L^{2}(\Omega)$ and $g \in H^{-1 / 2}(\partial \Omega)$, where $A$ is an elliptic operator and $\frac{\partial}{\partial n_{A}}$ is the conormal derivative associated with $A$ (see $(3.34))$. The cost functional to be minimized is the one introduced in (18.48). In this case $\mathscr{V}=H^{1}(\Omega), \mathscr{H}=L^{2}(\Omega)$, $\mathscr{U}=\mathscr{H}, B$ is the identity operator, $C$ is the injection map of $\mathscr{V}$ into $\mathscr{H}$,

$$
a(\psi, \varphi)=\mathscr{V}^{\prime}\langle A \psi, \varphi\rangle_{\mathscr{V}}, \quad F(\varphi)=\int_{\Omega} f \varphi d \Omega+\int_{\partial \Omega} g \varphi d \gamma
$$

If $A \varphi=-\Delta \varphi+\beta \varphi$, with $\beta>0$, then

$$
a(\psi, \varphi)=\int_{\Omega} \nabla \psi \cdot \nabla \varphi d \Omega+\int_{\Omega} \beta \psi \varphi d \Omega
$$

The variational formulation of the state problem $(18.50)$ is

$$
\text { find } y(u) \in H^{1}(\Omega): a(y(u), \varphi)=F(\varphi) \quad \forall \varphi \in H^{1}(\Omega)
$$

The adjoint problem is the following Neumann problem

$$
\begin{cases}A^{\prime} p(u)=y(u)-z_{d} & \text { in } \Omega \\ \frac{\partial p(u)}{\partial n_{A^{\prime}}}=0 & \text { on } \partial \Omega\end{cases}
$$

The optimal control can be obtained by solving the system formed by $(18.50),(18.52)$, and

$$
u \in \mathscr{U}_{a d}: \int_{\Omega}(p(u)+N u)(v-u) d \Omega \geq 0 \quad \forall v \in \mathscr{U}_{a d}
$$

\subsubsection{A Neumann problem with boundary control}

Consider now the problem

$$
\begin{cases}A y(u)=f & \text { in } \Omega \\ \frac{\partial y(u)}{\partial n_{A}}=g+u & & \text { on } \partial \Omega\end{cases}
$$

where the operator is the same as before and the cost functional is still that of $(18.48)$. In this case,

$$
\mathscr{V}=H^{1}(\Omega), \quad \mathscr{H}=L^{2}(\Omega), \quad \mathscr{U}=H^{-1 / 2}(\partial \Omega)
$$

For all $u \in \mathscr{U}, B u \in \mathscr{V}^{\prime}$ is given by $\mathscr{V}\langle B u, \varphi\rangle_{\mathscr{V}}=\int_{\partial \Omega} u \varphi d \gamma$, and $C$ is the injection map of $\mathscr{V}$ in $\mathscr{H}$.

The weak formulation of $(18.54)$ is

find $y(u) \in \mathscr{H}^{1}(\Omega): a(y(u), \varphi)=\int_{\Omega} f \varphi d \Omega+\int_{\partial \Omega}(g+u) \varphi d \gamma \quad \forall \varphi \in \mathscr{H}^{1}(\Omega)$. The adjoint problem is still given by (18.52), while the variational inequality yielding the optimal control is the third of (18.42). The interpretation of this inequality is far from trivial. If we choose

$$
(u, v)_{\mathscr{U}}=\int_{\partial \Omega}\left(-\Delta_{\partial \Omega}\right)^{-1 / 4} u\left(-\Delta_{\partial \Omega}\right)^{-1 / 4} v d \gamma=\int_{\partial \Omega}\left(-\Delta_{\partial \Omega}\right)^{-1 / 2} u v d \gamma
$$

as scalar product in $\mathscr{U}$, where $\Delta_{\partial \Omega}$ is the Laplace-Beltrami operator (see, e.g., [QV94]), it can be proven that the third inequality of (18.42) is equivalent to (see [Lio71, Chap. 1, Sect. 2.4])

$$
\int_{\partial \Omega}\left(p(u)_{\mid \partial \Omega}+\left(-\Delta_{\partial \Omega}\right)^{-1 / 2} N u\right)(v-u) d \gamma \geq 0 \quad \forall v \in \mathscr{U}_{a d}
$$

In Tables $18.1$ and $18.2$ we summarize the main conclusions that were drawn for the problems just considered.

Table 18.1. Summary of Dirichlet control problems

\begin{tabular}{|l|l|l|}
\hline Dirichlet conditions & Distributed Observation & Boundary Observation \\
\hline \multirow{3}{*}{ Boundary Control } & \begin{tabular}{ll}
$A y=f+u$ & in $\Omega$ \\
& $\begin{cases}A y=f+u & \text { in } \Omega \\
y=0 & \text { on } \partial \Omega\end{cases}$ & $\begin{cases}A y= \\
y=0 & \text { on } \partial \Omega\end{cases}$ \\
\cline { 1 - 4 } Distributed Control & $\begin{cases}A^{\prime} p=y-z_{d} & \text { in } \Omega \\
p=0 & \text { on } \partial \Omega\end{cases}$ & $\left\{\begin{array}{l}A^{\prime} p=y-z_{d} & \text { in } \Omega \\
p=0 & \text { on } \partial \Omega\end{array}\right.$ & $\left\{\begin{array}{l}A^{\prime} p=0 & \text { in } \Omega \\
J(y, u)=\int_{\Omega}\left(y-z_{d}\right)^{2} d \Omega+v \int u^{2} d \Omega \\
J(y, u)=\int_{\Omega}(y)\end{array}\right.$ \\
$\frac{1}{l \mid}\left\{\begin{array}{l}A, J) \\
p\end{array}\right.$ & $\left\{\begin{array}{l}A y=f & \text { in } \Omega \\
y=u & \text { on } \partial \Omega\end{array}\right.$ \\
$\qquad$ S $J_{A} J^{\prime}$ Con & In $\left.\Omega, J_{A}\right)^{2} d \gamma+v \int u^{2} d \Omega$ $y=u$ \\
Con Con \\
Co $)(u)=2$ Con \\
Co $)(u)$ ( $)=2$ \\
D $)(u)$ ( $)$ \\
D $)(y)$ Co \\
D $)(u)$ ( $)$ \\
D $)(y)$ on \\
$A_{A}(y)$ on \\
$\begin{aligned}&A^{\prime} p=0 & \left.\frac{\partial y}{\partial n_{A}}-z_{d}\right)^{2} d \gamma+v \int u^{2} d \gamma \\
&p=-\left(\frac{\partial y}{\partial n_{A}}-z_{d}\right)\end{aligned}$ \\
D on \\
D $)$ on \\
D $)$ ( $)$ \\
D $)$ ( $)$ \\
D $)$ ( $)$ \\
D $)$ ( $)$ \\
D 2 \\
D 2 \\
D $\boldsymbol{J}(u)+\boldsymbol{d}(\boldsymbol{x})$ $\boldsymbol{y}(\boldsymbol{x})$
\end{tabular} \\
$\boldsymbol{A}_{\boldsymbol{A}} \boldsymbol{l}(\boldsymbol{x})$ $\boldsymbol{y}(\boldsymbol{x})$
\end{tabular}

Table 18.2. Summary of Neumann control problems

\begin{tabular}{|c|c|c|}
\hline Neumann conditions & Distributed Observation & Boundary Observation \\
\hline \multirow{3}{*}{ Distributed Control } & \begin{tabular}{ll}
$A y=f+u$ & in $\Omega$ \\
$J(y, u)=\int\left(y-z_{d}\right)^{2} d \Omega+v \int u^{2} d \Omega$ & in $\Omega$ \\
& $\left\{\begin{array}{c}A y=f+u & \text { in } \\
\frac{\partial y}{\partial n_{A}}=g & \text { on } \partial \Omega\end{array}\right.$ & $\left\{\begin{array}{c}A y, u)=\int\left(y-z_{d}\right)^{2} d \gamma+v \int u^{2} d \Omega \\
\frac{\partial y}{\partial n_{A}}=g & \text { on } \partial \Omega\end{array}\right.$ \\
\hline Boundary Control & $\begin{array}{ll}A^{\prime} p=y-z_{d} & \text { in } \Omega \\
J(y, u)=\int_{\Omega}\left(y-z_{d}\right)^{2} d \Omega+v \int_{\Omega} u^{2} d \gamma \\
\frac{\partial p}{\partial n_{A^{\prime}}}=0 & \text { on } \partial \Omega & \text { in } & \text { in } \Omega \\
& \left\{\begin{array}{l}A y=f \\
\frac{1}{2} J^{\prime}(u)=p+v u\end{array}\right. & \text { in } \Omega & \begin{array}{l}A^{\prime} p=0 & \text { in } \Omega \\
\frac{\partial p}{\partial n_{A}}=y-z_{d}\end{array} & \text { on } \partial \Omega \\
\frac{\partial y}{\partial n}\end{array}$ & $\begin{aligned}&A^{\prime} p=y-z_{d} & \text { in } \Omega \\
&\frac{\partial p}{\partial n_{A^{\prime}}}=0 & \text { on } \partial \Omega & \left\{\begin{array}{l}A^{\prime} p=0 & \text { in } \Omega \\
\frac{\partial p}{\partial n_{A^{\prime}}}=y-z_{d}\end{array}\right. & \text { on } \partial \Omega \\
&\frac{1}{\partial} J^{\prime}(u)=p+v u & \text { on } \partial \Omega & \begin{array}{l}A y=f & \operatorname{l} \Omega \\
\frac{\partial y}{\partial n_{A}}\end{array}\end{aligned}$ \\
D $\frac{\partial}{\text { D }} \end{array}$ D D D D D D $\boldsymbol{S}_{\boldsymbol{A}}$ & $\begin{aligned}&\boldsymbol{A}(\boldsymbol{u})=p+v u & \text { in } \Omega\end{aligned}$ \\
\hline
\end{tabular}
\end{tabular}

\subsection{Numerical tests}

In this section we present some numerical tests for the solution of $1 \mathrm{D}$ optimal control problems similar to those summarized in Tables $18.1$ and 18.2. We postpone unitil Sect. $18.3$ the description of the numerical tecniques that are used for the approximation of $y, p$ and $u$.

For all numerical simulations we consider the domain $\Omega=(0,1)$, a simple diffusionreaction operator $A$

$$
A y=-\mu y^{\prime \prime}+\gamma y
$$

and the very same cost functional considered in the tables, with a regularization coefficient $v=10^{-2}$ (unless otherwise specified). We discretize both state and adjoint problems by means of piecewise-linear finite elements, with grid-size $h=10^{-2}$; for solving 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-142.jpg?height=280&width=710&top_left_y=114&top_left_x=100)

Fig. 18.3. Case D1. Initial and optimal state variables and the desired function (left); optimal control function (right)

Table 18.3. Case D1. Number of iterations and optimal cost functional corresponding to different values of $v$

\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline$v$ & $i t$ & $J$ & $v$ & $i t$ & $J$ & $v$ & $i t$ & $J$ \\
\hline $10^{-2}$ & 11 & $0.0202$ & $10^{-3}$ & 71 & $0.0047$ & $10^{-4}$ & 349 & $0.0011$ \\
\hline
\end{tabular}

the minimization problem we use the conjugate gradient method with an acceleration parameter $\tau^{k}$ initialized with $\tau^{0}=\bar{\tau}$ and then, when necessary for the convergence, reduce it by 2 at every subsequent step. This satisfies the Armijo rule (see Sect. 18.9). The tolerance tol for the iterative method is fixed to $10^{-3}$, with the following stopping criterium $\left\|J^{\prime}\left(u^{k}\right)\right\|<\operatorname{Tol}\left\|J^{\prime}\left(u^{0}\right)\right\|$.

- Case D1 (Table $18.1$ top left): distributed control and observation, with Dirichlet boundary conditions. We assume

$$
\mu=1, \quad \gamma=0, \quad f=1, \quad u^{0}=0, \quad z_{d}=\left\{\begin{array}{ll}
x & x \leq 0.5 \\
1-x & x>0.5
\end{array}, \quad \bar{\tau}=10\right.
$$

The value of the cost functional for $u^{0}$ is $J^{0}=0.0396$. Ater 11 iterations we obtain the optimal cost functional $J=0.0202$. In Fig. 18.3 we report the state variable for the initial and optimal control $u$ and the desired function $z_{d}$ (left), and the optimal control function (right).

As displayed in Table 18.3, the number of iterations increases as $v$ decreases. In the same Table, we also report, for the sake of comparison, the values of the cost functional $J$ corresponding to the optimal value of $u$ for different values of $v$.

In Fig. $18.4$ we report the optimal state (left) and the control functions (right) obtained for different values of $v$.

- Case D2 (Table $18.1$ top right): distributed control and boundary observation, with Dirichlet boundary condition. We consider $\mu=1, \gamma=0, f=1, u^{0}=0$, while the target function $z_{d}$ is such that $z_{d}(0)=-1$ and $z_{d}(1)=-4 ;$ finally, $\bar{\tau}=0.1$. At the initial step we have $J=12.5401$ and after 89 iterations $J=0.04305$; we can 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-143.jpg?height=282&width=688&top_left_y=112&top_left_x=112)

Fig. 18.4. Case D1. Optimal state variables $y_{2}$ (for $\left.v=1 e-2\right), y_{3}$ (for $\left.v=1 e-3\right), y_{4}$ (for $v=1 e-4)$ and desired function $z_{d}$ (left); optimal control $u_{2}$ (for $\left.v=1 e-2\right), u_{3}$ (for $\left.v=1 e-3\right)$, $u_{4}$ (for $\left.v=1 e-4\right)$ (right)

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-143.jpg?height=277&width=340&top_left_y=502&top_left_x=94)
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-143.jpg?height=278&width=708&top_left_y=500&top_left_x=94)

observe how the normal derivative of the state variable is "near" the desired value $\left.z_{d} \mid \mu \frac{\partial y}{\partial n}(0), \mu \frac{\partial y}{\partial n}(1)\right]=[-1.0511,-3.8695]$. In Fig. $18.5$ we report the initial and optimal state (left) and the corresponding optimal control function (right).

- Case D3 (Table $18.1$ bottom left): boundary control and distributed observation, with Dirichlet boundary condition. We consider $\mu=1, \gamma=0, f=1$, initial control $u^{0}$ such that $u^{0}(0)=u^{0}(1)=0, z_{d}=-1-3 x$ and $\bar{\tau}=0.1$. The initial functional value is $J=0.4204$, after 55 iterations we have $J=0.0364$ and the optimal control on the boundary is $[u(0), u(1)]=[0.6638,0.5541]$. In Fig. 18.6, left, we report the initial and final state variables and the desired observation function.

- Case D4 (Table $18.1$ bottom right): boundary control and observation, with Dirichlet boundary condition. We assume $\mu=1, \gamma=0, f=1$, initial control $u^{0}$ such that $u^{0}(0)=u^{0}(1)=0$, while the target function $z_{d}$ is such that $z_{d}(0)=-1$ and $z_{d}(1)=-4 ;$ finally, $\bar{\tau}=0.1$. For $i t=0$ the cost functional value is $J=12.5401$, after only 4 iterations $J=8.0513$ and the optimal control on the boundary is $[u(0), u(1)]=[0.7481,-0.7481]$. In Fig. 18.6, right, we report the state variable. 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-144.jpg?height=276&width=704&top_left_y=114&top_left_x=94)

Fig. 18.6. Left: Case D3. Initial and optimal state variables and desired observation function. Right: Case D4. Initial and optimal state variables

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-144.jpg?height=272&width=335&top_left_y=482&top_left_x=102)
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-144.jpg?height=276&width=710&top_left_y=478&top_left_x=102) control variable (right)

- Case N1 (Table $18.2$ top left): distributed control and observation, with Neumann boundary condition. We consider $\mu=1, \gamma=1, f=0, g=-1, u^{0}=0, z_{d}=1, \bar{\tau}=$ $0.1$. At the initial step we have $J=9.0053$, after 18 iterations the cost functional value is $J=0.0944$. In Fig. $18.7$ we report the state variable for the Neumann problem (left) and the final optimal control (right).

- Case N2 (Table $18.2$ top right): distributed control and boundary observation, with Neumann boundary condition. We consider $\mu=1, \gamma=1 f=0$, the function $g$ such that $g(0)=-1$ and $g(1)=-4, u^{0}=0$, while the target function $z_{d}$ is such that $z_{d}(0)=z_{d}(1)=1 ;$ finally, $\bar{\tau}=0.1$. For $i t=0, J=83.1329$, after 153 iterations $J=0.6280$, the optimal state on the boundary is $[y(0), y(1)]=[1.1613,0.7750]$. In Fig. $18.8$ we report the state variable (left) and the optimal control variable (right).

- Case N3 (Table $18.2$ bottom left): boundary control and distributed observation, with Neumann boundary condition. We assume $\mu=1, \gamma=1, f=0$, initial control $u^{0}$ such that $u^{0}(0)=u^{0}(1)=0, z_{d}=-1-3 x, \bar{\tau}=0.1$. The initial cost functional value is $J=9.0053$, after 9 iterations we have $J=0.0461$, and the optimal control 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-145.jpg?height=278&width=700&top_left_y=114&top_left_x=107)

Fig. 18.8. Case N2. Initial and optimal state variables (left); optimal control variable (right)

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-145.jpg?height=278&width=342&top_left_y=517&top_left_x=109)
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-145.jpg?height=278&width=698&top_left_y=517&top_left_x=108) Right: Case N4. Initial and optimal state variables.

is $[u(0), u(1)]=[1.4910,1.4910]$. In Fig. 18.9, left, we report the state variable for the initial and optimal control $u$, together with the desired observation function.

- Case N4 (Table $18.2$ bottom right): boundary control and observation, with Neumann boundary condition. We consider $\mu=1, \gamma=1, f=0, g$ such that $g(0)=-1$ and $g(1)=-4$, the initial control $u^{0}$ such that $u^{0}(0)=u^{0}(1)=0$ while the target function $z_{d}$ such that $z_{d}(0)=z_{d}(1)=1 ;$ finally, $\bar{\tau}=0.1$. At the initial step $J=83.1329$, after 37 iterations $J=0.2196$, the optimal control is $[u(0), u(1)]=[1.5817,4.3299]$ and the observed state on the boundary is $[y(0), y(1)]=[1.0445,0.9282]$. In Fig. 18.9, right, we report the initial and optimal state variables. 

\subsection{Lagrangian formulation of control problems}

In this section we present another methodological approach for the solution of optimal control problems, based on Lagrange multipliers; this approach better highlights the role played by the adjoint variable.

\subsubsection{Constrained optimization in $\mathbb{R}^{n}$}

Let us start by a simple example, the constrained optimization in $\mathbb{R}^{n}$. Given two functions $f, g \in C^{1}(X)$, where $X$ is an open set in $\mathbb{R}^{n}$, we look for the extrema of $f$ constrained to belong to

$$
E_{0}=\left\{\mathbf{x} \in \mathbb{R}^{n}: g(\mathbf{x})=0\right\}
$$

For simplicity of exposition we are considering the case where the constraint $g$ is a scalar function. We give the following definitions of regular points and of constrained critical points:

On the basis of these definitions the following result holds:

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-146.jpg?height=132&width=719&top_left_y=867&top_left_x=97)

We introduce the function $\mathscr{L}: X \times \mathbb{R} \mapsto \mathbb{R}$, called the Lagrangian (or Lagrangian functional),

$$
\mathscr{L}(\mathbf{x}, \lambda)=f(\mathbf{x})-\lambda g(\mathbf{x})
$$

From Theorem $18.4$ we deduce that $\mathbf{x}_{0}$ is a constrained critical point if and only if $\left(\mathbf{x}_{0}, \lambda_{0}\right)$ is a (free) critical point for $\mathscr{L}$. The number $\lambda_{0}$ is called Lagrange multiplier and can be obtained, together with $\mathbf{x}_{0}$, by solving the system

$$
\nabla \mathscr{L}(\mathbf{x}, \lambda)=\mathbf{0}
$$

that is

$$
\left\{\begin{array}{l}
\mathscr{L}_{\mathbf{x}}=\nabla f-\lambda \nabla g=\mathbf{0} \\
\mathscr{L}_{\lambda}=-g=0
\end{array}\right.
$$

\subsubsection{The solution approach based on the Lagrangian}

In this section we will extend the theory developed in Sect. 18.8.1 to optimal control problems. Also in this context, the Lagrangian approach can be used as an alternative to the approach "à la Lions" (see, for example, [BKR00]).

It is actually used to integrate the techniques of grid adaptivity based on a posteriori error estimates with optimal control problems (see [BKR00, Ded04]).

The approach based on Lagrange multipliers is also widely used to solve shape optimization problems in which the control is represented by the shape of the computational domain. The optimal control function $u$ is therefore a function defined on the boundary (or on a subset of it) which describes the optimal displacement from the original position. The interested reader can consult, e.g., [Jam88, MP01, SZ91].

In Sect. 18.8.1 the approach based on the Lagrangian allows the determination of the extrema of a function $f$ with constraint $g$. Instead, in control problems we look for a function $u \in \mathscr{U}_{a d} \subset \mathscr{U}$ satisfying the minimization problem (18.34), $y(u)$ being the solution of the state equation (18.28). As usual, $A$ is an elliptic differential operator applied to the state variable, while $B$ is an operator applied to the control function in the state equation. This problem can be regarded as a constrained minimization problem, provided we state a suitable correspondence between the cost functional $J$ and the function $f$ of Sect. 18.8.1), between the equation of state and the constraint equation $g=0$, and, finally, between the control function $u$ and the extremum $\mathbf{x}$.

We assume, for the sake of simplicity, that $\mathscr{U}_{a d}=\mathscr{U}$. For a more general treatment, see, e.g., [Gun03].

The solution of the optimal control problem can therefore be regarded as the search for the (unconstrained) critical "points" of the following Lagrangian functional

$$
\mathscr{L}(y, p, u)=J(y(u), u)+\mathscr{v}\langle p, f+B u-A y(u)\rangle_{\mathscr{V}^{\prime}}
$$

where $p$ is the Lagrange multiplier. In this framework the (unconstrained) critical points are represented by the functions $y, p$ and $u$. The problem therefore becomes

find $(y, p, u) \in \mathscr{V} \times \mathscr{V} \times \mathscr{U}: \nabla \mathscr{L}(y, p, u)[(\psi, \varphi, \phi)]=0 \quad \forall(\psi, \varphi, \phi) \in \mathscr{V} \times \mathscr{V} \times \mathscr{U}$

that is

$$
\begin{cases}\mathscr{L}_{p}(y, p, u)[\psi]=0 & \forall \psi \in \mathscr{V} \\ \mathscr{L}_{y}(y, p, u)[\varphi]=0 & \forall \varphi \in \mathscr{V} \\ \mathscr{L}_{u}(y, p, u)[\phi]=0 & \forall \phi \in \mathscr{U}\end{cases}
$$

We have used the abridged notation $\mathscr{L}_{y}$ to indicate the Gâteaux derivative of $\mathscr{L}$ with respect to $y$ (see Definition 2.6). The notations $\mathscr{L}_{p}$ and $\mathscr{L}_{u}$ have a similar meaning. Consider now as an example a state equation of elliptic type with two linear operators $A$ and $B$ that we rewrite in the weak form (18.29); given $u \in \mathscr{U}$ and $f \in \mathscr{H}$,

$$
\text { find } y=y(u) \in \mathscr{V}: a(y, \varphi)=(f, \varphi)+b(u, \varphi) \quad \forall \varphi \in \mathscr{V}
$$

The bilinear form $a(\cdot, \cdot)$ is associated with the operator $A, b(\cdot, \cdot)$ with $B$. The cost functional to be minimized can be expressed as follows

$$
J(y(u), u)=\left\|C y(u)-z_{d}\right\|^{2}+n(u, u)
$$

where $C$ is the operator that maps the state variable into the space $\mathscr{Z}$ of the observed functions, $z_{d}$ is the target observation and $n(\cdot, \cdot)$ is the symmetric bilinear form associated to $N$. Thus far, no assumption was made on the boundary conditions, the kind of control (either distributed or on the boundary) nor the kind of observation. This was done on purpose in order to consider a very general framework. In weak form, $(18.55)$ becomes

$$
\mathscr{L}(y, p, u)=J(y(u), u)+b(u, p)+(f, p)-a(y, p)
$$

Since

$$
\begin{cases}\mathscr{L}_{p}(y, p, u)[\varphi]=(f, \varphi)+b(u, \varphi)-a(y, \varphi) & \forall \varphi \in \mathscr{V} \\ \mathscr{L}_{y}(y, p, u)[\psi]=2\left(C y-z_{d}, C \psi\right)-a(\psi, p) & \forall \psi \in \mathscr{V} \\ \mathscr{L}_{u}(y, p, u)[\phi]=b(\phi, p)+2 n(u, \phi) \quad \forall \phi \in \mathscr{U}\end{cases}
$$

(18.56) yields the optimality system

$$
\begin{cases}y \in \mathscr{V}: a(y, \varphi)=b(u, \varphi)+(f, \varphi) & \forall \varphi \in \mathscr{V} \\ p \in \mathscr{V}: a(\psi, p)=2\left(C y-z_{d}, C \psi\right) & \forall \psi \in \mathscr{V} \\ u \in \mathscr{U}: b(\phi, p)+2 n(u, \phi)=0 & \forall \phi \in \mathscr{U}\end{cases}
$$

It is worth noticing that, upon rescaling $p$ as $\tilde{p}=p / 2$, the variables $(y, \tilde{p}, u)$ obtained from (18.60) actually satisfy the system (18.43) obtained in the framework of Lions' approach. Note that the vanishing of $\mathscr{L}_{p}$ yields the state equation (in weak form), that of $\mathscr{L}_{y}$ generates the equation for the Lagrange multiplier (which can be identified with the adjoint equation), and that of $\mathscr{L}_{u}$ yields the so-called sensitivity equation expressing the condition that the optimum is achieved. The adjoint variable, that is the Lagrange multiplier, is associated to the sensitivity of the cost functional J with respect to the variation of the observation function, and therefore to the variation of the control $u$.

It turns out to be very useful to express the Gâteaux derivative of the Lagrangian $\mathscr{L}_{u}$ in terms of the derivative of the cost functional $J$ with respect to $u,(18.60)$, according to what we have seen in Sect. 2.2. This correspondence is guaranteed by the Riesz representation theorem (see Theorem 2.1). Indeed, since $\mathscr{L}_{u}[\phi]$ is a linear and bounded functional, and $\phi$ belongs to the Hilbert space $\mathscr{U}$, we can compute $J^{\prime}$ case by case, that is, from the third of (18.59), and from $(18.45)$

$$
\mathscr{L}_{u}[\phi]=\left(J^{\prime}(u), \phi\right)_{\mathscr{U}}=b(\phi, p)+2 n(u, \phi)
$$

It is worth noticing how the adjoint equation is generated. According to Lions' theory the adjoint equation is based on the use of the adjoint operator (see equation (18.39)), whereas when using the approach based on the Lagrangian we obtain it by differentiating $\mathscr{L}$ with respect to the state variable. The adjoint variable $p(u)$, when computed on the optimal control $u$, corresponds to the Lagrange multiplier. In general, Lions' method and the method based on the use of the Lagrangian do not lead to the same definition of adjoint problem, so they give rise to numerical methods which can behave differently. For a correct solution of the optimal control problem it is therefore essential to be coherent with the kind of approach that we are considering. Another crucial issue is the derivation of the boundary conditions for the adjoint problem; the two different approaches may lead to different types of boundary conditions. In particular, the approach based on the Lagrangian yields the boundary conditions for the adjoint problem automatically, while this is not the case for the other approach.

\subsection{Iterative solution of the optimal control problem}

For the numerical solution of optimal control problems, two different paradigms can be adopted:

- optimize-then-discretize: we first apply the iterative method, then discretize the various steps of the algorithm, or

- discretize-then-optimize: we first discretize our optimal control problem and then apply an iterative algorithm to solve its discrete version.

This discussion is deferred until Sect. 18.12. In this section we illustrate the way an iterative algorithm can be used to generate a sequence that hopefully converges to the optimal control function $u$.

As previously discussed, an optimal control problem can be formulated according to the Lions approach, yielding the set of equations (18.43), or by means of the Lagrangian, in which case the equations to be solved are (18.60). In either case we end up with an optimality system made by:

i) the state equation,

ii) the adjoint equation,

iii) the equation expressing the optimality condition.

In particular, the third equation is related to the variation of the cost functional, either explicitly in the Lions approach, or implicitly (through the Riesz representation theorem) when using the Lagrangian approach. Indeed, in the case of linear elliptic equations previously examined we obtain, respectively:

- $\frac{1}{2} J^{\prime}(u)=B^{\prime} p(u)+\Lambda_{\mathscr{U}} N u$

- $\mathscr{L}_{u}[\phi]=b(\phi, p)+2 n(u, \phi) \quad \forall \phi \in \mathscr{U}$

In order to simplify our notation, in the remainder of this section we will use the symbol $J^{\prime}$ not only to indicate the derivative of the cost functional but also that of the Lagrangian, $\mathscr{L}_{u}[\phi]$. The evaluation of $J^{\prime}$ at a given point of the control region $(\Omega, \Gamma,$, or one of their subsets) provides an indication of the sensitivity of the cost functional $J$ (at that very point) with respect to the variations of the control function $u$. Otherwise said, an infinitesimal variation $\delta u$ of the control about a given value $u$, entails, up to infinitesimals of higher order, a variation $\delta J$ of the cost functional that is proportional to $J^{\prime}(u)$. This suggests the use of the following steepest descent iterative algorithm. If $u^{k}$ denotes the value of the control function at step $k$, the control function at the next step, $k+1$, can be obtained as follows

$$
u^{k+1}=u^{k}-\tau^{k} J^{\prime}\left(u^{k}\right)
$$

where $J^{\prime}$ rappresents the descent direction, and $\tau^{k}$ the acceleration parameter. Although not necessarily the most efficient, the choice (18.61) is however pedagogically useful to understand the role played by $J^{\prime}$ and therefore that of the adjoint variable $p$. A method for the search of an optimal control can therefore be devised in terms of the following iterative algorithm:

1. Find the expression of the adjoint equation and of the derivative $J^{\prime}$ by either one of the two approaches (Lions or Lagrangian);

2. Provide an initial guess $u^{0}$ of the control function $u$;

3. Solve the equation of state in $y$ using the above guess;

4. Knowing the state variable and the observation target $z_{d}$, evaluate the cost functional J;

5. Solve the adjoint equation for $p$, knowing $y$ and $z_{d}$;

6. Knowing the adjoint variable, compute $J^{\prime}$;

7. If the chosen stopping test is fulfilled (up to a given tolerance) then exit (jump to point 10 ), otherwise continue;

8. Compute the parameter(s) for the acceleration of the iterative algorithm (for instance $\tau^{k}$ );

9. Compute the new control function, e.g. through equation (18.61), and return to point 3.;

10. Take the last computed variable to generate the "converged" unknowns $u, y$ and $p$.

In Fig. $18.10$ we display a flow chart that illustrates the above algorithm.

Remark 18.4. A convenient stopping test can be built on the measure of the distance, in a suitable norm, between the observed variable $z$ and the (desired) target observation $z_{d}$, say

$$
\left\|z^{k}-z_{d}\right\|_{\mathscr{Z}} \leq \text { Tol. }
$$

However, in general this does not guarantee that $J\left(u^{k}\right) \rightarrow 0$ as $k \rightarrow \infty$. That is to say that $J$ might not converge to 0 . A different stopping criterion is based on the evaluation of the derivative of the cost functional

$$
\left\|J^{\prime}\left(u^{k}\right)\right\|_{\mathscr{U}^{\prime}} \leq \text { Tol } .
$$

The value of the tolerance must be sufficiently small with respect to both the value of ||$J^{\prime}\left(u^{0}\right) \|$ on the initial control and the proximity to the target observation that we want to achieve. 

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-151.jpg?height=689&width=371&top_left_y=112&top_left_x=271)

Fig. 18.10. Flow chart of a possible iterative algorithm for the solution of an optimal control problem

Remark 18.5. The adjoint variable is defined on the whole computational domain. For the evaluation of $J^{\prime}(u)$ it would be necessary to restrict the adjoint variable $p$ on that portion of the domain, or of its boundary, on which the control function $u$ is defined. See Tables $18.1$ and $18.2$ for several examples.

The descent iterative method requires the determination of a suitable parameter $\tau^{k}$. The latter should guarantee that the convergence of the cost functional to its minimum

$$
J_{*}=\inf _{u \in \mathscr{Y}} J(u) \geq 0
$$

is monotone, that is

$$
J\left(u^{k}-\tau^{k} J^{\prime}\left(u^{k}\right)\right)<J\left(u^{k}\right)
$$

In those cases in which the value of $J_{*}$ is known (e.g., $J_{*}=0$ ), then the parameter can be chosen as follows (see, e.g., [Ago03] and [Vas81])

$$
\tau^{k}=\frac{\left(J\left(u^{k}\right)-J_{*}\right)}{\left\|J^{\prime}\left(u^{k}\right)\right\|_{\mathscr{U}^{\prime}}^{2}}
$$

As an example, consider the following control problem

$$
\left\{\begin{array}{l}
A y=f+B u \\
\inf J(u), \quad \text { where } J(u)=v\|u\|_{\mathscr{U}}^{2}+\left\|C y-z_{d}\right\|_{\mathscr{Z}}^{2}, \quad v \geq 0
\end{array}\right.
$$

The previous iterative method becomes:

$$
\left\{\begin{array}{l}
A y^{k}=f+B u^{k} \\
A^{*} p^{k}=C^{\prime}\left(C y^{k}-z_{d}\right) \\
u^{k+1}=u^{k}-\tau^{k} 2\left(v u^{k}+B^{\prime} p^{k}\right)
\end{array}\right.
$$

If $\operatorname{Ker}\left(B^{\prime} A^{*-1} C^{\prime}\right)=\{0\}$ this problem admits a solution, moreover $J(u) \rightarrow 0$ if $v \rightarrow 0$ Thus, if $v \simeq 0^{+}$we can assume that $J_{*} \simeq 0$, whence, thanks to $(18.62)$,

$$
\tau^{k}=\frac{J\left(u^{k}\right)}{\left\|J^{\prime}\left(u^{k}\right)\right\|_{\mathscr{U}^{\prime}}^{2}}=\frac{v\left\|u^{k}\right\|_{\mathscr{\mathscr}}^{2}+\left\|C y^{k}-z_{d}\right\|_{\mathscr{\mathscr { Z }}}^{2}}{\left\|2 v u^{k}+B^{\prime} p^{k}\right\|_{\mathscr{U}^{\prime}}^{2}}
$$

When considering a discretized optimal control problem, for instance using the Galerkin-finite element method, as we will see in Sect. 18.12, instead of looking for the minimum of $J(u)$, with $J: \mathscr{U} \mapsto \mathbb{R}$, one looks for the minimum of $J(\mathbf{u})$, where $J: \mathbb{R}^{n} \mapsto \mathbb{R}$ and $\mathbf{u} \in \mathbb{R}^{n}$ is the vector whose components are the nodal values of the control $u \in \mathscr{U}$. We will make this assumption in the remainder of this section.

As previously noted, the steepest descent method (18.61) is one among several iterative algorithms that could be used for the solution of an optimal control problem. As a matter of fact, this method is a special case of gradient method

$$
\mathbf{u}^{k+1}=\mathbf{u}^{k}+\tau^{k} \mathbf{d}^{k}
$$

where $\mathbf{d}^{k}$ represents a descent direction, that is a vector that satisfies

$$
\mathbf{d}^{k^{T}} \cdot J^{\prime}\left(\mathbf{u}^{k}\right)<0 \quad \text { if } \nabla J\left(\mathbf{u}^{k}\right) \neq \mathbf{0}
$$

Depending upon the criterion that is followed to choose $\mathbf{d}^{k}$, we obtain several special cases:

- Newton method, for which

$$
\mathbf{d}^{k}=-H\left(\mathbf{u}^{k}\right)^{-1} \nabla J\left(\mathbf{u}^{k}\right)
$$

where $H\left(\mathbf{u}^{k}\right)$ is the Hessian matrix of $J(\mathbf{u})$ computed at $\mathbf{u}=\mathbf{u}^{k}$; 

$$
\mathbf{d}^{k}=-B_{k} \nabla J\left(\mathbf{u}^{k}\right)
$$

where $B_{k}$ is an approximation of the inverse of $H\left(\mathbf{u}^{k}\right)$;

- conjugate gradient method, for which

$$
\mathbf{d}^{k}=-\nabla J\left(\mathbf{u}^{k}\right)+\beta_{k} \mathbf{d}^{k-1}
$$

where $\beta_{k}$ is a scalar to be chosen in such a way that $\mathbf{d}^{k^{T}} \mathbf{d}^{k-1}=0$. (See also Chap. 7.)

Once $\mathbf{d}^{k}$ is computed, the parameter $\tau^{k}$ should be chosen in such a way to guarantee the monotonicity property

$$
J\left(\mathbf{u}^{k}+\tau^{k} \mathbf{d}^{k}\right)<J\left(\mathbf{u}^{k}\right)
$$

A more stringent requirement is that the following scalar minimization problem is solved

$$
\text { find } \tau^{k}: \phi\left(\tau^{k}\right)=J\left(\mathbf{u}^{k}+\tau^{k} \mathbf{d}^{k}\right) \text { minimum; }
$$

this would guarantee the following orthogonality property

$$
\mathbf{d}^{k^{T}} \cdot \nabla J\left(\mathbf{u}^{k}\right)=0
$$

Often, the computation of $\tau^{k}$ is based on heuristic methods. One way is to start from a relatively large value of the parameter $\tau^{k}$, which is then halved until $(18.65)$ is verified. However, this approach is not always successful. The idea is therefore to adopt more stringent criteria than (18.65) when choosing $\tau^{k}$, with the aim of achieving on one hand a high convergence rate, and on the other avoiding too small steps. The first goal is achieved by requiring that

$$
J\left(\mathbf{u}^{k}\right)-J\left(\mathbf{u}^{k}+\tau^{k} \mathbf{d}^{k}\right) \geq-\sigma \tau^{k} \mathbf{d}^{k^{T}} \cdot \nabla J\left(\mathbf{u}^{k}\right)
$$

for a suitable $\sigma \in(0,1 / 2)$. This inequality ensures that the average rate of decrease of $J$ at $\mathbf{u}^{k+1}$ along the direction $\mathbf{d}^{k}$ is at least equal to a given fraction of the initial decrease rate at $\mathbf{u}^{k}$. On the other hand, too small steps are avoided by requiring that

$$
\left|\mathbf{d}^{k^{T}} \cdot \nabla J\left(\mathbf{u}^{k}+\tau^{k} \mathbf{d}^{k}\right)\right| \leq \beta\left|\mathbf{d}^{k^{T}} \cdot \nabla J\left(\mathbf{u}^{k}\right)\right|
$$

for a suitable $\beta \in(\sigma, 1)$, so to guarantee that (18.66) is satisfied, too. In practice, $\sigma \in\left[10^{-5}, 10^{-1}\right]$ and $\beta \in\left[10^{-1}, 1 / 2\right]$. Several strategies can be chosen for the choice of $\tau^{k}$ that are compatible with conditions (18.66) and (18.67). A popular one is based on the Armijo formulae (see, e.g. [MP01]): for fixed $\sigma \in(0,1 / 2), \beta \in(0,1)$ and $\bar{\tau}>0$, one chooses $\tau^{k}=\beta^{m_{k}} \bar{\tau}, m_{k}$ being the first non-negative integer for which (18.66) is satisfied. One can even take $\tau^{k}=\bar{\tau}$ for all $k$, at least in those cases in which the evaluation of the cost functional $J$ is very involved.

For a more comprehensive discussion on this issue see, e.g., [GMSW89, KPTZ00, MP01, NW06]. 

\subsection{Numerical examples}

In this section we illustrate two examples of control problems inspired by real life applications. Both problems are analyzed by means of the Lagrangian approach outlined in Sect. 18.8.2; for simplicity the optimal control function is in fact a scalar value.

\subsubsection{Heat dissipation by a thermal fin}

Thermal fins are used to dissipate the heat produced by some devices with the goal of maintaining their temperature below some limit values. Typically, they are used for electronic devices such as transistors; when active, and depending on the electrical power, the latter could incur in failure with a higher frequency when the operational temperature increases. This represents a major issue when designing the dissipator, which is often used in combination with a fan able to improve considerably the thermal dissipation via forced convection, thus controlling the temperature of the device. For further details we refer the reader e.g. to [Ç07]; for another example in the field of parametrized problems see [OP07].

In our example, we aim at regulating the intensity of the forced convection associated with the fan in order to keep the temperature of the transistor as close as possible to a desired value. The control is represented by the scalar coefficient of forced convection, while the observation is the temperature on the boundary of the thermal fin which is in contact with the transistor.

Let us consider the following state problem, whose solution $y$ (in Kelvin degrees $[K]$ ) represents the temperature in the thermal fin

$$
\begin{cases}-\nabla \cdot(k \nabla y)=0 & \text { in } \Omega \\ -k \frac{\partial y}{\partial n}=-q & \text { on } \Gamma_{N} \\ -k \frac{\partial y}{\partial n}=(h+U)\left(y-y_{\infty}\right) & \text { on } \Gamma_{R}=\partial \Omega \backslash \Gamma_{N}\end{cases}
$$

where the domain $\Omega$ and its boundary are reported in Fig. 18.11. The coefficient $k$ $([W /(m m K)])$ represents the thermal conductivity (aluminium is considered in this case), while $h$ and $U$ (our control variable) are the natural and forced convection coefficients $\left(\left[W /\left(m m^{2} K\right)\right]\right)$, respectively. Let us remark that when the fan is active, the value of $U$ is greater than zero; if $U=0$ heat dissipation is due only to natural convection. The temperature $y_{\infty}$ corresponds to the temperature of the air far away from the dissipator, while $q\left(\left[W / m m^{2}\right]\right)$ is the heat per unit of area emitted by the transistor and entering in the thermal fin through the boundary $\Gamma_{N}$.

The weak form of problem (18.68) reads, for a given $U \in \mathscr{U}=\mathbb{R}$

$$
\text { find } y \in \mathscr{V}: a(y, \varphi ; U)=b(U, \varphi) \quad \forall \varphi \in \mathscr{V},
$$

where $\mathscr{V}=H^{1}(\Omega), a(\varphi, \psi ; \phi)=\int_{\Omega} k \nabla \varphi \cdot \nabla \psi d \Omega+\int_{\Gamma_{R}}(h+\phi) \varphi \psi d \gamma$ and $b(\phi, \psi)=$ $\int_{\Gamma_{R}}(h+\phi) y_{\infty} \psi d \gamma+\int_{\Gamma_{N}} q \psi d \gamma$. Existence and uniqueness of the solution of the RobinNeumann problem (18.69) are ensured by the Peetre-Tartar lemma (see Remark 3.5). 

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-155.jpg?height=263&width=337&top_left_y=112&top_left_x=288)

Fig. 18.11. Thermal fin: computational domain; unit measure in $\mathrm{mm}$

The optimal control problem consists in finding the value of the forced convection coefficient $U$ such that the following cost functional $J(y, U)$ is smallest, $y \in \mathscr{V}$ being the solution of $(18.69)$

$$
J(y, U)=v_{1} \int_{\Gamma_{N}}\left(y-z_{d}\right)^{2} d \gamma+v_{2} U^{2}
$$

This leads to keeping the temperature of the transistor as close as possible to the desired value $z_{d}([K])$ and the forced convection coefficient close to zero depending on the value of the coefficient $v_{2}>0 ;$ in particular we assume $v_{1}=1 / \int_{\Gamma_{N}} z_{d}^{2} d \gamma$ and $v_{2}=v_{2}^{0} / h^{2}$, for a suitable $v_{2}^{0}$ The analysis of the problem is carried out by means of the Lagrangian functional $\mathscr{L}(y, p, U)=J(y, U)+b(p ; U)-a(y, p ; U)$. In particular, we obtain via differentiation of $\mathscr{L}(\cdot)$ the following adjoint equation for a given $U \in \mathbb{R}$ and the corresponding $y=$ $y(U) \in \mathscr{V}$

$$
\text { find } p \in \mathscr{V} \quad: a(\psi, p ; U)=c(y, \psi) \quad \forall \psi \in \mathscr{V},
$$

where $c(\varphi, \psi)=2 v_{1} \int_{\Gamma_{N}}\left(\varphi-z_{d}\right) \psi d \gamma$. Similarly, from the optimality condition we deduce that

$$
J^{\prime}(U)=2 v_{2} U-\int_{\Gamma_{R}}\left(y(U)-y_{\infty}\right) p(U) d \gamma
$$

We assume now $k=2.20 W /(m m K), h=15.0 \cdot 10^{-6} W /\left(m m^{2} K\right), y_{\infty}=298.15 K$ $\left(=25^{\circ} \mathrm{C}\right), z_{d}=353.15 \mathrm{~K}\left(=80{ }^{\circ} \mathrm{C}\right)$ and $v_{2}^{0}=10^{-3}$. The problem is solved by means of the finite element method with piecewise quadratic basis functions on a triangular mesh with 1608 elements and 934 degrees of freedom. The steepest descent iterative method is used for the optimization with $\tau^{k}=\tau=10^{-9}$ (see (18.61)); the iterative procedure is stopped when $\left|J^{\prime}\left(U^{k}\right)\right| /\left|J^{\prime}\left(U^{0}\right)\right|<t o l=10^{-6}$. At the initial step we consider natural convection for the dissipation of the heat such that $U=0.0$, to which corresponds a cost functional $J=0.0377$. The optimum is reached after 132 iterations yielding the optimal cost functional $J=0.00132$ for the optimal value of the forced convection coefficient $U=16.1 \cdot 10^{-6} W /\left(m m^{2} K\right)$. Ideally, the fan should be designed in order to warrant this value of the forced convection coefficient. In Fig. $18.12$ 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-156.jpg?height=208&width=722&top_left_y=113&top_left_x=95)

Fig. 18.12. Thermal fin: state solution (temperature $\left[{ }^{\circ} \mathrm{C}\right]$ ) at the initial step (natural convection) (left) and at the optimum (right)

we display the state solution at the initial step and that at the optimum; we observe that the temperature on $\Gamma_{N}$ is not equal to $z_{d}$, because $v_{2}^{0} \neq 0$

\subsubsection{Thermal pollution in a river}

Industrial activities are often related with pollution phenomena which need to be properly taken into account while designing a new plant or planning its operations. In this field, thermal pollution could often affect a river or a channel used for cooling the hot liquids produced by industrial plants, thus affecting the vital flora and fauna.

In this case the goal could consist in regulating the heat emission in a branch of a river in order to maintain the temperature of the river close to a desired threshold without considerably affecting the ideal heat emission rate of the plant.

We introduce the following state problem, whose solution $y$ represents the temperature in the channels and branches of the river considered

$$
\begin{cases}\nabla \cdot(-k \nabla y+\mathbf{V} y)=f \chi_{1}+U \chi_{2} & \text { in } \Omega \\ y=0 & \text { on } \Gamma_{I N} \\ (-k \nabla y+\mathbf{V} y) \cdot \mathbf{n}=0 & \text { on } \Gamma_{N}\end{cases}
$$

The domain $\Omega$ and the boundary $\Gamma_{I N}$ are indicated in Fig. 18.13, while $\Gamma_{N}=\partial \Omega \backslash \Gamma_{I N}$ (note that the outflow boundary $\Gamma_{O U T}$ displayed in Fig. 18.13 is part of $\Gamma_{N}$ ). $\chi_{1}, \chi_{2}$ and $\chi_{O B S}$ represent the characteristic functions of the subdomains $\Omega_{1}, \Omega_{2}$ and $\Omega_{O B S}$, respectively. Dimensionless quantities are considered for this test case. The coefficient $k$ is the thermal diffusivity coefficient, which also accounts for the contribution to the diffusion of turbulence phenomena, while $\mathbf{V}$ is the advection field which describes the motion of the water in the domain $\Omega$ (we comment later on the way to find it). The source term $f \in \mathbb{R}$ and the control $U \in \mathscr{U}=\mathbb{R}$ represent the heat emission rates from two industrial plants; $f$ is given, whereas $U$ has to be determined on the basis of the solution of the optimal control problem. In particular, we want the following cost functional to be minimized

$$
J(y, U)=\int_{\Omega_{O B S}}\left(y-z_{d}\right)^{2} d \Omega+v\left(U-U_{d}\right)^{2}
$$



![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-157.jpg?height=266&width=517&top_left_y=112&top_left_x=198)

Fig. 18.13. Pollution in the river: computational domain

where $z_{d}$ is the desired temperature in $\Omega_{O B S}, U_{d}$ the ideal heat emission rate and $v>0$ is chosen conveniently.

The optimal control problem is set up by means of the Lagrangian approach. With this aim, (18.73) is rewritten in weak form, for a given $U$, as

$$
\text { find } y \in \mathscr{V}: a(y, \varphi)=b(U, \varphi) \quad \forall \varphi \in \mathscr{V}
$$

where $\mathscr{V}=H_{\Gamma_{N}}^{1}(\Omega), a(\varphi, \psi)=\int_{\Omega} k \nabla \varphi \cdot \nabla \psi d \Omega$ and $b(U, \psi)=f \int_{\Omega_{1}} \psi d \Omega+$ $U \int_{\Omega}, \psi d \Omega$. Existence and uniqueness of the solution of problem (18.75) can be proved by proceeding as indicated in Sect. $3.4$.

The Lagrangian functional is $\mathscr{L}(y, p, U)=J(y, U)+b(U, p)-a(y, p)$. Differentiating $\mathscr{L}(\cdot)$ in $y \in \mathscr{V}$ we obtain the adjoint equation

$$
\text { find } p \in \mathscr{V}: a(\psi, p)=c(y, \psi) \quad \forall \psi \in \mathscr{V},
$$

where $c(\varphi, \psi)=2 \int_{\Omega_{O B S}}\left(\varphi-z_{d}\right) \psi d \Omega$. Similarly, we deduce the following derivative of the cost functional

$$
J^{\prime}(U)=2 v\left(U-U_{d}\right)+\int_{\Omega_{2}} p(U) d \Omega
$$

We assume now $k=0.01, f=10.0, z_{d}=0$ and $U_{d}=f$. The advection field $\mathbf{V}$ is deduced by solving the Navier-Stokes equations (see Chap. 17) in the domain $\Omega$, with the following boundary conditions: on $\Gamma_{I N}$ a parabolic profile is prescribed for the velocity, with a maximum velocity equal to 1; on $\Gamma_{O U T}$ no stress conditions are assumed in the normal direction, together with the slip condition $\mathbf{V} \cdot \mathbf{n}=0 ;$ finally, noslip conditions are prescribed on $\partial \Omega \backslash\left(\Gamma_{I N} \cup \Gamma_{O U T}\right)$. The notations are those displayed in Fig. 18.13. The Reynolds number is equal to $\mathbb{R} e=500$. The Navier-Stokes problem is solved by means of the Taylor-Hood $\mathbb{P}^{2}-\mathbb{P}^{1}$ (see Sect. 17.4) pairs of finite elements on a mesh composed by 32248 triangles and 15989 nodes. In Fig. $18.14$ we report the intensity of the advection field $\mathbf{V}$ and the corresponding streamlines.

The optimal control problem is solved by means of the finite element method with $\mathbb{P}^{2}$ basis functions on a triangular mesh with 32812 elements and 16771 degrees of 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-158.jpg?height=118&width=702&top_left_y=113&top_left_x=102)

Fig. 18.14. Pollution in the river: intensity of the advection field $\mathbf{V}$, modulus (left) and streamlines (right)

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-158.jpg?height=112&width=298&top_left_y=316&top_left_x=103)
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-158.jpg?height=112&width=708&top_left_y=316&top_left_x=102) and at the optimum (right)

freedom, using the steepest descent method for the functional optimization; we select $\tau^{k}=\tau=5$ (see (18.61)) and the stopping criterium is $\left|J^{\prime}\left(U^{k}\right)\right| /\left|J^{\prime}\left(U^{0}\right)\right|<$ tol $=10^{-6}$. The advection field $\mathbf{V}$ obtained by solving the Navier-Stokes equations is interpolated on this new mesh. At the initial step we assume that $U=U_{d}$, thus obtaining a cost functional $J=1.884$. The optimal solution is obtained after 15 iterations, the corresponding optimal cost functional is $J=1.817$ obtained for an optimal heat emission rate of $U=6.685$. In practice the heat from the plant in $\Omega_{2}$ should be reduced in order to maintain the temperature in $\Omega_{O B S}$ low. In Fig. 18.15 we report the state solutions $y$ (temperature) before and after optimization.

\subsection{A few considerations about observability and controllability}

A few considerations can be made on the behaviour of iterative methods with respect to the kind of optimal control problem that we are solving, particularly on which kind of variable $z$ we are observing, and which kind of control function $u$ we are using. Briefly, on the relationship between observability and controllability. We warn the reader that the conclusions that we are going to draw are not supported by a general theory, nor they apply to any kind of numerical discretization method.

- Where we observe. In general, optimal control problems based on an observation variable distributed in the domain enjoy higher convergence rate than those for which the observation variable is concentrated on the domain boundary.

- Where we control. In general, the optimization process is more robust if also the control function is distributed in the domain (as a source term to the state equation, or as coefficient of the differential operator governing the state equation), rather than being concentrated on the domain boundary. More precisely, the convergence rate is higher and its sensitivity to the choice of the acceleration parameter lower for distributed control problems than for boundary control ones, provided of course all other parameters are the same.

- What we observe. Also the kind of variable that we observe affects the convergence behaviour of the iterative scheme. For instance, observing the state variable is less critical than observing either its gradients or some of its higher-order derivatives. The latter circumstance occurs e.g., in fluid dynamics problems, when for potential problems one observes the velocity field, or for Navier-Stokes equations one observes the fluid vorticity or its stresses.

- Shape optimization. Shape optimization problems are a special class of optimal control problems: as a matter of fact, in this case the control function is not only on the boundary, it is $t$ the boundary itself. The cost functional to be minimized is called shape functional as it depends on the domain itself. One looks for

$$
J\left(\Omega_{o p t}\right) \leq J(\Omega) \quad \forall \Omega \in \mathscr{D}_{a d}
$$

where $\mathscr{D}_{a d}$ is a set of admissible domains. Shape optimization problems are difficult to analyze theoretically and hard to solve numerically. The numerical grid needs to be changed at every iteration. Besides, non-admissible boundary shapes might be generated in the course of the iterations, unless additional geometrical constraints are imposed. Moreover, special stabilization and regularization techniques might be necessary to prevent numerical oscillations in the case of especially complex problems. More in general, shape optimization problems are more sensitive to the variation of the various parameters that characterize the control problem.

- Adjoint problem and state problem. For steady elliptic problems like those considered in this chapter, the use of the adjoint problem provides the gradient of the cost functional at the same computational cost of the state problem. This approach can be considered as an alternative to those based on inexact or automatic differentiation of the cost functional. In the case of shape optimization problems the use of the adjoint problem allows a computational saving with respect to the method based on the shape sensitivity analysis, as the latter depends on the (often prohibitive) number of parameters that characterize the shape (the control points). See, e.g., [KAJ02].

\subsection{Two alternative paradigms for numerical approximation}

Let us start by considering a simple example that illustrates some additional difficulties that arise when solving an optimal control problem numerically. For a more insightful analysis of the numerical discretization of optimal control problems, see, e.g., [FCZ03, Gun03]. Consider again the state equation (18.1) and the optimal control problem

$$
\begin{gathered}
\text { "find } u \in \mathscr{U}_{a d} \text { such that } \\
J(u) \leq J(v) \quad \forall v \in \mathscr{U}_{a d}
\end{gathered}
$$

$$
\text { where } J \text { is a given cost functional". }
$$

The question is: "How can this problem be conveniently approximated?" As already anticipated at the beginning of Sect. $18.8$, at least two alternative strategies can be pursued:

\section{1) Discretize-then-optimize}

According to this strategy, we discretize first the control space $\mathscr{U}_{a d}$ by a finite dimensional space $\mathscr{U}_{a d, h}$ and the state equation (18.1) by a discrete equation written for short, as

$$
A_{h} y_{h}\left(u_{h}\right)=f_{h} \text {. }
$$

In a finite element context, the parameter $h$ denotes the finite element grid-size. We assume that a "discrete state" $y_{h}\left(v_{h}\right)$ exists for every "admissible" discrete control $v_{h} \in \mathscr{U}_{a d, h}$

At this stage we look for a discrete optimal control, that is a function $u_{h} \in \mathscr{U}_{a d, h}$ such that

$$
J\left(u_{h}\right) \leq J\left(v_{h}\right) \quad \forall v_{h} \in \mathscr{U}_{a d, h}
$$

or, more precisely,

$$
J\left(y_{h}\left(u_{h}\right), u_{h}\right) \leq J\left(y_{h}\left(v_{h}\right), v_{h}\right) \quad \forall v_{h} \in \mathscr{U}_{a d, h} .
$$

This corresponds to the following scheme

$$
\mathrm{MODEL} \longrightarrow \text { DISCRETIZATION } \longrightarrow \text { CONTROL }
$$

for which the "discretize-then-optimize" expression was coined.

\section{2) Optimize-then-discretize}

Alternatively we could proceed as follows. We start from the control problem (18.1), (18.78) and we write down the corresponding optimality system based on the Euler-Lagrange equations :

$$
\begin{gathered}
A y(u)=f \\
A^{\prime} p=G(y(u)),
\end{gathered}
$$

for a suitable $G$ which depends on the state $y(u)$ and represents the right-hand side of the adjoint problem, plus an additional equation (formally corresponding to the third equation of (18.56)) relating the three variables $y, p$ and $u$

$$
Q(y, p, u)=0
$$

At this stage we discretize system (18.82), (18.83) and solve it numerically. This corresponds to the following procedure:

$\mathrm{MODEL} \longrightarrow \mathrm{CONTROL} \longrightarrow$ DISCRETIZATION, for which the expression "optimize-then-discretize" is used. With respect to the former approach, here we have swapped the last two steps.

The two strategies do not necessarily yield the same results. For instance, in [IZ99] it is shown that if the state equation is a dynamic problem that describes the vibrations of an elastic structure and a finite element approximation is used, then the first strategy yields wrong results. This can be attributed to the lack of accuracy of the finite element method for high frequency solutions of the wave equation (see [Zie00]).

At the same time it was also observed that for several shape optimization problems in optimal design, the former strategy should be preferred; see. e.g., [MP01, Pir84].

The strategy of choice certainly depends on the nature of the differential problem at hand. In this respect, control problems governed by elliptic or parabolic PDEs are less problematic than those governed by hyperbolic equations because of their intrinsic dissipative nature. See for a discussion [Zua03]. The reader should however keep abreast of the many important developments expected in this field in the coming years.

\subsection{A numerical approximation of an optimal control problem for advection-diffusion equations}

In this section we consider an optimal control problem for an advection-diffusion equation formulated with the Lagrangian approach. For its numerical discretization the two different strategies: "discretize-then-optimize" and "optimize-then-discretize" will be considered. The numerical approximation will be based on stabilized finite element methods, as seen in Chapter $13$. Besides, an a posteriori error analysis will be carried out, according to the guidelines illustrated in Chapter $4$. For more details we refer to [QRDQ06, DQ05], and the references therein.

We consider the following advection-diffusion boundary-value problem:

$$
\begin{cases}L(y)=-\nabla \cdot(\mu \nabla y)+\mathbf{V} \cdot \nabla y=u & \text { in } \Omega \\ y=0 & \text { on } \Gamma_{D} \\ \mu \frac{\partial y}{\partial n}=0 & \text { on } \Gamma_{N}\end{cases}
$$

where $\Omega$ is a two-dimensional domain, $\Gamma_{D}$ and $\Gamma_{N}$ provide a disjoint partition of the domain boundary $\partial \Omega, u \in L^{2}(\Omega)$ is the control variable while $\mu$ and $\mathbf{V}$ are two given functions (the former being a positive viscosity). Here $\Gamma_{D}=\{\mathbf{x} \in \partial \Omega: \mathbf{V}(\mathbf{x}) \cdot \mathbf{n}(\mathbf{x})<$ $0\}$ is the inflow boundary, $\mathbf{n}(\mathbf{x})$ is the outward unit normal, while $\Gamma_{N}=\partial \Omega \backslash \Gamma_{D}$ is the outflow boundary.

We assume that the observation function is restricted to a subdomain $D \subseteq \Omega$ and that the optimal control problem reads

$$
\text { find } u: J(u)=\int_{D}\left(g y(u)-z_{d}\right)^{2} d D \text { minimum, }
$$

where $g \in C^{\infty}(\Omega)$ maps the variabile $y$ into the space of observations, and $z_{d}$ is the desired observation (the target). By setting

$$
\mathscr{V}=H_{\Gamma_{D}}^{1}=\left\{v \in H^{1}(\Omega): v_{\mid \Gamma_{D}}=0\right\} \text { and } \mathscr{U}=L^{2}(\Omega)
$$

the Lagrangian functional introduced in Sect. $18.8$ becomes

$$
\mathscr{L}(y, p, u)=J(u)+F(p ; u)-a(y, p)
$$

where:

$$
\begin{gathered}
a(\varphi, \psi)=\int_{\Omega} \mu \nabla \varphi \cdot \nabla \psi d \Omega+\int_{\Omega} \mathbf{V} \cdot \nabla \varphi \psi d \Omega \\
F(\varphi ; u)=\int_{\Omega} u \varphi d \Omega
\end{gathered}
$$

By differentiating $\mathscr{L}$ with respect to the state variable $y$, we obtain the adjoint equation (in weak form)

$$
\text { find } p \in \mathscr{V}: a^{a d}(p, \psi)=F^{a d}(\psi ; \varphi) \quad \forall \psi \in \mathscr{V}
$$

where:

$$
\begin{gathered}
a^{a d}(p, \psi)=\int_{\Omega} \mu \nabla p \cdot \nabla \psi d \Omega+\int_{\Omega} \mathbf{V} \cdot \nabla \psi p d \Omega \\
F^{a d}(\psi ; y)=\int_{D} 2\left(g y-z_{d}\right) g \psi d D
\end{gathered}
$$

Its differential (distributional) counterpart reads

$$
\begin{cases}L^{a d}(p)=-\nabla \cdot(\mu \nabla p+\mathbf{V} p)=\chi_{D} g\left(g y-z_{d}\right) & \text { in } \Omega \\ p=0 & \text { on } \Gamma_{D} \\ \mu \frac{\partial p}{\partial n}+\mathbf{V} \cdot \mathbf{n} p=0 & \text { on } \Gamma_{N}\end{cases}
$$

where $\chi_{D}$ denotes the characteristic function of the region $D$. By differentiating $\mathscr{L}$ with respect to the control function $u$, we obtain the optimality equation (see the third equation of $(18.56))$, that is

$$
\int_{\Omega} \phi p d \Omega=0 \quad \forall \phi \in L^{2}(\Omega)
$$

This equation provides the sensitivity $J^{\prime}(u)$ of the cost functional with respect to the control variable. Denoting for simplicity this sensitivity by $\delta u$, in this case we obtain $\delta u=p(u)=p$. Finally, by differentiating $\mathscr{L}$ with respect to the adjoint variable $p$, as usual we obtain the state equation (in weak form)

$$
\text { find } y \in \mathscr{V}: a(y, \varphi)=F(\varphi ; u) \quad \forall \varphi \in \mathscr{V}
$$



\subsubsection{The strategies "optimize-then-discretize" and "discretize-then-optimize"}

From a numerical viewpoint the minimization algorithm introduced in Sect. $18.9$ requires, at every step, the numerical approximation of both the state and the adjoint boundary-value problems. For both problems we can use the stabilized Galerkin-LeastSquares finite element formulations introduced in Sect. 13.8.6. The corresponding discretized equations respectively read:

$$
\begin{array}{r}
\text { find } y_{h} \in \mathscr{V}_{h}: a\left(y_{h}, \varphi_{h}\right)+\bar{s}_{h}\left(y_{h}, \varphi_{h}\right)=F\left(\varphi_{h} ; u_{h}\right) \quad \forall \varphi_{h} \in \mathscr{V}_{h}, \\
\qquad \bar{s}_{h}\left(y_{h}, \varphi_{h}\right)=\sum_{K \in \mathscr{T}_{h}} \int_{K} \delta_{K} R\left(y_{h} ; u_{h}\right) L\left(\varphi_{h}\right) d K \\
\text { find } p_{h} \in \mathscr{V}_{h}: a^{a d}\left(p_{h}, \psi_{h}\right)+\bar{s}_{h}^{a d}\left(p_{h}, \psi_{h}\right)=F^{a d}\left(\psi_{h} ; y_{h}\right) \quad \forall \psi_{h} \in \mathscr{V}_{h}, \\
\bar{s}_{h}^{a d}\left(p_{h}, \psi_{h}\right)=\sum_{K \in \mathscr{T}_{h}} \int_{K} \delta_{K} R^{a d}\left(p_{h} ; y_{h}\right) L^{a d}\left(\psi_{h}\right) d K,
\end{array}
$$

where $\delta_{K}$ is a stabilization parameter, $R(y ; u)=L(y)-u, R^{a d}(p ; y)=L^{a d}(p)-G(y)$, with $G(y)=2 \chi_{D} g\left(g y-z_{d}\right)$. This is the paradigm "optimize-then-discretize"; see Sect. $18.12$ and, for the specific problem at hand, [Bec01, CH01, Gun03].

In the paradigm "discretize-then-optimize", the one that we will adopt in the following, we first discretize (by the same stabilized GLS formulation (Eq. (18.95) and (18.96)), and then introduce the discrete Lagrangian functional

$$
\mathscr{L}_{h}\left(y_{h}, p_{h}, u_{h}\right)=J\left(y_{h}, u_{h}\right)+F\left(p_{h} ; u_{h}\right)-a\left(y_{h}, p_{h}\right)-\bar{s}_{h}\left(y_{h}, p_{h}\right)
$$

At this stage, by differentiating with respect to $y_{h}$, we obtain the discrete adjoint equation (18.97), however this time the stabilization term is different, precisely

$$
\overline{\bar{s}}_{h}^{a d}\left(p_{h}, \psi_{h}\right)=\sum_{K \in \mathscr{T}_{h}} \int_{K} \delta_{K} L\left(\psi_{h}\right) L\left(p_{h}\right) d K
$$

Now, by differentiating $\mathscr{L}_{h}$ with respect to $u_{h}$ and using the Riesz representation theorem (Theorem 2.1), we obtain, noting that $u_{h} \in \mathscr{V}_{h}$,

$$
\delta u_{h}=p_{h}+\sum_{K \in \mathscr{T}_{h}} \int_{K} \delta_{K} L\left(p_{h}\right) d K
$$

In particular, the new stabilized Lagrangian reads [DQ05]

$$
\mathscr{L}_{h}^{s}\left(y_{h}, p_{h}, u_{h}\right)=\mathscr{L}\left(y_{h}, p_{h}, u_{h}\right)+S_{h}\left(y_{h}, p_{h}, u_{h}\right)
$$

where we have set

$$
S_{h}(y, p, u)=\sum_{K \in \mathscr{T}_{h}} \int_{K} \delta_{K} R(y ; u) R^{a d}(p ; y) d K
$$

By differentiating $\mathscr{L}_{h}^{s}$ we obtain the new discretized state and adjoint problems, which can still be written as in (18.95) and (18.97), however this time the stabilization terms read, respectively, as follows:

$$
\begin{gathered}
s_{h}\left(y_{h}, \varphi_{h} ; u_{h}\right)=-\sum_{K \in \mathscr{T} h} \int_{K} \delta_{K} R\left(y_{h} ; u_{h}\right) L^{a d}\left(\varphi_{h}\right) d K \\
s_{h}^{a d}\left(p_{h}, \psi_{h} ; y_{h}\right)=-\sum_{K \in \mathscr{T}_{h}} \int_{K} \delta_{K}\left(R^{a d}\left(p_{h} ; y_{h}\right) L\left(\psi_{h}\right)-R\left(y_{h} ; u_{h}\right) G^{\prime}\left(\psi_{h}\right)\right) d K
\end{gathered}
$$

having set $G^{\prime}(\psi)=2 \chi_{D} g^{2} \psi$. Finally, the sensitivity of the cost functional becomes now

$$
\delta u_{h}\left(p_{h}, y_{h}\right)=p_{h}-\sum_{K \in \mathscr{T}_{h}} \delta_{K} R^{a d}\left(p_{h} ; y_{h}\right)
$$

\subsubsection{A posteriori error estimates}

With the aim of obtaining a suitable a posteriori error estimate for the optimal control problem we shall use as error indicator the error on the cost functional, as done in [BKR00]. Moreover, we will split this error into two terms, that we will identify as iteration error and discretization error. In particular, for the discretization error we will make use of duality principle advocated in [BKR00] for the grid adaptivity.

\section{Iteration error and discretization error}

At every step $k$ of the iterative algorithm for the minimization of the cost functional we consider the error

$$
\varepsilon^{(k)}=J\left(y^{*}, u^{*}\right)-J\left(y_{h}^{k}, u_{h}^{k}\right)
$$

where the symbol $*$ identifies the variables corresponding to the optimal value of the control, while $y_{h}^{k}$ denotes the discrete state variable at step $k$. (The variables $y_{h}^{k}$ and $u_{h}^{k}$ have a similar meaning.) We call discretization error $\varepsilon_{D}^{(k)}$ [DQ05] the component of the total error $\varepsilon^{(k)}$ arising from step $k$, and iteration error $\varepsilon_{I T}^{(k)}[\mathrm{DQ} 05]$ the component of $\varepsilon^{(k)}$ that represents the difference between the value of the cost functional computed on the exact variables at step $k$ and the value $J^{*}=J\left(y^{*}, u^{*}\right)$ of the cost functional at the optimum. In conclusion, the total error $\varepsilon^{(k)}(18.106)$ can be written as

$$
\varepsilon^{(k)}=\left(J\left(y^{*}, u^{*}\right)-J\left(y^{k}, u^{k}\right)\right)+\left(J\left(y^{k}, u^{k}\right)-J\left(y_{h}^{k}, u_{h}^{k}\right)\right)=\varepsilon_{I T}^{(k)}+\varepsilon_{D}^{(k)}
$$

In the following we will apply an a posteriori error estimate only on $\varepsilon_{D}^{(k)}$, that is the only part of $\varepsilon^{(k)}$ that can be reduced by a grid refinement procedure. Since the gradient of $\mathscr{L}(\mathbf{x}), \mathbf{x}=(y, p, u)$, is linear in $\mathbf{x}$, when using algorithm (18.61) with $\tau^{k}=\tau=$ constant, the iteration error $\varepsilon_{I T}^{(k)}$ becomes $\varepsilon_{I T}^{(k)}=\frac{1}{2}\left(\delta u\left(p^{k}, u^{k}\right), u^{*}-u^{k}\right)$, which in the current case becomes ([DQ05])

$$
\varepsilon_{I T}^{(k)}=-\frac{1}{2} \tau\left\|p^{k}\right\|_{L^{2}(\Omega)}^{2}-\frac{1}{2} \tau \sum_{r=k+1}^{\infty}\left(p^{k}, p^{r}\right)_{L^{2}(\Omega)}
$$

Since the iteration error cannot be exactly defined by this formula, we will approximate $\varepsilon_{I T}^{(k)}$ as

$$
\left|\varepsilon_{I T}^{(k)}\right| \approx \frac{1}{2} \tau\left\|p^{k}\right\|_{L^{2}(\Omega)}^{2}
$$

or, more simply,

$$
\left|\varepsilon_{I T}^{(k)}\right| \approx\left\|p^{k}\right\|_{L^{2}(\Omega)}^{2}
$$

which yields the usual stopping criterium

$$
\left|\varepsilon_{I T}^{(k)}\right| \approx\left\|\delta u\left(p^{k}\right)\right\|_{L^{2}(\Omega)}
$$

In practice, $\varepsilon_{I T}^{(k)}$ is computed on the discrete variables, that is $\left|\varepsilon_{I T}^{(k)}\right| \approx\left\|\delta u_{h}\left(p_{h}^{k}\right)\right\|$. Suppose that at a given iteration $k$ the grid is adaptively refined, and denote with $\mathbf{x}_{h}=$ ( $\left.y_{h}, p_{h}, u_{h}\right)$ the variables computed on the old grid (before the refinement) $\mathscr{T}_{h}$, and with $\mathbf{x}_{h, r e f}=\left(y_{h}^{\text {ref }}, p_{h}^{\text {ref }}, u_{h}^{\text {ref }}\right)$ those of the refined grid $\mathscr{T}_{h, r e f}$. Then at step $k$ the discretization error associated with the grid $\mathscr{T}_{h, \text { ref }}$ is lower than the one associated to $\mathscr{T}_{h}$. However, the discretization error $\varepsilon_{I T}^{(k)}$ computed on $\mathbf{x}_{h, r e f}$, is lower than the iteration error computed on $\mathbf{x}_{h}$.

\section{A posteriori error estimate and adaptive strategy}

The a posteriori error estimate for the discretization error $\varepsilon_{D}^{(k)}$ can be characterized as follows ([DQ05]).

Theorem 18.5. For the linear advection-diffusion control problem under exam,

By adapting (18.110) to the specific problem at hand and expressing the contributions on the different finite elements $K \in \mathscr{T}_{h}$ ([BKR00]), the following estimate can be obtained

$$
\left|\varepsilon_{D}^{(k)}\right| \leq \eta_{D}^{(k)}=\frac{1}{2} \sum_{K \in \mathscr{T}_{h}}\left\{\left(\omega_{K}^{p} \rho_{K}^{y}+\omega_{K}^{y} \rho_{K}^{p}+\omega_{K}^{u} \rho_{K}^{u}\right)+\lambda_{K}\right\}
$$

where:

$$
\begin{aligned}
&\rho_{K}^{y}=\left\|R\left(y_{h}^{k} ; u_{h}^{k}\right)\right\|_{L^{2}(K)}+h_{K}^{-\frac{1}{2}}\left\|r\left(y_{h}^{k}\right)\right\|_{L^{2}(\partial K)} \\
&\omega_{K}^{p}=\left\|\left(p^{k}-p_{h}^{k}\right)-\delta_{K} L^{a d}\left(p^{k}-p_{h}^{k}\right)+\delta_{K} G^{\prime}\left(y^{k}-y_{h}^{k}\right)\right\|_{L^{2}(K)}+h_{K}^{\frac{1}{2}}\left\|p^{k}-p_{h}^{k}\right\|_{L^{2}(\partial K)} \\
&\rho_{K}^{p}=\left\|R^{a d}\left(p_{h}^{k} ; y_{h}^{k}\right)\right\|_{L^{2}(K)}+h_{K}^{-\frac{1}{2}}\left\|r^{a d}\left(p_{h}^{k}\right)\right\|_{L^{2}(\partial K)} \\
&\omega_{K}^{y}=\left\|\left(y^{k}-y_{h}^{k}\right)-\delta_{K} L\left(y^{k}-y_{h}^{k}\right)\right\|_{L^{2}(K)}+h_{K}^{\frac{1}{2}}\left\|y^{k}-y_{h}^{k}\right\|_{L^{2}(\partial K)} \\
&\rho_{K}^{u}=\left\|\delta u_{h}\left(p_{h}^{k}, y_{h}^{k}\right)+\delta u\left(p^{k}\right)\right\|_{L^{2}(K)}=\left\|p^{k}+p_{h}^{k}-\delta_{K} R^{a d}\left(p_{h}^{k} ; y_{h}^{k}\right)\right\|_{L^{2}(K)} \\
&\omega_{K}^{u}=\left\|u^{k}-u_{h}^{k}\right\|_{L^{2}(K)}, \\
&\lambda_{K}=2 \delta_{K}\left\|R\left(y_{h}^{k} ; u_{h}^{k}\right)\right\|_{L^{2}(K)}\left\|G\left(y_{h}^{k}\right)\right\|_{L^{2}(K)}, \\
&r\left(y_{h}^{k}\right)=\left\{\begin{array}{l}
-\frac{1}{2}\left[\mu \frac{\partial y_{h}^{k}}{\partial n}\right], \text { on } \partial K \backslash \partial \Omega, \\
-\mu \frac{\partial y_{h}^{k}}{\partial n}, \text { on } \partial K \subset \Gamma_{N},
\end{array}\right. \\
&r^{a d}\left(p_{h}^{k}\right)=\left\{\begin{array}{l}
-\frac{1}{2}\left[\mu \frac{\partial p_{h}^{k}}{\partial n}+\mathbf{V} \cdot \mathbf{n} p_{h}^{k}\right], \text { on } \partial K \backslash \partial \Omega \\
-\left(\mu \frac{\partial p_{h}^{k}}{\partial n}+\mathbf{V} \cdot \mathbf{n} p_{h}^{k}\right), \text { on } \partial K \subset \Gamma_{N} .
\end{array}\right.
\end{aligned}
$$

$(18.112)$

As usual, $\partial K$ denotes the boundary of $K \in \mathscr{T}_{h}$, and [.] the jump operator across $\partial K$.

For a practical use of estimate $(18.111)$ it is necessary to evaluate $y^{k}, p^{k}$ and $u^{k}$. With this in mind we replace $y^{k}$ and $p^{k}$ by their quadratic reconstructions $\left(y_{h}^{k}\right)^{q}$ and $\left(p_{h}^{k}\right)^{q}$, while $u^{k}$ is replaced by $\left(u_{h}^{k}\right)^{q}=u_{h}^{k}-\tau\left(\delta u_{h}\left(\left(p_{h}^{k}\right)^{q},\left(y_{h}^{k}\right)^{q}\right)-\delta u_{h}\left(p_{h}^{k}, y_{h}^{k}\right)\right)$, according to the steepest descent method with constant acceleration parameter $\tau^{k}=\tau$. Consider the following adaptive strategy for the iterative optimization algorithm:

1. use a coarse grid and iterate until the tolerance on the iterative error $\operatorname{Tol}_{I T}$ is achieved;

2. adapt the grid, by equi-distributing the error on the different elements $K \in \mathscr{T}_{h}$, according to the estimate (18.111), until convergence on the discretization error within a tolerance $\operatorname{Tol}_{D}$;

3. re-evaluate of the variables as well as $\varepsilon_{I T}^{(k)}$ on the refined grid: return to point 1 and repeat the procedure if $\varepsilon_{I T}^{(k)} \geq T o l_{I T}$, stop the algorithm if $\varepsilon_{I T}^{(k)}<T o l_{I T}$

\subsubsection{A test problem on control of pollutant emission}

As an example we are going to apply the a posteriori estimate (18.111) on the discretization error $\eta_{D}^{(k)}$ and the strategy illustrated in Sect. $18.13 .2$ to a test case on emission of pollutants into the atmosphere. The specific problem is how to regulate the emission from industrial chimneys so to keep the pollutant's concentration in a certain critical area below a prescribed admissible threshold.

With this aim we consider a state equation that is given by a quasi-3D advectiondiffusion boundary-value problem [DQ05]. The pollutant concentration $y$ at the source (the emission height) $\mathrm{z}=H$ is described by (18.84), while the concentration at ground level is obtained by applying the projection function $g(\mathrm{x}, \mathrm{y})$. The form of the diffusion coefficients $\mu(x, y)$ and $g(x, y)$ depends both on the distance $H$ of the source from the ground, and on the class of atmospheric stability (stable, neutral or unstable). We will refer to a neutral atmosphere and, with reference to the domain illustrated in Fig. 18.16, we assume that the wind field is $\mathbf{V}=V\left(\cos \frac{\pi}{30}, \sin \frac{\pi}{30}\right)$ with $V=2.5 \mathrm{~m} / \mathrm{s}$. Moreover, we assume that there are three chimneys (represented by the three aligned small circles in Fig.18.16), all at the same height $H=100 \mathrm{~m}$, and that the maximum discharge allowed from every chimney is $u_{\max }=800 \mathrm{~g} / \mathrm{s}$. We assume that the pollutant emitted be $S O_{2}$ and we fix at $z_{d}=100 \mu \mathrm{g} / \mathrm{m}^{3}$ (the target observation in our control problem) the desired concentration in the region of observation, a circular region of the computational domain that we indicate by $D$ in Fig.18.16. In (18.84) we have considered the case of a distributed control $u$ on the whole computational domain $\Omega$, whereas in the current example $u=\sum_{i=1}^{N} u_{i} \chi_{i}$, where $\chi_{i}$ is the characteristic function of the (tiny) region $U_{i}$ which represents the location of the $i$-th chimney.

Finally, we choose

$$
g(\mathbf{x})=2 e^{-\frac{1}{2}\left(\frac{H}{0.04 r\left(1+2 \cdot 10^{-4} r\right)^{-1 / 2}}\right)}
$$

where $r$ is the distance (in mt) from the source.

In Fig.18.17(a) we display the concentration at ground level corresponding to the highest possible discharge $\left(u_{\max }=800 \mathrm{~g} / \mathrm{s}\right.$ ) from each chimney, while in Fig. 18.17(b) we display the concentration that we have after having applied the optimal control procedure (the cost functional being the square of the $L^{2}(D)$ norm of the distance from the target concentration $\left.z_{d}\right)$. We observe that the "optimal" emission rates become $u_{1}=0.0837 \cdot u_{\max }, u_{2}=0.0908 \cdot u_{\max }$ and $u_{3}=1.00 \cdot u_{\max }$

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-167.jpg?height=260&width=318&top_left_y=931&top_left_x=288)

Fig. 18.16. Computational domain for the test problem on pollutant control 

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-168.jpg?height=303&width=333&top_left_y=115&top_left_x=115)

(a)

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-168.jpg?height=303&width=335&top_left_y=115&top_left_x=464)

(b)

Fig. 18.17. Pollutant concentration measured in $\left[\mu g / m^{3}\right]$ at the ground level: (a) before and (b) after regulating the emissions from the chimneys

In Fig.18.18(a) we report the grid obtained by the a posteriori estimate on $\eta_{D}^{(k)}$, whereas in Fig. 18. 18 (b) the one obtained by the following energy norm error indicator $\left(\eta_{E}\right)^{(k)}=\Sigma_{K \in \mathscr{T}_{h}} h_{K} \rho_{K}^{y}$. The symbols adopted are those of $(18.112)$

These results are then compared with those that are obtained with a very fine grid of about 80000 elements. The grid adaptivity driven by the error indicator $\eta_{D}^{(k)}$ tends to concentrate nodes in those areas that are more relevant for the optimal control. This is confirmed by comparing the errors on the cost functional using the same number of gridpoints. For instance, the indicator $\eta_{D}^{(k)}$ yields an error of about $20 \%$, against the $55 \%$ that would be obtained using the indicator $\left(\eta_{E}\right)^{(k)}$ on a grid of about 4000 elements, while on a grid of about 14000 elements it would be $6 \%$ against $15 \%$.

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-168.jpg?height=276&width=335&top_left_y=868&top_left_x=114)

(a)

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-168.jpg?height=273&width=335&top_left_y=868&top_left_x=464)

(b)

Fig. 18.18. Adapted grids (of about 14000 elements) obtained using the error indicator $\eta_{D}^{(j)}$ (see $(18.111))$ (a) and $\left(\eta_{E}\right)^{(j)}$ ) 

\subsection{Exercises}

1. Consider the optimal control problem with boundary control

$$
\begin{cases}-\nabla \cdot(\alpha \nabla y)+\beta \cdot \nabla y+\gamma y=f & \text { in } \Omega=(0,1)^{2} \\ \frac{\partial y}{\partial n}=u & \text { on } \partial \Omega\end{cases}
$$

$u \in L^{2}(\Omega)$ being the control function and $f \in L^{2}(\Omega)$ a given function. Consider the cost functional

$$
J(u)=\frac{1}{2}\left\|\eta y-z_{d}\right\|_{L^{2}(\Omega)}^{2}+v\|u\|_{L^{2}(\partial \Omega)}^{2}
$$

with $\eta \in L^{\infty}(\Omega)$.

Write the equations (equation of state, adjoint equation and equation of optimality) of the optimal control problem (18.113)-(18.114) based on the Lagrangian approach, and those based on Lions' approach.

2. Consider the optimal control problem

$$
\begin{cases}-\nabla \cdot(\alpha \nabla y)+\beta \cdot \nabla y+\gamma y=f+c u & \text { in } \Omega=(0,1)^{2} \\ \frac{\partial y}{\partial n}=g & \text { on } \partial \Omega\end{cases}
$$

where $u \in L^{2}(\Omega)$ is a distributed control, $c$ a given constant, $f \in L^{2}(\Omega)$ and $g \in$ $H^{-1 / 2}(\partial \Omega)$ two given functions. Consider the cost functional

$$
J(u)=\frac{1}{2}\left\|\eta y-z_{d}\right\|_{L^{2}(\Omega)}^{2}+v\|u\|_{L^{2}(\Omega)}^{2}
$$

with $\eta \in L^{\infty}(\Omega)$.

Find the formulation of the optimal control problem (18.115)-(18.116) by the Lagrangian-based approach, then the one based on Lions' formulation. 

\section{Domain decomposition methods}

In this chapter we will introduce the domain decomposition method (DD, in short). In its most common version, DD can be used in the framework of any discretization method for partial differential equations (such as, e.g. finite elements, finite volumes, finite differences, or spectral element methods) to make their algebraic solution more efficient on parallel computer platforms. In addition, DD methods allow the reformulation of any given boundary-value problem on a partition of the computational domain into subdomains. As such, it provides a very convenient framework for the solution of heterogeneous or multiphysics problems, i.e. those that are governed by differential equations of different kinds in different subregions of the computational domain.

The basic idea behind DD methods consists in subdividing the computational domain $\Omega$, on which a boundary-value problem is set, into two or more subdomains on which discretized problems of smaller dimension are to be solved, with the further potential advantage of using parallel solution algorithms. More in particular, there are two ways of subdividing the computational domain into subdomains: one with disjoint subdomains, the others with overlapping subdomains (for an example, see Fig. 19.1). Correspondingly, different DD algorithms will be set up. For reference lectures on DD methods we refer to [BGS96, QV99, TW05].

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-170.jpg?height=217&width=719&top_left_y=934&top_left_x=97)

Fig. 19.1. Two examples of subdivision of the domain $\Omega$, with and without overlap



\subsection{Some classical iterative DD methods}

In this section we introduce four different iterative schemes starting from the model problem: find $u: \Omega \rightarrow \mathbb{R}$ such that:

$$
\begin{cases}L u=f & \text { in } \Omega \\ u=0 & \text { on } \partial \Omega\end{cases}
$$

$L$ being a generic second order elliptic operator, whose weak formulation reads

$$
\text { find } u \in V=H_{0}^{1}(\Omega): a(u, v)=(f, v) \quad \forall v \in V,
$$

being $a(\cdot, \cdot)$ the bilinear form associated with $L$ and $(\cdot, \cdot)$ the scalar product of $L^{2}(\Omega)$.

\subsubsection{Schwarz method}

Consider a decomposition of the domain $\Omega$ in two subdomains $\Omega_{1}$ and $\Omega_{2}$ such that $\bar{\Omega}=\bar{\Omega}_{1} \cup \bar{\Omega}_{2}, \Omega_{1} \cap \Omega_{2}=\Omega_{12} \neq \emptyset\left(\right.$ see Fig. 19.1) and let $\Gamma_{i}=\partial \Omega_{i} \backslash\left(\partial \Omega \cap \partial \Omega_{i}\right)$ Consider the following iterative method. Given $u_{2}^{(0)}$ on $\Gamma_{1}$, solve the following problems for $k \geq 1$ :

$$
\begin{array}{ll} 
\begin{cases}L u_{1}^{(k)}=f & \text { in } \Omega_{1} \\
u_{1}^{(k)}=u_{2}^{(k-1)} & & \text { on } \Gamma_{1} \\
u_{1}^{(k)}=0 & & \text { on } \partial \Omega_{1} \backslash \Gamma_{1}\end{cases} \\
\left(\begin{array}{ll}
L u_{2}^{(k)}=f & \text { in } \Omega_{2} \\
u_{2}^{(k)}= \begin{cases}u_{1}^{(k)} & & \\
u_{1}^{(k-1)}\end{cases} & \text { on } \Gamma_{2} \\
u_{2}^{(k)}=0 & \text { on } \partial \Omega_{2} \backslash \Gamma_{2}
\end{array}\right.
\end{array}
$$

In the case in which one chooses $u_{1}^{(k)}$ on $\Gamma_{2}$ in (19.4) the method is named multiplicative Schwarz, whereas that in which we choose $u_{1}^{(k-1)}$ is named additive Schwarz. The reason will be clarified in Sect.19.6. We have thus two elliptic boundary-value problems with Dirichlet conditions for the two subdomains $\Omega_{1}$ and $\Omega_{2}$, and we would like the two sequences $\left\{u_{1}^{(k)}\right\}$ and $\left\{u_{2}^{(k)}\right\}$ to converge to the restrictions of the solution $u$ of problem (19.1), that is

$$
\lim _{k \rightarrow \infty} u_{1}^{(k)}=\left.u\right|_{\Omega_{1}} \text { and } \quad \lim _{k \rightarrow \infty} u_{2}^{(k)}=\left.u\right|_{\Omega_{2}}
$$

It can be proven that the Schwarz method applied to problem (19.1) always converges, with a rate that increases as the measure $\left|\Omega_{12}\right|$ of the overlapping region $\Omega_{12}$ increases. Let us show this result on a simple one-dimensional case. 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-172.jpg?height=196&width=702&top_left_y=115&top_left_x=97)

Fig. 19.2. Example of a decomposition with overlap in dimension 1 (left). A few iterations of the multiplicative Schwarz method for problem (19.7) (right)

Example 19.1. Let $\Omega=(a, b)$ and let $\gamma_{1}, \gamma_{2} \in(a, b)$ be such that $a<\gamma_{2}<\gamma_{1}<b$ (see Fig. 19.2). The two problems (19.3) and (19.4) become:

$$
\begin{aligned}
& \begin{cases}L u_{1}^{(k)}=f, & a<x<\gamma_{1} \\
u_{1}^{(k)}=u_{2}^{(k-1)}, & x=\gamma_{1} \\
u_{1}^{(k)}=0, & x=a\end{cases} \\
&\qquad \begin{array}{ll}
L u_{2}^{(k)}=f, & \gamma_{2}<x<b \\
u_{2}^{(k)}=u_{1}^{(k)}, & x=\gamma_{2} \\
u_{2}^{(k)}=0, & x=b
\end{array}
\end{aligned}
$$

To show that this scheme converges, let us bound ourselves to the simpler problem

$$
\begin{cases}-u^{\prime \prime}(x)=0, & a<x<b \\ u(a)=u(b)=0, & \end{cases}
$$

that is the model problem (19.1) with $L=-d^{2} / d x^{2}$ and $f=0$, whose solution clearly is $u=0$ in $(a, b)$. This is not restrictive since at every step the error: $u-u_{1}^{(k)}$ in $\Omega_{1}$, $u-u_{2}^{(k)}$ in $\Omega_{2}$, satisfies a problem like (19.5)-(19.6) with null forcing term.

Let $k=1$; since $\left(u_{1}^{(1)}\right)^{\prime \prime}=0, u_{1}^{(1)}(x)$ is a linear function; moreover, it vanishes at $x=a$ and takes the value $u_{2}^{(0)}$ at $x=\gamma_{1}$. As we know the value of $u_{1}^{(1)}$ at $\gamma_{2}$, we can solve the problem (19.6) which, in its turn, features a linear solution. Then we proceed in a similar manner. In Fig. $19.2$ we show a few iterations: we clearly see that the method converges, moreover the convergence rate reduces as the length of the interval $\left(\gamma_{2}, \gamma_{1}\right)$ gets smaller.

At each iteration the Schwarz iterative method $(19.3)-(19.4)$ requires the solution of two subproblems with boundary conditions of the same kind as those of the original problem: indeed, by starting with a Dirichlet boundary-value problem in $\Omega$ we end up with two subproblems with Dirichlet conditions on the boundary of $\Omega_{1}$ and $\Omega_{2}$.

Should the differential problem (19.1) had been completed by a Neumann boundary condition on the whole boundary $\partial \Omega$, we would have been led to the solution of a mixed Dirichlet-Neumann boundary-value problem on either subdomain $\Omega_{1}$ and $\Omega_{2}$. 

\subsubsection{Dirichlet-Neumann method}

Let us partition the domain $\Omega$ in two disjoint subdomains (as in Fig. 19.1): let then $\Omega_{1}$ and $\Omega_{2}$ be two subdomains providing a partition of $\Omega$, i.e. $\bar{\Omega}_{1} \cup \bar{\Omega}_{2}=\bar{\Omega}, \bar{\Omega}_{1} \cap \bar{\Omega}_{2}=\Gamma$ and $\Omega_{1} \cap \Omega_{2}=\emptyset$. We denote by $\mathbf{n}_{i}$ the outward unit normal vector to $\Omega_{i}$ and will use the following notational convention: $\mathbf{n}=\mathbf{n}_{1}=-\mathbf{n}_{2}$.

The following result holds (for its proof see [QV99]):

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-173.jpg?height=370&width=722&top_left_y=310&top_left_x=97)

Thanks to this result we could split problem (19.1) by assigning the interface conditions (19.9)-(19.10) the role of "boundary conditions" for the two subproblems on the interface $\Gamma$. In particular, we can set up the following Dirichlet-Neumann (DN) iterative algorithm : given $u_{2}^{(0)}$ on $\Gamma$, for $k \geq 1$ solve the problems:

$$
\begin{array}{ll}
L u_{1}^{(k)}=f & \text { in } \Omega_{1} \\
u_{1}^{(k)}=u_{2}^{(k-1)} & \text { on } \Gamma, \\
u_{1}^{(k)}=0 & \text { on } \partial \Omega_{1} \backslash \Gamma
\end{array}
$$

Condition (19.9) has generated a Dirichlet boundary condition on $\Gamma$ for the subproblem in $\Omega_{1}$ whereas (19.10) has generated a Neumann boundary condition on $\Gamma$ for the subproblem in $\Omega_{2}$

Differently than Schwarz's method, the DN algorithm yields a Neumann boundaryvalue problem on the subdomain $\Omega_{2}$. Theorem $19.1$ guarantees that when the two sequences $\left\{u_{1}^{(k)}\right\}$ and $\left\{u_{2}^{(k)}\right\}$ converge, then their limit will be perforce the solution to the exact problem (19.1). The DN algorithm is therefore consistent. Its convergence however is not always guaranteed, as we can see on the following simple example.

Example 19.2. Let $\Omega=(a, b), \gamma \in(a, b), L=-d^{2} / d x^{2}$ and $f=0$. At every $k \geq 1$ the DN algorithm generates the two subproblems:

$$
\begin{array}{ll}
-\left(u_{1}^{(k)}\right)^{\prime \prime}=0, & a<x<\gamma \\
u_{1}^{(k)}=0, & x=a \\
u_{1}^{(k)}=u_{2}^{(k-1)}, & x=\gamma \\
\left(\begin{array}{ll}
-\left(u_{2}^{(k)}\right)^{\prime \prime}=0, & \gamma<x<b \\
\left(u_{2}^{(k)}\right)^{\prime}=\left(u_{1}^{(k)}\right)^{\prime}, & x=\gamma \\
u_{2}^{(k)}=0, & x=b
\end{array}\right.
\end{array}
$$

Proceeding as done in Example 19.1, we can prove that the two sequences converge only if $\gamma>(a+b) / 2$, as shown graphycally in Fig. 19.3.

In general, for a problem in arbitrary dimension $d>1$, the measure of the "Dirichlet" subdomain $\Omega_{1}$ must be larger than that of the "Neumann" one $\Omega_{2}$ in order to guarantee the convergence of (19.11)-(19.12). This however yields a severe constraint to fulfill, especially if several subdomains will be used.

To overcome such limitation, a variant of the DN algorithm can be set up by replacing the Dirichlet condition $(19.11)_{2}$ in the first subdomain by

$$
u_{1}^{(k)}=\theta u_{2}^{(k-1)}+(1-\theta) u_{1}^{(k-1)} \quad \text { on } \Gamma
$$

that is by introducing a relaxation which depends on a positive parameter $\theta$. In such a way it is always possible to reduce the error between two subsequent iterates.
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-174.jpg?height=238&width=524&top_left_y=926&top_left_x=195)

Fig. 19.3. Example of converging (left) and diverging (right) iterations for the DN method in $1 \mathrm{D}$ In the case displayed in Fig. $19.3$ we can easily verify that, by choosing

$$
\theta_{o p t}=-\frac{u_{1}^{(k-1)}}{u_{2}^{(k-1)}-u_{1}^{(k-1)}}
$$

the algorithm converges to the exact solution in a single iteration (this is not surprising!).

More in general, it can be proven that in any dimension $d \geq 1$, there exists a suitable value $\theta_{\max }<1$ such that the DN algorithm converges for any possible choice of the relaxation parameter $\theta$ in the interval $\left(0, \theta_{\max }\right)$.

\subsubsection{Neumann-Neumann algorithm}

Consider again a partition of $\Omega$ into two disjoint subdomains and denote by $\lambda$ the (unknown) value of the solution $u$ at their interface $\Gamma$. Consider the following iterative algorithm: for any given $\lambda^{(0)}$ on $\Gamma$, for $k \geq 0$ and $i=1,2$ solve the following problems:

$$
\begin{aligned}
&u_{i} \begin{cases}L u_{i}^{(k+1)}=f & \text { in } \Omega_{i} \\
u_{i}^{(k+1)}=\lambda^{(k)} & \text { on } \Gamma, \\
u_{i}^{(k+1)}=0 & \text { on } \partial \Omega_{i} \backslash \Gamma\end{cases} \\
& \begin{cases}L \psi_{i}^{(k+1)}=0 & \text { in } \Omega_{i} \\
\frac{\partial \psi_{i}^{(k+1)}}{\partial n}=\sigma_{i}\left(\frac{\partial u_{1}^{(k+1)}}{\partial n_{L}}-\frac{\partial u_{2}^{(k+1)}}{\partial n_{L}}\right) & \text { on } \Gamma \\
\psi_{i}^{(k+1)}=0 & \text { on } \partial \Omega_{i} \backslash \Gamma\end{cases}
\end{aligned}
$$

with

$$
\lambda^{(k+1)}=\lambda^{(k)}-\theta\left(\sigma_{1} \psi_{1 \mid \Gamma}^{(k+1)}-\sigma_{2} \psi_{2 \mid \Gamma}^{(k+1)}\right)
$$

where $\theta$ is a positive acceleration parameter, while $\sigma_{1}$ and $\sigma_{2}$ are two positive coefficients such that $\sigma_{1}+\sigma_{2}=1$. This iterative algorithm is named Neumann-Neumann (NN). Note that in the first stage (19.17) we care about the continuity on $\Gamma$ of the functions $u_{1}^{(k+1)}$ and $u_{2}^{(k+1)}$ but not that of their conormal derivatives. The latter are addressed in the second stage (19.18), (19.19) by means of the correcting functions $\psi_{1}^{(k+1)}$ and $\psi_{2}^{(k+1)}$ 

\subsubsection{Robin-Robin algorithm}

At last, we consider the following iterative algorithm, named Robin-Robin (RR). For every $k \geq 0$ solve the following problems:

$$
\begin{cases}L u_{1}^{(k+1)}=f & \text { in } \Omega_{1} \\ u_{1}^{(k+1)}=0 & \text { on } \partial \Omega_{1} \cap \partial \Omega \\ \frac{\partial u_{1}^{(k+1)}}{\partial n_{L}}+\gamma_{1} u_{1}^{(k+1)}=\frac{\partial u_{2}^{(k)}}{\partial n_{L}}+\gamma_{1} u_{2}^{(k)} & \text { on } \Gamma\end{cases}
$$

then

$$
\begin{cases}-\triangle u_{2}^{(k+1)}=f & \text { in } \Omega_{2} \\ u_{2}^{(k+1)}=0 & \text { on } \partial \Omega_{2} \cap \partial \Omega \\ \frac{\partial u_{2}^{(k+1)}}{\partial n_{L}}+\gamma_{2} u_{2}^{(k+1)}=\frac{\partial u_{1}^{(k+1)}}{\partial n_{L}}+\gamma_{2} u_{1}^{(k+1)} & \text { on } \Gamma\end{cases}
$$

where $u_{0}$ is assigned and $\gamma_{1}, \gamma_{2}$ are non-negative acceleration parameters that satisfy $\gamma_{1}+\gamma_{2}>0$. Aiming at the algorithm parallelization, in (19.21) we could use $u_{1}^{(k)}$ instead of $u_{1}^{(k+1)}$, provided in such a case an initial value for $u_{1}^{0}$ is assigned as well.

\subsection{Multi-domain formulation of Poisson problem and interface conditions}

In this section, for the sake of exposition, we choose $L=-\triangle$ and consider the Poisson problem with homogeneous Dirichlet boundary conditions (3.13). Generalization to an arbitrary second order elliptic operator with different boundary conditions is in order.

In the case addressed in Sect. 19.1.2 of a domain partitioned into two disjoint subdomains, the equivalence Theorem 19.1 allows the following multidomain formulation of problem (19.1), in which $u_{i}=\left.u\right|_{\Omega_{i}}, i=1,2$ :

$$
\begin{cases}-\triangle u_{1}=f & \text { in } \Omega_{1} \\ u_{1}=0 & \text { on } \partial \Omega_{1} \backslash \Gamma, \\ -\Delta u_{2}=f & \text { in } \Omega_{2}, \\ u_{2}=0 & \text { on } \partial \Omega_{2} \backslash \Gamma, \\ u_{1}=u_{2} & \text { on } \Gamma \\ \frac{\partial u_{1}}{\partial n}=\frac{\partial u_{2}}{\partial n} & \text { on } \Gamma\end{cases}
$$



\subsubsection{The Steklov-Poincaré operator}

We denote again by $\lambda$ the unknown value of the solution $u$ of problem (3.13) on the interface $\Gamma$, that is $\lambda=u_{\Gamma}$. Should we know a priori the value $\lambda$ on $\Gamma$, we could solve the following two independent boundary-value problems with Dirichlet condition on $\Gamma(i=1,2):$

$$
\left\{\begin{array}{lll}
-\Delta w_{i}=f & & \text { in } \Omega_{i} \\
w_{i}=0 & & \text { on } \partial \Omega_{i} \backslash \Gamma \\
w_{i}=\lambda & & \text { on } \Gamma
\end{array}\right.
$$

With the aim of obtaining the value $\lambda$ on $\Gamma$, let us split $w_{i}$ as follows

$$
w_{i}=w_{i}^{*}+u_{i}^{0}
$$

where $w_{i}^{*}$ and $u_{i}^{0}$ represent the solutions of the following problems $(i=1,2)$ :

$$
\left\{\begin{array}{lll}
-\Delta w_{i}^{*}=f & & \text { in } \Omega_{i} \\
w_{i}^{*}=0 & & \text { on } \partial \Omega_{i} \cap \partial \Omega \\
w_{i}^{*}=0 & & \text { on } \Gamma
\end{array}\right.
$$

and

$$
\left\{\begin{array}{lll}
-\Delta u_{i}^{0}=0 & \text { in } \Omega_{i} \\
u_{i}^{0}=0 & & \text { on } \partial \Omega_{i} \cap \partial \Omega \\
u_{i}^{0}=\lambda & & \text { on } \Gamma
\end{array}\right.
$$

respectively. Note that the functions $w_{i}^{*}$ depend solely on the source data $f$, while $u_{i}^{0}$ solely on the value $\lambda$ on $\Gamma$, henceforth we can write $w_{i}^{*}=G_{i} f$ and $u_{i}^{0}=H_{i} \lambda$. Both operators $G_{i}$ and $H_{i}$ are linear; $H_{i}$ is the so-called harmonic extension operator of $\lambda$ on the domain $\Omega_{i}$.

By a formal comparison of problem (19.22) with problem (19.23), we infer that the equality

$$
u_{i}=w_{i}^{*}+u_{i}^{0}, i=1,2,
$$

holds iff the condition $(19.22)_{6}$ on the normal derivatives on $\Gamma$ is satisfied, that is iff

$$
\frac{\partial w_{1}}{\partial n}=\frac{\partial w_{2}}{\partial n} \quad \text { on } \Gamma
$$

By using the previously introduced notations the latter condition can be reformulated as

$$
\frac{\partial}{\partial n}\left(G_{1} f+H_{1} \lambda\right)=\frac{\partial}{\partial n}\left(G_{2} f+H_{2} \lambda\right)
$$

and therefore

$$
\left(\frac{\partial H_{1}}{\partial n}-\frac{\partial H_{2}}{\partial n}\right) \lambda=\left(\frac{\partial G_{2}}{\partial n}-\frac{\partial G_{1}}{\partial n}\right) f \quad \text { on } \Gamma
$$



![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-178.jpg?height=169&width=338&top_left_y=113&top_left_x=286)

Fig. 19.4. Harmonic extensions in one dimension

In this way we have obtained an equation for the unknown $\lambda$ on the interface $\Gamma$, named Steklov-Poincaré equation, that can be rewritten in compact form as

$$
S \lambda=\chi \quad \text { on } \Gamma
$$

$S$ is the Steklov-Poincaré pseudo-differential operator; its formal definition is

$$
S \mu=\frac{\partial}{\partial n} H_{1} \mu-\frac{\partial}{\partial n} H_{2} \mu=\sum_{i=1}^{2} \frac{\partial}{\partial n_{i}} H_{i} \mu=\sum_{i=1}^{2} S_{i} \mu
$$

while $\chi$ is a linear functional which depends on $f$

$$
\chi=\frac{\partial}{\partial n} G_{2} f-\frac{\partial}{\partial n} G_{1} f=-\sum_{i=1}^{2} \frac{\partial}{\partial n_{i}} G_{i} f
$$

The operator

$$
S_{i}: \mu \rightarrow S_{i} \mu=\left.\frac{\partial}{\partial n_{i}}\left(H_{i} \mu\right)\right|_{\Gamma}, \quad i=1,2
$$

is called local Steklov-Poincaré operator. Note that $S, S_{1}$ and $S_{2}$ operate between the trace space

$$
\Lambda=\left\{\mu|\exists v \in V: \mu=v|_{\Gamma}\right\}
$$

(that is $H_{00}^{1 / 2}(\Gamma)$, see [QV99] $)$, and its dual $\Lambda^{\prime}$, whereas $\chi \in \Lambda^{\prime}$.

Example 19.3. With the aim of providing a practical (elementary) example of operator $S$, let us consider a simple one-dimensional problem. Let $\Omega=(a, b) \subset \mathbb{R}$ as shown in Fig. 19.4 and $L u=-u^{\prime \prime}$. By subdividing $\Omega$ in two disjoint subdomains, the interface $\Gamma$ reduces to a single point $\gamma \in(a, b)$, and the Steklov-Poincaré operator $S$ becomes

$$
S \lambda=\left(\frac{d H_{1}}{d x}-\frac{d H_{2}}{d x}\right) \lambda=\left(\frac{1}{l_{1}}+\frac{1}{l_{2}}\right) \lambda
$$

with $l_{1}=\gamma-a$ and $l_{2}=b-\gamma$. 

\subsubsection{Equivalence between Dirichlet-Neumann and Richardson methods}

The Dirichlet-Neumann (DN) method introduced in Sect. 19.1.2 can be reinterpreted as a (preconditioned) Richardson method for the solution of the Steklov-Poincaré interface equation. To check this statement, consider again, for the sake of simplicity, a domain $\Omega$ partitioned into two disjoint subdomains $\Omega_{1}$ and $\Omega_{2}$ with interface $\Gamma$. Then we re-write the DN algorithm (19.11), (19.12), (19.15) in the case of the operator $L=-\Delta$ : for a given $\lambda^{0}$, for $k \geq 1$ solve:

$$
\begin{aligned}
& \begin{cases}-\Delta u_{1}^{(k)}=f_{1} & \text { in } \Omega_{1} \\
u_{1}^{(k)}=\lambda^{(k-1)} & \text { on } \Gamma \\
u_{1}^{(k)}=0 & \text { on } \partial \Omega_{1} \backslash \Gamma\end{cases} \\
&\left\{\begin{array}{l}
-\Delta u_{2}^{(k)}=f_{2} & \text { in } \Omega_{2} \\
\frac{\partial u_{2}^{(k)}}{\partial n_{2}}=\frac{\partial u_{1}^{(k)}}{\partial n_{2}} & \text { on } \Gamma \\
u_{2}^{(k)}=0 & \text { on } \partial \Omega_{2} \backslash \Gamma
\end{array}\right. \\
&\lambda^{(k)}=\theta u_{2}^{(k)}+(1-\theta) \lambda^{(k-1)}
\end{aligned}
$$

The following result holds:

Proof. The solution $u_{1}^{(k)}$ of (19.31) can be written as

$$
u_{1}^{(k)}=H_{1} \lambda^{(k-1)}+G_{1} f_{1}
$$

Since $G_{2} f_{2}$ satisfies the differential problem

$$
\begin{cases}-\triangle\left(G_{2} f_{2}\right)=f_{2} & \text { in } \Omega_{2}, \\ G_{2} f_{2}=0 & \text { on } \partial \Omega_{2},\end{cases}
$$

thanks to $(19.32)$ the function $u_{2}^{(k)}-G_{2} f_{2}$ satisfies the differential problem

$$
\begin{cases}-\Delta\left(u_{2}^{(k)}-G_{2} f_{2}\right)=0 \quad \text { in } \Omega_{2} \\ \frac{\partial}{\partial n_{2}}\left(u_{2}^{(k)}-G_{2} f_{2}\right)=-\frac{\partial u_{1}^{(k)}}{\partial n}+\frac{\partial}{\partial n}\left(G_{2} f_{2}\right) & \text { on } \Gamma \\ u_{2}^{(k)}-G_{2} f_{2}=0 & \text { on } \partial \Omega_{2} \backslash \Gamma\end{cases}
$$

In particular $u_{2}^{(k)}{\mid r}=\left(u_{2}^{(k)}-G_{2} f_{2}\right)_{\mid \Gamma}$. Since the operator $S_{i}$ (19.29) maps a Dirichlet data to a Neumann data on $\Gamma$, its inverse $S_{i}^{-1}$ transforms a Neumann data in a Dirichlet one on $\Gamma$. Otherwise said, $S_{2}^{-1} \eta=\left.w_{2}\right|_{\Gamma}$, where $w_{2}$ is the solution of

$$
\begin{cases}-\Delta w_{2}=0 & \text { in } \Omega_{2} \\ \frac{\partial w_{2}}{\partial n}=\eta & \text { on } \Gamma \\ w_{2}=0 \quad & \text { on } \partial \Omega_{2} \backslash \Gamma\end{cases}
$$

Setting now

$$
\eta=-\frac{\partial u_{1}^{(k)}}{\partial n}+\frac{\partial}{\partial n}\left(G_{2} f_{2}\right)
$$

and comparing (19.36) with (19.37), we conclude that

$$
u_{2}^{(k)}{ }_{\Gamma}=\left(u_{2}^{(k)}-G_{2} f_{2}\right)_{\mid \Gamma}=S_{2}^{-1}\left(-\frac{\partial u_{1}^{(k)}}{\partial n}+\frac{\partial}{\partial n}\left(G_{2} f_{2}\right)\right)
$$

On the other hand, owing to $(19.35)$ and to the definition (19.28) of $\chi$, we obtain

$$
\begin{aligned}
u_{2}^{(k)}{ }_{\Gamma} &=S_{2}^{-1}\left(-\frac{\partial}{\partial n}\left(H_{1} \lambda^{(k-1)}\right)-\frac{\partial}{\partial n}\left(G_{1} f_{1}\right)+\frac{\partial}{\partial n}\left(G_{2} f_{2}\right)\right) \\
&=S_{2}^{-1}\left(-S_{1} \lambda^{(k-1)}+\chi\right)
\end{aligned}
$$

Using (19.33) we can therefore write

$$
\lambda^{(k)}=\theta\left[S_{2}^{-1}\left(-S_{1} \lambda^{(k-1)}+\chi\right)\right]+(1-\theta) \lambda^{(k-1)}
$$

that is

$$
\lambda^{(k)}-\lambda^{(k-1)}=\theta\left[S_{2}^{-1}\left(-S_{1} \lambda^{(k-1)}+\chi\right)-\lambda^{(k-1)}\right]
$$

Since $-S_{1}=S_{2}-S$, we finally obtain

$$
\begin{aligned}
\lambda^{(k)}-\lambda^{(k-1)} &=\theta\left[S_{2}^{-1}\left(\left(S_{2}-S\right) \lambda^{(k-1)}+\chi\right)-\lambda^{(k-1)}\right] \\
&=\theta S_{2}^{-1}\left(\chi-S \lambda^{(k-1)}\right)
\end{aligned}
$$

that is (19.34). The preconditioned $D N$ operator is therefore $S_{2}^{-1} S=I+S_{2}^{-1} S_{1}$

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-180.jpg?height=26&width=34&top_left_y=1212&top_left_x=762)

Using an argument similar to that used for the proof of Theorem 19.2, also the Neumann-Neumann (NN) algorithm (19.17)-(19.19) can be interpreted as a preconditioned Richardson algorithm, that is

$$
P_{N N}\left(\lambda^{(k)}-\lambda^{(k-1)}\right)=\theta\left(\chi-S \lambda^{(k-1)}\right)
$$

this time the preconditioner being $P_{N N}=\left(D_{1} S_{1}^{-1} D_{1}+D_{2} S_{2}^{-1} D_{2}\right)^{-1}$ where $D_{i}$ is a diagonal matrix whose entries are equal to $\sigma_{i}$. Note that the preconditioned operator becomes (if $\left.D_{i}=I\right) S_{2}^{-1} S_{1}+2 I+\left(S_{2}^{-1} S_{1}\right)^{-1}$. Consider at last the Robin-Robin iterative algorithm $(19.20)-(19.21)$. Denoting by $\mu_{i}^{(k)} \in \Lambda$ the approximation at step $k$ of the trace of $u_{i}^{(k)}$ on the interface $\Gamma, i=1,2$, it can be proven that $(19.20)-(19.21)$ is equivalent to the following alternating direction (ADI) algorithm:

$$
\begin{aligned}
&\left(\gamma_{1} i_{\Lambda}+S_{1}\right) \mu_{1}^{(k)}=\chi+\left(\gamma_{1} i_{\Lambda}+S_{2}\right) \mu_{2}^{(k-1)} \\
&\left(\gamma_{2} i_{\Lambda}+S_{2}\right) \mu_{2}^{(k)}=\chi+\left(\gamma_{2} i_{\Lambda}+S_{1}\right) \mu_{1}^{(k-1)}
\end{aligned}
$$

where $i_{\Lambda}: \Lambda \rightarrow \Lambda^{\prime}$ here denotes the Riesz isomorphism between the Hilbert space $\Lambda$ and its dual $\Lambda^{\prime}$ (see $(2.5)$; however, mind the different notation).

Should, for a convenient choice of the two parameters $\gamma_{1}$ and $\gamma_{2}$, the algorithm converge to two limit functions $\mu_{1}$ and $\mu_{2}$, then $\mu_{1}=\mu_{2}=\lambda$, the latter function being the solution to the Steklov-Poincaré equation $(19.26)$.

The RR preconditioner reads $P_{R R}=\left(\gamma_{1}+\gamma_{2}\right)^{-1}\left(\gamma_{1} i_{\Lambda}+S_{1}\right)\left(\gamma_{2} i_{\Lambda}+S_{2}\right)$.

Remark 19.1. In the Dirichlet-Neumann algorithm, the value $\lambda$ of the solution $u$ at the interface $\Gamma$ is the principal unknown. Once it has been determined, we can use it as Dirichlet data to recover the original solution in the whole domain. Alternatively, one could use the normal derivative $\eta=\frac{\partial u}{\partial n}$ on $\Gamma$ as principal unknown (or, for a more general partial differential operator, the conormal derivative - or flux). By proceeding as above, we can show that $\eta$ satisfies the new Steklov-Poincaré equation

$$
\left(S_{1}^{-1}+S_{2}^{-1}\right) \eta=T_{1} f_{1}+T_{2} f_{2} \quad \text { on } \Gamma
$$

where for $i=1,2, T_{i} f_{i}$ is the solution of the following Neumann problem

$$
\begin{cases}-\triangle\left(T_{i} f_{i}\right)=f_{i} & \text { in } \Omega_{i} \\ \frac{\partial}{\partial n_{i}}\left(T_{i} f_{i}\right)=0 & \text { on } \Gamma \\ T_{i} f_{i}=0 & \text { on } \partial \Omega \backslash \Gamma\end{cases}
$$

The so-called FETI algorithms (see Sect. 19.5.4) are examples of iterative algorithms designed for the solution of problems like (19.38). The FETI preconditioner is $P_{F E T I}=S_{1}+S_{2}$, hence the preconditioned FETI operator is $\left(S_{1}+S_{2}\right)\left(S_{1}^{-1}+S_{2}^{-1}\right)$. 

\subsection{Multidomain formulation of the finite element approximation of the Poisson problem}

What seen thus far can be regarded as propedeutical to numerical solution of boundaryvalue problems. In this section we will see how the previous ideas can be reshaped in the framework of a numerical discretization method. Although we will only address the case of finite element discretization, this is however not restrictive. We refer, e.g., to [CHQZ07] and [TW05] for the case of spectral or spectral element discretizations and to [Woh01] for discretization based on DG and mortar methods.

Consider the Poisson problem (3.13), its weak formulation (3.18) and its Galerkin finite element approximation $(4.40)$ on a triangulation $\mathscr{T}_{h}$. Recall that $V_{h}=\dot{X}_{h}^{r}=\left\{v_{h} \in\right.$ $\left.X_{h}^{r}:\left.v_{h}\right|_{\partial \Omega}=0\right\}$ is the space of finite element functions of degree $r$ vanishing on $\partial \Omega$, whose basis is $\left\{\varphi_{j}\right\}_{j=1}^{N_{h}}$ (see Sect. 4.5.1).

For the finite element nodes in the domain $\Omega$ we consider the following partition: let $\left\{x_{j}^{(1)}, 1 \leq j \leq N_{1}\right\}$ be the nodes located in the subdomain $\Omega_{1},\left\{x_{j}^{(2)}, 1 \leq j \leq N_{2}\right\}$ those in $\Omega_{2}$ and, finally, $\left\{x_{j}^{(\Gamma)}, 1 \leq j \leq N_{\Gamma}\right\}$ those lying on the interface $\Gamma$. Let us split the basis functions accordingly: $\varphi_{j}^{(1)}$ will denote those associated to the nodes $x_{j}^{(1)}, \varphi_{j}^{(2)}$ those associated with the nodes $x_{j}^{(2)}$, and $\varphi_{j}^{(\Gamma)}$ those associated with the nodes $x_{j}^{(\Gamma)}$ lying on the interface. This yields

$$
\varphi_{j}^{(\alpha)}\left(x_{j}^{(\beta)}\right)=\delta_{i j} \delta_{\alpha \beta}, \quad 1 \leq i \leq N_{\alpha}, \quad \leq j \leq \mathbb{N}_{\beta}
$$

with $\alpha, \beta=1,2, \Gamma ; \delta_{i j}$ is the Kronecker symbol.

By letting $v_{h}$ in $(4.40)$ to coincide with a test function, $(4.40)$ can be given the following equivalent formulation: find $u_{h} \in V_{h}$ such that:

$$
\begin{cases}a\left(u_{h}, \varphi_{i}^{(1)}\right)=F\left(\varphi_{i}^{(1)}\right) & \forall i=1, \ldots, N_{1} \\ a\left(u_{h}, \varphi_{j}^{(2)}\right)=F\left(\varphi_{j}^{(2)}\right) & \forall j=1, \ldots, N_{2} \\ a\left(u_{h}, \varphi_{k}^{(\Gamma)}\right)=F\left(\varphi_{k}^{(\Gamma)}\right) & \forall k=1, \ldots, N_{\Gamma}\end{cases}
$$

having set $F(v)=\int_{\Omega} f v d \Omega$. Let now

$$
a_{i}(v, w)=\int_{\Omega_{i}} \nabla_{v} \cdot \nabla w d \Omega \quad \forall v, w \in V, i=1,2
$$

be the restriction of the bilinear form $a(., .)$ to the subdomain $\Omega_{i}$ and define $V_{i, h}=\{v \in$ $H^{1}\left(\Omega_{i}\right) \mid v=0$ on $\left.\partial \Omega_{i} \backslash \Gamma\right\}(i=1,2)$. Similarly we set $F_{i}(v)=\int_{\Omega_{i}} f v d \Omega$ and denote by $u_{h}^{(i)}=\left.u_{h}\right|_{\Omega_{i}}$ the restriction of $u_{h}$ to the subdomain $\Omega_{i}$, with $i=1,2$. Problem (19.41) can be rewritten in the equivalent form: find $u_{h}^{(1)} \in V_{1, h}, u_{h}^{(2)} \in V_{2, h}$ such that

$$
\begin{cases}a_{1}\left(u_{h}^{(1)}, \varphi_{i}^{(1)}\right)=F_{1}\left(\varphi_{i}^{(1)}\right) & \forall i=1, \ldots, N_{1} \\ a_{2}\left(u_{h}^{(2)}, \varphi_{j}^{(2)}\right)=F_{2}\left(\varphi_{j}^{(2)}\right) & \forall j=1, \ldots, N_{2} \\ a_{1}\left(u_{h}^{(1)},\left.\varphi_{k}^{(\Gamma)}\right|_{\Omega_{1}}\right)+a_{2}\left(u_{h}^{(2)},\left.\varphi_{k}^{(\Gamma)}\right|_{\Omega_{2}}\right) & \\ =F_{1}\left(\left.\varphi_{k}^{(\Gamma)}\right|_{\Omega_{1}}\right)+F_{2}\left(\left.\varphi_{k}^{(\Gamma)}\right|_{\Omega_{2}}\right) & \forall k=1, \ldots, N_{\Gamma}\end{cases}
$$

The interface continuity condition $(19.22)_{5}$ is automatically satisfied thanks to the continuity of the functions $u_{h}^{(i)}$. Moreover, equations $(19.42)_{1}-(19.42)_{3}$ correspond to the finite element discretization of equations $(19.22)_{1}-(19.22)_{6}$, respectively. In particular, the third of equations $(19.42)$ must be regarded as the discrete counterpart of condition (19.22) $_{6}$ expressing the continuity of normal derivatives on $\Gamma$.

Let us expand the solution $u_{h}$ with respect to the basis functions $V_{h}$

$$
u_{h}(x)=\sum_{j=1}^{N_{1}} u_{h}\left(x_{j}^{(1)}\right) \varphi_{j}^{(1)}(x)+\sum_{j=1}^{N_{2}} u_{h}\left(x_{j}^{(2)}\right) \varphi_{j}^{(2)}(x)+\sum_{j=1}^{N_{\Gamma}} u_{h}\left(x_{j}^{(\Gamma)}\right) \varphi_{j}^{(\Gamma)}(x)
$$

From now on, the nodal values $u_{h}\left(x_{j}^{(\alpha)}\right)$, for $\alpha=1,2, \Gamma$ and $j=1, \ldots, N_{\alpha}$, which are the expansion coefficients, will be indicated with the shorthand notation $u_{j}^{(\alpha)}$. Using (19.43), we can rewrite (19.42) as follows:

$$
\begin{aligned}
&\sum_{j=1}^{N_{1}} u_{j}^{(1)} a_{1}\left(\varphi_{j}^{(1)}, \varphi_{i}^{(1)}\right)+\sum_{j=1}^{N_{\Gamma}} u_{j}^{(\Gamma)} a_{1}\left(\varphi_{j}^{(\Gamma)}, \varphi_{i}^{(1)}\right)=F_{1}\left(\varphi_{i}^{(1)}\right) \quad \forall i=1, \ldots, N_{1} \\
&\sum_{j=1}^{N_{2}} u_{j}^{(2)} a_{2}\left(\varphi_{j}^{(2)}, \varphi_{i}^{(2)}\right)+\sum_{j=1}^{N_{\Gamma}} u_{j}^{(\Gamma)} a_{2}\left(\varphi_{j}^{(\Gamma)}, \varphi_{i}^{(2)}\right)=F_{2}\left(\varphi_{i}^{(2)}\right) \quad \forall i=1, \ldots, N_{2} \\
&\sum_{j=1}^{N_{\Gamma}} u_{j}^{(\Gamma)}\left[a_{1}\left(\varphi_{j}^{(\Gamma)}, \varphi_{i}^{(\Gamma)}\right)+a_{2}\left(\varphi_{j}^{(\Gamma)}, \varphi_{i}^{(\Gamma)}\right)\right] \\
&\quad+\sum_{j=1}^{N_{1}} u_{j}^{(1)} a_{1}\left(\varphi_{j}^{(1)}, \varphi_{i}^{(\Gamma)}\right)+\sum_{j=1}^{N_{2}} u_{j}^{(2)} a_{2}\left(\varphi_{j}^{(2)}, \varphi_{i}^{(\Gamma)}\right) \\
&=F_{1}\left(\left.\varphi_{i}^{(\Gamma)}\right|_{\Omega_{1}}\right)+F_{2}\left(\left.\varphi_{i}^{(\Gamma)}\right|_{\Omega_{2}}\right) \quad \forall i=1, \ldots, N_{\Gamma}
\end{aligned}
$$

Let us introduce the following arrays:

$$
\begin{array}{ll}
\left(A_{11}\right)_{i j}=a_{1}\left(\varphi_{j}^{(1)}, \varphi_{i}^{(1)}\right), & \left(A_{1 \Gamma}\right)_{i j}=a_{1}\left(\varphi_{j}^{(\Gamma)}, \varphi_{i}^{(1)}\right) \\
\left(A_{22}\right)_{i j}=a_{2}\left(\varphi_{j}^{(2)}, \varphi_{i}^{(2)}\right), & \left(A_{2 \Gamma}\right)_{i j}=a_{2}\left(\varphi_{j}^{(\Gamma)}, \varphi_{i}^{(2)}\right) \\
\left(A_{\Gamma \Gamma}^{1}\right)_{i j}=a_{1}\left(\varphi_{j}^{(\Gamma)}, \varphi_{i}^{(\Gamma)}\right), & \left(A_{\Gamma \Gamma}^{2}\right)_{i j}=a_{2}\left(\varphi_{j}^{(\Gamma)}, \varphi_{i}^{(\Gamma)}\right), \\
\left(A_{\Gamma 1}\right)_{i j}=a_{1}\left(\varphi_{j}^{(1)}, \varphi_{i}^{(\Gamma)}\right), & \left(A_{\Gamma 2}\right)_{i j}=a_{2}\left(\varphi_{j}^{(2)}, \varphi_{i}^{(\Gamma)}\right) \\
\left(\mathbf{f}_{1}\right)_{i}=F_{1}\left(\varphi_{i}^{(1)}\right), & \left(\mathbf{f}_{2}\right)_{i}=F_{2}\left(\varphi_{i}^{(2)}\right) \\
\left(\mathbf{f}_{1}^{\Gamma}\right)_{i}=F_{1}\left(\varphi_{i}^{(\Gamma)}\right), & \left(\mathbf{f}_{2}^{\Gamma}\right)_{i}=F_{2}\left(\varphi_{i}^{(\Gamma)}, \varphi_{i}^{(1)}\right),
\end{array}
$$

then set

$$
\mathbf{u}=\left(\mathbf{u}_{1}, \mathbf{u}_{2}, \boldsymbol{\lambda}\right)^{T}, \text { with } \mathbf{u}_{1}=\left(u_{j}^{(1)}\right), \mathbf{u}_{2}=\left(u_{j}^{(2)}\right) \text { and } \boldsymbol{\lambda}=\left(u_{j}^{(\Gamma)}\right)
$$

Problem (19.44) can be casted in the following algebraic form

$$
\left\{\begin{array}{l}
A_{11} \mathbf{u}_{1}+A_{1 \Gamma} \boldsymbol{\lambda}=\mathbf{f}_{1} \\
A_{22} \mathbf{u}_{2}+A_{2 \Gamma} \boldsymbol{\lambda}=\mathbf{f}_{2} \\
A_{\Gamma 1} \mathbf{u}_{1}+A_{\Gamma 2} \mathbf{u}_{2}+\left(A_{\Gamma \Gamma}^{(1)}+A_{\Gamma \Gamma}^{(2)}\right) \boldsymbol{\lambda}=\mathbf{f}_{1}^{\Gamma}+\mathbf{f}_{2}^{\Gamma}
\end{array}\right.
$$

or, equivalently,

$$
A \mathbf{u}=\mathbf{f}, \text { that is }\left[\begin{array}{ccc}
A_{11} & 0 & A_{1 \Gamma} \\
0 & A_{22} & A_{2 \Gamma} \\
A_{\Gamma 1} & A_{\Gamma 2} & A_{\Gamma \Gamma}
\end{array}\right]\left[\begin{array}{l}
\mathbf{u}_{1} \\
\mathbf{u}_{2} \\
\lambda
\end{array}\right]=\left[\begin{array}{l}
\mathbf{f}_{1} \\
\mathbf{f}_{2} \\
\mathbf{f}_{\Gamma}
\end{array}\right]
$$

having set $A_{\Gamma \Gamma}=\left(A_{\Gamma \Gamma}^{(1)}+A_{\Gamma \Gamma}^{(2)}\right)$ and $\mathbf{f}_{\Gamma}=\mathbf{f}_{1}^{\Gamma}+\mathbf{f}_{2}^{\Gamma} .(19.47)$ is nothing but a blockwise representation of the finite element system (4.46), the blocks being determined by the partition (19.45) of the vector of unknowns.

More precisely, the first and second equations of (19.46) are discretizations of the given Poisson problems in $\Omega_{1}$ and $\Omega_{2}$, respectively for the interior values $\mathbf{u}_{1}$ and $\mathbf{u}_{2}$, with Dirichlet data vanishing on $\partial \Omega_{i} \backslash \Gamma$ and equal to the common value $\lambda$ on $\Gamma$. Alternatively, by setting (from the third equation of $(19.46)$ )

$$
A_{\Gamma_{1}} \mathbf{u}_{1}+A_{\Gamma \Gamma}^{(1)} \boldsymbol{\lambda}-\mathbf{f}_{\Gamma}^{1}=-\left(A_{\Gamma_{2}} \mathbf{u}_{2}+A_{\Gamma \Gamma}^{(2)} \boldsymbol{\lambda}-\mathbf{f}_{\Gamma}^{2}\right) \equiv \boldsymbol{\eta}
$$

the first and third equations of (19.46) provide a discretization of the Poisson problem in $\Omega_{1}$ with vanishing Dirichlet data on $\partial \Omega_{1} \backslash \Gamma$ and with Neumann data $\eta$ on $\Gamma$.

Similar considerations apply to the second and third equations of (19.46): they represent the discretization of a Poisson problem in $\Omega_{2}$ with zero Dirichlet data in $\partial \Omega_{2} \backslash \Gamma$ and Neumann data equal to $\eta$ on $\Gamma$.

\subsubsection{The Schur complement}

Consider now the Steklov-Poincaré interface equation (19.26) and look for its finite element counterpart. Since $\lambda$ represents the unknown value of $u$ on $\Gamma$, its finite element correspondent is the vector $\lambda$ of the values of $u_{h}$ at the interface nodes.

By gaussian elimination operated on system (19.47), we can obtain a new reduced system on the sole unknown $\boldsymbol{\lambda}$.

Matrices $A_{11}$ and $A_{22}$ are invertible since they are associated with two homogeneous Dirichlet boundary-value problems for the Laplace operator, hence

$$
\mathbf{u}_{1}=A_{11}^{-1}\left(\mathbf{f}_{1}-A_{1 \Gamma} \boldsymbol{\lambda}\right) \quad \text { and } \quad \mathbf{u}_{2}=A_{22}^{-1}\left(\mathbf{f}_{2}-A_{2 \Gamma} \boldsymbol{\lambda}\right)
$$

From the third equation in (19.46), we obtain

$$
\begin{aligned}
&{\left[\left(A_{\Gamma \Gamma}^{(1)}-A_{\Gamma 1} A_{11}^{-1} A_{1 \Gamma}\right)+\left(A_{\Gamma \Gamma}^{(2)}-A_{\Gamma 2} A_{22}^{-1} A_{2 \Gamma}\right)\right] \boldsymbol{\lambda}} \\
&=\mathbf{f}_{\Gamma}-A_{\Gamma 1} A_{11}^{-1} \mathbf{f}_{1}-A_{\Gamma 2} A_{22}^{-1} \mathbf{f}_{2}=\left(\mathbf{f}_{\Gamma}^{(1)}-A_{\Gamma_{1}} A_{11}^{-1} \mathbf{f}_{1}\right)+\left(\mathbf{f}_{\Gamma}^{(2)}-A_{\Gamma_{2}} A_{22}^{-1} \mathbf{f}_{2}\right)
\end{aligned}
$$

Using the following definitions:

$$
\Sigma=\Sigma_{1}+\Sigma_{2}, \quad \Sigma_{i}=A_{\Gamma \Gamma}^{(i)}-A_{\Gamma i} A_{i i}^{-1} A_{i \Gamma}, \quad i=1,2
$$

and

$$
\chi_{\Gamma}=\chi_{\Gamma}^{(1)}+\chi_{\Gamma}^{(2)}, \quad \chi_{\Gamma}^{(i)}=\mathbf{f}_{\Gamma}^{(i)}-A_{\Gamma_{i}} A_{i i} \mathbf{f}_{i}
$$

(19.50) becomes

$$
\Sigma \boldsymbol{\lambda}=\chi_{\Gamma}
$$

Since $\Sigma$ and $\chi_{\Gamma}$ approximate $S$ and $\chi$, respectively, (19.53) can be considered as a finite element approximation to the Steklov-Poincaré equation (19.26). Matrix $\Sigma$ is the so-called Schur complement of $A$ with respect to $\mathbf{u}_{1}$ and $\mathbf{u}_{2}$, whereas matrices $\Sigma_{i}$ are the Schur complements related to the subdomains $\Omega_{i}(i=1,2)$.

Once system (19.53) is solved w.r.t the unknown $\lambda$, by virtue of (19.49) we can compute $\mathbf{u}_{1}$ and $\mathbf{u}_{2}$. This computation amounts to solve numerically two Poisson problems on the two subdomains $\Omega_{1}$ and $\Omega_{2}$, with Dirichlet boundary conditions $\left.u_{h}^{(i)}\right|_{\Gamma}=\lambda_{h}(i=$ $1,2)$ on the interface $\Gamma$.

The Schur complement $\Sigma$ inherits some of the properties of its generating matrix $A$, as stated by the following result:

Recall that the condition number of the finite element stiffness matrix $A$ satisfies $K_{2}(A) \simeq C h^{-2}$ (see (4.50)). As of $\Sigma$, it can be proven that

$$
K_{2}(\Sigma) \simeq C h^{-1} .
$$

In the specific case under consideration, $A$ (and therefore $\Sigma$, thanks to Lemma 19.1) is symmetric and positive definite. It is therefore convenient to use the conjugate gradient method (with a suitable preconditioner) for the solution of system (19.53). At every iteration, the computation of the residue will involve the finite element solution of two independent Dirichlet boundary-value problems on the subdomains $\Omega_{i}$. By employing a similar procedure we can derive instead of (19.53) an interface equation for the flux $\eta$ introduced in (19.48). From (19.47) and (19.48) we derive

$$
\left[\begin{array}{cc}
A_{11} & A_{1 \Gamma} \\
A_{\Gamma 1} & A_{\Gamma \Gamma}^{(1)}
\end{array}\right]\left[\begin{array}{c}
\mathbf{u}_{1} \\
\boldsymbol{\lambda}
\end{array}\right]=\left[\begin{array}{c}
\mathbf{f}_{1} \\
\mathbf{f}_{\Gamma}^{(1)}+\boldsymbol{\eta}
\end{array}\right]
$$

By eliminating $\mathbf{u}_{1}$ from the first row and replacing it in the second one we obtain

$$
\Sigma_{1} \boldsymbol{\lambda}=\chi_{\Gamma}^{(1)}+\boldsymbol{\eta}, \text { that is } \boldsymbol{\lambda}=\Sigma_{1}^{-1}\left(\chi_{\Gamma}^{(1)}+\boldsymbol{\eta}\right)
$$

Proceeding in a similar way we obtain

$$
\Sigma_{2} \boldsymbol{\lambda}=\chi_{\Gamma}^{(2)}-\boldsymbol{\eta}, \text { that is } \boldsymbol{\lambda}=\Sigma_{2}^{-1}\left(\chi_{\Gamma}^{(2)}-\boldsymbol{\eta}\right)
$$

By equating the last two equations (whose common value is $\boldsymbol{\lambda}$ ) we finally obtain the Schur-complement equation for the flux $\eta$ :

$$
T \eta=\psi_{\Gamma}, \quad \text { with } T=\Sigma_{1}^{-1}+\Sigma_{2}^{-1}, \psi_{\Gamma}=\Sigma_{2}^{-1} \chi_{\Gamma}^{(2)}-\Sigma_{1}^{-1} \chi_{\Gamma}^{(1)}
$$

This algebraic equation can be regarded as a direct discretization of the Steklov-Poincaré problem for the flux (19.38).

\subsubsection{The discrete Steklov-Poincaré operator}

In this section we will find the discrete operator associated with the Schur complement. With this aim, besides the space $V_{i, h}$ previously introduced, we will need the one $V_{i, h}^{0}$ generated by the functions $\left\{\varphi_{j}^{(i)}\right\}$ exclusively associated to the internal nodes of the subdomain $\Omega_{i}$, and the space $\Lambda_{h}$ generated by the set of functions $\left.\left\{\varphi_{j}^{(\Gamma)}\right\}_{\Gamma}\right\}$. We have $\Lambda_{h}=\left\{\mu_{h}\left|\exists v_{h} \in V_{h}: v_{h}\right|_{\Gamma}=\mu_{h}\right\}$, whence $\Lambda_{h}$ represents a finite element subspace of the trace functions space $\Lambda$ introduced in $(19.30)$.

Consider now the problem: find $H_{i, h} \eta_{h} \in V_{i, h}$, with $H_{i, h} \eta_{h}=\eta_{h}$ on $\Gamma$, such that

$$
\int_{\Omega_{i}} \nabla\left(H_{i, h} \eta_{h}\right) \cdot \nabla v_{h} d \Omega_{i}=0 \quad \forall v_{h} \in V_{i, h}^{0}
$$

Clearly, $H_{i, h} \eta_{h}$ represents a finite element approximation of the harmonic extension $H_{i} \eta_{h}$, and the operator $H_{i, h}: \eta_{h} \rightarrow H_{i, h} \eta_{h}$ can be regarded as an approximation of $H_{i}$. By expanding $H_{i, h} \eta_{h}$ in terms of the basis functions

$$
H_{i, h} \eta_{h}=\sum_{j=1}^{N_{i}} u_{j}^{(i)} \varphi_{j}^{(i)}+\left.\sum_{k=1}^{N_{\Gamma}} \eta_{k} \varphi_{k}^{(\Gamma)}\right|_{\Omega_{i}}
$$

we can rewrite (19.59) in matrix form

$$
A_{i i} \mathbf{u}^{(i)}=-A_{i \Gamma} \boldsymbol{\eta}
$$

The following result, called the uniform discrete extension theorem, holds:

For the proof see, e.g., [QV99].

Now for $i=1,2$ the (local) discrete Steklov-Poincaré operator $S_{i, h}: \Lambda_{h} \rightarrow \Lambda_{h}^{\prime}$ is defined as follows

$$
\left\langle S_{i, h} \eta_{h}, \mu_{h}\right\rangle=\int_{\Omega_{i}} \nabla\left(H_{i, h} \eta_{h}\right) \cdot \nabla\left(H_{i, h} \mu_{h}\right) d \Omega_{i} \quad \forall \eta_{h}, \mu_{h} \in \Lambda_{h}
$$

then we define the (global) discrete Steklov-Poincaré operator as $S_{h}=S_{1, h}+S_{2, h}$.

Proof. For $i=1,2$ we have

$$
\begin{aligned}
\left\langle S_{i, h} \eta_{h}, \mu_{h}\right\rangle &=a_{i}\left(H_{i, h} \eta_{h}, H_{i, h} \mu_{h}\right) \\
&=a_{i}\left(\sum_{j=1}^{N_{\Gamma}} u_{j} \varphi_{j}^{(i)}+\left.\sum_{k=1}^{N_{\Gamma}} \eta_{k} \varphi_{k}^{(\Gamma)}\right|_{\Omega_{i}}, \sum_{l=1}^{N_{\Gamma}} w_{l} \varphi_{l}^{(i)}+\left.\sum_{m=1}^{N_{\Gamma}} \mu_{m} \varphi_{m}^{(\Gamma)}\right|_{\Omega_{i}}\right)
\end{aligned}
$$



$$
\begin{aligned}
=& \sum_{j, l=1}^{N_{\Gamma}} w_{l} a_{i}\left(\varphi_{j}^{(i)}, \varphi_{l}^{(i)}\right) u_{j}+\sum_{j, m=1}^{N_{\Gamma}} \mu_{m} a_{i}\left(\varphi_{j}^{(i)},\left.\varphi_{m}^{(\Gamma)}\right|_{\Omega_{i}}\right) u_{j} \\
&+\sum_{k, l=1}^{N_{\Gamma}} w_{l} a_{i}\left(\left.\varphi_{k}^{(\Gamma)}\right|_{\Omega_{i}}, \varphi_{l}^{(i)}\right) \eta_{k}+\sum_{k, m=1}^{N_{\Gamma}} \mu_{m} a_{i}\left(\left.\varphi_{k}^{(\Gamma)}\right|_{\Omega_{i}},\left.\varphi_{m}^{(\Gamma)}\right|_{\Omega_{i}}\right) \eta_{k} \\
=& \mathbf{w}^{T} A_{i i} \mathbf{u}+\boldsymbol{\mu}^{T} A_{\Gamma i} \mathbf{u}+\mathbf{w}^{T} A_{i \Gamma} \boldsymbol{\eta}+\boldsymbol{\mu}^{T} A_{\Gamma \Gamma}^{(i)} \boldsymbol{\eta}
\end{aligned}
$$

Thanks to $(19.60)$ we obtain

$$
\begin{aligned}
\left\langle S_{i, h} \eta_{h}, \mu_{h}\right\rangle &=-\mathbf{w}^{T} A_{i \Gamma} \eta-\boldsymbol{\mu}^{T} A_{\Gamma i} A_{i i}^{-1} A_{i \Gamma} \boldsymbol{\eta}+\mathbf{w}^{T} A_{i \Gamma} \boldsymbol{\eta}+\boldsymbol{\mu}^{T} A_{\Gamma \Gamma}^{(i)} \boldsymbol{\eta} \\
&=\boldsymbol{\mu}^{T}\left(A_{\Gamma \Gamma}^{(i)}-A_{\Gamma i} A_{i i}^{-1} A_{i \Gamma}\right) \eta \\
&=\boldsymbol{\mu}^{T} \Sigma_{i} \boldsymbol{\eta}
\end{aligned}
$$

From Theorem $19.3$ and thanks to the representation (19.63), we deduce that there exist two constants $\hat{K}_{1}, \hat{K}_{2}>0$, independent of $h$, such that

$$
\hat{K}_{1}\left\langle S_{1, h} \mu_{h}, \mu_{h}\right\rangle \leq\left\langle S_{2, h} \mu_{h}, \mu_{h}\right\rangle \leq \hat{K}_{2}\left\langle S_{1, h} \mu_{h}, \mu_{h}\right\rangle \quad \forall \mu_{h} \in \Lambda_{h}
$$

Thanks to (19.64) we can infer that there exist two constants $\tilde{K}_{1}, \tilde{K}_{2}>0$, independent of $h$, such that

$$
\tilde{K}_{1}\left(\boldsymbol{\mu}^{T} \Sigma_{1} \boldsymbol{\mu}\right) \leq \boldsymbol{\mu}^{T} \Sigma_{2} \boldsymbol{\mu} \leq \tilde{K}_{2}\left(\boldsymbol{\mu}^{T} \Sigma_{1} \boldsymbol{\mu}\right) \quad \forall \boldsymbol{\mu} \in \mathbb{R}^{N_{\Gamma}}
$$

This amounts to say that the two matrices $\Sigma_{1}$ and $\Sigma_{2}$ are spectrally equivalent, that is their spectral condition number features the same asymptotic behaviour w.r.t $h$. Henceforth, both $\Sigma_{1}$ and $\Sigma_{2}$ provide an optimal preconditioner of the Schur complement $\Sigma$, that is there exists a constant $C$, independent of $h$, such that

$$
K_{2}\left(\Sigma_{i}^{-1} \Sigma\right) \leq C, \quad i=1,2
$$

As we will see in Sect. 19.3.3, this property allows us to prove that the discrete version of the Dirichlet-Neumann algorithm converges with a rate independent of $h$. A similar result holds for the discrete Neumann-Neumann and Robin-Robin algorithms.

\subsubsection{Equivalence between the Dirichlet-Neumann algorithm and a preconditioned Richardson algorithm in the discrete case}

Let us now prove the analogue of the equivalence theorem $19.2$ in the algebraic case. The finite element approximation of the Dirichlet problem (19.31) has the following algebraic form

$$
A_{11} \mathbf{u}_{1}^{(k)}=\mathbf{f}_{1}-A_{1 \Gamma} \boldsymbol{\lambda}^{(k-1)}
$$

whereas that of the Neumann problem (19.32) reads

$$
\left[\begin{array}{cc}
A_{22} & A_{2 \Gamma} \\
A_{\Gamma 2} & A_{\Gamma \Gamma}^{(2)}
\end{array}\right]\left[\begin{array}{c}
\mathbf{u}_{2}^{(k)} \\
\boldsymbol{\lambda}^{(k-1 / 2)}
\end{array}\right]=\left[\begin{array}{c}
\mathbf{f}_{2} \\
\mathbf{f}_{\Gamma}-A_{\Gamma 1} \mathbf{u}_{1}^{(k)}-A_{\Gamma \Gamma}^{(1)} \boldsymbol{\lambda}^{(k-1)}
\end{array}\right]
$$

In its turn, (19.33) becomes

$$
\boldsymbol{\lambda}^{(k)}=\theta \boldsymbol{\lambda}^{(k-1 / 2)}+(1-\theta) \boldsymbol{\lambda}^{(k-1)}
$$

By eliminating $\mathbf{u}_{2}^{(k)}$ from (19.70) we obtain

$$
\left(A_{\Gamma \Gamma}^{(2)}-A_{\Gamma 2} A_{22}^{-1} A_{2 \Gamma}\right) \boldsymbol{\lambda}^{(k-1 / 2)}=\mathbf{f}_{\Gamma}-A_{\Gamma 1} \mathbf{u}_{1}^{(k)}-A_{\Gamma \Gamma}^{(1)} \boldsymbol{\lambda}^{(k-1)}-A_{\Gamma 2} A_{22}^{-1} \mathbf{f}_{2}
$$

By the definition (19.51) of $\Sigma_{2}$ and by (19.69), one has

$$
\Sigma_{2} \boldsymbol{\lambda}^{(k-1 / 2)}=\mathbf{f}_{\Gamma}-A_{\Gamma 1} A_{11}^{-1} \mathbf{f}_{1}-A_{\Gamma 2} A_{22}^{-1} \mathbf{f}_{2}-\left(A_{\Gamma \Gamma}^{(1)}-A_{\Gamma 1} A_{11}^{-1} A_{1 \Gamma}\right) \boldsymbol{\lambda}^{(k-1)}
$$

that is, owing to the definition (19.51) of $\Sigma_{1}$ and to $(19.52)$,

$$
\boldsymbol{\lambda}^{(k-1 / 2)}=\Sigma_{2}^{-1}\left(\chi_{\Gamma}-\Sigma_{1} \boldsymbol{\lambda}^{(k-1)}\right)
$$

Now, by virtue of (19.71) we deduce

$$
\boldsymbol{\lambda}^{(k)}=\theta \Sigma_{2}^{-1}\left(\chi_{\Gamma}-\Sigma_{1} \boldsymbol{\lambda}^{(k-1)}\right)+(1-\theta) \boldsymbol{\lambda}^{(k-1)}
$$

that is, since $-\Sigma_{1}=-\Sigma+\Sigma_{2}$

$$
\boldsymbol{\lambda}^{(k)}=\theta \Sigma_{2}^{-1}\left(\chi_{\Gamma}-\Sigma \boldsymbol{\lambda}^{(k-1)}+\Sigma_{2} \boldsymbol{\lambda}^{(k-1)}\right)+(1-\theta) \boldsymbol{\lambda}^{(k-1)}
$$

whence

$$
\Sigma_{2}\left(\boldsymbol{\lambda}^{(k)}-\boldsymbol{\lambda}^{(k-1)}\right)=\theta\left(\chi_{\Gamma}-\Sigma \boldsymbol{\lambda}^{(k-1)}\right)
$$

The latter is nothing but a Richardson iteration on the system (19.53) using the local Schur complement $\Sigma_{2}$ as preconditioner.

Remark 19.2. The Richardson preconditioner induced by the Dirichlet-Neumann algorithm is in fact the local Schur complement associated to that subdomain on which we solve a Neumann problem. So, in the so-called Neumann-Dirichlet algorithm, in which at every iteration we solve a Dirichlet problem in $\Omega_{2}$ and a Neumann one in $\Omega_{1}$, the preconditioner of the associated Richardson algorithm would be $\Sigma_{1}$ and not $\Sigma_{2}$

Remark 19.3. An analogous result can be proven for the discrete version of the Neumann-Neumann algorithm introduced in Sect. 19.1.3. Precisely, the NeumannNeumann algorithm is equivalent to the Richardson algorithm applied to system (19.53) with a preconditioner whose inverse is given by $P_{h}^{-1}=\sigma_{1} \Sigma_{1}^{-1}+\sigma_{2} \Sigma_{2}^{-1}, \sigma_{1}$ and $\sigma_{2}$ being the coefficients used for the (discrete) interface equation which corresponds to (19.19). Moreover we can prove that there exists a constant $C>0$, independent of $h$, such that

$$
K_{2}\left(\left(\sigma_{1} \Sigma_{1}^{-1}+\sigma_{2} \Sigma_{2}^{-1}\right) \Sigma\right) \leq C
$$

Proceeding in a similar way we can show that the discrete version of the Robin-Robin algorithm (19.20)-(19.21) is also equivalent to a Richardson algorithm for (19.53), using this time as preconditioner the matrix $\left(\gamma_{1}+\gamma_{2}\right)^{-1}\left(\gamma_{1} I+\Sigma_{1}\right)\left(\gamma_{2} I+\Sigma_{2}\right)$. Let us recall that a matrix $P_{h}$ is an optimal preconditioner for $\Sigma$ if the condition number of $P_{h}^{-1} \Sigma$ is bounded uniformely w.r.t the dimension $N$ of the matrix $\Sigma$ (and therefore from $h$ in the case in which $\Sigma$ arises from a finite element discretization).

We can therefore summarize by saying that for the solution of system $\Sigma \boldsymbol{\lambda}=\chi_{\Gamma}$, we can make use of the following preconditioners, all of them being optimal:

When solving the flux equation (19.58), the FETI preconditioner reads $P_{h}=\left(\Sigma_{1}+\right.$ $\left.\Sigma_{2}\right)^{-1}$, yelding the preconditioned matrix $\left(\Sigma_{1}+\Sigma_{2}\right)\left(\Sigma_{1}^{-1}+\Sigma_{2}^{-1}\right)$. For all these preconditioners, optimality follows from the spectral equivalence $(19.67)$, hence $K_{2}\left(P_{h}^{-1} \Sigma\right)$ is bounded independently of $h$.

From the convergence theory of Richardson method we know that if both $\Sigma$ and $P_{h}$ are symmetric and positive definite, one has $\left\|\boldsymbol{\lambda}^{n}-\boldsymbol{\lambda}\right\|_{\Sigma} \leq \rho^{n}\left\|\boldsymbol{\lambda}^{0}-\boldsymbol{\lambda}\right\|_{\Sigma}, n \geq 0$, being $\|\boldsymbol{v}\|_{\Sigma}=\left(\boldsymbol{v}^{T} \Sigma \boldsymbol{v}\right)^{1 / 2}$. The optimal convergence rate is given by

$$
\rho=\frac{K_{2}\left(P_{h}^{-1} \Sigma\right)-1}{K_{2}\left(P_{h}^{-1} \Sigma\right)+1}
$$

and is therefore independent of $h$.

\subsection{Generalization to the case of many subdomains}

To generalize the previous DD algorithms to the case in which the domain $\Omega$ is partitioned into an arbitrary number $M>2$ of subdomains we proceed as follows. Let $\Omega_{i}, i=1, \ldots, M$, denote a family of disjoint subdomains such that $\cup \bar{\Omega}_{i}=\bar{\Omega}$, and denote $\Gamma_{i}=\partial \Omega_{i} \backslash \partial \Omega$ and $\Gamma=\cup \Gamma_{i}$ (the skeleton).

Let us consider the Poisson problem (3.13). In the current case the equivalence Theorem $19.1$ generalizes as follows:

$$
\begin{cases}-\Delta u_{i}=f & \text { in } \Omega_{i}, \\ u_{i}=u_{k} \quad & \text { on } \Gamma_{i k}, \quad \forall k \in \mathscr{A}(i) \\ \frac{\partial u_{i}}{\partial n_{i}}=\frac{\partial u_{k}}{\partial n_{i}} & \text { on } \Gamma_{i k}, \forall k \in \mathscr{A}(i) \\ u_{i}=0 & \text { on } \partial \Omega_{i} \cap \partial \Omega\end{cases}
$$

for $i=1, \ldots, M$, being $\Gamma_{i k}=\partial \Omega_{i} \cap \partial \Omega_{k} \neq \emptyset, \mathscr{A}(i)$ the set of indices $k$ such that $\Omega_{k}$ is adjacent to $\Omega_{i}$; as usual, $\mathbf{n}_{i}$ denotes the outward unit normal vetor to $\Omega_{i}$.

Assume now that (3.13) has been approximated by the finite element method. Following the ideas presented in Sect. 19.3 and denoting by $\mathbf{u}=\left(\mathbf{u}_{I}, \mathbf{u}_{\Gamma}\right)^{T}$ the vector of unknowns split in two subvectors, the one $\left(\mathbf{u}_{I}\right)$ related with the internal nodes, and that $\left(\mathbf{u}_{\Gamma}\right.$ ) related with the nodes lying on the skeleton $\Gamma$, the finite element algebraic system can be reformulated in blockwise form as follows

$$
\left[\begin{array}{cc}
A_{I I} & A_{I \Gamma} \\
A_{\Gamma I} & A_{\Gamma \Gamma}
\end{array}\right]\left[\begin{array}{c}
\mathbf{u}_{I} \\
\mathbf{u}_{\Gamma}
\end{array}\right]=\left[\begin{array}{c}
\mathbf{f}_{I} \\
\mathbf{f}_{\Gamma}
\end{array}\right]
$$

being $A_{\Gamma I}=A_{I \Gamma}^{T}$. Matrix $A_{I \Gamma}$ is banded, while $A_{I I}$ has the block diagonal form

$$
A_{I I}=\left[\begin{array}{cccc}
A_{\Omega_{1}, \Omega_{1}} & 0 & \ldots & 0 \\
0 & \ddots & & \vdots \\
\vdots & & \ddots & 0 \\
0 & \ldots & 0 & A_{\Omega_{M}, \Omega_{M}}
\end{array}\right]
$$

We are using the following notations:

$$
\begin{array}{ll}
\left(A_{\Omega_{i}, \Omega_{i}}\right)_{l j}=a_{i}\left(\varphi_{j}, \varphi_{l}\right), & 1 \leq l, j \leq N_{i} \\
\left(A_{\Gamma \Gamma}^{(i)}\right)_{s r}=a_{i}\left(\psi_{r}, \psi_{s}\right), \quad 1 \leq r, s \leq N_{\Gamma_{i}} \\
\left(A_{\Omega_{i}, \Gamma}\right)_{l r}=a_{i}\left(\psi_{r}, \varphi_{l}\right), & 1 \leq r \leq N_{\Gamma_{i}}, \quad 1 \leq l \leq N_{i}
\end{array}
$$

where $N_{i}$ is the number of nodes internal to $\Omega_{i}, N_{\Gamma_{i}}$ that of the nodes sitting on the interface $\Gamma_{i}, \varphi_{j}$ and $\psi_{r}$ the basis functions associated with the internal and interface nodes, respectively.

Let us remark that on every subdomain $\Omega_{i}$ the matrix

$$
A_{i}=\left[\begin{array}{cc}
A_{\Omega_{i}, \Omega_{i}} & A_{\Omega_{i}, \Gamma} \\
A_{\Gamma, \Omega_{i}} & A_{\Gamma \Gamma}^{(i)}
\end{array}\right]
$$

represents the local finite element stiffness matrix associated to a Neumann problem on $\Omega_{i}$. Since $A_{I I}$ is non-singular, from (19.74) we can formally derive

$$
\mathbf{u}_{I}=A_{I I}^{-1}\left(\mathbf{f}_{I}-A_{I \Gamma} \mathbf{u}_{\Gamma}\right)
$$

By eliminating the unknown $\mathbf{u}_{I}$ from system (19.74), it follows

$$
A_{\Gamma \Gamma} \mathbf{u}_{\Gamma}=\mathbf{f}_{\Gamma}-A_{\Gamma I} A_{I I}^{-1}\left(\mathbf{f}_{I}-A_{I \Gamma} \mathbf{u}_{\Gamma}\right)
$$

that is

$$
\left(\begin{array}{cc}
A_{I I} & A_{I \Gamma} \\
0 & \Sigma
\end{array}\right)\left(\begin{array}{c}
u_{I} \\
u_{\Gamma}
\end{array}\right)=\left(\begin{array}{c}
\mathbf{f}_{I} \\
\chi_{\Gamma}
\end{array}\right)
$$

having set

$$
\Sigma=A_{\Gamma \Gamma}-A_{\Gamma I} A_{I I}^{-1} A_{I \Gamma} \text { and } \chi_{\Gamma}=\mathbf{f}_{\Gamma}-A_{\Gamma I} A_{I I}^{-1} \mathbf{f}_{I}
$$

Denoting, as usual, $\boldsymbol{\lambda}=\mathbf{u}_{\Gamma},(19.78)$ yields

$$
\Sigma \boldsymbol{\lambda}=\chi_{\Gamma}
$$

This is the Schur complement system in the multidomain case. It can be regarded as a finite element approximation of the interface Steklov-Poincaré problem in the case of $M$ subdomains.

The local Schur complements are defined as

$$
\Sigma_{i}=A_{\Gamma \Gamma}^{(i)}-A_{\Gamma, \Omega_{i}} A_{\Omega_{i}, \Omega_{i}}^{-1} A_{\Omega_{i}, \Gamma}, \quad i=1, \ldots M
$$

hence

$$
\Sigma=R_{\Gamma_{1}}^{T} \Sigma_{1} R_{\Gamma_{1}}+\ldots+R_{\Gamma_{M}}^{T} \Sigma_{M} R_{\Gamma_{M}}
$$

where $R_{\Gamma_{i}}$ is a restriction operator, that is a rectangular matrix of zeros and ones that map values on $\Gamma$ onto those on $\Gamma_{i}, i=1, \ldots, M$. Note that the r.h.s. of $(19.79)$ can be written as a sum of local contributions,

$$
\chi_{\Gamma}=\sum_{i=1}^{M} R_{\Gamma_{i}}^{T}\left(\mathbf{f}_{\Gamma}^{(i)}-A_{\Gamma, \Omega_{i}} A_{\Omega_{i}, \Omega_{i}}^{-1} \mathbf{f}_{I}^{(i)}\right)
$$

A general algorithm to solve the finite element Poisson problem in $\Omega$ could be formulated as follows:

1. compute the solution of (19.79) to obtain the value of $\lambda$ on the skeleton $\Gamma$;

2. solve (19.77); since $A_{I I}$ is block-diagonal, this step yields the solution of $M$ independent subproblems of reduced dimension, $A_{\Omega_{i}, \Omega_{i}} \mathbf{u}_{I}^{i}=\mathbf{g}^{i}, i=1, \ldots, M$.

About the condition number of $\Sigma$, the following estimate can be proven: there exists a constant $C>0$, independent of $h$ and $H_{\min }, H_{\max }$, such that

$$
K_{2}(\Sigma) \leq C \frac{H_{\max }}{h H_{\min }^{2}}
$$

$H_{\max }$ being the maximum diameter of the subdomains and $H_{\min }$ the minimum one.

Remark 19.4 (Approximation of the inverse of $\boldsymbol{A}$ ). The inverse of the block matrix $A$ in (19.74) admits the following LDU factorization

$$
A^{-1}=\left[\begin{array}{cc}
I & -A_{I I}^{-1} A_{I \Gamma} \\
0 & I
\end{array}\right]\left[\begin{array}{cc}
A_{I I}^{-1} & 0 \\
0 & \Sigma^{-1}
\end{array}\right]\left[\begin{array}{cc}
I & 0 \\
-A_{\Gamma I} A_{I I}^{-1} & I
\end{array}\right]
$$

Should we have suitable preconditioners $B_{I I}$ of $A_{I I}$ and $P$ of $\Sigma$, an approximation of $A^{-1}$ would be given by

$$
P_{A}^{-1}=\left[\begin{array}{cc}
I & -B_{I I}^{-1} A_{I \Gamma} \\
0 & I
\end{array}\right]\left[\begin{array}{cc}
B_{I I}^{-1} & 0 \\
0 & P^{-1}
\end{array}\right]\left[\begin{array}{cc}
I & 0 \\
-A_{\Gamma I} B_{I I}^{-1} & I
\end{array}\right]
$$

An application of $P_{A}^{-1}$ to a given vector involves $B_{I I}^{-1}$ in two matrix-vector multiplies and $P^{-1}$ in only one matrix-vector multiply (see [TW05, Sect. 4.3]). Remark 19.5 (Saddle-point systems). In case of a saddle-point (block) matrix like the one in (17.58), an LDU factorization can be obtained as follows

$$
K=\left[\begin{array}{cc}
A & B^{T} \\
B & -C
\end{array}\right]=\left[\begin{array}{cc}
I_{A} & 0 \\
B A^{-1} & I_{C}
\end{array}\right]\left[\begin{array}{cc}
A & 0 \\
0 & S
\end{array}\right]\left[\begin{array}{cc}
I_{A} & A^{-1} B^{T} \\
0 & I_{C}
\end{array}\right]
$$

where $S=-C-B A^{-1} B^{T}$ is the Schur complement computed with respect to the second variable (e.g. $\mathbf{P}$ in the case of system (17.58)). An inverse of $K$ is obtained as

$$
K^{-1}=\left[\begin{array}{cc}
A^{-1} & 0 \\
0 & 0
\end{array}\right]+Q S^{-1} Q^{T}, \quad Q=\left[\begin{array}{c}
-A^{-1} B^{T} \\
I
\end{array}\right]
$$

being $I_{A}$ and $I_{C}$ two identity matrices having the size of $A$ and $C$, respectively. A preconditioner for $K$ can be constructed by replacing in $(19.87) A^{-1}$ and $S^{-1}$ by suitable domain decomposition preconditioners of $A$ and $S$, respectively.

This observation stands at the ground of the design of the so-called FETI-DP and BDDC preconditioners, see Sects. 19.5.5 and 19.5.6.

\subsubsection{Some numerical results}

Consider the Poisson problem (3.13) on the domain $\Omega=(0,1)^{2}$ whose finite element approximation was given in (4.40).

Let us partition $\Omega$ into $M$ disjoint squares $\Omega_{i}$ whose sidelength is $H$, such that $\cup_{i=1}^{M} \overline{\Omega_{i}}=$ $\bar{\Omega}$. An example with four subdomains is displayed in Fig. $19.5$ (left).

In Table $19.1$ we report the numerical values of $K_{2}(\Sigma)$ for the problem at hand, for several values of the finite element grid-size $h$; it grows linearly with $1 / h$ and with $1 / H$, as predicted by the formula (19.83). In Fig. $19.5$ (right) we display the pattern of the Schur complement matrix $\Sigma$ in the particular case of $h=1 / 8$ and $H=1 / 2$. The matrix has a blockwise structure that accounts for the interfaces $\Gamma_{1}, \Gamma_{2}, \Gamma_{3}$ and $\Gamma_{4}$, plus the contribution arising from the crosspoint $\Gamma_{c}$. Since $\Sigma$ is a dense matrix, when solving the linear system (19.79) the explicit computation of its entries is not convenient. Instead, we can use the following Algorithm $18.1$ to compute the matrixvector product $\sum \mathbf{x}_{\Gamma}$, for any vector $\mathbf{x}_{\Gamma}$ (and therefore the residue at every step of an iterative algorithm). We have denoted by $R_{\Gamma_{i}}$ the rectangular matrix associated to the

Table 19.1. Condition number of the Schur complement $\Sigma$

\begin{tabular}{lccc}
\hline$K_{2}(\Sigma)$ & $H=1 / 2$ & $H=1 / 4$ & $H=1 / 8$ \\
\hline$h=1 / 8$ & $9.77$ & $14.83$ & $25.27$ \\
$h=1 / 16$ & $21.49$ & $35.25$ & $58.60$ \\
$h=1 / 32$ & $44.09$ & $75.10$ & $137.73$ \\
$h=1 / 64$ & $91.98$ & $155.19$ & $290.43$ \\
\hline
\end{tabular}


![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-194.jpg?height=270&width=584&top_left_y=116&top_left_x=164)

Fig. 19.5. Example of partition of $\Omega=(0,1)^{2}$ into four squared subdomains (left). Pattern of the Schur complement $\Sigma$ (right) corresponding to the domain partition displayed on the left

restriction operator $R_{\Gamma_{i}}: \Gamma \rightarrow \Gamma_{i}=\partial \Omega_{i} \backslash \partial \Omega$, while $\mathbf{x} \leftarrow \mathbf{y}$ indicates the algebraic operation $\mathbf{x}=\mathbf{x}+\mathbf{y}$

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-194.jpg?height=352&width=729&top_left_y=540&top_left_x=87)

Since no communication is required among the subdomains, this is a fully parallel algorithm.

Before using for the first time the Schur complement, a start-up phase, described in Algorithm 18.2, is requested. Note that this is an off-line procedure. 

\subsection{DD preconditioners in case of many subdomains}

Before introducing the preconditioners for the Schur complement in the case in which $\Omega$ is partitioned in many subdomains we recall the following definition:

Definition 19.1. A preconditioner $P_{h}$ of $\Sigma$ is said to be scalable if the condition

Iterative methods using scalable preconditioners allow henceforth to achieve convergence rates independent of the subdomain number. This is a very desirable property in those cases where a large number of subdomains is used.

Let $R_{i}$ be a restriction operator which, to any vector $\mathbf{v}_{h}$ of nodal values on the global domain $\Omega$, associates its restriction to the subdomain $\Omega_{i}$

$$
R_{i}: \mathbf{v}_{h \mid} \rightarrow \mathbf{v}_{\left.h\right|_{\Omega_{i} \cup \Gamma_{i}} ^{i}}
$$

Let moreover

$$
R_{i}^{T}: \mathbf{v}_{h \mid}^{i} \rightarrow \mathbf{v}_{\left.h\right|_{\Omega_{i} \cup I_{i}}}^{i}
$$

be the prolongation (or extension-by-zero) operator. In algebraic form $R_{i}$ can be represented by a matrix that coincides with the identity matrix in correspondence with the subdomain $\Omega_{i}$

$$
R_{i}=\left[\begin{array}{ccc|c|ccc}
0 & \ldots & 0 \\
\vdots & \ddots & \vdots & 1 & 0 & \ldots & 0 \\
0 & \ldots & 0 & & \left.{\Omega_{i}} \mid \begin{array}{cccc}
0 & \ddots & \vdots \\
0 & \ldots & 0
\end{array}\right]
\end{array}\right.
$$

Similarly we can define the restriction and prolongation operators $R_{\Gamma_{i}}$ and $R_{\Gamma_{i}}^{T}$, respectively, that act on the vector of interface nodal values (as done in (19.81)). In order to find a preconditioner for $\Sigma$ the strategy consists of combining the contributions of local subdomain preconditioners with that of a global contribution referring to a coarse grid whose elements are the subdomains themselves. Without the latter coarse grid term the preconditioner could not be scalable since it would lack any mechanism for global communication of information across the domain in each iteration step. This idea can be formalized through the following relation that provides the inverse of the preconditioner

$$
\left(P_{h}\right)^{-1}=\sum_{i=1}^{M} R_{\Gamma_{i}}^{T} P_{i, h}^{-1} R_{\Gamma_{i}}+R_{\Gamma}^{T} P_{H}^{-1} R_{\Gamma}
$$

.
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-196.jpg?height=324&width=678&top_left_y=128&top_left_x=126)

Fig. 19.6. A decomposition into 9 subdomains (left) with a fine triangulation in small triangles and a coarse triangulation in large quadrilaterals (the 9 subdomains) (right)

We have denoted by $H$ the maximum value of the diameters $H_{i}$ of the subdomains $\Omega_{i}$; moreover, $P_{i, h}$ is either the local Schur complement $\Sigma_{i}$, or (more frequently) a suitable preconditioner of $\Sigma_{i}$, while $R_{\Gamma}$ and $P_{H}$ refer to operators that act on the global scale (that of the coarse grid).

Many different choices are possible for the local Schur complement preconditioner $P_{i, h}$; they will give rise to different condition numbers of the preconditioned matrix $P_{h}^{-1} \Sigma$

\subsubsection{Jacobi preconditioner}

Let $\left\{e_{1}, \ldots, e_{m}\right\}$ be the set of edges and $\left\{v_{1}, \ldots, v_{n}\right\}$ that of vertices of a partition of $\Omega$ into subdomains (see Fig. $19.6$ for an example).

The Schur complement $\Sigma$ features the following blockwise representation

$$
\Sigma=\left[\begin{array}{cc}
\Sigma_{e e} & \Sigma_{e v} \\
\Sigma_{e v}^{T} & \Sigma_{v v}
\end{array}\right]
$$

having set

$$
\Sigma_{e e}=\left[\begin{array}{ccc}
\Sigma_{e_{1} e_{1}} & \ldots & \Sigma_{e_{1} e_{m}} \\
\vdots & \ddots & \vdots \\
\Sigma_{e_{m} e_{1}} & \ldots & \Sigma_{e_{m} e_{m}}
\end{array}\right], \quad \Sigma_{e v}=\left[\begin{array}{ccc}
\Sigma_{e_{1} v_{1}} & \ldots & \Sigma_{e_{1} v_{n}} \\
\vdots & \ddots & \vdots \\
\Sigma_{e_{m} v_{1}} & \ldots & \Sigma_{e_{m} v_{n}}
\end{array}\right]
$$

and

$$
\Sigma_{v v}=\left[\begin{array}{cccc}
\Sigma_{v_{1} v_{1}} & 0 & \ldots & 0 \\
0 & \ddots & & \vdots \\
\vdots & & \ddots & 0 \\
0 & \ldots & 0 & \Sigma_{v_{n} v_{n}}
\end{array}\right]
$$

In $3 \mathrm{D}$ there should be a further block row and column due to the presence of faces.

The Jacobi preconditioner of the Schur complement $\Sigma$ is a block diagonal matrix defined by

$$
P_{h}^{J}=\left[\begin{array}{cc}
\hat{\Sigma}_{e e} & 0 \\
0 & \Sigma_{v v}
\end{array}\right]
$$

where $\hat{\Sigma}_{e e}$ is either $\Sigma_{e e}$ or a suitable approximation of it. This preconditioner does not account for the interaction between the basis functions associated with edges and those associated with vertices. The matrix $\hat{\Sigma}_{e e}$ is also diagonal

$$
\hat{\Sigma}_{e e}=\left[\begin{array}{cccc}
\hat{\Sigma}_{e_{1} e_{1}} & 0 & \ldots & 0 \\
0 & \ddots & & \vdots \\
\vdots & & \ddots & 0 \\
0 & \ldots & 0 & \hat{\Sigma}_{e_{m} e_{m}}
\end{array}\right]
$$

Here $\hat{\Sigma}_{e_{k} e_{k}}$ denotes $\Sigma_{e_{k} e_{k}}$ or a suitable approximation of it.

The preconditioner $P_{h}^{J}$ can also be expressed in terms of restriction and prolongation operators as follows

$$
\left(P_{h}^{J}\right)^{-1}=\sum_{k=1}^{m} R_{e_{k}}^{T} \hat{\Sigma}_{e_{k} e_{k}}^{-1} R_{e_{k}}+R_{v}^{T} \Sigma_{v v}^{-1} R_{v}
$$

where $R_{e_{k}}$ and $R_{v}$ denote edge and vertices restriction operators, respectively.

Regarding the condition number of the preconditioned Schur complement, there exists a constant $C>0$, indipendent of both $h$ and $H$, such that

$$
K_{2}\left(\left(P_{h}^{J}\right)^{-1} \Sigma\right) \leq C H^{-2}\left(1+\log \frac{H}{h}\right)^{2}
$$

Should the conjugate gradient method be used to solve the preconditioned Schur complement system (19.79) with preconditioner $P_{h}^{J}$, the number of iterations necessary to converge (within a prescribed tolerance) would be proportional to $H^{-1}$. The presence of $H$ indicates that the Jacobi preconditioner is not scalable.

Moreover, we notice that the presence of the logarithmic term $\log (H / h)$ introduces a relation between the size of the subdomains and the size of the computational grid $\mathscr{T}_{h}$. This generates a propagation of information among subdomains characterized by a finite (rather than infinite) speed of propagation. Note that the ratio $H / h$ measures the maximum number of elements across any subdomain.

\subsubsection{Bramble-Pasciak-Schatz preconditioner}

With the aim of accelerating the speed of propagation of information among subdomains we can devise a mechanism of global coupling among subdomains. As already anticipated, the family of subdomains can be regarded as a coarse grid, say $\mathscr{T}_{H}$, of the original domain. For instance, in Fig. $19.6 \mathscr{T}_{H}$ is made of 9 (macro) elements and 4 internal nodes. It identifies a stiffness matrix of piecewise bilinear elements, say $A_{H}$, of dimension $4 \times 4$ which guarantees a global coupling in $\Omega$. We can now introduce a restriction operator that, for simplicity, we indicate $R_{H}: \Gamma_{h} \rightarrow \Gamma_{H}$. More precisely, this operator transforms a vector of nodal values on the skeleton $\Gamma_{h}$ into a vector of nodal values on the internal vertices of the coarse grid (4 in the case at hand). Its transpose $R_{H}^{T}$ is an extension operator. The matrix $P_{h}^{B P S}$, whose inverse is

$$
\left(P_{h}^{B P S}\right)^{-1}=\sum_{k=1}^{m} R_{e_{k}}^{T} \hat{\Sigma}_{e_{k} e_{k}}^{-1} R_{e_{k}}+R_{H}^{T} A_{H}^{-1} R_{H}
$$

is named Bramble-Pasciak-Schatz preconditioner. The main difference with Jacobi preconditioner $(19.88)$ is due to the presence of the global (coarse-grid) stiffness matrix $A_{H}$ instead of the diagonal vertex matrix $\Sigma_{v v}$. The following results hold:

$$
\begin{gathered}
K_{2}\left(\left(P_{h}^{B P S}\right)^{-1} \Sigma\right) \leq C\left(1+\log \frac{H}{h}\right)^{2} \text { in } 2 \mathrm{D} \\
K_{2}\left(\left(P_{h}^{B P S}\right)^{-1} \Sigma\right) \leq C \frac{H}{h} \text { in } 3 \mathrm{D}
\end{gathered}
$$

Note that the factor $H^{-2}$ does not show up anymore. The number of iterations of the conjugate gradient method with preconditioner $P_{h}^{B P S}$ is now proportional to $\log (H / h)$ in $2 \mathrm{D}$ and to $(\mathrm{H} / \mathrm{h})^{1 / 2}$ in $3 \mathrm{D}$.

\subsubsection{Neumann-Neumann preconditioner}

Although the Bramble-Pasciak-Schatz preconditioner has better properties than Jacobi's, yet in $3 \mathrm{D}$ the condition number of the preconditioned Schur complement still contains a linear dependence on $H / h$. In this respect, a further improvement is achievable using the so-called Neumann-Neumann preconditioner, whose inverse is

$$
\left(P_{h}^{N N}\right)^{-1}=\sum_{i=1}^{M} R_{\Gamma_{i}}^{T} D_{i} \Sigma_{i}^{*} D_{i} R_{\Gamma_{i}}
$$

As before, $R_{\Gamma_{i}}$ denotes the restriction from the nodal values on the whole skeleton $\Gamma$ to those on the local interface $\Gamma_{i}$, whereas $\Sigma_{i}^{*}$ is either $\Sigma_{i}^{-1}$ (should the local inverse exist) or an approximation of $\Sigma_{i}^{-1}$, e.g. the pseudo-inverse $\Sigma_{i}^{+}$of $\Sigma_{i}$. The matrix $D_{i}$ is a diagonal matrix of positive weights $d_{j}>0$, for $j=1, \ldots, n, n$ being the number of nodes on $\Gamma_{i}$. For instance, $d_{j}$ coincides with the inverse of the number of subdomains that share the $j-t h$ node. If we still consider the 4 internal vertices of Fig. 19.6, we will have $d_{j}=1 / 4$, for $j=1, \ldots, 4$.

For the preconditioner $(19.90)$ the following estimate (similar to that of Jacobi preconditioner) holds: there exists a constant $C>0$, indipendent of both $h$ and $H$, such that

$$
K_{2}\left(\left(P_{h}^{N N}\right)^{-1} \Sigma\right) \leq C H^{-2}\left(1+\log \frac{H}{h}\right)^{2}
$$

The last (logarithmic) factor drops out in case the subdomains partition features no cross points.

The presence of $D_{i}$ and $R_{\Gamma_{i}}$ in (19.90) only entails matrix-matrix multiplications. On the other hand, if $\Sigma_{i}^{*}=\Sigma_{i}^{-1}$, applying $\Sigma_{i}^{-1}$ to a given vector can be reconducted to the use of local inverses. As a matter of fact, let $\mathbf{q}$ be a vector whose components are the nodal values on the local interface $\Gamma_{i}$; then

$$
\Sigma_{i}^{-1} \mathbf{q}=[0, I] A_{i}^{-1}[0, I]^{T} \mathbf{q}
$$

In particular, $[0, I]^{T} \mathbf{q}=[0, \mathbf{q}]^{T}$, and the matrix-vector product

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-199.jpg?height=167&width=312&top_left_y=382&top_left_x=298)

corresponds to the solution on $\Omega_{i}$ of the Neumann boundary-value problem:

$$
\begin{cases}-\Delta w_{i}=0 & \text { in } \Omega_{i} \\ \frac{\partial w_{i}}{\partial n}=q & \text { on } \Gamma_{i}\end{cases}
$$

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-199.jpg?height=324&width=724&top_left_y=740&top_left_x=88)

Also in this case a start-up phase is required, consisting in the preparation for the solution of linear systems with local stiffness matrices $A_{i}$. Note that in the case of the model problem (3.13), $A_{i}$ is singular if $\Omega_{i}$ is an internal subdomain, that is if $\partial \Omega_{i} \backslash \partial \Omega=\emptyset$. One of the following strategies should be adopted:

1. compute a (either LU or Cholesky) factorization of $A_{i}+\varepsilon I$, for a given $\varepsilon>0$ sufficiently small; Table 19.2. Condition number of the preconditioned matrix $\left(P_{h}^{N N}\right)^{-1} \Sigma$

\begin{tabular}{lcccc}
\hline$K_{2}\left(\left(P_{h}^{N N}\right)^{-1} \Sigma\right)$ & $H=1 / 2$ & $H=1 / 4$ & $H=1 / 8$ & $H=1 / 16$ \\
\hline$h=1 / 16$ & $2.55$ & $15.20$ & $47.60$ & $-$ \\
$h=1 / 32$ & $3.45$ & $20.67$ & $76.46$ & $194.65$ \\
$h=1 / 64$ & $4.53$ & $26.25$ & $105.38$ & $316.54$ \\
$h=1 / 128$ & $5.79$ & $31.95$ & $134.02$ & $438.02$ \\
\hline
\end{tabular}

2. compute a factorization of $A_{i}+\frac{1}{H^{2}} M_{i}$, where $M_{i}$ is the mass matrix,

$$
\left(M_{i}\right)_{k, j}=\int_{\Omega_{i}} \varphi_{k} \varphi_{j} d \Omega_{i}
$$

3. compute the singular-value decomposition of $A_{i}$.

The matrix $\Sigma_{i}^{*}$ is defined accordingly. In our numerical results we have adopted the third approach.

The convergence history of the preconditioned conjugate gradient method with preconditioner $P_{h}^{N N}$ in the case $h=1 / 32$ is displayed in Fig. 19.7. In Table $19.2$ we report the values of the condition number of $\left(P_{h}^{N N}\right)^{-1} \Sigma$ for several values of $H$.

As already pointed out, the Neumann-Neumann preconditioner of the Schur complement matrix is not scalable. A substantial improvement of $(19.90)$ can be achieved by adding a coarse grid correction mechanism, yielding the following new precondi-

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-200.jpg?height=425&width=513&top_left_y=745&top_left_x=200)

Fig. 19.7. Convergence history for the preconditioned conjugate gradient method with preconditioner $P_{h}^{N N}$ when $h=1 / 32$ tioned Schur complement matrix (see, e.g., [TW05, Sect. 6.2.1])

$$
\left(P_{h}^{B N N}\right)^{-1} \Sigma=P_{0}+\left(I-P_{0}\right)\left(\left(P_{h}^{N N}\right)^{-1} \Sigma\right)\left(I-P_{0}\right)
$$

in which we have used the shorthand notation $P_{0}=\bar{R}_{0}^{T} \Sigma_{0}^{-1} \bar{R}_{0} \Sigma, \Sigma_{0}=\bar{R}_{0} \Sigma \bar{R}_{0}^{T}$, and $\bar{R}_{0}$ denotes restriction from $\Gamma$ onto the coarse level skeleton. The matrix $P_{h}^{B N N}$ is called balanced Neumann-Neumann preconditioner. It can be proven that there exists a constant $C>0$, independent of $h$ and $H$, such that

$$
K_{2}\left(\left(P_{h}^{B N N}\right)^{-1} \Sigma\right) \leq C\left(1+\log \frac{H}{h}\right)^{2}
$$

both in $2 \mathrm{D}$ and 3D. The balanced Neumann-Neumann preconditioner therefore guarantees optimal scalability up to a light logarithmic dependence on $H$ and $h$.

The coarse grid matrix $\Sigma_{0}$ that is a constituent of $\Sigma_{H}$ can be built up using the Algorithm 18.4:

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-201.jpg?height=258&width=720&top_left_y=533&top_left_x=89)

Step a. of this Algorithm is computationally very cheap, whereas step $\mathrm{b}$. requires several (e.g., $\ell$ ) matrix-vector products involving the Schur complement matrix $\Sigma$. Since $\Sigma$ is never built explicitly, this involves the finite element solution of $\ell \times M$ Dirichlet boundary value problems to generate $A_{H}$. Observe moreover that the restriction operator introduced at step a. implicitly defines a coarse space whose functions are piecewise constant on every $\Gamma_{i}$. For this reason the balanced Neumann-Neumann preconditioner is especially convenient when either the finite element grid or the subdomain partition (or both) are unstructured, as in Fig. 19.8). An algorithm that im-

Table 19.3. Condition number of $\left(P_{h}^{B N N}\right)^{-1} \Sigma$ for several values of $H$

\begin{tabular}{lcccc}
\hline$K_{2}\left(\left(P_{h}^{B N N}\right)^{-1} \Sigma\right)$ & $H=1 / 2$ & $H=1 / 4$ & $H=1 / 8$ & $H=1 / 16$ \\
\hline$h=1 / 16$ & $1.67$ & $1.48$ & $1.27$ & $-$ \\
$h=1 / 32$ & $2.17$ & $2.03$ & $1.47$ & $1.29$ \\
$h=1 / 64$ & $2.78$ & $2.76$ & $2.08$ & $1.55$ \\
$h=1 / 128$ & $3.51$ & $3.67$ & $2.81$ & $2.07$ \\
\hline
\end{tabular}


![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-202.jpg?height=288&width=714&top_left_y=110&top_left_x=100)

Fig. 19.8. Example of an unstructured subdomain partition in 8 subdomains for a finite element grid which is either structured (left) or unstructured (right)

plements the BNN preconditioner within a conjugate gradient method to solve the interface problem (19.79) is reported in [TW05, Sect. 6.2.2].

By a comparison of the results obtained using the Neumann-Neumann preconditioner (with and without balancing), the following conclusions can be drawn:

- although featuring a better condition number than $A, \Sigma$ is still ill-conditioned. The use of a suitable preconditioner is therefore mandatory;

- the Neumann-Neumann preconditioner can be satisfactorily used for partitions featuring a moderate number of subdomains;

- the balancing Neumann-Neumann preconditioner is almost optimally scalable and therefore recommandable for partitions with a large number of subdomains.

\subsubsection{FETI (Finite Element Tearing $\&$ Interconnecting) methods}

In this section we will denote by $H_{i}=\operatorname{diam}\left(\Omega_{i}\right), W_{i}=W^{h}\left(\partial \Omega_{i}\right)$ (the space of traces of finite element functions on the boundaries $\partial \Omega_{i}$ ), and by $W=\prod_{i=1}^{M} W_{i}$ the product space of such trace spaces.

At a later stage we will need two further finite element trace spaces, $\widehat{W} \subset W$ a subspace of continuous traces across the skeleton $\Gamma$, and $\widetilde{W}$, a possible intermediate space $\widehat{W} \subset \widetilde{W} \subset W$ that will fulfill a smaller number of continuity constraints.

We will consider the variable coefficient elliptic problem:

$$
\begin{cases}-\operatorname{div}(\rho \nabla u)=f & \text { in } \Omega \\ u=0 & \text { on } \partial \Omega\end{cases}
$$

where $\rho$ is piecewise constant, $\rho=\rho_{i} \in \mathbb{R}^{+}$in $\Omega_{i}$.

Finally, we will denote by $\Omega_{i h}$ the nodes in $\Omega_{i}, \partial \Omega_{i h}$ the nodes on $\partial \Omega_{i}, \partial \Omega_{h}$ the nodes on $\partial \Omega$, and $\Gamma_{h}$ the nodes on $\Gamma$. See Fig. 19.9. 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-203.jpg?height=384&width=534&top_left_y=115&top_left_x=187)

Fig. 19.9. Finite element sets of nodes $\partial \Omega_{h}, \Gamma_{h}, \Omega_{i h}$, and $\partial \Omega_{i h}$

Let us introduce the following scaling counting functions: $\forall x \in \Gamma_{h} \cup \partial \Omega_{h}$

$$
\delta_{i}(x)=\left\{\begin{array}{cl}
1 & x \in \partial \Omega_{i h} \cap\left(\partial \Omega_{h} \backslash \Gamma_{h}\right) \\
\sum_{j \in N_{x}} \rho_{j}^{\gamma}(x) / \rho_{i}^{\gamma}(x) & x \in \partial \Omega_{i h} \cap \Gamma_{h} \\
0 & \text { elsewhere }
\end{array}\right.
$$

where $\gamma \in[1 / 2,+\infty)$ and $N_{x}$ is the set of indices of the subregions having $x$ on their boundary. Then we set

$$
\delta_{i}^{\dagger}(x) \quad(=\text { pseudo inverses })=\left\{\begin{array}{cl}
\delta_{i}^{-1}(x) & \text { if } \delta_{i}(x) \neq 0 \\
0 & \text { if } \delta_{i}(x)=0
\end{array}\right.
$$

Based on the finite element approximation of (19.93), let us consider the local Schur complements (19.80), which are positive semi-definite matrices. In this section we will indicate the interface nodal values on $\partial \Omega_{i}$ as $\mathbf{u}_{i}$, and we set $\mathbf{u}=\left(\mathbf{u}_{1}, \ldots, \mathbf{u}_{M}\right)$, the local load vectors on $\partial \Omega_{i}$ as $\chi_{i}$ and we set $\chi_{\Delta}=\left(\chi_{1}, \ldots, \chi_{M}\right)$. Finally, we set

$$
\Sigma_{\Delta}=\operatorname{diag}\left(\Sigma_{1}, \ldots, \Sigma_{M}\right)=\left[\begin{array}{cccc}
\Sigma_{1} & 0 & \cdots & 0 \\
\vdots & \Sigma_{2} & & \vdots \\
& & \ddots & \\
0 & 0 & \cdots & \Sigma_{M}
\end{array}\right]
$$

a block diagonal matrix. The original FEM problem, when reduced to the interface $\Gamma$, reads: find $\mathbf{u} \in W$ such that

$$
\left\{\begin{aligned}
J(\mathbf{u}) &=\frac{1}{2}\left\langle\Sigma_{\Delta} \mathbf{u}, \mathbf{u}\right\rangle-\left\langle\chi_{\Delta}, \mathbf{u}\right\rangle \rightarrow \min \\
B_{\Gamma} \mathbf{u} &=\mathbf{0}
\end{aligned}\right.
$$

$B_{\Gamma}$ is not unique, so that we should impose continuity when $\mathbf{u}$ belongs to more than one subdomain; $B_{\Gamma}$ is made of $\{0,-1,1\}$, since it enforces continuity constraints at interfaces' nodes. Here, we are using the same notation $W$ to denote the finite element space trace and that of their nodal values at points of $\Gamma_{h}$.

In $2 \mathrm{D}$, there is a little choice on how to write the constraint of continuity at a point sitting on an edge, there are many options for a vertex point. For the edge node we only need to choose the sign, whereas for a vertex node, e.g. one common to 4 subdomains, a minimum set of three constraints can be chosen in many different ways to assure continuity at the node in question. See, e.g., Fig. 19.10.
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-204.jpg?height=180&width=448&top_left_y=380&top_left_x=232)

Fig. 19.10. Continuity constraints enforced by 3 (non-redundant) conditions on the left, by 6 (redundant) conditions on the right

Problem (19.96) admits a unique solution iff $\operatorname{Ker}\left\{\Sigma_{\Delta}\right\} \cap \operatorname{Ker}\left\{B_{\Gamma}\right\}=0$, that is if $\Sigma_{\Delta}$ is invertible on $\operatorname{Ker}\left(B_{\Gamma}\right)$.

We can reformulate (19.96) using Lagrange multipliers: find $(\mathbf{u}, \boldsymbol{\lambda}) \in W \times U$ such that

$$
\begin{cases}\Sigma_{\Delta} \mathbf{u}+B_{\Gamma}^{T} \boldsymbol{\lambda} & =\boldsymbol{\chi}_{\Delta} \\ B_{\Gamma} \mathbf{u} & =\mathbf{0}\end{cases}
$$

Because of the inf-sup (LBB) condition (see Chap. 17), the component $\boldsymbol{\lambda}$ of the solution to $(19.97)$ is unique up to an additive vector of $\operatorname{Ker}\left(B_{\Gamma}^{T}\right)$, so we choose $U=$ $\operatorname{range}\left(B_{\Gamma}\right)$

Let $R=\operatorname{diag}\left(R^{(1)}, \ldots, R^{(M)}\right)$ be made of null-space elements of $\Sigma_{\Delta} .\left(\right.$ E.g. $R^{(i)}$ corresponds to the rigid body motions of $\Omega_{i}$, in case of linear elasticity operator.)

$R$ is a full column rank matrix. The solution of the first equation of $(19.97)$ exists iff $\chi_{\Delta}-B_{\Gamma}^{T} \boldsymbol{\lambda} \in \operatorname{range}\left(\Sigma_{\Delta}\right)$, a limitation that will be resolved by introducing a suitable projection operator $P$. Then,

$$
\mathbf{u}=\Sigma_{\Delta}^{\dagger}\left(\chi_{\Delta}-B_{\Gamma}^{T} \boldsymbol{\lambda}\right)+R \alpha \quad \text { if } \chi_{\Delta}-B_{\Gamma}^{T} \boldsymbol{\lambda} \perp \operatorname{Ker}\left(\Sigma_{\Delta}\right)
$$

where $\alpha$ is an arbitrary vector and $\Sigma_{\Delta}^{\dagger}$ is a pseudoinverse of $\Sigma_{\Delta}$. (Even though there are several pseudo-inverses of a given matrix, the following algorithm will be invariant to the specific choice.) It is convenient to choose a symmetric $\Sigma_{\Delta}^{\dagger}, \mathrm{e} . \mathrm{g}$. that of MoorePenrose, see [QSS07]. Substituting $\mathbf{u}$ into the second equation of (19.97) yields

$$
B_{\Gamma} \Sigma_{\Delta}^{\dagger} B_{\Gamma}^{T} \boldsymbol{\lambda}=B_{\Gamma} \Sigma_{\Delta}^{\dagger} \chi_{\Delta}+B_{\Gamma} R \boldsymbol{\alpha}
$$

Let us set $F=B_{\Gamma} \Sigma_{\Delta}^{\dagger} B_{\Gamma}^{T}$ and $\mathbf{d}=B_{\Gamma} \Sigma_{\Delta}^{\dagger} \chi_{\Delta}$. Then choose $P^{T}$ to be a suitable projection matrix, e.g. $P^{T}=I-G\left(G^{T} G\right)^{-1} G^{T}$, with $G=B_{\Gamma} R$. Then:

$$
\begin{cases}P^{T} F \boldsymbol{\lambda} & =P^{T} \mathbf{d} \\ G^{T} \boldsymbol{\lambda} & =\mathbf{e}\left(=R^{T} \chi_{\Delta}\right)\end{cases}
$$

More in general, one can introduce a s.p.d. matrix $Q$, and set

$$
P^{T}=I-G\left(G^{T} Q G\right)^{-1} G^{T} Q
$$

The operator $P^{T}$ is the projection from $U$ onto the space of Lagrange multipliers that are Q-orthogonal to $\operatorname{range}(G)$, while $P=I-Q G\left(G^{T} Q G\right)^{-1} G^{T}$ is a projection from $U$ onto $\operatorname{Ker}\left(G^{T}\right)$ (it is indeed the orthogonal projection with respect to the $Q^{-1}$-inner $\left.\operatorname{product}(\boldsymbol{\lambda}, \boldsymbol{\mu})_{Q^{-1}}=\left(\boldsymbol{\lambda}, \boldsymbol{Q}^{-1} \boldsymbol{\mu}\right)\right)$

Upon multiplication of (19.98) by $H=\left(G^{T} Q G\right)^{-1} G^{T} Q$ we find

$$
\boldsymbol{\alpha}=H(\mathbf{d}-F \boldsymbol{\lambda})
$$

which fully determines the primal variables in terms of $\boldsymbol{\lambda}$.

If the differential operator has constant coefficients, choosing $Q=I$ suffices. In case of jumps in the coefficients, $Q$ is typically chosen as a scaling diagonal matrix and can be regarded as a scaling from the left of matrix $B_{\Gamma}$ by $Q^{1 / 2}$.

The original one-level FETI method is a CG method in the space $V$ applied to

$$
P^{T} F \boldsymbol{\lambda}=P^{T} \mathbf{d}, \quad \boldsymbol{\lambda} \in \boldsymbol{\lambda}_{0}+V
$$

with an initial $\boldsymbol{\lambda}_{0}$ such that $G^{T} \boldsymbol{\lambda}_{0}=\mathbf{e}$. Here

$$
V=\left\{\boldsymbol{\lambda} \in U:\langle\boldsymbol{\lambda}, B \mathbf{z}\rangle=0, \mathbf{z}=\operatorname{Ker}\left(\Sigma_{\Delta}\right)\right\}
$$

is the so-called space of admissible increments, $\operatorname{Ker}\left(G^{T}\right)=\operatorname{range}(P)$ and

$$
V^{\prime}=\left\{\boldsymbol{\mu} \in U:\langle\boldsymbol{\mu}, B \mathbf{z}\rangle_{Q}=0, \mathbf{z} \in \operatorname{Ker}\left(\Sigma_{\Delta}\right)\right\}=\operatorname{range}\left(P^{T}\right)
$$

The above simplest version of FETI with no preconditioner (or only a diagonal preconditioner) in the subdomain is scalable with the number of subdomains, but the condition number grows polynomially with the number of elements per subdomain. The original, most basic FETI preconditioner is

$$
P_{h}^{-1}=B_{\Gamma} \Sigma_{\Delta} B_{\Gamma}^{T}=\sum_{i=1}^{M} B^{(i)} \Sigma_{i} B^{(i)^{T}}
$$

It is called a Dirichlet preconditioner since its application to a given vector involves the solution of $M$ independent Dirichlet problems, one in every subdomain. The coarse space in FETI consists of the nullspace on each substructure. To keep the search directions of the resulting preconditioned $\mathrm{CG}$ method in the space $V$, the application of $P_{h}^{-1}$ is followed by an application of the projection $P$. Thus, the so-called Dirichlet variant of the FETI method is the CG algorithm applied to the modified equation

$$
P P_{h}^{-1} P^{T} F \boldsymbol{\lambda}=P P_{h}^{-1} P^{T} \mathbf{d}, \quad \boldsymbol{\lambda} \in \boldsymbol{\lambda}_{0}+V
$$

Since, for $\boldsymbol{\lambda} \in V, P P_{h}^{-1} P^{T} F \boldsymbol{\lambda}=P P_{h}^{-1} P^{T} P^{T} F P \boldsymbol{\lambda}$, the matrix on the left of $(19.102)$ can be regarded as the product of two symmetric matrices. In case $B_{\Gamma}$ has full row rank, i.e. the constraints are linearly independent and there are no redundant Lagrange multipliers, a better preconditioner can be defined as follows

$$
\widehat{P}_{h}^{-1}=\left(B_{\Gamma} D^{-1} B_{\Gamma}^{T}\right)^{-1} B_{\Gamma} D^{-1} \Sigma_{\Delta} D^{-1} B_{\Gamma}^{T}\left(B_{\Gamma} D^{-1} B_{\Gamma}^{T}\right)^{-1}
$$

where $D$ is a block diagonal matrix $D=\operatorname{diag}\left(D^{(1)}, \ldots, D^{(M)}\right)$ and each block $D^{(i)}$ is a diagonal matrix whose elements are $\delta_{i}^{\dagger}(x)$ (see (19.95)) corresponding to the point $x$ of $\partial \Omega_{i, h}$

Since $B_{\Gamma} D^{-1} B_{\Gamma}^{T}$ is block-diagonal, its inverse can be easily computed by inverting small blocks whose size is $n_{x}$, the number of Lagrange multipliers used to enforce continuity at point $x$.

The matrix $D$, that operates on elements of the product space $W$, can be regarded as a scaling from the right of $B_{\Gamma}$ by $D^{-1 / 2}$. With this choice

$$
K_{2}\left(P \widehat{P}_{h}^{-1} P^{T} F\right) \leq C(1+\log (H / h))^{2}
$$

where $K_{2}(\cdot)$ is the spectral condition number and $C$ is a constant independent of $h, H$, $\gamma$ and the values of the $\rho_{i}$.

\subsubsection{FETI-DP (Dual Primal FETI) methods}

The FETI-DP method is a domain decomposition method introduced in [FLT $\left.^{+} 01\right]$ that enforces equality of the solution at subdomains interfaces by Lagrange multipliers except at subdomains corners, which remain primal variables. The first mathematical analysis of the method was provided by Mandel and Tezaur [MT01]. The method was further improved by enforcing the equality of averages across the edges or faces on subdomain interfaces [FLP00], [KWD02]. This is important for parallel scalability.

Let us consider a $2 \mathrm{D}$ case for simplicity. As anticipated at the beginning of Sect. 19.5.4, this idea is implemented by introducing an additional space $\widetilde{W}$ such that $\widehat{W} \subset \widetilde{W} \subset W$ for which we have continuity of the primal variables at subdomain vertices, and also common values of the averages over all edges of the interface. However, for simplicity we will confine ourselves to the case of primal variables associated to subdomain vertices only. This space can be written as the sum of two subspaces

$$
\widetilde{W}=\widehat{W}_{\Pi} \oplus \widetilde{W}_{\Delta}
$$

where $\widehat{W}_{\Pi} \subset \widehat{W}$ is the space of continuous interface functions that vanish at all nodal points of $\Gamma_{h}$ except at the subdomain vertices. $\widehat{W}_{\Pi}$ is given in terms of the vertex variables and the averages of the values over the individual edges of the set of interface nodes $\Gamma_{h} . \widetilde{W}_{\Delta}$ is the direct sum of local subspaces $\widetilde{W}_{\Delta, i}$ :

$$
\widetilde{W}_{\Delta}=\prod_{i=1}^{M} \widetilde{W}_{\Delta, i}
$$

where $\widetilde{W}_{\Delta, i} \subset W_{i}$ consists of local functions on $\partial \Omega_{i}$ that vanish at the vertces of $\Omega_{i}$ and have zero average on each individual edge.

According to this space splitting, the continuous degrees of freedom associated with the subdomain vertices and with the subspace $\widehat{W}_{\Pi}$ are called primal $(\Pi)$, while those (that are potentially discontinuous across $\Gamma$ ) that are associated with the subspaces $\widetilde{W}_{\Delta, i}$ and with the interior of the subdomain edges are called $\operatorname{dual}(\Delta)$.

The subspace $\widehat{W}_{\Pi}$, together with the interior subspace, defines the subsystem which is fully assembled, factored, and stored in each iteration step.

At this stage, all unknowns of the first subspace as well as the interior variables are eliminated to obtain a new Schur complement $\widetilde{\Sigma}_{\Delta}$. More precisely, we proceed as follows.

Let $\widetilde{A}$ denote the stiffness matrix obtained by restricting $\operatorname{diag}\left(A_{1}, \ldots, A_{M}\right)(\mathrm{see}(19.76))$ from $\prod_{i=1}^{M} W^{h}\left(\Omega_{i}\right)$ to $\widetilde{W}^{h}(\Omega)$ (these spaces now refer to subdomains, not to their boundaries). Then $A$ is no longer block diagonal because of the coupling that now exists between subdomains sharing a common vertex. According to the previous space decomposition, $\widetilde{A}$ can be split as follows

$$
\widetilde{A}=\left[\begin{array}{ccc}
A_{I I} & A_{I \Pi} & A_{I \Delta} \\
A_{I \Pi}^{T} & A_{\Pi \Pi} & A_{\Pi \Delta} \\
A_{I \Delta}^{T} & A_{\Pi \Delta}^{T} & A_{\Delta \Delta}
\end{array}\right]
$$

Here the subscript $I$ refers to the internal degrees of freedom of the subdomains, $\Pi$ to those associated to the subdomains vertices, and $\Delta$ to those of the interior of the subdomains edges, see Fig. 19.11, right. The matrices $A_{I I}$ and $A_{\Delta \Delta}$ are block diagonal (one block per subdomain). Any non-zero entry of $A_{I \Delta}$ represents a coupling between degrees of freedom associated with the same subdomain. Upon eliminating the variables of the $I$ and $\Pi$ sets, a Schur complement associated with the variables of the $\Delta$ sets (interior and edges) is obtained as follows

$$
\tilde{\Sigma}=A_{\Delta \Delta}-\left[A_{I \Delta}^{T} A_{\Pi \Delta}^{T}\right]\left[\begin{array}{cc}
A_{I I} & A_{I \Pi} \\
A_{I \Pi}^{T} & A_{\Pi \Pi}
\end{array}\right]^{-1}\left[\begin{array}{c}
A_{I \Delta} \\
A_{\Pi \Delta}
\end{array}\right]
$$

Correspondingly we obtain a reduced right hand side $\tilde{\chi}_{\Delta}$. By indicating with $\mathbf{u}_{\Delta} \in \widetilde{W}_{\Delta}$ the vector of degrees of freedom associated with the edges, similarly to what done in (19.96) for FETI, the finite element problem can be reformulated as a minimization problem with constraints given by the requirement of continuity across all of $\Gamma:$ find $\mathbf{u}_{\Delta} \in \widetilde{W}_{\Delta}$

$$
\left\{\begin{aligned}
J\left(\mathbf{u}_{\Delta}\right) &=\frac{1}{2}\left\langle\widetilde{\Sigma} \mathbf{u}_{\Delta}, \mathbf{u}_{\Delta}\right\rangle-\left\langle\tilde{\chi}_{\Delta}, \mathbf{u}_{\Delta}\right\rangle \rightarrow \min \\
B_{\Delta} \mathbf{u}_{\Delta} &=\mathbf{0}
\end{aligned}\right.
$$


![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-208.jpg?height=240&width=576&top_left_y=114&top_left_x=164)

Fig. 19.11. Degrees of freedom of the space $W$ for one-level FETI (left) and those of the space $W$ for one-level FETI-DP (right) in the case of primal vertices only

The matrix $B_{\Delta}$ is made of $\{0,-1,1\}$ as it was for $B_{\Gamma}$. Note however that this time the constraints associated with the vertex nodes are dropped since they are assigned to the primal set. Note also that since all the constraints refer to edge points, no distinction needs to be made between redundant and non-redundant constraints and Lagrange multipliers.

A saddle point formulation of (19.108), similar to (19.97), can be obtained by introducing a set of Lagrange multipliers $\boldsymbol{\lambda} \in V=\operatorname{range}\left(B_{\Delta}\right)$. Indeed, since $\widetilde{A}$ is s.p.d., so is $\widetilde{\Sigma}:$ by eliminating the subvectors $\mathbf{u}_{\Delta}$ we obtain the reduced system

$$
F_{\Delta} \boldsymbol{\lambda}=\mathbf{d}_{\Delta}
$$

where $F_{\Delta}=B_{\Delta} \widetilde{\Sigma}^{-1} B_{\Delta}^{T}$ and $\mathbf{d}_{\Delta}=B_{\Delta} \widetilde{\Sigma}^{-1} \tilde{\chi}_{\Delta}$

Note that once $\boldsymbol{\lambda}$ is found, $\mathbf{u}_{\Delta}=\widetilde{\Sigma}^{-1}\left(\tilde{\chi}_{\Delta}-B_{\Delta}^{T} \boldsymbol{\lambda}\right) \in \widetilde{W}_{\Delta}$, while the interior variables $\mathbf{u}_{I}$ and the vertex variables $\mathbf{u}_{\Pi}$ are obtained by back-solving the system associated with $\widetilde{A}$

A preconditioner for $F$ is introduced as done in (19.103) for FETI (in case of non-redundant Lagrange multipliers)

$$
P_{\Delta}^{-1}=\left(B_{\Delta} D_{\Delta}^{-1} B_{\Delta}^{T}\right)^{-1} B_{\Delta} D_{\Delta}^{-1} S_{\Delta \Delta} D_{\Delta}^{-1} B_{\Delta}^{T}\left(B_{\Delta} D_{\Delta}^{-1} B_{\Delta}^{T}\right)^{-1}
$$

Here $D_{\Delta}$ is a block diagonal scaling matrix with blocks $D_{\Delta}^{(i)}$ : each of their diagonal elements corresponds to a Lagrange multiplier that enforces continuity between the nodal values of some $w_{i} \in W_{i}$ and $w_{j} \in W_{j}$ at some point $x \in \Gamma_{h}$ and it is given by $\delta_{j}^{\dagger}(x)$. Moreover, $\Sigma_{\Delta \Delta}=\operatorname{diag}\left(\Sigma_{1, \Delta \Delta}, \ldots, \Sigma_{M, \Delta \Delta}\right)$ with $\Sigma_{i, \Delta \Delta}$ being the restriction of the local Schur complement $\Sigma_{i}$ to $\widetilde{W}_{\Delta, i} \subset W_{i}$

When using the conjugate gradient method for the preconditioned system

$$
P_{\Delta}^{-1} F_{\Delta} \boldsymbol{\lambda}=P_{\Delta}^{-1} \mathbf{d}_{\Delta}
$$

in contrast with one level FETI methods we can use an arbitrary initial guess $\boldsymbol{\lambda}^{0}$.

For an efficient implementation of this algorithm see [TW05, Sect. 6.4.1]. Also in this case we have a condition number that scales polylogarithmically, that is

$$
K_{2}\left(P_{\Delta}^{-1} F_{\Delta}\right) \leq C(1+\log (H / h))^{2}
$$

where $C$ is independent of $h, H, \gamma$ and the values of the $\rho_{i}$. For a comprehensive presentation and analysis, see [KWD02] and [TW05].

For a conclusive comparative remark between FETI and FETI-DP methods, by following [TW05] we can note that FETI-DP algorithms do not require the characterization of the kernels of local Neumann problems (as required by one-level methods), because the enforcement of the additional constraints in each iteration always makes the local problems nonsingular and at the same time provides an underlying coarse global problem. FETI-DP methods do not require the introduction of a scaling matrix $Q$, which enters in the construction of a coarse solver for one-level FETI algorithms.

Finally, it is worth noticing that one-level FETI methods are projected conjugate gradient algorithms that cannot start from an arbitrary initial guess. In contrast, FETI-DP methods are standard preconditioned conjugate algorithms and can therefore employ an arbitrary initial guess $\boldsymbol{\lambda}^{0}$.

\subsubsection{BDDC (Balancing Domain Decomposition with Constraints) methods}

This method was introduced by Dohrmann [Doh03] as a simpler primal alternative to the FETI-DP domain decomposition method. The name BDDC was coined by Mandel and Dohrmann because it can be understood as further development of the balancing domain decomposition method [Man93] with the coarse, global component of a BDDC algorithm expressed interms of a set of primal constraints.

In contrast to the original Neumann-Neumann and one-levet FETI methods, FETIDP and BDDC algorithms do not require the solution of any singular linear systems of equations (those associated with a pure Neumann problem). In fact, any given choice of the primal set of variables determines a FETI-DP method and an associated BDDC method. This pair defines a duality, and features the same spectrum of eigenvalues (up to the eigenvalues 0 and 1 ) (see [LW06]). The choice of the primal constraints is of course a crucial question in order to obtain an efficient FETI-DP or BDDC algorithm.

BDDC is used as a preconditioner for the conjugate gradient method. A specific version of BDDC is characterized by the choice of coarse degrees of freedom, which can be values at the corners of the subdomains, or averages over the edges of the interface between the subdomains. One application of the BDDC preconditioner then combines the solution of local problems on each subdomain with the solution of a global coarse problem with the coarse degrees of freedom as the unknowns. The local problems on different subdomains are completely independent of each other, so the method is suitable for parallel computing. A BDDC preconditioner reads

$$
P_{B D D C}^{-1}=\widetilde{R}_{D \Gamma}^{T} \widetilde{\Sigma}^{-1} \widetilde{R}_{D \Gamma}
$$

where $\widetilde{R}_{\Gamma}: \widehat{W} \rightarrow \widetilde{W}$ is a restriction matrix, $\widetilde{R}_{D \Gamma}$ is a scaled variant of $\widetilde{R}_{\Gamma}$ with scale factor $\delta_{i}^{\top}$ (featuring the same sparsity pattern of $R_{\Gamma}$ ). This scaling is chosen in such a way that $\widetilde{R}_{\Gamma} \widetilde{R}_{D \Gamma}^{T}$ is a projection (then it coincides with its square).

Theoretical analysis of BDDC preconditioner (and its spectral analogy with FETIDP preconditioner) was first provided in [MDT05] and later in [LW06] and [BS07]. 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-210.jpg?height=228&width=576&top_left_y=103&top_left_x=170)

Fig. 19.12. Two examples for which the Schwarz method in its classical form applies

\subsection{Schwarz iterative methods}

Schwarz method, in its original form described in Sect. 19.1.1, was proposed by H. Schwarz [Sch69] as an iterative scheme to prove existence of solutions to elliptic equations set in domains whose shape inhibits a direct application of Fourier series. Two elementary examples are displayed in Fig. 19.12. This method is still used in some quarters as solution method for elliptic equations in arbitrarily shaped domains. However, nowadays it is mostly used in a somehow different version, that of DD preconditioner of conjugate gradient (or, more generally, Krylov) iterations for the solution of algebraic systems arising from finite element (or other kind of) discretizations of boundary-value problems.

As seen in Sect. 19.1.1, the distinctive feature of Schwarz method is that it is based on an overlapping subdivision of the original domain. Let us still denote $\left\{\Omega_{m}\right\}$ these subdomains.

To start with, in the following subsection we will show how the Schwarz method can be formulated as an iterative algorithm to solve the algebraic system associated with the finite element discretization of problem (19.1).

\subsubsection{Algebraic form of Schwarz method for finite element discretizations}

Consider as usual a finite element triangulation $\mathscr{T}_{h}$ of the domain $\Omega$. Then assume that $\Omega$ is decomposed in two overlapping subdomains, $\Omega_{1}$ and $\Omega_{2}$, as shown in Fig. $19.1$ (left).

Denote with $N_{h}$ the total number of nodes of the triangulation that are internal to $\Omega$ (i.e., they don't sit on its boundary), and with $N_{1}$ and $N_{2}$, respectively, those internal to $\Omega_{1}$ and $\Omega_{2}$, as done in Sect. 19.3. Note that $N_{h} \leq N_{1}+N_{2}$ and that equality holds only if the overlap reduces to a single layer of elements. Indeed, if we denote with $I=\left\{1, \ldots, N_{h}\right\}$ the set of indices of the nodes of $\Omega$, and with $I_{1}$ and $I_{2}$ those associated with the internal nodes of $\Omega_{1}$ and $\Omega_{2}$, respectively, one has $I=I_{1} \cup I_{2}$, while $I_{1} \cap I_{2} \neq \emptyset$ unless the overlap consists of a single layer of elements.

Let us order the nodes in such a way that the first block corresponds to those in $\Omega_{1} \backslash \Omega_{2}$, the second to those in $\Omega_{1} \cap \Omega_{2}$, and the third to those in $\Omega_{2} \backslash \Omega_{1}$. The stiffness matrix $A$ of the finite element discretization contains two submatrices, $A_{1}$ and $A_{2}$, corresponding to the local stiffness matrices in $\Omega_{1}$ e $\Omega_{2}$, respectively (see Fig. 19.13). They are related to $A$ as follows

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-211.jpg?height=235&width=285&top_left_y=204&top_left_x=315)

Fig. 19.13. The submatrices $A_{1}$ and $A_{2}$ of the stiffness matrix $A$

$$
A_{1}=R_{1} A R_{1}^{T} \in \mathbb{R}^{N_{1} \times N_{1}} \quad \text { and } \quad A_{2}=R_{2} A R_{2}^{T} \in \mathbb{R}^{N_{2} \times N_{2}}
$$

being $R_{i}$ and $R_{i}^{T}$, for $i=1,2$, the restriction and prolongation operators, respectively. The matrix representation of the latter is

$$
R_{1}^{T}=\left[\begin{array}{ccc}
1 & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & 1 \\
0 &
\end{array}\right] \in \mathbb{R}^{N_{h} \times N_{1}}, \quad R_{2}^{T}=\left[\begin{array}{ccc} 
& 0 & \\
1 & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & 1
\end{array}\right] \in \mathbb{R}^{N_{h} \times N_{2}}
$$

If $\mathbf{v}$ is a vector of $\mathbb{R}^{N_{h}}$, then $R_{1} \mathbf{v}$ is a vector of $\mathbb{R}^{N_{1}}$ whose components coincide with the first $N_{1}$ components of $\mathbf{v}$. Should $\mathbf{v}$ instead be a vector of $\mathbb{R}^{N_{1}}$, then $R_{1}^{T} \mathbf{v}$ would be a vector of dimension $N_{h}$ whose last $N_{h}-N_{1}$ components are all zero.

By using these definitions, an iteration of the multiplicative Schwarz method applied to system $A \mathbf{u}=\mathbf{f}$ can be expressed as follows:

$$
\begin{gathered}
\mathbf{u}^{(k+1 / 2)}=\mathbf{u}^{(k)}+R_{1}^{T} A_{1}^{-1} R_{1}\left(\mathbf{f}-A \mathbf{u}^{(k)}\right) \\
\mathbf{u}^{(k+1)}=\mathbf{u}^{(k+1 / 2)}+R_{2}^{T} A_{2}^{-1} R_{2}\left(\mathbf{f}-A \mathbf{u}^{(k+1 / 2)}\right)
\end{gathered}
$$

Equivalently, by setting

$$
P_{i}=R_{i}^{T} A_{i}^{-1} R_{i} A, \quad i=1,2,
$$

we have

$$
\begin{gathered}
\mathbf{u}^{(k+1 / 2)}=\left(I-P_{1}\right) \mathbf{u}^{(k)}+P_{1} \mathbf{u} \\
\mathbf{u}^{(k+1)}=\left(I-P_{2}\right) \mathbf{u}^{(k+1 / 2)}+P_{2} \mathbf{u}=\left(I-P_{2}\right)\left(I-P_{1}\right) \mathbf{u}^{(k)}+\left(P_{1}+P_{2}-P_{2} P_{1}\right) \mathbf{u}
\end{gathered}
$$

Similarly, an iteration of the additive Schwarz method reads

$$
\mathbf{u}^{(k+1)}=\mathbf{u}^{(k)}+\left(R_{1}^{T} A_{1}^{-1} R_{1}+R_{2}^{T} A_{2}^{-1} R_{2}\right)\left(\mathbf{f}-A \mathbf{u}^{(k)}\right)
$$

that is

$$
\mathbf{u}^{(k+1)}=\left(I-P_{1}-P_{2}\right) \mathbf{u}^{(k)}+\left(P_{1}+P_{2}\right) \mathbf{u} .
$$

Introducing the matrices

$$
Q_{i}=R_{i}^{T} A_{i}^{-1} R_{i}=P_{i} A^{-1}, i=1,2
$$

from (19.113) and (19.114) we derive the following recursive formula for the multiplicative Schwarz method

$$
\begin{aligned}
\mathbf{u}^{(k+1)} &=\mathbf{u}^{(k)}+Q_{1}\left(\mathbf{f}-A \mathbf{u}^{(k)}\right)+Q_{2}\left[\mathbf{f}-A\left(\mathbf{u}^{(k)}+Q_{1}\left(\mathbf{f}-A \mathbf{u}^{(k)}\right)\right)\right] \\
&=\mathbf{u}^{(k)}+\left(Q_{1}+Q_{2}-Q_{2} A Q_{1}\right)\left(\mathbf{f}-A \mathbf{u}^{(k)}\right)
\end{aligned}
$$

whereas for the additive Schwarz method we obtain from (19.116) that

$$
\mathbf{u}^{(k+1)}=\mathbf{u}^{(k)}+\left(Q_{1}+Q_{2}\right)\left(\mathbf{f}-A \mathbf{u}^{(k)}\right)
$$

This last formula can easily be extended to the case of a decomposition of $\Omega$ into $M \geq 2$ overlapping subdomains $\left\{\Omega_{i}\right\}$ (see Fig. 19.14 for an example). In this case we have

$$
\mathbf{u}^{(k+1)}=\mathbf{u}^{(k)}+\left(\sum_{i=1}^{M} Q_{i}\right)\left(\mathbf{f}-A \mathbf{u}^{(k)}\right)
$$

\subsubsection{Schwarz preconditioners}

Denoting with

$$
P_{a s}=\left(\sum_{i=1}^{M} Q_{i}\right)^{-1}
$$

from (19.119) it follows that an iteration of the additive Schwarz method corresponds to an iteration of the preconditioned Richardson method applied to the solution of the linear system $A \mathbf{u}=\mathbf{f}$ using $P_{a s}$ as preconditioner (see Sect. 7.2.1). For this reason the matrix $P_{a s}$ is named additive Schwarz preconditioner.

In case of disjoint subdomains (no overlap), $P_{a s}$ coincides with the block Jacobi preconditioner

$$
P_{J}=\left[\begin{array}{ccc}
A_{1} & & 0 \\
& \ddots & \\
0 & & A_{M}
\end{array}\right], \quad P_{J}^{-1}=\left[\begin{array}{ccc}
A_{1}^{-1} & & 0 \\
& \ddots & \\
0 & & A_{M}^{-1}
\end{array}\right]
$$

in which we have removed the off-diagonal blocks of $A$. Equivalently, one iteration of the additive Schwarz method corresponds to an iteration by the Richardson method on the preconditioned linear system $Q_{a} \mathbf{u}=\mathbf{g}_{a}$, with $\mathbf{g}_{a}=P_{a s}^{-1} \mathbf{f}$, and the preconditioned matrix $Q_{a}$ is

$$
Q_{a}=P_{a s}^{-1} A=\sum_{i=1}^{M} P_{i}
$$

By proceeding similarly, using the multiplicative Schwarz method would yield the following preconditioned matrix

$$
Q_{M}=P_{m s}^{-1} A=I-\left(I-P_{M}\right) \ldots\left(I-P_{1}\right)
$$

Lemma 19.3. Matrices $P_{i}$ defined in (19.115) are symmetric and non-negative

Proof. For $i=1,2$, we have

$$
\begin{aligned}
\left(P_{i} \mathbf{w}, \mathbf{v}\right)_{A} &=\left(A P_{i} \mathbf{w}, \mathbf{v}\right)=\left(R_{i}^{T} A_{i}^{-1} R_{i} A \mathbf{w}, A \mathbf{v}\right)=\left(A \mathbf{w}, R_{i}^{T} A_{i}^{-1} R_{i} A \mathbf{v}\right) \\
&=\left(\mathbf{w}, P_{i} \mathbf{v}\right)_{A} \quad \forall \mathbf{v}, \mathbf{w} \in \mathbb{R}^{N_{h}}
\end{aligned}
$$

Moreover, $\forall \mathbf{v} \in \mathbb{R}^{N_{h}}$

$$
\left(P_{i} \mathbf{v}, \mathbf{v}\right)_{A}=\left(A P_{i} \mathbf{v}, \mathbf{v}\right)=\left(R_{i}^{T} A_{i}^{-1} R_{i} A \mathbf{v}, A \mathbf{v}\right)=\left(A_{i}^{-1} R_{i} A \mathbf{v}, R_{i} A \mathbf{v}\right) \geq 0
$$

Lemma 19.4. The preconditioned matrix $Q_{a}$ of the additive Schwarz method is symmetric and positive definite w.r.t the scalar product induced by $A$.

Proof. Let us first prove the symmetry: for all $\mathbf{u}, \mathbf{v} \in \mathbb{R}^{N_{h}}$, since $A$ and $P_{i}$ are both symmetric, we obtain

$$
\begin{aligned}
\left(Q_{a} \mathbf{u}, \mathbf{v}\right)_{A} &=\left(A Q_{a} \mathbf{u}, \mathbf{v}\right)=\left(Q_{a} \mathbf{u}, A \mathbf{v}\right)=\sum_{i}\left(P_{i} \mathbf{u}, A \mathbf{v}\right) \\
&=\sum_{i}\left(P_{i} \mathbf{u}, \mathbf{v}\right)_{A}=\sum_{i}\left(\mathbf{u}, P_{i} \mathbf{v}\right)_{A}=\left(\mathbf{u}, Q_{a} \mathbf{v}\right)_{A}
\end{aligned}
$$

Concerning the positivity, choosing in the former identities $\mathbf{u}=\mathbf{v}$, we obtain

$$
\left(Q_{a} \mathbf{v}, \mathbf{v}\right)_{A}=\sum_{i}\left(P_{i} \mathbf{v}, \mathbf{v}\right)_{A}=\sum_{i}\left(R_{i}^{T} A_{i}^{-1} R_{i} A \mathbf{v}, A \mathbf{v}\right)=\sum_{i}\left(A_{i}^{-1} \mathbf{q}_{i}, \mathbf{q}_{i}\right) \geq 0
$$

having set $\mathbf{q}_{i}=R_{i} A \mathbf{v}$. It follows that $\left(Q_{a} \mathbf{v}, \mathbf{v}\right)_{A}=0$ iff $\mathbf{q}_{i}=\mathbf{0}$ for every $i$, that is iff $A \mathbf{v}=\mathbf{0}$. Since $A$ is positive definite, this holds $\mathbf{i f f} \mathbf{v}=\mathbf{0}$. Owing to the previous properties we can deduce that a more efficient iterative method can be generated by replacing the preconditioned Richardson iterations with the preconditioned conjugate gradient iterations, yet using the same additive Schwarz preconditioner $P_{a s}$. Unfortunately, this preconditioner is not scalable. In fact, the condition number of the preconditioned matrix $Q_{a}$ can only be bounded as

$$
K_{2}\left(P_{a s}^{-1} A\right) \leq C \frac{1}{\delta H}
$$

being $C$ a constant independent of $h, H$ and $\delta ;$ here $\delta$ is a characteristic linear measure of the overlapping regions and, as usual, $H=\max _{i=1, \ldots, M}\left\{\operatorname{diam}\left(\Omega_{i}\right)\right\}$. This is due to the fact that the exchange of information only occurs among neighbooring subdomains, as the application of $\left(P_{a s}\right)^{-1}$ involves only local solvers. This limitation can be overcome by introducing, also in the current context, a global coarse solver defined on the whole domain $\Omega$ and apt at guaranteing a global communication among all of the subdomains. This leads to devise two-level domain decomposition strategies, see Sect. 19.6.3

Let us address some algorithmic aspects. Let us subdivide the domain $\Omega$ in $M$ subdomains $\left\{\Omega_{i}\right\}_{i=1}^{M}$ such that $\cup_{i=1}^{M} \bar{\Omega}_{i}=\bar{\Omega}$. Neighbooring subdomains share an overlapping region of size at least equal to $\delta=\xi h$, for a suitable $\xi \in \mathbb{N}$. In particular, $\xi=1$ corresponds to the case of minimum overlap, that is the overlapping strip reduces to a single layer of finite elements. The following algorithm can be used.

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-214.jpg?height=196&width=717&top_left_y=663&top_left_x=100)

In Fig. $19.14$ a rectangular two-dimensional domain is subdivided into 9 disjoint subdomains $\hat{\Omega}_{i}$ (on the left); also shown is one of the extended (overlapping) subdomains (on the right).

To apply the Schwarz preconditioner (19.120) we can proceed as indicated in Algorithm 18.5. We recall that $N_{i}$ is the number of internal nodes of $\Omega_{i}, R_{i}^{T}$ and $R_{i}$ are the prolongation and restriction matrices, respectively, introduced in (19.112) and $A_{i}$ are the local stiffness matrices introduced in (19.111). In Fig. 19.15 we display an example of sparsity pattern of $R_{i}$.

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-214.jpg?height=155&width=547&top_left_y=1094&top_left_x=97)



![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-215.jpg?height=150&width=712&top_left_y=114&top_left_x=100)

A few general comments on Algorithm $18.5$ and Algorithm $18.6$ are in order:

- steps a. and b. of algorithm $18.5$ can be carried out in reverse order, that is we could first subdivide the computational domain into subdomains (based, for instance, on physical considerations), then set up a triangulation;

- depending upon the general code structure, steps b. and c. of the algorithm $18.6$ could be glued together with the scope of optimizing memory requirements and CPU time.

In other circumstances we could interchange steps $b$. and $c$., that is the local stiffness matrices $A_{i}$ can be built at first (using the single processors), then assembled to construct the global stiffness matrix $A$.

Indeed, a crucial factor for an efficient use of a parallel computer platform is keeping data locality since in most cases the time necessary for moving data among processors can be higher than that needed for computation.

Other codes (e.g. AztecOO, Trilinos, IFPACK) instead move from the global stiffness
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-215.jpg?height=238&width=538&top_left_y=683&top_left_x=186)

Fig. 19.14. Partition of a rectangular region $\Omega$ in 9 disjoint subregions $\hat{\Omega}_{i}$ (on the left), and an example of an extended subdomain $\Omega_{5}$ (on the right)

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-215.jpg?height=216&width=534&top_left_y=1006&top_left_x=190)

Fig. 19.15. The sparsity pattern of the matrix $R_{i}$ for a partition of the domain in 4 subdomains Table 19.4. Condition number of $P_{a s}^{-1} A$ for several values of $h$ and $H$

\begin{tabular}{lcccc}
\hline$K_{2}\left(P_{a s}^{-1} A\right)$ & $H=1 / 2$ & $H=1 / 4$ & $H=1 / 8$ & $H=1 / 16$ \\
\hline$h=1 / 16$ & $15.95$ & $27.09$ & $52.08$ & $-$ \\
$h=1 / 32$ & $31.69$ & $54.52$ & $104.85$ & $207.67$ \\
$h=1 / 64$ & $63.98$ & $109.22$ & $210.07$ & $416.09$ \\
$h=1 / 128$ & $127.99$ & $218.48$ & $420.04$ & $832.57$ \\
\hline
\end{tabular}

matrix distributed rowise and deduce the local stiffness matrices $A_{i}$ without performing matrix-matrix products but simply using the column indices. In MATLAB, however, it seems more convenient to build $A$ at first, next the restriction matrices $R_{i}$, and finally to carry out matrix multiplications $R_{i} A R_{i}^{T}$ to generate the $A_{i}$.

In Table $19.4$ we analyze the case of a decomposition with minimum overlap $(\delta=$ $h$ ), considering several values for the number $M$ of subdomains. The subdomains $\Omega_{i}$ are overlapping squares of area $H^{2}$. Note that the theoretical estimate (19.122) is satisfied by our results.

\subsubsection{Two-level Schwarz preconditioners}

As anticipated in Sect. 19.6.2, the main limitation of Schwarz methods is to propagate information only among neighbooring subdomains. As for the Neumann-Neumann method, a possible remedy consists of introducing a coarse grid mechanism that allows for a sudden information diffusion on the whole domain $\Omega$. The idea is still that of considering the subdomains as macro-elements of a new coarse grid $\mathscr{T}_{H}$ and to build a corresponding stiffness matrix $A_{H}$. The matrix

$$
Q_{H}=R_{H}^{T} A_{H}^{-1} R_{H}
$$

where $R_{H}$ is the restriction operator from the fine to the coarse grid, represents the coarse level correction for the new two-level preconditioner. More precisely, setting for notational convenience $Q_{0}=Q_{H}$, the two-level preconditioner $P_{\text {cas }}$ is defined through its inverse as

$$
P_{c a s}^{-1}=\sum_{i=0}^{M} Q_{i}
$$

The following result can be proven in $2 \mathrm{D}$ : there exists a constant $C>0$, independent of both $h$ and $H$, such that

$$
K_{2}\left(P_{\text {cas }}^{-1} A\right) \leq C\left(1+\frac{H}{\delta}\right) .
$$

The ratio $H / \delta$ measures the relative overlap between neighboring overlapping subdomains. For "generous" overlap, that is if $\delta$ is a fraction of $H$, the preconditioner $P_{\text {cas }}$ is scalable. Consequently, conjugate gradient iterations on the original finite element system using the preconditioner $P_{\text {cas }}$ converges with a rate independent of $h$ and $H$ (and therefore of the number of subdomains). Moreover, thanks to the additive structure (19.123), the preconditioning step is fully parallel as it involves the solution of $M$ independent systems, one per each local matrix $A_{i}$.

In $3 \mathrm{D}$, we would get a bound with a factor $H / h$, unless the elliptic differential operator has constant coefficients (or variable coefficients which don't vary too much).

The use of $P_{\text {cas }}$ involves the same kind of operations required by $P_{a s}$, plus those of the following algorithm.

Algorit\}

hm $18.7$ (start-up phase for the use of $\boldsymbol{P}_{\text {cas }}$ )

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-217.jpg?height=32&width=93&top_left_y=349&top_left_x=121)

a. Execute Algorithm $\mathbf{1 8 . 6}$

$$
A_{H}(i, j)=a\left(\Phi_{j}, \Phi_{i}\right)=\int_{\Omega} \sum_{\ell=1}^{d} \frac{\partial \Phi_{i}}{\partial x_{\ell}} \frac{\partial \Phi_{j}}{\partial x_{\ell}}
$$

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-217.jpg?height=22&width=14&top_left_y=794&top_left_x=775)

$$
A_{H}=R_{H} A R_{H}^{T}
$$
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-217.jpg?height=220&width=490&top_left_y=918&top_left_x=213)

Fig. 19.16. On the left, example of a coarse grid for a 2D domain, based on a structured mesh. The triangles of the fine grid has thin edges; thick edges identify the boundaries of the coarse grid elements. On the right, a similar construction is displayed, this time for an unstructured fine grid For a computational domain with a simple shape (like the one we are considering) one typically generates the coarse grid $\mathscr{T}_{H}$ first, and then, by multiple refinements, the fine grid $\mathscr{T}_{h}$. In other cases, when the domain has a complex shape and/or a non structured fine grid $\mathscr{T}_{h}$ is already available, the generation of a coarse grid might be difficult or computationally expensive. A first option would be to generate $\mathscr{T}_{H}$ by successive derefinements of the fine grid, in which case the nodes of the coarse grid will represent a subset of those of the fine grid. This approach, however, might not be very efficient in $3 \mathrm{D}$.

Alternatively, one could generate the two (not necessarily nested) grids $\mathscr{T}_{h}$ and $\mathscr{T}_{H}$ independently, then generate the corresponding restriction and prolongation operators from the fine to the coarse grid, $R_{H}$ and $R_{H}^{T}$.

The final implementation of $P_{\text {cas }}$ could therefore be made as follows:

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-218.jpg?height=467&width=727&top_left_y=439&top_left_x=90)

In Table $19.5$ we report the condition number of $P_{\text {cas }}^{-1} A$ in the case of a minimum overlap $\delta=h$. Note that the condition number is almost the same on each NW-SE diagonal (i.e. for fixed values of the ratio $H / \delta$ ).

Table 19.5. Condition number of $P_{\text {cas }}^{-1} A$ for several values of $h$ and $H$

\begin{tabular}{lcccc}
\hline$K_{2}\left(P_{\text {cas }}^{-1} A\right)$ & $H=1 / 4$ & $H=1 / 8$ & $H=1 / 16$ & $H=1 / 32$ \\
\hline$h=1 / 32$ & $7.03$ & $4.94$ & $-$ & $-$ \\
$h=1 / 64$ & $12.73$ & $7.59$ & $4.98$ & $-$ \\
$h=1 / 128$ & $23.62$ & $13.17$ & $7.66$ & $4.99$ \\
$h=1 / 256$ & $45.33$ & $24.34$ & $13.28$ & $-$ \\
\hline
\end{tabular}

An alternative approach to the coarse grid correction can be devised as follows. Suppose that the coefficients of the restriction matrix be given by

$$
\hat{R}_{H}(i, j)= \begin{cases}1 & \text { if the } j-t h \text { node is in } \Omega_{i} \\ 0 & \text { otherwise }\end{cases}
$$

then we set $\hat{A}_{H}=\hat{R}_{H} A \hat{R}_{H}^{T}$. This procedure is named aggregation because the elements of $\hat{A}_{H}$ are obtained by simply summing up the entries of $A$. Note that we don't need to construct a coarse grid in this case. The corresponding preconditioner, denoted by $P_{\text {aggre }}$, has an inverse that reads

$$
P_{\text {aggre }}^{-1}=\hat{R}_{H}^{T} \hat{A}_{H}^{-1} \hat{R}_{H}+P_{a s}
$$

It can be proven that

$$
K_{2}\left(P_{\text {aggre }}^{-1} A\right) \leq C\left(1+\frac{H}{\delta}\right)
$$

In Table $19.6$ we report several numerical values of the condition number for different values of $h$ and $H$.

Table 19.6. Condition number of $P_{\text {aggre }}^{-1} A$ for several values of $h$ and $H$

\begin{tabular}{lccc}
\hline$P_{a g g r e}^{-1} A$ & $H=1 / 4$ & $H=1 / 8$ & $H=1 / 16$ \\
\hline$h=1 / 16$ & $13.37$ & $8.87$ & $-$ \\
$h=1 / 32$ & $26.93$ & $17.71$ & $9.82$ \\
$h=1 / 64$ & $54.33$ & $35.21$ & $19.70$ \\
$h=1 / 128$ & $109.39$ & $70.22$ & $39.07$ \\
\hline
\end{tabular}

If $H / \delta=$ constant, this two-level preconditioner is either optimal and scalable, that is the condition number of the preconditioned stiffness matrix is independent of both $h$ and $H$.

We can conclude this section with the following practical indications:

- for decompositions with a small number of subdomains, the single level Schwarz preconditioner $P_{a s}$ is very efficient;

- when the number $M$ of subdomains gets large, using two-level preconditioners becomes crucial; aggregation techniques can be adopted, in alternative to the use of a coarse grid in those cases in which the generation of the latter is difficult.

\subsection{An abstract convergence result}

The analysis of overlapping and non-overlapping domain decomposition preconditioners is based on the following abstract theory, due to P.L. Lions, J. Bramble, M. Dryja, O. Wildlund. Let $V_{h}$ be a Hilbert space of finite dimension. In our applications, $V_{h}$ is one of the finite element spaces or spectral element spaces. Let $V_{h}$ be decomposed as follows:

$$
V_{h}=V_{0}+V_{1}+\cdots+V_{M}
$$

Let $F \in V^{\prime}$ and $a: V \times V \rightarrow \mathbb{R}$ be a symmetric, continuous and coercive bilinear form. Consider the problem

$$
\text { find } u_{h} \in V_{h}: a\left(u_{h}, v_{h}\right)=F\left(v_{h}\right) \quad \forall v_{h} \in V_{h} .
$$

Let $P_{i}: V_{h} \rightarrow V_{i}$ be a projection operator defined by

$$
b_{i}\left(P_{i} u_{h}, v_{h}\right)=a\left(u_{h}, v_{h}\right) \quad \forall v_{h} \in V_{i}
$$

with $b_{i}: V_{i} \times V_{i} \rightarrow \mathbb{R}$ being a local symmetric, continuous and coercive bilinear form on each subspace $V_{i}$. Assume that the following properties hold:

a. stable subspace decomposition:

$\exists C_{0}>0$ such that every $u_{h} \in V_{h}$ admits a decomposition $u_{h}=\sum_{i=0}^{M} u_{i}$ with $u_{i} \in V_{i}$ and

$$
\sum_{i=0}^{M} b_{i}\left(u_{i}, u_{i}\right) \leq C_{0}^{2} a\left(u_{h}, u_{h}\right)
$$

b. strengthened Cauchy-Schwarz inequality:

$\exists \varepsilon_{i j} \in[0,1], i, j=0, \ldots, M$ such that

$$
a\left(u_{i}, u_{i}\right) \leq \varepsilon_{i j} \sqrt{a\left(u_{i}, u_{i}\right)} \sqrt{a\left(u_{j}, u_{j}\right)} \quad \forall u_{i} \in V_{i}, u_{j} \in V_{j}
$$

c. local stability:

$\exists \omega \geq 1$ such that $\forall i=0, \ldots, M$

$$
a\left(u_{i}, u_{i}\right) \leq \omega b_{i}\left(u_{i}, u_{i}\right) \quad \forall u_{i} \in \operatorname{Range}\left(P_{i}\right) \subset V_{i} .
$$

Then, $\forall u_{h} \in V_{h}$,

$$
C_{0}^{-2} a\left(u_{h}, u_{h}\right) \leq a\left(P_{a s} u_{h}, u_{h}\right) \leq \omega(\rho(E)+1) a\left(u_{h}, u_{h}\right)
$$

where $\rho(E)$ is the spectral radius of the matrix $E=\left(\varepsilon_{i j}\right)$, and $P_{a s}=P_{0}+\cdots+P_{M}$ is the domain decomposition preconditioner.

From inequality (19.125) the following bound holds for the preconditioned system

$$
K\left(B^{-1} A\right) \leq C_{0}^{2} \omega(\rho(E)+1)
$$

where $K(\cdot)$ denotes the spectral condition number, $A$ the matrix associated with the original system $(19.124), B$ the matrix associated to the operator $P_{a s}$. For the proof, see e.g. [TW05]. 

\section{$19.8$ Interface conditions for other differential problems}

Theorem $19.1$ in Sect. 19.1.2 allows a second order elliptic problem (19.1) to be reformulated in a DD version thanks to suitable interface conditions (19.9) and (19.10). On the other hand, as we have extensively discussed, such reformulation sets the ground for several iterative algorithms on disjoint DD partitions. They comprise DirichletNeumann, Neumann-Neumann, Robin-Robin algorithms and, more generally, all of the preconditioned iterative algorithms of the Schur complement system (19.53) using suitable DD preconditioners.

In this section we consider other kind of boundary-value problems and formulate the associated interface conditions. Table $19.7$ displays the interface conditions for these problems. For more details, analysis and investigation of associated iterative DD algorithms, the interested reader can consult [QV99]. Here we limit ourselves to provide a few additional insights in the case of advection and Stokes equations.

Advection (transport) problems. Consider the differential problem

$$
L u=\nabla \cdot(\mathbf{b} u)+a_{0} u=f \quad \text { in } \Omega
$$

supplemented by suitable conditions on the boundary $\partial \Omega$. Consider a partition of the computational domain $\Omega$ into two disjoint subdomains whose interface is $\Gamma$. Let us partition the latter as follows (see Fig. 19.17): $\Gamma=\Gamma_{\text {in }} \cup \Gamma_{\text {out }}$, where

$$
\Gamma_{i n}=\{x \in \Gamma \mid \mathbf{b}(x) \cdot \mathbf{n}(x)>0\} \text { and } \Gamma_{\text {out }}=\Gamma \backslash \Gamma_{\text {in }} .
$$

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-221.jpg?height=309&width=351&top_left_y=890&top_left_x=275)

Fig. 19.17. Domain partition and interface splitting for the advection problem (19.126) Example 19.4. The Dirichlet-Neumann method for the problem at hand could be generalized as follows: being given two functions $u_{1}^{(0)}, u_{2}^{(0)}$ on $\Gamma, \forall k \geq 0$ solve:

$$
\begin{aligned}
& \begin{cases}L u_{1}^{(k+1)}=f & \text { in } \Omega_{1} \\
(\mathbf{b} \cdot \mathbf{n}) u_{1}^{(k+1)}=(\mathbf{b} \cdot \mathbf{n}) u_{2}^{(k)} & \text { on } \Gamma_{o u t}\end{cases} \\
&\left\{\begin{array}{ll}
L u_{2}^{(k+1)}=f & \text { in } \Omega_{2} \\
(\mathbf{b} \cdot \mathbf{n}) u_{2}^{(k+1)}=\theta(\mathbf{b} \cdot \mathbf{n}) u_{1}^{(k+1)}+(1-\theta)(\mathbf{b} \cdot \mathbf{n}) u_{2}^{(k)}
\end{array} \text { on } \Gamma_{i n}\right.
\end{aligned}
$$

where $\theta>0$ denotes a suitable relaxation parameter. The adaptation to the case of a finite element discretization is straightforward.

Stokes problem. The Stokes equations (17.11) feature two fields of variables: fluid velocity and fluid pressure. When considering a DD partition, at subdomain interface only the velocity field is requested to be continuous. Pressure needs not necessarily be continuous, since in the weak formulation of the Stokes equations it is "only" requested to be in $L^{2}$. Moreover, on the interface $\Gamma$ the continuity of the normal Cauchy stress $v \frac{\partial \mathrm{u}}{\partial n}-p \mathbf{n}$ needs only be satisfied in weak (natural) form.

Example 19.5. A Dirichlet-Neumann algorithm for the Stokes problem would entail at each iteration the solution of the following subproblems (we use the short-hand notation $\mathscr{S}$ to indicate the Stokes operator):

$$
\begin{aligned}
& \begin{cases}\mathscr{S}\left(\mathbf{u}_{2}^{(k+1)}, p_{2}^{(k+1)}\right)=\mathbf{f} & \text { in } \Omega_{2} \\
v \frac{\partial \mathbf{u}_{2}^{(k+1)}}{\partial n}-p_{2}^{(k+1)}=v \frac{\partial \mathbf{u}_{1}^{(k)}}{\partial n}-p_{1}^{(k)} & \text { on } \Gamma \\
\mathbf{u}_{2}^{(k+1)}=\mathbf{0} & \text { on } \partial \Omega_{2} \backslash \Gamma\end{cases} \\
& \begin{cases}\mathscr{S}\left(\mathbf{u}_{1}^{(k+1)}, p_{1}^{(k+1)}\right)=\mathbf{f} & \text { in } \Omega_{1} \\
\mathbf{u}_{1}^{(k+1)}=\theta \mathbf{u}_{2}^{(k+1)}+(1-\theta) \mathbf{u}_{1}^{(k)} & \text { on } \Gamma \\
\mathbf{u}_{1}^{(k+1)}=\mathbf{0} & \text { on } \partial \Omega_{1} \backslash \Gamma\end{cases}
\end{aligned}
$$

Should the boundary conditions of the original problem be prescribed on the velocity field, e.g. $\mathbf{u}=\mathbf{0}$, pressure $p$ would be defined only up to an additive constant, which could be fixed by, e.g., imposing the constraint $\int_{\Omega} p d \Omega=0$.

To fulfill this constraint we can proceed as follows. When solving the Neumann problem (19.127) on the subdomain $\Omega_{2}$, both the velocity $\mathbf{u}_{2}^{(k+1)}$ and the pressure $p_{2}^{(k+1)}$ are univocally determined. When solving the Dirichlet problem (19.128) on $\Omega_{1}$, the pressure is defined only up to an additive constant; we fix it by imposing the additional equation

$$
\int_{\Omega_{1}} p_{1}^{(k+1)} d \Omega_{1}=-\int_{\Omega_{2}} p_{2}^{(k+1)} d \Omega_{2}
$$

Should the four sequences $\left\{\mathbf{u}_{1}^{(k)}\right\},\left\{\mathbf{u}_{2}^{(k)}\right\},\left\{p_{1}^{(k)}\right\}$ and $\left\{p_{2}^{(k)}\right\}$ converge, the null average condition on the pressure would be automatically verified.

Example 19.6. Suppose now that the Schwarz iterative method is used on an overlapping subdomain decomposition of the domain like that on Fig. 19.1, left. At every step we have to solve two Dirichlet problems for the Stokes equations:

$$
\begin{array}{ll} 
\begin{cases}\mathscr{S}\left(\mathbf{u}_{1}^{(k+1)}, p_{1}^{(k+1)}\right)=\mathbf{f} & \text { in } \Omega_{1} \\
\mathbf{u}_{1}^{(k+1)}=\mathbf{u}_{2}^{(k)} & & \text { on } \Gamma_{1} \\
\mathbf{u}_{1}^{(k+1)}=0 & & \text { on } \partial \Omega_{1} \backslash \Gamma_{1}\end{cases} \\
\begin{cases}\mathscr{S}\left(\mathbf{u}_{2}^{(k+1)}, p_{2}^{(k+1)}\right)=\mathbf{f} & \text { in } \Omega_{2} \\
\mathbf{u}_{2}^{(k+1)}=\mathbf{u}_{1}^{(k+1)} & & \text { on } \Gamma_{2} \\
\mathbf{u}_{2}^{(k+1)}=0 & & \text { on } \partial \Omega_{2} \backslash \Gamma_{2}\end{cases}
\end{array}
$$

No continuity is required on the pressure field at subdomain boundaries.

The constraint on the fluid velocity to be divergence free on the whole domain $\Omega$ requires special care. Indeed, after solving $(19.129)$, we have $\operatorname{div} \mathbf{u}_{1}^{(k+1)}=0$ in $\Omega_{1}$, hence, thanks to the Green formula,

$$
\int_{\partial \Omega_{1}} \mathbf{u}_{1}^{(k+1)} \cdot \mathbf{n} d \gamma=0
$$

This relation implies a similar relation for $\mathbf{u}_{2}^{(k)}$ in $(19.129)_{2}$; indeed

$$
0=\int_{\partial \Omega_{1}} \mathbf{u}_{1}^{(k+1)} \cdot \mathbf{n} d \gamma=\int_{\Gamma_{1}} \mathbf{u}_{1}^{(k+1)} \cdot \mathbf{n} d \gamma=\int_{\Gamma_{1}} \mathbf{u}_{2}^{(k)} \cdot \mathbf{n} d \gamma
$$

At the very first iteration we can select $\mathbf{u}_{2}^{(0)}$ in such a way that the compatibility condition (19.131) be satisfied, however this control is lost, a priori, in the course of the subsequent iterations. For the same reason, the solution of $(19.130)$ yields the compatibility condition

$$
\int_{\Gamma_{2}} \mathbf{u}_{1}^{(k+1)} \cdot \mathbf{n} d \gamma=0
$$

Fortunately, Schwarz method automatically guarantees that this condition holds. Indeed, in $\Gamma_{12}=\Omega_{1} \cap \Omega_{2}$ we have $\operatorname{divu}_{1}^{(k+1)}=0$, moreover on $\Gamma_{12} \backslash\left(\Gamma_{1} \cup \Gamma_{2}\right), \mathbf{u}_{1}^{(k+1)}=\mathbf{0}$ because of the given homogeneous Dirichlet boundary conditions. Thus

$$
0=\int_{\partial \Gamma_{12}} \mathbf{u}_{1}^{(k+1)} \cdot \mathbf{n} d \gamma=\int_{\Gamma_{1}} \mathbf{u}_{1}^{(k+1)} \cdot \mathbf{n} d \gamma+\int_{\Gamma_{2}} \mathbf{u}_{1}^{(k+1)} \cdot \mathbf{n} d \gamma
$$

The first integral on the right hand side vanishes because of (19.131), therefore (19.132) is satisfied. Table 19.7. Interface continuity conditions for several kind of differential operators; D stands for Dirichlet condition, $\mathrm{N}$ for Neumann

\begin{tabular}{ccc}
\hline Operator & Problem & $D$ & $N$ \\
\hline Laplace & $-\Delta u=f,$, & $u$ & $\frac{\partial u}{\partial n}$ \\
\hline Elasticity & \\
$-\nabla \cdot(\sigma(\mathbf{u}))=\mathbf{f},$, & \\
with \\
Transport-diffusion $-\sum_{k j} D_{k}\left(A_{k j} D_{j} u\right)+\operatorname{div}(\mathbf{b} u)+a_{0} u=f$ & $u$ & \\
& \\
$\qquad \begin{gathered}\sigma_{k j}=\hat{\lambda}\left(D_{k} u_{j}+D_{j} u_{k}\right)+\hat{\lambda} \operatorname{divu} \delta_{k j}, \\
\mathbf{u}^{*}\end{gathered}=\left\{\begin{array}{ccc}-\operatorname{div}(\mathbf{u}, p)+\left(\mathbf{u}^{*} \cdot \nabla\right) \mathbf{u}=\mathbf{f}, \\
\operatorname{u} \text { in-plane membrane displacement }\end{array}\right.$ \\
divu $=0$, with Incompressible vis \\
compressible vis \\
(harmonic regime) \\
compressible vis \\
cote inviscid flows \\
cote viscous flows \\
us u \\
$\begin{gathered}\text { (Stokes equations) } \\
\text { ( }) & \text { (Oseen equations) } \\
\text { (Navier-Stokes equations) }\end{gathered}$ & u \\
(u) cote \\
(u) cote \\
$\begin{gathered}\text { cote } u) \cdot \mathbf{u}_{k j} D_{k} & \\
\text { (u) } & \text { with } & \text { u } & \\
\text { (is) } & \text { with } & \text { u } & \text { u } & \text { ( })\left(D_{k j}+\left(g-\frac{2 v}{d}\right) \operatorname{divu} \delta_{k j},\right. & \\
\text { (u) } & \text { un } & \text { (u) } & \text { (u) } & \text { (u) } & \text { (u) } & \text { (u) } & \text { (u) } & \text { (u) } & \text { (u) } & \text { (u) } & \text { (i) } & \text { (i) } & \text { ( }), & \text { (i) } \\
\text { (I) } & \text { (I) } & \text { (I) } & \text { ( } u) \cdot \mathbf{n} \\
\text { (o) } & \text { (I) } & \text { (I) } & \text { ( } u) & \\
\text { (is) } & \text { (i) } & \text { (o) } & \text { (o) } \\
\text { (is) } & \text { (i) } & \text { (o) } & \\
\text { (is) } & \text { (i) } & \text { (I) } & \text { (I) } \\
\text { (I) } & \text { (I) } & \text { (I) } & \text { (I) } \\
\text { (I) } & \text { (I) } & \text { (I) } & \\
\text { (is) } & \mathrm{~ ( o}\end{gathered}$
\end{tabular}



\section{$19.9$ Exercises}

1. Consider the one-dimensional advection-transport-reaction problem

$$
\begin{cases}-\left(\alpha u_{x}\right)_{x}+(\beta u)_{x}+\gamma u=f & \text { in } \Omega=(a, b) \\ u(a)=0, \quad \alpha u_{x}(b)-\beta u(b)=g\end{cases}
$$

with $\alpha$ and $\gamma \in \mathrm{L}^{\infty}(a, b), \beta \in \mathrm{W}^{1, \infty}(a, b)$ and $f \in L^{2}(a, b)$.

a) Write the addititive Schwarz iterative method, then the multiplicative one, on the two overlapping intervals $\Omega_{1}=\left(a, \gamma_{2}\right)$ and $\Omega_{2}=\left(\gamma_{1}, b\right)$, with $a<\gamma_{1}<\gamma_{2}<$ $b$

b) Interpret these methods as suitable Richardson algorithms to solve the given differential problem.

c) In case we approximate (19.133) by the finite element method, write the corresponding additive Schwarz preconditioner, with and without coarse-grid component. Then provide an estimate of the condition number of the preconditioned matrix, in both cases.

2. Consider the one-dimensional diffusion-transport-reaction problem

$$
\begin{cases}-\left(\alpha u_{x}\right)_{x}+(\beta u)_{x}+\delta u=f & \text { in } \Omega=(a, b) \\ \alpha u_{x}(a)-\beta u(a)=g, \quad u_{x}(b)=0\end{cases}
$$

with $\alpha$ and $\gamma \in \mathrm{L}^{\infty}(a, b), \alpha(x) \geq \alpha_{0}>0, \beta \in \mathrm{W}^{1, \infty}(a, b), f \in L^{2}(a, b)$ and $g$ a given real number.

a) Consider two disjoined subdomains of $\Omega, \Omega_{1}=(a, \gamma)$ and $\Omega_{2}=(\gamma, b)$, with $a<\gamma<b$. Formulate problem (19.134) using the Steklov-Poincaré operator, both in differential and variational form. Analyze the properties of this operator starting from those of the bilinear form associated with problem (19.134).

b) Apply the Dirichlet-Neumann method to problem (19.134) using the same domain partition introduced at point a).

c) In case of finite element approximation, derive the expression of the DirichletNeumann preconditioner of the Schur complement matrix.

3. Consider the one-dimensional Poisson problem

$$
\begin{cases}-u_{x x}(x)=f(x) & \text { in } \Omega=(0,1) \\ u(0)=0, \quad u_{x}(1)=0, & \end{cases}
$$

with $f \in L^{2}(\Omega)$.

a) If $\mathscr{T}_{h}$ indicates a partition of the interval $\Omega$ with step-size $h$, write the Galerkinfinite element approximation of problem (19.135).

b) Consider now a partition of $\Omega$ into the subintervals $\Omega_{1}=(0, \gamma)$ and $\Omega_{2}=(\gamma, 1)$, being $0<\gamma<1$ a node of the partition $\mathscr{T}_{h}$ (See Fig. 19.18). Write the algebraic blockwise form of the Galerkin-finite element stiffness matrix relative to this subdomain partition.

c) Derive the discrete Steklov-Poincaré interface equation which corresponds to the DD formulation at point b). Which is the dimension of the Schur complement?

d) Consider now two overlapping subdomains $\Omega_{1}=\left(0, \gamma_{2}\right)$ and $\Omega_{2}=\left(\gamma_{1}, 1\right)$, with $0<\gamma_{1}<\gamma_{2}<1$, the overlap being reduced to a single finite element of the partition $\mathscr{T}_{h}$ (see Fig. 19.19). Provide the algebraic formulation of the additive Schwarz iterative method.

e) Provide the general expression of the two-level additive Shwarz preconditioner, by assuming as coarse matrix $A_{H}$ that associated with only two elements, as displayed in Fig. 19.20.

4. Consider the diffusion-transport-reaction problem

$$
\begin{cases}L u=-\nabla \cdot(\alpha \nabla u)+\nabla \cdot(\boldsymbol{\beta} u)+\gamma u=f & \text { in } \Omega=(0,2) \times(0,1) \\ u=0 & \text { on } \Gamma_{D} \\ \alpha \frac{\partial u}{\partial n}+\delta u=0 & \text { on } \Gamma_{R}\end{cases}
$$

with $\alpha=\alpha(\mathbf{x}), \boldsymbol{\beta}=\boldsymbol{\beta}(\mathbf{x}), \gamma=\gamma(\mathbf{x}), \delta=\delta(\mathbf{x})$ and $f=f(\mathbf{x})$ being given functions, and $\partial \Omega=\bar{\Gamma}_{D} \cup \bar{\Gamma}_{R}$, with $\Gamma_{D} \cap \Gamma_{R}=\emptyset$

Let $\Omega$ in (19.136) be partitioned into two disjoined subdomains $\Omega_{1}=(0,1) \times(0,1)$ and $\Omega_{2}=(1,2) \times(0,1)$.

a) Formulate problem (19.136) in terms of the Steklov-Poincaré operator, both in differential and variational form.

b) Apply the Dirichlet-Neumann method to problem (19.136) using the same decomposition introduced before.

c) Prove the equivalence between the Dirichlet-Neumann method at point $\mathrm{b}$ ) and a suitable preconditioned Richardson operator, after setting $\alpha=1, \beta=0, \gamma=1$ and $\Gamma_{R}=\emptyset$ in (19.136). Do the same for the Neumann-Neumann method.

5. Consider the two-dimensional diffusion-transport-reaction problem

$$
\begin{cases}L u=-\nabla \cdot(\mu \nabla u)+b \cdot \nabla u+\sigma u=f & \text { in } \Omega=(a, c) \times(d, e) \\ u=0 & \text { on } \partial \Omega .\end{cases}
$$

Consider a decomposition of $\Omega$ made of the overlapping subdomains $\Omega_{3}=(a, f) \times$ $(d, e)$ and $\Omega_{4}=(g, c) \times(d, e)$, with $g<f$. On such a decomposition, write for problem (19.137) the Schwarz method in both multiplicative and additive versions.

Fig. 19.18. Subdomain partition $\mathscr{T}_{h}$ of the interval $(0,1)$ 

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-227.jpg?height=96&width=327&top_left_y=115&top_left_x=293)

Fig. 19.19. Overlapping decomposition of the interval $(0,1)$

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-227.jpg?height=170&width=364&top_left_y=274&top_left_x=273)

Fig. 19.20. Coarse-grid partition made of two macro elements for the construction of matrix $A_{H}$ and Lagrangian characteristic function associated with the node $\gamma$

Then interpret these methods as suitable preconditioned Richardson iterative algorithms. Finally, comment on the convergence properties of these methods. 


\section{Reduced basis approximation for parametrized partial differential equations}

Reduced basis (RB) methods are computational reduction techniques for the rapid and reliable evaluation of input-output relationships: the output is expressed as a functional of the solution of a parametrized partial differential equation (PDE), the input being the set of parameters.

Parametrized PDEs can model several processes that are relevant in applications, such as, e.g., steady and unsteady heat and mass transfer, acoustics, solid and fluid mechanics, but also electromagnetics or even finance. The input-parameter vector may characterize either the geometric configuration, or some physical properties, or else boundary conditions and source terms. The outputs of interest might be the maximum system temperature, an added mass coefficient, a crack stress intensity factor, an effective constitutive property, an acoustic waveguide transmission loss, or a channel flowrate or pressure drop, just to mention a few. Finally, the field variables that connect the input parameters to the outputs can represent a distribution function, temperature or concentration, displacement, pressure, or velocity, etc.

The goal of an RB approximation is to capture the essential features of the input/output behaviour of a system $(i)$ by improving computational performances and (ii) by keeping the approximation error between the reduced-order solution and the full-order one (the parametrized PDE) under control. In particular, the aim is to approximate a PDE solution using a handful of degrees of freedom instead of the many more (thousands, or millions, sometimes even billions) that would be needed for a fullorder approximation. In fact, the idea at the heart of computational reduction strategies is the assumption (often verified) that the behaviour of a system can be well described by a small number of dominant modes.

In this way, we need to solve the full-order problem only for few instances of the input through a computationally demanding Offline stage, in order to construct a reduced space of basis solutions. This makes possible to perform many low-cost reduced-order simulations at a very inexpensive Online stage for new instances of the input, by expressing the reduced solution as a linear combination of the basis solutions and exploiting a Galerkin projection onto this reduced space.

RB methods do not replace Galerkin methods (or any other method suitable to approximate PDEs). Rather, they build upon, a given approximation method, for instance the finite element method (FEM): the RB solution does not approximate directly the exact solution, but rather a "given" finite element discretization of (typically) very large dimension $N_{h}$ of it.

In particular, we shall consider in this chapter the case of linear functional outputs of affinely parametrized linear elliptic coercive PDEs. This class of problems - relatively simple, yet relevant to many important applications, such as conduction and convection-diffusion, linear elasticity, etc. - proves to be a convenient expository vehicle for the methodology, which can be applied, up to suitable extension, to more general equations. We refer the reader interested in, say, linear parabolic PDEs to the review presented in [QRM11], for instance.

Although our focus is on the affine linear elliptic coercive case, the reduced basis approximation and a posteriori error estimation we discuss in this chapter are for more general, and their combination is a key factor for reduction techniques to be computationally successful. We also point out that, despite the increasing computer power nowadays makes the numerical solution of problems of very large dimensions and/or modelling complex phenomena essential, a computational reduction is still crucial whenever one is interested in real-time simulations and/or repeated output evaluations for different values of some inputs of interest. Typical cases are, for instance, real time visualization, the sensitivity analysis of PDE solutions with respect to parameters, or optimization problems under PDE constraints (such as optimal control problems, as the ones addressed in Chap. 18).

We outline below the chapter's content. In Sect. $20.1$ we introduce the affine linear elliptic coercive setting in the case of the so-called compliant problems; the most relevant examples of parametrizations will be addressed in Sect. 20.9. In Sect. $20.2$ we illustrate some basic ingredients shared by several computational reduction approaches. Then we describe the main features of the reduced basis method for parametrized problems in Sect. 20.3: reduced spaces, Galerkin projection and an Offline/Online procedure ensuring computational efficiency. Moreover, we provide in Sect. $20.4$ both an algebraic and a geometrical interpretation of the reduced basis approximation problem. We address in Sect. $20.5$ the most popular strategies to construct snapshots (that is, basis functions) and reduced spaces: greedy algorithms (the core of $\mathrm{RB}$ methods for parametrized PDEs) and proper orthogonal decomposition, POD. In Sect. $20.6$ we sketch some ideas related to a priori convergence theory, whereas in Sect. $20.7$ we present rigorous and relatively sharp a posteriori output error bounds for RB approximations. We extend both the RB approximation and a posteriori error bounds to the case of non-compliant problems in Sect. 20.8. We discuss a simple numerical test case and provide a brief overview for more general classes of problems in Sect. 20.10. 

\subsection{Elliptic coercive parametric PDEs}

Before introducing the main features of computational reduction, let us describe the class of problems we deal with throughout the chapter; more details will be given in Sect. 20.9. We denote by $\mathscr{D} \subset \mathbb{R}^{p}$, for an integer $p \geq 1$, a set of input parameters which may describe physical properties of the system, as well as boundary terms, source terms or the geometry of the computational domain. The problems we focus on can be written in the following form:

given $\mu \in \mathscr{D}$, evaluate the output of interest $s(\mu)=J(u(\mu))$ where $u(\mu) \in V=$ $V(\Omega)$ is the solution of the following parametrized PDE

$$
L(\mu) u(\mu)=F(\mu)
$$

Here $\Omega \subset \mathbb{R}^{d}, d=1,2,3$ is a regular domain, $V$ a suitable Hilbert space, $V^{\prime}$ its dual, $L(\mu): V \rightarrow V^{\prime}$ a second-order differential operator and $F(\mu) \in V^{\prime}$. The weak formulation of problem $(20.1)$ reads: find $u(\mu) \in V=V(\Omega)$ such that

$$
a(u(\mu), v ; \mu)=f(v ; \mu) \quad \forall v \in V,
$$

where the bilinear form $^{1}$ is obtained from $L(\mu)$ :

$$
a(u, v ; \mu)={v^{\prime}}\langle L(\mu) u, v\rangle_{V} \quad \forall u, v \in V,
$$

while

$$
f(v ; \mu)={ }_{V^{\prime}}\langle F(\mu), v\rangle_{V}
$$

is a continuous linear form. We assume, for each $\mu \in \mathscr{D}, a(\cdot, \cdot ; \mu)$ to be continuous and coercive, i.e. $\exists \bar{\gamma}<+\infty, \alpha_{0}>0$ :

$$
\gamma(\mu)=\sup _{u \in V} \sup _{v \in V} \frac{a(u, v ; \mu)}{\|u\|_{V}\|v\|_{V}}<\bar{\gamma}, \quad \alpha(\mu)=\inf _{u \in V} \frac{a(u, u ; \mu)}{\|u\|_{V}^{2}} \geq \alpha_{0}
$$

If the coercivity assumption is not satisfied, we have stability in the more general sense guaranteed by the inf-sup condition. $J$ (the output functional) is a linear and bounded form on $V$. Under these standard hypotheses on $a$ and $f,(20.2)$ admits a unique solution, due to the Lax-Milgram lemma. We shall exclusively consider second-order elliptic partial differential equations, in which case $V=H_{\Gamma_{D}}^{1}(\Omega)-$ see $(3.26)$. Furthermore, we assume that $a$ is symmetric and that $J=f$. The latter is merely a simplifying assumption and it means that we are in the so-called compliant case [PR07], a situation occurring quite frequently in engineering problems (see Sect. 20.1.1). The generalization to the non-compliant case, where $a$ may be non-symmetric and $J$ may be any bounded linear functional over $V$, is provided in Sect. $20.8$.

${ }^{1}$ To be rigorous, we should introduce the Riesz identification operator $R: V^{\prime} \rightarrow V$ by which we identify $V$ and its dual, so that, given a third Hilbert space $H$ such that $V \hookrightarrow H^{\prime}$ and $H^{\prime} \hookrightarrow V^{\prime}, V^{\prime}\langle L(\mu) u, v\rangle_{V}=(R L(\mu) u, v)_{H} ;$ see also Sect. 2.1. However, the Riesz operator will be omitted for the sake of simplicity. We make one last assumption, crucial to enhance the computational efficiency: we require both the parametric bilinear form $a$ and the parametric linear form $f$ to be affine with respect to the parameter $\mu$, that is:

$$
\begin{gathered}
a(w, v ; \mu)=\sum_{q=1}^{Q_{a}} \Theta_{a}^{q}(\mu) a^{q}(w, v) \quad \forall v, w \in V, \mu \in \mathscr{D} \\
f(v ; \mu)=\sum_{q=1}^{Q_{f}} \Theta_{f}^{q}(\mu) f^{q}(w) \quad \forall w \in V, \mu \in \mathscr{D}
\end{gathered}
$$

Here $\Theta_{a}^{q}: \mathscr{D} \rightarrow \mathbb{R}, q=1, \ldots, Q_{a}$ and $\Theta_{f}^{q}: \mathscr{D} \rightarrow \mathbb{R}, q=1, \ldots, Q_{f}$, are $\mu$-dependent functions, whereas $a^{q}: V \times V \rightarrow \mathbb{R}, f^{q^{\prime}}: V \rightarrow \mathbb{R}$ are $\mu$-independent. As a general principle, parameter-independent terms will be computed Offline, thus making Online computation much lighter.

Let us also remark that, since $a$ is symmetric, we can define the energy inner product and the energy norm for elements of $V$ as follows:

$$
\begin{aligned}
(w, v)_{\mu} &=a(w, v ; \mu) & \forall w, v \in V, \\
\|w\|_{\mu} &=(w, w)_{\mu}^{1 / 2} & \forall w \in V .
\end{aligned}
$$

Next, for given $\bar{\mu} \in \mathscr{D}$ and non-negative real $\tau$

$$
\begin{aligned}
(w, v)_{V} &=(w, v)_{\mu}+\tau(w, v)_{L^{2}(\Omega)} & & \forall w, v \in V \\
\|w\|_{V} &=(w, w)_{V}^{1 / 2} & \forall w \in V
\end{aligned}
$$

shall define the inner product and norm on our $V$, respectively. The role of this scalar product will be clear in Sect.20.5.

\subsubsection{Two simple examples}

Before describing the main features shared by several computational reduction approaches, we provide two simple examples of parametrized problems which fit the framework and the methodology presented in this chapter. More involved examples, as well as a general formulation of physical and/or geometrical parametrizations fulfilling the key assumptions of RB methods, will be addressed later on, in Sect.20.9.

The simplest elliptic coercive parametrized problem we may think of is a Poisson problem, defined over a domain $\Omega$, modelling the diffusion/reaction of e.g. a pollutant. Here $u=u(\mu)$ denotes the pollutant concentration, and the diffusion coefficient $\mu$ plays the role of input parameter; the output of interest is the average of the concentration over the domain,

$$
s(\mu)=\int_{\Omega} u(\mu) d \Omega .
$$

Consider homogeneous Neumann boundary conditions over $\partial \Omega$, a unit source term over the domain $\Omega$ and set $V=H^{1}(\Omega)$. We recover the abstract formulation of $(20.2)$ by defining

$$
a(w, v ; \mu)=\mu \int_{\Omega} \nabla w \cdot \nabla v d \Omega+\int_{\Omega} w v d \Omega, \quad f(v ; \mu)=\int_{\Omega} v d \Omega
$$

This problem is coercive, symmetric, and compliant. In this case we deal with $p=1$ parameters, encoding a physical property; $f$ does not depend on $\mu$, while $a$ is affine in $\mu ;$ we have $Q_{a}=2, Q_{f}=1, \Theta_{a}^{1}(\mu)=\mu, \Theta_{a}^{2}(\mu)=\Theta_{f}^{1}(\mu)=1$, and

$$
a^{1}(w, v)=\int_{\Omega} \nabla w \cdot \nabla v d \Omega, \quad a^{2}(w, v)=\int_{\Omega} w v d \Omega, \quad f^{1}(v)=\int_{\Omega} v d \Omega
$$

A slightly more involved case, still dealing with physical parameters only, is given by a heat conduction problem in a square domain $\Omega$ which comprises $B_{1} \times B_{2}$ blocks, each one representing a subregion with (a priori different) constant thermal conductivity; the geometry is depicted in Fig. 20.1. Here

$$
\bar{\Omega}=\bigcup_{i=1}^{P+1} \overline{\mathscr{R}}_{i}
$$

where the $\mathscr{R}_{i}, i=1 \ldots, P+1$, correspond to the subregions featuring a conductivity equal to $\mu_{i}>0$. Inhomogeneous Neumann (non-zero flux) boundary conditions on $\Gamma_{\text {base }}$, homogeneous Dirichlet (temperature) conditions on $\Gamma_{\text {top }}$, and homogeneous (zero flux) Neumann conditions are imposed on the two vertical sides. The output of interest is the average temperature over $\Gamma_{\text {base }}$.

The parameters $\mu=\left(\mu_{1}, \ldots, \mu_{P}\right)$ are then the conductivities in the first $P=B_{1} B_{2}-$ 1 blocks (with the blocks numbered as shown in Figure 20.1); the conductivity of the last block, which serves for normalization, is one. By setting $V=\left\{v \in H^{1}(\Omega)|v|_{\Gamma_{\text {top }}}=0\right\}$, we recover the abstract formulation (20.2), with

$$
a(w, v ; \mu)=\sum_{i=1}^{P} \mu_{i} \int_{\mathscr{R}_{i}} \nabla w \cdot \nabla v d \Omega+\int_{\mathscr{R}_{P+1}} \nabla w \cdot \nabla v d \Omega
$$

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-232.jpg?height=319&width=317&top_left_y=880&top_left_x=295)

Fig. 20.1. Thermal block problem for $B_{1}=B_{2}=3$ which is associated to the Laplace operator with homogeneous Neumann conditions (as well as internal flux continuity conditions), and

$$
f(v ; \mu)=\int_{\Gamma_{\text {base }}} v d \Gamma
$$

which imposes the inhomogeneous Neumann conditions and yields the mean temperature. Moreover, $F$ is independent of $\mu$. The dependence of $(20.13)$ on the parameters is affine; by direct inspection we note that $Q_{a}=P+1, Q_{f}=1, \Theta_{a}^{q}(\mu)=\mu_{q}, 1 \leq q \leq P$, $\Theta_{a}^{P+1}=1, \Theta_{f}^{1}(\mu)=1$ and

$$
a^{q}(w, v)=\int_{\mathscr{R}_{q}} \nabla w \cdot \nabla v d \Omega, \quad 1 \leq q \leq P+1, \quad f^{1}(v)=\int_{\Gamma_{\text {base }}} v d \Gamma
$$

\subsection{Main components of computational reduction techniques}

By computational reduction techniques (CRT) we denote problem-dependent methods which aim at reducing the dimension of the algebraic system arising from the discretization of a given PDE problem, for instance $(20.2)$.

The reduced solution is obtained through a projection onto a small subspace made by global basis functions, constructed for the specific problem at hand, rather than onto a large space of generic basis functions (either local, like in finite elements or global, like in spectral methods).

In order to highlight the essential features of CRTs, in this section we rely on the strong form (20.1) of the PDE problem. In fact, the CRTs we are going to introduce can be built upon any discretization technique described throughout the book, and not necessarily on those based on the weak form of the PDE problem.

The goal of a computational reduction technique for parametrized PDE problems is to compute, in a cheap way, a low-dimensional approximation of the PDE solution. The most common choices, like proper orthogonal decomposition (POD) or (greedy) reduced basis (RB) methods, seek a reduced solution through a projection onto suitable low-dimensional subspaces $^{2}$. The essential constituents of a computational reduction technique can be described as follows:

- High-fidelity discretization technique: as previously observed, a CRT is not intended to replace a high-fidelity (sometimes denoted as truth) discretization method (obtained e.g. by any kind of Galerkin method).

In the case of problem (20.1), the truth, high-fidelity approximation can be expressed in the following compact way: given $\mu \in \mathscr{D}$, evaluate $s_{h}(\mu)=f\left(u_{h}(\mu)\right)$ where $u_{h}(\mu) \in V^{N_{h}}$ is such that

$$
L_{h}(\mu) u_{h}(\mu)=F_{h}(\mu)
$$

${ }^{2}$ Indeed, several CRTs, like POD, were originally introduced in order to speed-up the solution of complex time-dependent problems, like those modelling turbulent flows, without being specifically designed for parametrized problems (otherwise said, time was considered as the only parameter). Here $V^{N_{h}} \subset V$ is a finite-dimensional space of very large dimension $N_{h}, L_{h}(\mu)$ a suitable discrete operator and $F_{h}(\mu)$ a given term. Recall that compliance means $J=f$.

For instance, assume that the truth approximation is based on the following Galerkin high-fidelity approximation of problem $(20.2):$ find $u_{h}(\mu) \in V^{N_{h}}$ such that

$$
a\left(u_{h}(\mu), v_{h} ; \mu\right)=f\left(v_{h} ; \mu\right) \quad \forall v_{h} \in V^{N_{h}} .
$$

Moreover, let us introduce the injection operator $Q_{h}: V^{N_{h}} \rightarrow V$, and its adjoint $Q_{h}^{\prime}: V^{\prime} \rightarrow\left(V^{N_{h}}\right)^{\prime}$ between the dual spaces. The Galerkin problem (20.15) reads

$$
Q_{h}^{\prime}\left(L(\mu) Q_{h} u_{h}(\mu)-F(\mu)\right)=0
$$

which corresponds to $(20.15)$ upon defining

$$
L_{h}(\mu)=Q_{h}^{\prime} L(\mu) Q_{h}, \quad F_{h}(\mu)=Q_{h}^{\prime} F(\mu)
$$

Note that $\left(L_{h}(\mu)\right)^{-1}=\Pi_{h}(L(\mu))^{-1} \Pi_{h}^{\prime}$, where $\Pi_{h}: V \rightarrow V^{N_{h}}$ is the $L^{2}$-projection operator and $\Pi_{h}^{\prime}:\left(V^{N_{h}}\right)^{\prime} \rightarrow V^{\prime}$ its adjoint. It follows directly from our assumptions on $a, f$, and $V^{N_{h}}$ that (20.16) admits a unique solution. Let us assume that

$$
\left\|u(\mu)-u_{h}(\mu)\right\|_{V} \leq \mathscr{E}(h) \quad \forall \mu \in \mathscr{D},
$$

$\mathscr{E}(h)$ being an estimate of the discretization error, which can be made as small as desired by choosing a suitable discretization space. Moreover, we define the coercivity and continuity constants (related to the subspace $\left.V^{N_{h}}\right)$ as

$$
\alpha^{N_{h}}(\mu)=\inf _{w \in V^{N_{h}}} \frac{a(w, w ; \mu)}{\|w\|_{V}^{2}}, \quad \gamma^{N_{h}}(\mu)=\sup _{w \in V^{N_{h}}} \sup _{v \in V^{N_{h}}} \frac{a(w, v ; \mu)}{\|w\|_{V}\|v\|_{V}}
$$

respectively. By $(20.5)$, from the continuity and coercivity of $a$ it follows that

$$
\alpha^{N_{h}}(\mu) \geq \alpha(\mu), \quad \gamma^{N_{h}}(\mu) \leq \gamma(\mu) \quad \forall \mu \in \mathscr{D}
$$

- (Galerkin) projection: a CRT usually consists in selecting a reduced basis of few high-fidelity PDE solutions $\left\{u_{h}\left(\mu^{i}\right)\right\}_{i=1}^{N}$ (called snapshots) and seeking a reduced approximation $u_{N}(\mu)$ expressed as a linear combination of these basis functions. The coefficients of this combination are determined through a projection of the equations onto the reduced space

$$
V_{N}=\operatorname{span}\left\{u_{h}\left(\mu^{i}\right), i=1, \ldots, N\right\}
$$

with $N=\operatorname{dim}\left(V_{N}\right) \ll N_{h}$. The reduced problem reads, therefore: given $\mu \in \mathscr{D}$, evaluate $s_{N}(\mu)=f\left(u_{N}(\mu)\right)$, where $u_{N}(\mu) \in V_{N}$ solves

$$
a\left(u_{N}(\mu), v_{N} ; \mu\right)=f\left(v_{N} ; \mu\right) \quad \forall v_{N} \in V_{N}
$$

The smaller $N$, the cheaper the reduced problem to solve. We remark that our $\mathrm{RB}$ field and RB output approximate, for given $N_{h}$, the high-fidelity solution $u_{h}(\mu)$ and output $s_{h}(\mu)$ (hence, indirectly, $u(\mu)$ and $\left.s(\mu)\right)$.

As before, we can interpret $(20.21)$ with the aid of suitable operators as

$$
L_{N}(\mu) u_{N}(\mu)=F_{N}(\mu)
$$

Indeed, let us introduce the injection operator $Q_{N}: V_{N} \rightarrow V^{N_{h}}$, and its adjoint $Q_{N}^{\prime}$ : $\left(V^{N_{h}}\right)^{\prime} \rightarrow V_{N}^{\prime}$ operating between the dual spaces. Then, since

$$
Q_{N}^{\prime}\left(L_{h}(\mu) Q_{N} u_{N}(\mu)-F_{h}(\mu)\right)=0
$$

we can obtain $(20.22)$ from (20.23) by identifying

$$
L_{N}(\mu)=Q_{N}^{\prime} L_{h}(\mu) Q_{N}
$$

$$
F_{N}(\mu)=Q_{N}^{\prime} F_{h}(\mu)
$$

Similarly to what was done before, here $\left(L_{N}(\mu)\right)^{-1}=\Pi_{N}\left(L_{h}(\mu)\right)^{-1} \Pi_{N}^{\prime}$, where $\Pi_{N}: V^{N_{h}} \rightarrow V_{N}$ is the $L^{2}$-projection operator and $\Pi_{N}^{\prime}: V_{N}^{\prime} \rightarrow\left(V^{N_{h}}\right)^{\prime}$.

- Offline/Online procedure: under suitable assumptions (see Sect. 20.3.3) the extensive generation of the snapshots database can be performed Offline once, and is completely decoupled from each new subsequent input-output Online query. Clearly, during the Online stage, the goal is to solve the reduced problem for parameter instances $\mu \in \mathscr{D}$ not selected during the Offline stage. In addition, the expensive Offline computations have to be amortized over the Online stage $-$ in the RB context the break-even point is usually reached with $\mathscr{O}\left(10^{2}\right)$ Online queries.

- Error estimation procedure: sharp, inexpensive bounds $\Delta_{N}(\mu)$ such that

$$
\left\|u_{h}(\mu)-u_{N}(\mu)\right\|_{V} \leq \Delta_{N}(\mu) \quad \forall \mu \in \mathscr{D}, N=1, \ldots, N_{\max }
$$

may be available [PR07], as well as output error bounds $\Delta_{N}^{s}(\mu)$ such that

$$
\left|s_{h}(\mu)-s_{N}(\mu)\right| \leq \Delta_{N}^{s}(\mu)
$$

These error estimators might also be employed to generate a clever parameter sampling during the construction of the reduced space, as we will see in Sect. 20.5.1. Their construction in the case of elliptic coercive problems is describe in detail in Sect. $20.7$.

By putting (20.19) and (20.25) together we finally obtain, for all $\mu \in \mathscr{D}$, the error bound

$$
\left\|u(\mu)-u_{N}(\mu)\right\|_{V} \leq\left\|u(\mu)-u_{h}(\mu)\right\|_{V}+\left\|u_{h}(\mu)-u_{N}(\mu)\right\|_{V} \leq \mathscr{E}(h)+\Delta_{N}(\mu)
$$

In the following section we provide further details on the construction of a reduced basis approximation, which is the main goal of this chapter. 

\subsection{The reduced basis method}

We now specify the general features presented in Sect. $20.2$ for the case of the reduced basis method. Reduced Basis (RB) discretization is, in brief, a Galerkin (sometimes, a Petrov-Galerkin) projection on an $N$-dimensional approximation space $V_{N}$ that approximates the manifold

$$
\mathscr{M}_{h}=\left\{u_{h}(\mu) \in V^{N_{h}}: \mu \in \mathscr{D}\right\}
$$

given by the set of fields generated as the input varies over the whole parameter domain $\mathscr{D}$. We assume that this manifold is sufficiently smooth; in the case of a single parameter, the parametrically induced manifold is a one-dimensional filament within the infinite-dimensional space which characterizes all possible solutions to the given $\mathrm{PDE}$. We depict the retained snapshots in Fig. $20.2$.

If indeed the manifold is low-dimensional and smooth (a point we will return to later), we expect any point of the manifold $-$ any solution $u_{h}(\mu)$ for some $\mu$ in $\mathscr{D}$ - to be well approximated in terms of relatively few retained snapshots. However, we must ensure that not only we can choose our retained snapshots optimally (see Sect. $20.5$ ), but also that we can $(i)$ select a good combination of the available retained snapshots, $(i i)$ represent the retained snapshots in a stable reduced basis and (iii) obtain the associated basis coefficients efficiently. These three points will be discussed in the following section.

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-236.jpg?height=338&width=607&top_left_y=784&top_left_x=153)

Fig. 20.2. The "snapshots" $u_{h}\left(\mu^{n}\right), 1 \leq n \leq N$, on the parametric manifold $\mathscr{M}_{h}$ 

\subsubsection{RB Spaces}

We restrict our attention to reduced spaces generated by snapshots that represent highfidelity solutions of the $\mathrm{PDE}^{3}$, and review the construction of the RB approximation in the elliptic case, also from an algebraic standpoint. Given a positive integer $N_{\max }$, we define a (hierarchical) sequence of RB spaces $V_{N}^{R B}, 1 \leq N \leq N_{\max }$, such that each $V_{N}^{R B}$ is an $N$-dimensional subspace of $V^{N_{h}}$. We further suppose that

$$
V_{1}^{R B} \subset V_{2}^{R B} \subset \cdots V_{N_{\max }}^{R B} \subset V^{N_{h}}
$$

As we shall see, the nesting or hierarchy condition $(20.27)$ is important in ensuring memory efficiency of the resulting RB approximation.

In order to define a (hierarchical) sequence of spaces $V_{N}^{R B}, 1 \leq N \leq N_{\max }$, we first introduce, for given $N \in\left\{1, \ldots, N_{\max }\right\}$, a sample

$$
S_{N}=\left\{\mu^{1}, \ldots, \mu^{N}\right\}
$$

of parameter points $\mu^{n} \in \mathscr{D}, 1 \leq n \leq N$, to be properly selected (e.g. by means of the greedy procedure that will be presented in Sect. 20.5.1). This produces corresponding snapshots $u_{h}\left(\mu^{n}\right) \in V^{N_{h}}$. The associated greedy-RB spaces are thus given by

$$
V_{N}^{R B}=\operatorname{span}\left\{u_{h}\left(\mu^{n}\right), 1 \leq n \leq N\right\}
$$

In the rest of the section the superscript ${ }^{R B}$ will often be omitted for ease of notation. We observe that, by construction, the spaces $V_{N}$ satisfy $(20.27)-$ i.e. RB spaces (20.29) are hierarchical - and the samples $(20.28)$ are nested, that is $S_{1}=\left\{\mu^{1}\right\} \subset S_{2}=$ $\left\{\mu^{1}, \mu^{2}\right\} \subset \cdots \subset S_{N_{\max }}$

\subsubsection{Galerkin projection}

For our particular class of equations, Galerkin projection is a natural choice. Thus, given $\mu \in \mathscr{D}$, we evaluate $s_{N}(\mu)=J\left(u_{N}(\mu)\right)$, where $u_{N}(\mu) \in V_{N} \subset V^{N_{h}}$ is such that

$$
a\left(u_{N}(\mu), v_{N} ; \mu\right)=f\left(v_{N} ; \mu\right) \quad \forall v_{N} \in V_{N} .
$$

From now on, problem (20.30) will be called Galerkin Reduced Basis (G-RB) approximation of the given problem (20.2). By comparison between $(20.16)$ and $(20.30)$ we immediately obtain the property

$$
a\left(u_{h}(\mu)-u_{N}(\mu), v_{N} ; \mu\right)=0 \quad \forall v_{N} \in V_{N}
$$

which is a Galerkin orthogonality property for the reduced problem (see Chap. 4).

${ }^{3}$ This is the most common way of constructing reduced subspaces, which are also called Lagrange RB spaces. Other known methods are based on Taylor [Por85] and Hermite [IR98] spaces. Moreover, we obtain from (20.31) and Céa's lemma - see Sect. $4.2$ - the classical optimality result in the energy norm $(20.9)$

$$
\left\|u_{h}(\mu)-u_{N}(\mu)\right\| \mu \leq \inf _{w \in V_{N}}\left\|u_{h}(\mu)-w\right\| \mu
$$

In other words, in the energy norm the Galerkin procedure automatically selects the best combination of snapshots. It is also clear that

$$
s_{h}(\mu)-s_{N}(\mu)=\left\|u_{h}(\mu)-u_{N}(\mu)\right\|_{\mu}^{2},
$$

i.e. the output converges as the "square" of the energy error. In fact, using the compliance assumption, we can write

$$
\begin{aligned}
s_{h}(\mu) &-s_{N}(\mu)=a\left(u_{h}(\mu), u_{h}(\mu) ; \mu\right)-a\left(u_{N}(\mu), u_{N}(\mu) ; \mu\right) \\
&=a\left(u_{h}(\mu), u_{h}(\mu)-u_{N}(\mu) ; \mu\right)+a\left(u_{h}(\mu), u_{N}(\mu) ; \mu\right)-a\left(u_{N}(\mu), u_{N}(\mu) ; \mu\right) \\
&=a\left(u_{h}(\mu)-u_{N}(\mu), u_{h}(\mu)-u_{N}(\mu) ; \mu\right)+a\left(u_{h}(\mu)-u_{N}(\mu), u_{N}(\mu) ; \mu\right)
\end{aligned}
$$

where the second term in the last row vanishes thanks to (20.31). Although this result depends critically on the compliance assumption, a generalisation via adjoint approximations to the non-compliant case is possible; see Sect. $20.8$.

Let us remark that, by choosing the $V$-norm (20.11) instead of $(20.32)$, we would find

$$
\left\|u_{h}(\mu)-u_{N}(\mu)\right\|_{V} \leq\left(\frac{\bar{\gamma}}{\alpha_{0}}\right)^{1 / 2} \inf _{w \in V_{N}}\left\|u_{h}(\mu)-w\right\|_{V}
$$

$\bar{\gamma}$ and $\alpha_{0}$ being the uniform continuity and coercivity constants defined in $(20.5)$.

We now consider the discrete equations associated with the Galerkin approximation $(20.30)$. We must first choose an appropriate basis for our space; note that an ill-advised choice of the RB basis can lead to very poorly conditioned systems. Moreover, if $V_{N}$ provides rapid convergence, the snapshots of $(20.29)$ will be increasingly co-linear as $N$ increases, by construction. To avoid this situation (and generate an independent set of snapshots) we apply the Gram-Schmidt process [Mey00, TI97] in the $(\cdot, \cdot)_{V}$ inner product to the snapshots $u_{h}\left(\mu^{n}\right), 1 \leq n \leq N_{\max }$, to obtain mutually orthonormal functions $\zeta_{n}, 1 \leq n \leq N_{\max }:\left(\zeta_{n}, \zeta_{m}\right)_{V}=\delta_{n m}, 1 \leq n, m \leq N_{\max }$, where $\delta_{n m}$ is the Kronecker delta symbol. We then choose the set $\left\{\zeta_{n}\right\}_{n=1, \ldots, N}$ as our basis of $V_{N}, 1 \leq N \leq N_{\max }$

We now insert

$$
u_{N}(\mu)=\sum_{m=1}^{N} u_{N}^{(m)}(\mu) \zeta_{m}
$$

and then $v_{N}=\zeta_{n}, 1 \leq n \leq N$, into $(20.30)$ to obtain the RB algebraic system

$$
\sum_{m=1}^{N} a\left(\zeta_{m}, \zeta_{n} ; \mu\right) u_{N}^{(m)}(\mu)=f\left(\zeta_{n} ; \mu\right)
$$

for the RB coefficients $u_{N}^{(m)}(\mu), 1 \leq m, n \leq N$. We can subsequently evaluate the $\mathrm{RB}$ output prediction as

$$
s_{N}(\mu)=\sum_{m=1}^{N} u_{N}^{(m)}(\mu) f\left(\zeta_{m} ; \mu\right)
$$

By using the Rayleigh quotient, as explained in (4.51), we can show that the condition number of the matrix $a\left(\zeta_{m}, \zeta_{n} ; \mu\right), 1 \leq n, m \leq N$, is bounded by $\gamma(\mu) / \alpha(\mu)$, independently of $N$ and $N_{h}$, owing to the orthogonality of the $\left\{\zeta_{n}\right\}$ and to $(20.5)$; see e.g. [PR07] for further details. For the sake of simplicity, from now on we consider the case where $f$ does not depend on the parameter $\mu$.

\subsubsection{Offline-Online computational procedure}

System (20.36) is nominally of small size, yet it involves entities $\zeta_{n}, 1 \leq n \leq N$, associated with our $N_{h}$-dimensional high-fidelity approximation space. If we have to invoke high-fidelity fields in order to form the RB stiffness matrix for each new value of $\mu$ the marginal cost per input-output evaluation $\mu \rightarrow s_{N}(\mu)$ will remain unacceptably large. Fortunately, the crucial assumption of affine parametric dependence will cause a major increase in computational speed. In particular, thanks to $(20.6)$, system (20.36) can be expressed as

$$
\sum_{m=1}^{N}\left(\sum_{q=1}^{Q_{a}} \Theta_{a}^{q}(\mu) \mathbb{A}_{N}^{q}\right) \mathbf{u}_{N}(\mu)=\mathbf{f}_{N}
$$

and $(20.37)$ reads

$$
s_{N}(\mu)=\mathbf{f}_{N} \cdot \mathbf{u}_{N}(\mu)
$$

where $\left(\mathbf{u}_{N}(\mu)\right)_{m}=u_{N}^{(m)}(\mu),\left(\mathbb{A}_{N}^{q}\right)_{m n}=a^{q}\left(\zeta_{n}, \zeta_{m}\right),\left(\mathbf{f}_{N}\right)_{n}=f\left(\zeta_{n}\right)$, for $1 \leq m, n \leq N$

The computation thus entails an expensive $\mu$-independent Offline stage, performed only once, and an inexpensive Online stage for any chosen parameter value $\mu \in \mathscr{D}$ :

- in the Offline stage, we first compute the $u_{h}\left(\mu^{n}\right)$, and subsequently the $\zeta_{n}$ by GramSchmidt orthonormalization, $1 \leq n \leq N_{\max } ;$ we then form and store the terms

$$
\begin{gathered}
f\left(\zeta_{n}\right), \quad 1 \leq n \leq N_{\max } \\
a^{q}\left(\zeta_{n}, \zeta_{m}\right), \quad 1 \leq n, m \leq N_{\max }, 1 \leq q \leq Q_{a}
\end{gathered}
$$

The Offline operation count depends on $N_{\max }, Q_{a}$, and $N_{h}$;

- in the Online stage, we retrieve $(20.41)$ to form

$$
\sum_{q=1}^{Q_{a}} \Theta_{a}^{q}(\mu) a^{q}\left(\zeta_{n}, \zeta_{m}\right), \quad 1 \leq n, m \leq N
$$

we solve the resulting $N \times N$ stiffness system $(20.38)$ to obtain the $u_{N}^{(m)}(\mu)$, $1 \leq m \leq N ;$ finally, we access $(20.40)$ to evaluate the output $(20.37)$. The Online operation count is $O\left(Q_{a} N^{2}\right)$ to perform the sum $(20.42), O\left(N^{3}\right)$ to invert $(20.38)-$ note that the RB stiffness matrix is full - and finally $O(N)$ to evaluate the inner product $(20.37)$. The Online storage (the data archived in the Offline stage) is only $O\left(Q_{a} N_{\max }^{2}\right)+O\left(N_{\max }\right)$, because of the hierarchical condition $(20.27)$ : for any given $N$, we extract the necessary $N \times N$ RB matrices (resp. $N$-vectors) as principal submatrices (resp. subvectors) of the corresponding $N_{\max } \times N_{\max }\left(\right.$ resp. $\left.N_{\max }\right)$ quantities.

The Online cost (operation count and storage) to evaluate $\mu \rightarrow s_{N}(\mu)$ is thus independent of $N_{h}$. The consequence is two-fold: first, if $N$ is indeed small, we will achieve very fast response both in real-time and many-query contexts; secondly, we may choose $N_{h}$ large enough, to make sure that the error $\left\|u(\mu)-u_{h}(\mu)\right\|_{V}$ is very small, without affecting the Online marginal cost.

\subsection{Algebraic and geometric interpretations of the RB problem}

We now discuss the relationship between the Galerkin Reduced Basis (G-RB) approximation (20.30) and the Galerkin high-fidelity approximation (20.16) from both an algebraic and a geometric point of view.

Let us denote by $\mathbf{u}_{h}(\mu) \in \mathbb{R}^{N_{h}}$ and $\mathbf{u}_{N}(\mu) \in \mathbb{R}^{N}$ the vectors of degrees of freedom associated to the functions $u_{h}(\mu) \in V^{N_{h}}$ and $u_{N}(\mu) \in V_{N}$, respectively, which are given by

$$
\mathbf{u}_{h}(\mu)=\left(u_{h}^{(1)}(\mu), \ldots, u_{h}^{\left(N_{h}\right)}(\mu)\right)^{T}, \quad \mathbf{u}_{N}(\mu)=\left(u_{N}^{(1)}(\mu), \ldots, u_{N}^{(N)}(\mu)\right)^{T}
$$

Let $\left\{\tilde{\varphi}^{r}\right\}_{r=1}^{N_{h}}$ denote the standard FE basis, orthogonal with respect to a discrete scalar product

$$
\left(u_{h}, v_{h}\right)_{h}=\sum_{r=1}^{N_{h}} w_{r} u_{h}\left(\mathbf{x}_{r}\right) v_{h}\left(\mathbf{x}_{r}\right)
$$

$\left\{\mathbf{x}_{r}\right\}_{r=1}^{N_{h}}$ being the set of $\mathrm{FE}$ nodes such that $\tilde{\varphi}^{r}\left(\mathbf{x}_{s}\right)=\delta_{r s}$ and $\left\{w_{r}\right\}_{r=1}^{N_{h}}$ a set of weights such that $\sum_{r=1}^{N_{h}} w_{r}=|\Omega|$, for $r, s=1, \ldots, N_{h} ;$ note that other choices of scalar products can be made. It is useful to normalize these basis functions by defining

$$
\varphi^{r}=\frac{1}{\sqrt{w_{r}}} \tilde{\varphi}^{r}, \quad\left(\varphi^{r}, \varphi^{s}\right)_{h}=\delta_{r s}, \quad r, s=1, \ldots, N_{h}
$$

As we saw in Chapter $4-$ see relation (4.7) - we can consider the following bijection between the spaces $\mathbb{R}^{N_{h}}$ and $V^{N_{h}}$ :

$$
\left\{\begin{array}{l}
\mathbf{v}_{h} \in \mathbb{R}^{N_{h}} \leftrightarrow v_{h} \in V^{N_{h}} \\
\mathbf{v}_{h}=\left(v_{h}^{(1)}, \ldots, v_{h}^{\left(N_{h}\right)}\right)^{T} \leftrightarrow v_{h}(\mathbf{x})=\sum_{r=1}^{N_{h}} v_{h}^{(r)} \varphi^{r}(\mathbf{x})
\end{array}\right.
$$

For ease of notation, we will express the bijection $(20.44)$ as follows: $\mathbf{v}_{h} \in \mathbb{R}^{N_{h}} \rightarrow$ $I_{h} \mathbf{v}_{h}=v_{h} \in V_{h}, I_{h}^{T} v_{h}=\mathbf{v}_{h}$. Similarly, for the reduced basis approximation we use the notation $\mathbf{v}_{N} \in \mathbb{R}^{N} \rightarrow I_{N} \mathbf{v}_{N}=v_{N} \in V_{N}, I_{N}^{T} v_{N}=\mathbf{v}_{N}$. Thanks to the ortonormality of the basis functions, $v_{h}^{(r)}=\left(v_{h}, \varphi^{r}\right)_{h}, \quad r=1, \ldots, N_{h}$

Using bijection (20.44), the following algebraic relations hold:

$$
\left.\left(\mathbf{u}_{h}, \mathbf{v}_{h}\right)_{2}=\left(u_{h}, v_{h}\right)_{h} \quad \forall \mathbf{u}_{h}, \mathbf{v}_{h} \in \mathbb{R}^{N_{h}} \text { (equivalently, } \forall u_{h}, v_{h} \in V^{N_{h}}\right) .
$$

Indeed:

$$
\begin{aligned}
\left(u_{h}, v_{h}\right)_{h} &=\left(\sum_{r=1}^{N_{h}} u_{h}^{(r)} \varphi^{r}, \sum_{s=1}^{N_{h}} v_{h}^{(s)} \varphi^{s}\right)_{h} \\
&=\sum_{r, s=1}^{N_{h}} u_{h}^{(r)} v_{h}^{(s)}\left(\varphi^{r}, \varphi^{s}\right)_{h}=\sum_{r, s=1}^{N_{h}} u_{h}^{(r)} v_{h}^{(r)}=\left(\mathbf{u}_{h}, \mathbf{v}_{h}\right)_{2}
\end{aligned}
$$

\subsubsection{Algebraic interpretation of the (G-RB) problem}

We first discuss the algebraic connection between the (G-RB) problem (20.30) and the Galerkin high-fidelity approximation (20.16), which has strong consequences on the computational aspects related with RB methods.

In matrix form, the (G-RB) problem (20.36) can be written as

$$
\mathbb{A}_{N}(\mu) \mathbf{u}_{N}(\mu)=\mathbf{f}_{N}
$$

with $\mathbf{f}_{N}=\left(f_{N}^{(1)}, \ldots, f_{N}^{(N)}\right)^{T}, f_{N}^{(k)}=f\left(\zeta_{k}\right),\left(\mathbb{A}_{N}(\mu)\right)_{k m}=a\left(\zeta_{m}, \zeta_{k} ; \mu\right)$, for $k, m=$ $1, \ldots, N$. On the other hand, the Galerkin high-fidelity approximation (20.16) reads in matrix form as

$$
\mathbb{A}_{h}(\mu) \mathbf{u}_{h}(\mu)=\mathbf{f}_{h}
$$

with $\mathbf{f}_{h}=\left(f_{h}^{(1)}, \ldots, f_{h}^{\left(N_{h}\right)}\right)^{T}, f_{h}^{(r)}=f\left(\varphi^{r}\right),\left(\mathbb{A}_{h}(\mu)\right)_{r s}=a\left(\varphi^{s}, \varphi^{r} ; \mu\right)$, for $r, s=$ $1, \ldots, N_{h}$

For the sake of notation, we omit the dependence on $\mu$ in the rest of the section.

Let $\mathbb{V} \in \mathbb{R}^{N_{h} \times N}$ be the transformation matrix whose entries are

$$
(\mathbb{V})_{r k}=\left(\zeta_{k}, \varphi^{r}\right)_{h}, \quad r=1, \ldots, N_{h}, k=1, \ldots, N
$$

Using this matrix, we can easily obtain the following algebraic identities:

$$
\mathbf{f}_{N}=\mathbb{V}^{T} \mathbf{f}_{h}, \quad \mathbb{A}_{N}=\mathbb{V}^{T} \mathbb{A}_{h} \mathbb{V}
$$

which represent the algebraic counterparts of the operator identities $(20.24)$ (see Fig. 20.3). Indeed,

$$
\begin{aligned}
\left(\mathbb{V}^{T} \mathbb{A}_{h} \mathbb{V}\right)_{k m} &=\sum_{r, s=1}^{N_{h}}(\mathbb{V})_{k r}^{T}\left(\mathbb{A}_{h}\right)_{r s}(\mathbb{V})_{s m}=\sum_{r, s=1}^{N_{h}}\left(\zeta_{k}, \varphi^{r}\right)_{h} a\left(\varphi^{s}, \varphi^{r}\right)\left(\zeta_{m}, \varphi^{s}\right)_{h} \\
&=a\left(\sum_{s=1}^{N_{h}}\left(\zeta_{m}, \varphi^{s}\right)_{h} \varphi^{s}, \sum_{r=1}^{N_{h}}\left(\zeta_{k}, \varphi^{r}\right)_{h} \varphi^{r}\right)=a\left(\zeta_{m}, \zeta_{k}\right)=\left(\mathbb{A}_{N}\right)_{k m}
\end{aligned}
$$



![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-242.jpg?height=157&width=439&top_left_y=112&top_left_x=238)

Fig. 20.3. Schematic representation of how to assemble the RB "stiffness" matrix

and, in the same way,

$$
\begin{aligned}
\left(\mathbb{V}^{T} \mathbf{f}_{h}\right)^{(k)} &=\sum_{r=1}^{N_{h}}(\mathbb{V})_{k r}^{T}\left(\mathbf{f}_{h}\right)^{(r)}=\sum_{r=1}^{N_{h}}\left(\zeta_{k}, \varphi^{r}\right)_{h} f\left(\varphi^{r}\right) \\
&=f\left(\sum_{r=1}^{N_{h}}\left(\zeta_{k}, \varphi^{r}\right)_{h} \varphi^{r}\right)=f\left(\zeta_{k}\right)=\left(\mathbf{f}_{N}\right)_{k} .
\end{aligned}
$$

Thanks to $(20.49)$, each $\mu$-independent $\mathrm{RB}$ "stiffness" matrix $\mathbb{A}_{N}^{q}$ can be assembled once the corresponding high-fidelity "stiffness" matrix $\mathbb{A}_{h}^{q}$ has been computed.

The (vector representation of the) error between the solution of the (G-RB) problem and the Galerkin high-fidelity approximation is

$$
\mathbf{e}_{N}=\mathbf{u}_{h}-\mathbb{V} \mathbf{u}_{N}
$$

Similarly, the (vector representation of the) high-fidelity residual of the (G-RB) solution reads

$$
\mathbf{r}_{h}\left(\mathbf{u}_{N}\right)=\mathbf{f}_{h}-\mathbb{A}_{h} \mathbb{V} \mathbf{u}_{N}
$$

The following lemma provides the main algebraic connection between the $(\mathrm{G}-\mathrm{RB})$ problem and the Galerkin high-fidelity approximation:

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-242.jpg?height=232&width=724&top_left_y=852&top_left_x=96)

Proof. Equation (20.53) follows directly from (20.51) and (20.47).

By left multiplication of $(20.47)$ by $\mathbb{V}^{T}$, we immediately obtain $(20.54)$, thanks to (20.49).

Finally, (20.55) follows from (20.52) using the identities (20.49) and problem (20.46). Note that condition $(20.53)$ is the algebraic counterpart of the Galerkin orthogonality property $(20.31)$ valid for the (G-RB) problem.

In summary, for a given matrix $\mathbb{V}$ of reduced bases, the Galerkin Reduced Basis (G-RB) problem (20.46) can be formally obtained as follows:

If $\mathbb{A}_{h}$ is symmetric and positive definite, then the G-RB solution satisfies the following residual minimization property:

$$
\mathbf{u}_{N}=\arg \min _{\tilde{\mathbf{u}}_{N} \in \mathbb{R}^{N}}\left\|\mathbf{r}_{h}\left(\tilde{\mathbf{u}}_{N}\right)\right\|_{\mathbb{A}_{h}^{-1}}^{2}
$$

In fact, by indicating with $\mathbb{K}^{1 / 2}$ the square root of a (symmetric and positive definite) matrix $\mathbb{K}$, we have

$$
\begin{aligned}
\left\|\mathbf{r}_{h}\left(\tilde{\mathbf{u}}_{N}\right)\right\|_{\mathbb{A}_{h}^{-1}}^{2} &=\left(\mathbf{f}_{h}-\mathbb{A}_{h} \mathbb{V} \mathbf{u}_{N}, \mathbf{f}_{h}-\mathbb{A}_{h} \mathbb{V} \mathbf{u}_{N}\right)_{\mathbb{A}_{h}^{-1}}=\\
&=\left(\mathbb{A}_{h}^{-1 / 2} \mathbf{f}_{h}-\mathbb{A}_{h}^{1 / 2} \mathbb{V} \mathbf{u}_{N}, \mathbb{A}_{h}^{-1 / 2} \mathbf{f}_{h}-\mathbb{A}_{h}^{1 / 2} \mathbb{V} \mathbf{u}_{N}\right)
\end{aligned}
$$

This can be regarded as the least-squares solution of the system $\mathbb{A}_{h}^{1 / 2} \mathbb{V} \mathbf{u}_{N}=\mathbb{A}_{h}^{-1 / 2} \mathbf{f}_{h}$, whose corresponding normal equations ${ }^{4}$ are

$$
\mathbb{V}^{T} \mathbb{A}_{h}^{1 / 2} \mathbb{A}_{h}^{1 / 2} \mathbb{V} \mathbf{u}_{N}=\mathbb{V}^{T} \mathbb{A}_{h}^{1 / 2} \mathbb{A}_{h}^{-1 / 2} \mathbf{f}_{h}=\mathbb{V}^{T} \mathbf{f}_{h}
$$

Note that the latter coincide with the (G-RB) problem (20.46).

\subsubsection{Geometric interpretation of the (G-RB) problem}

We can also characterize from a geometric standpoint the RB approximation obtained by solving the (G-RB) problem, as well as the error $\mathbf{e}_{N}=\mathbf{u}_{h}-\mathbb{V} \mathbf{u}_{N}$ between the solution of the (G-RB) problem and the Galerkin high-fidelity approximation.

${ }^{4}$ We recall that, given $\mathbf{c} \in \mathbb{R}^{N_{h}}$ and $\mathbb{B} \in \mathbb{R}^{N_{h} \times N}$, the overdetermined system $\mathbb{B} \tilde{\mathbf{u}}=\mathbf{b}$ can be solved in the least-squares sense, by seeking $\mathbf{u}=\arg \min _{\tilde{\mathbf{u}} \in \mathbb{R}^{N}}\|\mathbf{c}-\mathbb{B} \tilde{\mathbf{u}}\|_{2}^{2}$. The solution is unique provided that the $N$ columns of $\mathbb{B}$ are linearly independent, and can be obtained through the following normal equations:

$$
\left(\mathbb{B}^{T} \mathbb{B}\right) \mathbf{u}=\mathbb{B}^{T} \mathbf{c}
$$

To this end, we exploit the fact that the transformation matrix $\mathbb{V}$ defined by $(20.48)$ identifies an orthogonal projection on the reduced subspace $\mathbf{V}_{N}=\operatorname{span}\left\{\mathbf{v}_{1}, \ldots, \mathbf{v}_{N}\right\}$ of $\mathbb{R}^{N_{h}}$ generated by the column vectors of the matrix $\mathbb{V}$. Then $\operatorname{dim}\left(\mathbf{V}_{N}\right)=N$ because of the linear independence of the columns of $\mathbb{V}$.

Assume that the basis functions $\left\{\zeta_{k}\right\}_{k=1, \ldots, N}$ are orthonormal with respect to the scalar product $(\cdot, \cdot)_{h}$, that is

$$
\left(\zeta_{k}, \zeta_{m}\right)_{h}=\sum_{j=1}^{N_{h}} w_{j} \zeta_{k}\left(x_{j}\right) \zeta_{m}\left(x_{j}\right)=\delta_{k m}
$$

Then

$$
\mathbb{V}^{T} \mathbb{V} \in \mathbb{R}^{N \times N}, \quad \mathbb{V}^{T} \mathbb{V}=\mathbb{I}_{N}
$$

where $\mathbb{I}_{N}$ denotes the identity matrix of dimension $N$.

As a matter of fact

$$
\left(\mathbb{V}^{T} \mathbb{V}\right)_{m k}=\left(\zeta_{m}, \zeta_{k}\right)_{h} \quad \forall k, m=1, \ldots, N
$$

1. The matrix $\boldsymbol{\Pi}=\mathbb{V} \mathbb{V}^{T} \in \mathbb{R}^{N_{h} \times N_{h}}$ is a projection matrix from the whole space
$\mathbb{R}^{N_{h}}$ onto the subspace $\mathbf{V}_{N}$;
Tlo watwix

2. The matrix $\mathbb{I}_{N_{h}}-\boldsymbol{\Pi}=\mathbb{I}_{N_{h}}-\mathbb{V} \mathbb{V}^{T} \in \mathbb{R}^{N_{h} \times N_{h}}$ is a projection matrix from the

whole space $\mathbb{R}^{N_{h}}$ onto the space $\mathbf{V}_{N}^{\perp}$, which is the subspace of $\mathbb{R}^{N_{h}}$ orthogonal

Proof. Property 1 is a direct consequence of the orthonormality property (20.58). In fact:

$$
\forall \mathbf{w}_{N} \in \mathbf{V}_{N} \text { there exists } \mathbf{v}_{N} \in \mathbb{R}^{N} \text { s.t. } \mathbf{w}_{N}=\mathbb{V} \mathbf{v}_{N} .
$$

Then, $\forall \mathbf{v}_{h} \in \mathbb{R}^{N_{h}}, \forall \mathbf{w}_{N} \in \mathbf{V}_{N}$

$$
\left(\boldsymbol{\Pi} \mathbf{v}_{h}, \mathbf{w}_{N}\right)_{2}=\left(\boldsymbol{\Pi} \mathbf{v}_{h}, \mathbb{V} \mathbf{v}_{N}\right)_{2}=\left(\mathbb{V}^{T} \mathbf{v}_{h}, \mathbb{V}^{T} \mathbb{V} \mathbf{v}_{N}\right)_{2}=\left(\mathbf{v}_{h}, \mathbb{V} \mathbf{v}_{N}\right)_{2}=\left(\mathbf{v}_{h}, \mathbf{w}_{N}\right)_{2}
$$

Property 2 follows from property 1. Finally, $(20.60)$ follows from (20.55).

If we assume the basis to be orthonormal, the error $\mathbf{e}_{N}=\mathbf{u}_{h}-\mathbb{V} \mathbf{u}_{N}$ can be decomposed into two orthogonal terms:

$$
\begin{aligned}
\mathbf{e}_{N} &=\mathbf{u}_{h}-\mathbb{V} \mathbf{u}_{N}=\left(\mathbf{u}_{h}-\Pi \mathbf{u}_{h}\right)+\left(\Pi \mathbf{u}_{h}-\mathbb{V} \mathbf{u}_{N}\right) \\
&=\left(\mathbb{I}_{N_{h}}-\Pi\right) \mathbf{u}_{h}+\mathbb{V}\left(\mathbb{V}^{T} \mathbf{u}_{h}-\mathbf{u}_{N}\right)=\mathbf{e}_{\mathbf{V}_{N}^{\perp}}+\mathbf{e}_{\mathbf{V}_{N}}
\end{aligned}
$$



![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-245.jpg?height=306&width=436&top_left_y=116&top_left_x=240)

Fig. 20.4. The subspace $\mathbf{V}_{N}$ of $\mathbb{R}^{N_{h}}$ and the vectors $\mathbf{u}_{N} \in \mathbb{R}^{N}, \mathbb{V} \mathbf{u}_{N} \in \mathbf{V}_{N}$ and $\mathbf{u}_{h} \in \mathbb{R}^{N_{h}}$

The first term, orthogonal to $\mathbf{V}_{N}$, accounts for the fact that the high-fidelity solution does not strictly belong to the reduced subspace $\mathbf{V}_{N}$, whereas the second one, parallel to $\mathbf{V}_{N}$, accounts for the fact that a (slightly) different problem from the original one is solved; see Fig. $20.4$.

We remark that the result $(20.58)$ follows from property $(20.57)$ on the $h$ orthogonality of the reduced basis functions. Actually, the orthonormalization can be done with respect to a different scalar product that we generically denote $(\cdot, \cdot)_{N}$, i.e.

$$
\left(\zeta_{k}, \zeta_{m}\right)_{N}=\delta_{k m} \quad \forall k, m=1, \ldots, N
$$

In this case, instead of $(20.59)$ we have

$$
\delta_{k m}=\left(\zeta_{k}, \zeta_{m}\right)_{N}=\sum_{r=1}^{N_{h}} \sum_{s=1}^{N_{h}}\left(\mathbb{V}_{r k} \varphi^{r}, \mathbb{V}_{s m} \varphi^{s}\right)_{N}=\sum_{r=1}^{N_{h}} \sum_{s=1}^{N_{h}} \mathbb{V}_{m s} \mathbb{M}_{s r} \mathbb{V}_{r k}
$$

where we have used (20.48) and defined the mass matrix for $(\cdot, \cdot)_{N}$ as

$$
\mathbb{M}_{s r}=\left(\varphi^{r}, \varphi^{s}\right)_{N}, \quad 1 \leq r, s \leq N
$$

Consequently, instead of (20.58) we obtain the new orthonormality relation

$$
\mathbb{Y}^{T} \mathbb{Y}=\mathbb{V}^{T} \mathbb{M} \mathbb{V}=\mathbb{I}_{N}
$$

where $\mathbb{Y}^{T}=\mathbb{V}^{T} \mathbb{M}^{1 / 2}, \mathbb{Y}=\mathbb{M}^{1 / 2} \mathbb{V}$. In the same way, the projection matrix is $\Pi=$ $\mathbb{Y} \mathbb{Y}^{T}$. The results 1 . and 2. of Lemma $20.2$ proven above will still hold, provided the matrix $\mathbb{V}$ is replaced by $\mathbb{Y}$ and the subspace $\mathbf{V}_{N}$ by $\mathbf{Y}_{N}=\mathbb{Y} \mathbf{V}_{N}$.

\subsubsection{Alternative formulations: Least-Squares and Petrov-Galerkin RB problems}

The Galerkin projection, which leads to the (G-RB) problem discussed so far, is the most common strategy to build a reduced-order method, since it yields, up to constants, the best approximation with respect to the energy norm. In this case, the trial space (namely, the space where we seek the solution) and the test space are the same; from an algebraic standpoint this is reflected by the identities (20.49), where the matrix by which we pre- and post-multiply the high-fidelity stiffness matrix is the same. However, the trial and the test space may be chosen in a different way, giving rise to what we have called a Petrov-Galerkin formulation. In this section we provide some ideas about this approach.

\section{The Least Squares Reduced Basis method}

An alternative approach to (G-RB) is the so-called Least Squares Reduced Basis (LSRB) - sometimes also called Minimum Residual - method, where the (LS-RB) solution satisfies

$$
\mathbf{u}_{N}=\arg \min _{\tilde{\mathbf{u}}_{N} \in \mathbb{R}^{N}}\left\|\mathbf{r}_{h}\left(\tilde{\mathbf{u}}_{N}\right)\right\|_{2}^{2}
$$

Note that the minimization criterion (20.61) applies for any matrix $\mathbb{A}_{h}$, whereas (20.56), which characterizes the (G-RB) method, requires $\mathbb{A}_{h}$ to be symmetric and positive definite. The solution to $(20.61)$ coincides with the solution of the normal equations

$$
\left(\mathbb{A}_{h} \mathbb{V}\right)^{T} \mathbb{A}_{h} \mathbb{V} \mathbf{u}_{N}=\left(\mathbb{A}_{h} \mathbb{V}\right)^{T} \mathbf{f}_{h}
$$

that is

$$
\left(\mathbb{A}_{h} \mathbb{V}\right)^{T} \mathbf{r}_{h}\left(\mathbf{u}_{N}\right)=\mathbf{0}
$$

For a given matrix $\mathbb{V}$, the Least Squares (Minimum Residual) RB problem can therefore be obtained as follows:

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-246.jpg?height=321&width=715&top_left_y=823&top_left_x=96)

Note that (20.63) can be rewritten in the form (20.46) provided we set

$$
\mathbf{f}_{N}=\left(\mathbb{A}_{h} \mathbb{V}\right)^{T} \mathbf{f}_{h}, \quad \mathbb{A}_{N}=\left(\mathbb{A}_{h} \mathbb{V}\right)^{T} \mathbb{A}_{h} \mathbb{V}
$$



\section{The Petrov-Galerkin Reduced Basis method}

Problem (20.63) can be regarded as a special instance of the following Petrov-Galerkin (rather than Galerkin) method: find $u_{N}(\mu) \in V_{N}$ such that

$$
a\left(u_{N}(\mu), w_{N} ; \mu\right)=f\left(w_{N} ; \mu\right) \quad \forall w_{N} \in W_{N}
$$

where $W_{N} \subset V^{N_{h}}$ is a subspace of dimension $N$, different from $V_{N}$. If we denote by $\left\{\eta_{k}, k=1, \ldots, N\right\}$ a basis for $W_{N}$, and by $\mathbb{W} \in \mathbb{R}^{N_{h} \times N}$ the matrix whose entries are

$$
(\mathbb{W})_{r k}=\left(\eta_{k}, \varphi^{r}\right)_{h}, \quad r=1, \ldots, N_{h}, k=1, \ldots, N
$$

we can still express (20.64) in the algebraic form (20.46); this time, however, instead of $(20.49)$ we have

$$
\mathbf{f}_{N}=\mathbb{W}^{T} \mathbf{f}_{h}, \quad \mathbb{A}_{N}=\mathbb{W}^{T} \mathbb{A}_{h} \mathbb{V}
$$

For two given matrices $\mathbb{V}$ and $\mathbb{W}$, the Petrov-Galerkin $\mathrm{RB}$ (PG-RB) method can be obtained as follows:

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-247.jpg?height=283&width=712&top_left_y=534&top_left_x=99)

As already anticipated, the (LS-RB) problem (20.63) is a special case of the (PGRB) problem $(20.66)$ corresponding to the choice $\mathbb{W}=\mathbb{A}_{h} \mathbb{V}$. In fact, we can show (see Exercise 1) the following result:

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-247.jpg?height=199&width=721&top_left_y=920&top_left_x=96)

The (LS-RB) method is therefore equivalent to the Petrov-Galerkin method (20.64) provided the test space

$$
W_{N}=\operatorname{span}\left\{\eta_{k}^{\mu}, k=1, \ldots, N\right\} \subset V^{N_{h}}
$$

is defined by taking $\eta_{k}^{\mu}=\eta_{k}^{\mu}\left(\zeta_{k}\right)$ as the solution to

$$
\left(\eta_{k}^{\mu}, z_{h}\right)_{h}=a\left(\zeta_{k}, z_{h} ; \mu\right) \quad \forall z_{h} \in V^{N_{h}}
$$

Remark $20.1$. The choice $(20.68)$ is optimal in the sense that the error in the (PG-RB) problem equals the best approximation error - i.e. the ratio of continuity constant to stability constant is 1 - provided we endow the trial space with a suitable energy norm; the interested reader can refer, for instance, to [DG11].

We also note that in the (LS-RB) case the basis functions of the test space depend on $\mu$ because of the parametric dependence of the bilinear form $a(\cdot, \cdot ; \mu)$. Several options for their efficient construction and orthonormalization are available in this case; see for instance [RHM13] for further details.

\subsection{Construction of reduced spaces}

We now describe how to sample the parameter space in order to compute the retained snapshots which actually form the reduced basis. We start by illustrating a sample strategy, the greedy algorithm, which was introduced in [PR07, RHP08], and is based on the idea of selecting at each step the locally optimal element. Next we will address an alternative procedure, the so-called proper orthogonal decomposition.

\subsubsection{Greedy algorithm}

We start by formulating this algorithm in an abstract setting, then we characterize the case of the reduced basis method by providing a computable version.

A greedy algorithm is a general procedure to approximate each element of a compact set $K$ in a Hilbert space $V$ by a subspace of properly selected elements of $K$. For a given $N \geq 1$, we seek functions $\left\{f_{1}, f_{1}, \ldots, f_{N}\right\}$ such that each $f \in K$ is well approximated by the elements of the subspace $K_{N}=\operatorname{span}\left\{f_{1}, \ldots, f_{N}\right\}$. The algorithm can be described as follows:

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-248.jpg?height=204&width=712&top_left_y=934&top_left_x=96)

$\Pi_{K_{N-1}}$ is the orthogonal projection operator on $K_{N-1}$ with respect to the scalar product $(\cdot, \cdot)_{V}$ and $\varepsilon_{\mathrm{tol}}^{*}$ is a chosen tolerance; $f_{N}$ is called the worst case element, i.e. the element of $K$ that maximizes the projection error on $K_{N-1}$. At each step, the elements provided by the previous algorithm are orthonormalized by the following Gram-Schmidt procedure: we define

$$
\zeta_{1}=\frac{f_{1}}{\left\|f_{1}\right\|_{V}}, \quad \zeta_{N}=\frac{f_{N}-\Pi_{K_{N-1}} f_{N}}{\left\|f_{N}-\Pi_{K_{N-1}} f_{N}\right\|_{V}}, \quad N=2, \ldots, N_{\max }
$$

and assume that the construction ends when $N=N_{\max }$. In particular, for any $f \in V$,

$$
\Pi_{K_{N}} f=\sum_{n=1}^{N} \pi_{K_{N}}^{n}(f) \zeta_{n}, \quad \text { with } \quad \pi_{K_{N}}^{n}(f)=\left(f, \zeta_{n}\right)_{V}
$$

We are interested in the case where $K$ is the parametrically induced manifold (20.26); the greedy algorithm for the parameters selection takes the following form:

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-249.jpg?height=216&width=716&top_left_y=431&top_left_x=94)

Here $\Pi_{N-1}^{\mu}: V^{N_{h}} \rightarrow V_{N-1}$ denotes the elliptic (Galerkin) projection onto $V_{N-1}$ :

$$
a\left(\Pi_{N-1}^{\mu} u_{h}, v_{h} ; \mu\right)=a\left(u_{h}, v_{h} ; \mu\right) \quad \forall v \in V_{N-1}
$$

Note that, thanks to $(20.16)$ and $(20.21), \Pi_{N}^{\mu} u_{h}(\mu)=u_{N}(\mu)$ for all $\mu \in \mathscr{D}$. The set $\left\{u_{h}\left(\mu^{1}\right), \ldots, u_{h}\left(\mu^{N}\right)\right\}$ generated above is then orthonormalized with respect to the scalar product $(\cdot, \cdot)_{V}$, yielding a new orthonormal basis $\left\{\zeta_{1} \ldots, \zeta_{N}\right\}$ of $V_{N}$

In the Gram-Schmidt orthonormalization process we cannot use the elliptic (Galerkin) projection $\Pi_{N}^{\mu}$, since the latter depends on $\mu$; instead, we employ the orthogonal projection $\Pi_{N}: V^{N_{h}} \rightarrow V_{N}$ with respect to the scalar product $(\cdot, \cdot)_{V}$ corresponding to a specific value of $\bar{\mu}$, see $(20.10)$. The orthonormalization process gives:

$$
\zeta_{1}=\frac{u_{h}\left(\mu^{1}\right)}{\left\|u_{h}\left(\mu^{1}\right)\right\|_{V}}, \quad \zeta_{N}=\frac{u_{h}\left(\mu^{N}\right)-\Pi_{N-1} u_{h}\left(\mu^{N}\right)}{\left\|u_{h}\left(\mu^{N}\right)-\Pi_{N-1} u_{h}\left(\mu^{N}\right)\right\|_{V}}, \quad N=2, \ldots, N_{\max }
$$

In particular,

$$
\Pi_{N} u_{h}(\mu)=\sum_{n=1}^{N} \pi_{N}^{n}(\mu) \zeta_{n}, \quad \text { with } \quad \pi_{N}^{n}(\mu)=\left(u_{h}(\mu), \zeta_{n}\right)_{V}
$$

Computationally, the greedy algorithm (20.69) is rather expensive: at each step, seeking the best snapshot entails solving an optimization problem, where the evaluation of the approximation error $\left\|u_{h}(\mu)-\Pi_{N-1}^{\mu} u_{h}(\mu)\right\|_{V}$ requires many expensive evaluations of the high-fidelity solution $u_{h}(\mu)$. In practice, this cost is alleviated by replacing the sup over $\mathscr{D}$ with a sup over a very fine sample $^{5} \Xi_{\text {train }} \subset \mathscr{D}$, of cardinality $\left|\Xi_{\text {train }}\right|=n_{\text {train }}$, which shall serve to select our RB space - or train our RB approximation. This nevertheless still requires solving several high-fidelity approximation problems.

A further simplification is then adopted, which consists of replacing the approximation error with an inexpensive a posteriori error estimator $\Delta_{N-1}(\mu)$ such that

$$
\left\|u_{h}(\mu)-\Pi_{N-1}^{\mu} u_{h}(\mu)\right\|_{V} \leq \Delta_{N-1}(\mu) \quad \forall \mu \in \mathscr{D}
$$

The complete greedy algorithm reads as follows:

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-250.jpg?height=373&width=713&top_left_y=425&top_left_x=97)

Otherwise said, at the $N$-th iteration of this algorithm to the retained snapshots, over all possible candidate $u_{h}(\mu), \mu \in \Xi_{\text {train }}$, we append the particular candidate snapshot that the a posteriori error bound (20.96) predicts will be the worst approximated by the RB prediction associated to $V_{N-1}$.

A similar greedy procedure can also be developed with respect to the energy norm [RHP08]. This is particularly relevant in the compliant case, since the error in the energy norm is directly related to the error in the output (see Sect. 20.3.2).

5 Typically these samples are chosen by Monte Carlo methods with respect to a uniform or log-uniform density: $\Xi$ is however sufficiently large to ensure that the reported results are insensitive to further refinement of the parameter sample. We usually make a distinction between the test sample $\Xi$, which serves to assess Online the quality of the RB approximation and the a posteriori error estimators, and the train samples $\Xi_{\text {train }}$, which serves to generate the RB approximation; the choice of the latter has important (both Offline and Online) computational implications. 

\subsubsection{Proper Orthogonal Decomposition}

A technique alternative to greedy $\mathrm{RB}$ algorithms for the construction of reduced spaces in computational reduction of parametrized systems is the proper orthogonal decomposition (POD). POD is also popular in multivariate statistical analysis (where it is called principal component analysis) or in the theory of stochastic processes (under the name of Karhunen-Loève decomposition). The first applications of POD in scientific computing were concerned with the simulation of turbulent flows and date back to the early '90s [Aub91, BHL93]; the interested reader can find further details for instance in [HLB98].

POD techniques $^{6}$ reduce the dimensionality of a system by transforming the original variables into a new set of uncorrelated variables (called POD modes, or principal components), the first few modes ideally retaining most of the energy present in all of the original variables.

The POD method relies on the use of the singular value decomposition (SVD) algorithm that we briefly describe below. Consider a discrete set of $n_{\text {train }}$ snapshot vectors $\left\{\mathbf{u}_{1}, \ldots, \mathbf{u}_{n_{\text {train }}}\right\}$ belonging to $\mathbb{R}^{N_{h}}$, and form the snapshot matrix $\mathbb{U} \in \mathbb{R}^{N_{h} \times n_{\text {train }}}$ having them as column vectors:

$$
\mathbb{U}=\left[\begin{array}{llll}
\mathbf{u}_{1} & \mathbf{u}_{2} & \ldots & \mathbf{u}_{n_{\text {train }}}
\end{array}\right]
$$

with $n_{\text {train }}=\left|\Xi_{\text {train }}\right| \ll N_{h}($ see $(20.44))$ :

$$
\mathbf{u}_{j}=\left(u_{j}^{(1)}, \ldots, u_{j}^{\left(N_{h}\right)}\right) \in \mathbb{R}^{N_{h}}, \quad u_{j}^{(r)}=u_{h}\left(x_{r} ; \mu^{j}\right) \Leftrightarrow u_{h}\left(x ; \mu^{j}\right)=\sum_{r=1}^{N_{h}} u_{j}^{(r)} \varphi^{r}(\mathbf{x})
$$

The SVD decomposition of $\mathbb{U}$ reads

$$
\mathbb{V}^{T} \mathbb{U Z}=\left(\begin{array}{cc}
\Sigma & 0 \\
0 & 0
\end{array}\right)
$$

where $\mathbb{V}=\left[\begin{array}{llll}\boldsymbol{\zeta}_{1} & \zeta_{2} & \ldots & \zeta_{N^{h}}\end{array}\right] \in \mathbb{R}^{N_{h} \times N_{h}}$ and $\mathbb{Z}=\left[\begin{array}{llll}\boldsymbol{\psi}_{1} & \boldsymbol{\psi}_{2} & \ldots & \psi_{n_{\text {train }}}\end{array}\right] \in \mathbb{R}^{n_{\text {train }} \times n_{\text {train }}}$ are orthogonal matrices and $\Sigma=\operatorname{diag}\left(\sigma_{1}, \ldots, \sigma_{r}\right)$ with $\sigma_{1} \geq \sigma_{2} \geq \ldots \geq \sigma_{r}$; here $r \leq n_{\text {train }}$ is the rank of $\mathbb{U}$, which is strictly smaller than $n_{\text {train }}$ if the snapshot vectors are not all linearly independent.

Then, we can write

$$
\mathbb{U} \boldsymbol{\psi}_{i}=\sigma_{i} \zeta_{i} \quad \text { and } \quad \mathbb{U}^{T} \zeta_{i}=\sigma_{i} \psi_{i}, \quad i=1, \ldots, r
$$

or, equivalently,

$$
\mathbb{U}^{T} \mathbb{U} \psi_{i}=\sigma_{i}^{2} \psi_{i} \quad \text { and } \quad \mathbb{U U}^{T} \zeta_{i}=\sigma_{i}^{2} \zeta_{i}, \quad i=1, \ldots, r
$$

${ }^{6}$ For a general and concise introduction to POD techniques for the reduction of a (timedependent) dynamical system $-$ the first (and most used) application of this strategy $-$ the interested reader may refer to [Pin08, Voll1]. Two additional techniques - indeed quite close to POD - for generating reduced spaces are the Centroidal Voronoi Tessellation (see Chap. 9, [BGL06a, BGL06b]) and the Proper Generalized Decomposition (see for instance [CAC10, CLC11, Nou10]). i.e. $\sigma_{i}^{2}, i=1, \ldots, r$ are the nonzero eigenvalues of the matrix $\mathbb{U}^{T} \mathbb{U}$ (and also of $\mathbb{U U}^{T}$ ), listed in nondecreasing order; $\mathbb{C}=\mathbb{U}^{T} \mathbb{U}$ is the correlation matrix

$$
\mathbb{C}_{i j}=\mathbf{u}_{i}^{T} \mathbf{u}_{j}, \quad 1 \leq i, j \leq n_{\text {train }}
$$

For any $N \leq n_{\text {train }}$, the POD basis of dimension $N$ is defined as the set of the first $N$ left singular vectors $\zeta_{1}, \ldots, \zeta_{N}$ of $\mathbb{U}$ or, alternatively, the set of vectors

$$
\zeta_{j}=\frac{1}{\sigma_{j}} \mathbb{U} \psi_{j}, \quad 1 \leq j \leq N
$$

obtained from the first $N$ eigenvectors $\psi_{1}, \ldots, \psi_{N}$ of the correlation matrix $\mathbb{C}$

By construction, the POD basis is orthonormal. Moreover, if $\left\{\mathbf{z}_{1}, \ldots, \mathbf{z}_{N}\right\}$ is an arbitrary set of $N$ orthonormal vectors in $\mathbb{R}^{N_{h}}$, and $\Pi_{Z_{N}} \mathbf{w}$ the projection of a vector $\mathbf{w} \in \mathbb{R}^{N_{h}}$ onto $Z_{N}=\operatorname{span}\left\{\mathbf{z}_{1}, \ldots, \mathbf{z}_{N}\right\}$, that is

$$
\Pi_{Z_{N}} \mathbf{u}=\sum_{n=1}^{N} \pi_{Z_{N}}^{n}(\mathbf{u}) \mathbf{z}_{n}, \quad \text { with } \quad \pi_{Z_{N}}^{n}(\mathbf{u})=\mathbf{u}^{T} \mathbf{z}_{n}
$$

the POD basis $(20.73)$ generated from the set of snapshot vectors $\mathbf{u}_{1}, \ldots, \mathbf{u}_{n_{\text {train }}}$ solves the following minimization problem:

$$
\left\{\begin{array}{c}
\min \left\{E\left(\mathbf{z}_{1}, \ldots, \mathbf{z}_{N}\right), \mathbf{z}_{i} \in \mathbb{R}^{N_{h}}, \mathbf{z}_{i}^{T} \mathbf{z}_{j}=\delta_{i j}, \forall 1 \leq i, j \leq N\right\} \\
\text { with } E\left(\mathbf{z}_{1}, \ldots, \mathbf{z}_{N}\right)=\sum_{i=1}^{n_{\operatorname{train}}}\left\|\mathbf{u}_{i}-\Pi_{Z_{N}} \mathbf{u}_{i}\right\|_{2}^{2}
\end{array}\right.
$$

Thus, the POD basis minimizes, over all possible $N$-dimensional orthonormal sets $\left\{\mathbf{z}_{1}, \ldots, \mathbf{z}_{N}\right\}$ in $\mathbb{R}^{N_{h}}$, the sum of the squares of the error $E\left(\mathbf{z}_{1}, \ldots, \mathbf{z}_{N}\right)$ between each snapshot vector $\mathbf{u}_{i}$ and its projection $\Pi_{Z_{N}} \mathbf{u}_{i}$ onto the subspace $Z_{N} . E\left(\mathbf{z}_{1}, \ldots, \mathbf{z}_{N}\right)$ is often referred to as the POD energy.

The previous constructive presentation of the POD method was based on the socalled method of snapshots, introduced by Sirovich [Sir87]. Alternatively, a POD basis corresponding to a set of snapshot vectors $\mathbf{u}_{1}, \ldots, \mathbf{u}_{n_{\text {train }}}$ can be (and often is) defined by $(20.74)$. In this case, its connection with the SVD of the correlation matrix, as well as relation (20.72), follow by imposing that the POD basis $\left\{\zeta_{1}, \ldots, \zeta_{N}\right\}$ fulfill the first-order necessary optimality conditions (see Exercise 2).

Furthermore, it can be shown that

$$
E\left(\zeta_{1}, \ldots, \zeta_{N}\right)=\sum_{i=N+1}^{r} \sigma_{i}^{2}
$$

so that the error in the POD basis is equal to the squares of the singular values corresponding to the neglected POD modes. In this way, we can select $N_{\max }$ so that $E\left(\zeta_{1}, \ldots, \zeta_{N}\right) \leq \varepsilon_{\text {tol }}^{*}$, for a prescribed tolerance $\varepsilon_{\text {tol }}^{*}$

To do this, it is sufficient to choose $N_{\max }$ as the smallest $N$ such that

$$
I(N)=\sum_{i=1}^{N} \sigma_{i}^{2} / \sum_{i=1}^{r} \sigma_{i}^{2} \geq 1-\delta
$$

that is the energy retained by the last $r-N_{\max }$ modes equals to $\delta>0$, as small as desired; $I(N)$ is referred to as the relative information content of the POD basis.

A key feature is that although $\delta$ is chosen to be very small, e.g., $\delta=10^{-\beta}$ with $\beta=$ $3,4, \ldots$, in several problems $N_{\max }$ is relatively small (and in particular much smaller than $r$ ). This happens because, very often, the singular values of the snapshot matrix decrease very fast (e.g. with exponential rate).

Let us now cast the POD method described so far into the reduced basis context. As already pointed out, see $(20.71)$, the set of snapshot vectors is obtained from a set $\left\{u_{h}\left(\mu^{1}\right), \ldots, u_{h}\left(\mu^{n}\right.\right.$ train $\left.)\right\}$ of high-fidelity approximation functions belonging to the space $V^{N_{h}}$. In this case the minimization problem (20.74) can be equivalently reformulated as: find the POD basis $\left\{\zeta_{1}, \ldots, \zeta_{N}\right\}$ that solves the minimization problem:

$$
\left\{\begin{array}{c}
\min \left\{E\left(z_{1}, \ldots, z_{N}\right), z_{i} \in V^{N_{h}},\left(z_{i}, z_{j}\right)_{L^{2}(\Omega)}=\delta_{i j}, \forall 1 \leq i, j \leq N\right\} \\
\text { with } E\left(z_{1}, \ldots, z_{N}\right)=\sum_{i=1}^{n_{\text {train }}}\left\|u_{h}\left(\mu^{i}\right)-\Pi_{z} u_{h}\left(\mu^{i}\right)\right\|_{L^{2}(\Omega)}^{2}
\end{array}\right.
$$

where $\Pi_{z}: V^{N_{h}} \rightarrow Z_{N}$ is the $L^{2}(\Omega)$-projection onto $Z_{N}=\operatorname{span}\left\{z_{1}, \ldots, z_{N}\right\}$.

To solve (20.77) we proceed as follows:

- we form the rank $r\left(\leq n_{\text {train }}\right)$ correlation matrix

$$
\mathbb{C}_{i j}=\left(u_{h}\left(\mu^{i}\right), u_{h}\left(\mu^{j}\right)\right)_{L^{2}(\Omega)}, \quad 1 \leq i, j \leq n_{\text {train }}
$$

- we solve the $n_{\text {train }} \times n_{\text {train }}$ eigenvalue problem: for $i=1, \ldots, r$,

$$
\mathbb{C} \psi_{i}=\sigma_{i}^{2} \psi_{i}, \quad \boldsymbol{\psi}_{i}^{T} \boldsymbol{\psi}_{j}=\delta_{i j}, \quad 1 \leq i, j \leq r
$$

- finally we set

$$
\zeta_{i}=\sum_{j=1}^{n_{\text {train }}} \frac{1}{\sigma_{i}} \psi_{i}^{(j)} u_{h}\left(\mu^{j}\right), \quad 1 \leq i \leq N
$$

$\psi_{i}^{(j)}$ being the $j$-th component of the eigenvector $\psi_{i}$ and $\sigma_{i} \geq \sigma_{i-1}>0$

The correlation matrix factorizes as $\mathbb{C}=\mathbb{U}^{T} \tilde{\mathbb{M}} \mathbb{U}$, where $\mathbb{U}$ is the snapshot matrix while

$$
(\tilde{\mathbf{M}})_{i j}=\left(\varphi^{i}, \varphi^{j}\right)_{L^{2}(\Omega)}, \quad 1 \leq i, j \leq N_{h}
$$

is the mass matrix $\tilde{\mathbb{M}}$ of the high-fidelity approximation space. This allows us to exploit again a SVD to compute the POD basis functions. By using the Cholesky factorization $\tilde{\mathbb{M}}=\mathbb{H}^{T} \mathbb{H}$, where $\mathbb{H} \in \mathbb{R}^{N_{h} \times N_{h}}$ is the Cholesky factor of $\tilde{\mathbb{M}}$, we find $\tilde{U}=\mathbb{H U}$, hence $\mathbb{C}=\mathbb{U}^{T} \tilde{\mathbb{M}} \mathbb{U}=\tilde{\mathbb{U}}^{T} \tilde{\mathbb{U}}$ and therefore $\psi_{i}, i=1, \ldots, N$ represent the first $N$ singular vectors of $\tilde{U}$.

Typically a POD approach to build an $\mathrm{RB}$ space is more expensive than the greedy approach. In the latter, we only need to compute the $N$ - typically very few - highfidelity retained snapshots, whereas in the POD approach we must compute all $n_{\text {train }}-$ typically/desirably very many - high-fidelity candidate snapshots, as well as the solution of an eigenproblem for the correlation matrix $\mathbb{C} \in \mathbb{R}^{N_{h} \times N_{h}}$. Note that (20.76) provides information about the amount of energy neglected by the selected POD modes, that is an indication in the $L^{2}$-norm instead than in the $V$-norm, as it was the case of the a posteriori error estimates used in the greedy algorithm.

\subsection{Convergence of RB approximations}

In this section we illustrate some convergence results for problems depending on one or several parameters.

\subsubsection{A priori convergence theory: a simple case}

An a priori exponential convergence result, with respect to the number $N$ of basis functions, is known in the case of elliptic PDEs depending on one-dimensional parameters, for instance in [MPT02a, MPT02b]; furthermore, several computational tests shown e.g. in [RHP08] provide a numerical assessment of this behaviour, also for larger dimensions of the parameter space. Here we describe the simplest case, associated with specific non-hierarchical spaces $V_{N}^{\ln }, 1 \leq N \leq N_{\max }$, given by

$$
\begin{gathered}
V_{N}^{\ln }=\operatorname{span}\left\{u_{h}\left(\mu_{N}^{n}\right), \quad 1 \leq n \leq N\right\} \\
\mu_{N}^{n}=\mu^{\min } \exp \left\{\frac{n-1}{N-1} \ln \left(\frac{\mu^{\max }}{\mu^{\min }}\right)\right\}, \quad 1 \leq n \leq N, 1 \leq N \leq N_{\max } .
\end{gathered}
$$

We denote the corresponding RB approximation by $u_{N}^{\ln }(\mu)$. The a priori theory described below suggests that the spaces $(20.79)-$ which we shall call "equi-ln" spaces - do display optimality features; as we will see in the next subsection, a greedy selection would act as just as well. Here we consider the thermal block problem of Sect. $20.1 .1$ for the case in which $B_{1}=2, B_{2}=1$, as shown in Fig. $20.5$.

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-254.jpg?height=255&width=234&top_left_y=944&top_left_x=335)

Fig. 20.5. Thermal block problem: $B_{1}=2, B_{2}=1$ The governing equations are then given by (20.13) and (20.14) for two blocks/regions $\mathscr{R}_{1}$ and $\mathscr{R}_{2}$, the single parameter $\mu=\mu_{1}$ representing the conductivity of region $\mathscr{R}_{1}$ (the conductivity of region $\mathscr{R}_{2}$ is one), and the parameter domain $\mathscr{D}=\left[\mu^{\min }, \mu^{\max }\right]=\left[1 / \sqrt{\mu_{r}}, \sqrt{\mu_{r}}\right]$ for $\mu_{r}=100 ;$ the associated affine expansion (20.6) now comprises only $Q_{a}=2$ terms.

The analysis presented here is, as a matter of fact, relevant to a large class of single-parameter coercive problems. For the RB approximation of this problem, the following result holds:

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-255.jpg?height=258&width=718&top_left_y=358&top_left_x=99)

The proof is a "parametric" version of the standard (finite element) variational arguments of Chap. 4. In particular, we first invoke $(20.32)$ and take as our candidate $w$ a high-order polynomial interpolant in the parameter $\mu$ of $u_{h}(\mu)$; we next apply the standard Lagrange interpolant remainder formula; finally, we resort to an eigenfunction expansion to bound the parametric (sensitivity) derivatives and optimize the order of the polynomial interpolant.

For the complete proof and more considerations, see [PR07]. We note that the RB convergence estimate (20.81), relative to the model problem we have considered, relies on parameter smoothness and not on the computational grid (through $N_{h}$ ); the exponent in the convergence rate depends on $N$ and logarithmically on $\mu_{r}$.

\subsubsection{A priori convergence theory: greedy algorithms}

Several recent results carry out an a priori convergence analysis in the more general case of reduced spaces built through the greedy algorithm. Below we provide a few preliminary results that explain why approximation spaces $V_{N}$ built in this way exhibit exponential convergence in $N$; see e.g. $\left[\mathrm{BMP}^{+} 12, \mathrm{BCD}^{+} 11\right]$ for further details.

Let us denote, for any $u_{h} \in \mathscr{M}_{h}$,

$$
\sigma_{N}\left(u_{h}, \mathscr{M}_{h}\right)=\inf _{v_{N} \in V_{N}}\left\|u_{h}-v_{N}\right\|_{V}=\left\|u_{h}-\Pi_{N}^{\mu} u_{h}\right\|_{V}
$$

where $\Pi_{N}^{\mu}$ denotes the projector onto $V_{N}$. Then

$$
\sigma_{N}\left(\mathscr{M}_{h}\right)=\sup _{u_{h} \in \mathscr{M}_{h}} \sigma_{N}\left(u_{h}, \mathscr{M}_{h}\right)
$$

is the best approximation error when approximating the set $\mathscr{M}_{h}$ by $V_{N}$. Indeed, this is the quantity which has to be maximized at each step of the greedy algorithm, in order to select the current snapshot. For instance, referring to the case addressed in Sect. 20.6.1, with the specific choice $(20.80)$ we can achieve exponential convergence, that is

$$
\sigma_{N}\left(\mathscr{M}_{h}\right) \leq C \exp \left(-N^{\alpha}\right), \quad \text { for some } \alpha>0
$$

A priori convergence analysis aims at providing upper bounds for the sequence $\sigma_{N}\left(\mathscr{M}_{h}\right)$ in terms of the best $N$-dimensional subspace, i.e. the one that would minimize the projection error for the whole set $\mathscr{M}_{h}$ among all $N$-dimensional subspaces. This minimal error is given by the Kolmogorov $N$-width, which is defined as

$$
d_{N}\left(\mathscr{M}_{h}\right)=\inf _{Z_{N} \subset V} \sup _{u_{h} \in \mathscr{M}_{h}} \operatorname{dist}\left(u_{h}, Z_{N}\right)
$$

where

$$
\operatorname{dist}\left(u_{h}, Z_{N}\right)=\min _{w_{N} \in Z_{N}}\left\|u_{h}-w_{N}\right\|_{V}=\left\|u_{h}-\Pi_{Z_{N}} u_{h}\right\|_{V}
$$

and the first infimum is taken over all linear subspaces $Z_{N} \subset V$ of dimension $N$. We refer the reader to [LvGM96, Pin85] for a general discussion on Kolmogorov width.

$d_{N}\left(\mathscr{M}_{h}\right)$ measures the degree to which a subset of the space $V$ can be approximated using finite-dimensional subspaces $Z_{N}$. If $\sigma_{N}\left(\mathscr{M}_{h}\right)$ decayed at a rate comparable to $d_{N}\left(\mathscr{M}_{h}\right)$, the greedy selection would essentially provide the best possible accuracy attainable by $N$-dimensional subspaces.

In general the optimal subspace with respect to the Kolmogorov $N$-width $(20.82)$ is not spanned by elements of the set $\mathscr{M}_{h}$ being approximated, thus we possibly have that $d_{N}\left(\mathscr{M}_{h}\right) \ll \sigma_{N}\left(\mathscr{M}_{h}\right)$. However, if the $N$-width converges at exponential rate, then also the error of the best approximation in $V_{N}$ does. This is the meaning of the following result, shown in $\left[\mathrm{BMP}^{+} 12\right]$; we recall that $\gamma(\mu)$ and $\alpha(\mu)$ are the continuity and the coercivity constants of the bilinear form $a(\cdot, \cdot ; \mu)$ :

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-256.jpg?height=294&width=723&top_left_y=839&top_left_x=95)



![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-257.jpg?height=252&width=713&top_left_y=113&top_left_x=97)

Although interesting from a theoretical viewpoint, the previous comparison is only useful if $d_{N}\left(\mathscr{M}_{h}\right)$ decays to zero faster than $N^{-1} \delta_{0}^{-N}$. We remark that the factor $\sqrt{\bar{\gamma} / \alpha_{0}}$ is related to the $V$-norm chosen to measure the error in the greedy algorithm. Moreover, the expression of the a posteriori error estimate $\Delta_{N}(\mu)$, defined in (20.96), involves a computable ( $\mu$-dependent) lower bound $\alpha_{\mathrm{LB}}^{N_{h}}(\mu) \leq \alpha^{N_{h}}(\mu)$ of the high-fidelity coercivity constant $\alpha^{N_{h}}(\mu)$; in particular, the parametric lower bound is uniformly bounded below by $\alpha_{0}^{N_{h}}>0$.

In fact, we have exponential convergence of the Kolmogorov $N$-width when the dependence on the parameter is analytic'. This result has been further improved in $\left[\mathrm{BCD}^{+} 11\right]$, where it was shown that if $d_{N}\left(\mathscr{M}_{h}\right) \leq C \exp \left(-c N^{\beta}\right)$ for all $N>0$ and some $C, c>0$, then, for some $\widetilde{C}, \widetilde{c}>0$ we have:

$$
\sigma_{N}\left(\mathscr{M}_{h}\right) \leq \widetilde{C} \exp \left(-\widetilde{c} N^{\beta /(\beta+1)}\right)
$$

with $\widetilde{C}$ independent of $N$. In the case of algebraic convergence, $d_{N}\left(\mathscr{M}_{h}\right) \leq M N^{-\alpha}$ for all $N>0$ and some $M, \alpha>0$, it was proved that

$$
\sigma_{N}\left(\mathscr{M}_{h}\right) \leq C M N^{-\alpha}, \quad C=C\left(\gamma, \alpha_{0}\right)
$$

Furthermore, it was shown that

$$
\sigma_{N}\left(\mathscr{M}_{h}\right) \leq \frac{2^{N+1}}{\sqrt{3}} d_{N}\left(\mathscr{M}_{h}\right)
$$

and that this estimate cannot, in general, be improved.

To conclude, the reader of this book will remind that the fast (exponential) convergence of numerical approximations is a distinctive feature of spectral methods (see Chap. 10). The reduced basis method shares indeed some aspects with spectral methods: similarly to the latter, it makes use of basis functions with global support (which are problem-dependent in the RB case, whereas global orthogonal polynomials in the pure spectral case).

${ }^{7}$ However, analytic regularity of the solution manifold $\mathscr{M}_{h}$ is not necessary in order to apply the reduced basis method. In general, the reduced basis approximation of solutions of elliptic equations with regular coefficients has indeed been very successful. Further convergence analyses for the greedy algorithm and a numerical proof of these results, extending the ones presented in [MPT02a, MPT02b], can be found e.g. in [LMQR13].

\subsection{A posteriori error estimation}

Effective a posteriori error bounds for field variables and outputs of interest are crucial for both the efficiency and the reliability of RB approximations. As regards efficiency, a posteriori error estimation allows to control the error, and also to minimize the computational effort by controlling the dimension of the RB space. Not only, for the greedy algorithm the application of error bounds (as surrogates for the actual error) allows significantly larger training samples $\Xi_{\text {train }} \subset \mathscr{D}$ and a better parameter space exploration at greatly reduced Offline computational costs. Concerning reliability, a posteriori error bounds allow a confident exploitation of the rapid predictive power of the $\mathrm{RB}$ approximation and provide an error quantification for each new parameter value $\mu$ in the Online stage.

In turn, error bounds must be rigorous - valid for all $N$ and for all parameter values in the parameter domain $\mathscr{D}$ : non-rigorous error indicators would not ensure reliability. Secondly, the bounds must be reasonably sharp: an overly conservative error bound can yield inefficient approximations ( $N$ too large) or even dangerous suboptimal engineering results (unnecessary safety margins). In the third place, the bounds must be very efficient: the Online operation count and storage to compute the RB error bounds - the marginal average cost $-$ must be independent of $N_{h}$ (and commensurate with the cost associated with the RB output prediction).

\subsubsection{Some preliminary estimates}

The central equation for a posteriori error estimates is the error-residual relationship (see Sect. 4.6.2). It follows from the problems statements for $u_{h}(\mu),(20.16)$, and $u_{N}(\mu),(20.30)$, by introducing the residual $r(v ; \mu) \in\left(V^{N_{h}}\right)^{\prime}$

$$
r(v ; \mu)=f(v ; \mu)-a\left(u_{N}(\mu), v ; \mu\right) \quad \forall v \in V^{N_{h}}
$$

that the error $e_{h}(\mu)=u_{h}(\mu)-u_{N}(\mu) \in V^{N_{h}}$ satisfies

$$
a\left(e_{h}(\mu), v ; \mu\right)=r(v ; \mu) \quad \forall v \in V^{N_{h}}
$$

The Riesz representation $\hat{e}_{h}(\mu) \in V^{N_{h}}$ of $r(\cdot ; \mu)$ (see Theorem 2.1) satisfies

$$
\left(\hat{e}_{h}(\mu), v\right)_{V}=r(v ; \mu) \quad \forall v \in V^{N_{h}}
$$

From the error residual equation (20.86) we obtain

$$
a\left(e_{h}(\mu), v ; \mu\right)=\left(\hat{e}_{h}(\mu), v\right)_{V} \quad \forall v \in V^{N_{h}}
$$

and therefore

$$
\|r(\cdot ; \mu)\|_{\left(V^{\left.N_{h}\right)^{\prime}}\right.}=\sup _{v \in V^{N_{h}}} \frac{r(v ; \mu)}{\|v\|_{V}}=\left\|\hat{e}_{h}(\mu)\right\|_{V}
$$

Computing the dual norm of the residual through the Riesz representation theorem allows to develop a suitable Offline-Online procedure also for the evaluation of the error bounds, as remarked below.

We recall the definition of the exact and $\mathrm{FE}$ coercivity constants, respectively (20.5) and (20.20). Moreover, we shall require a (parametric) lower bound $\alpha_{\mathrm{LB}}^{N_{h}}$ : $\mathscr{D} \rightarrow \mathbb{R}$ to the (parametric) coercivity constant $\alpha^{N_{h}}(\mu)$,

$$
0<\alpha_{\mathrm{LB}}^{N_{h}}(\mu) \leq \alpha^{N_{h}}(\mu) \quad \forall \mu \in \mathscr{D}
$$

\subsubsection{Error bounds}

We define error estimators for the energy norm and the output respectively as

$$
\Delta_{N}^{\mathrm{en}}(\mu)=\frac{\left\|\hat{e}_{h}(\mu)\right\|_{V}}{\left(\alpha_{\mathrm{LB}}^{N_{h}}(\mu)\right)^{1 / 2}} \quad \text { and } \quad \Delta_{N}^{s}(\mu)=\frac{\left\|\hat{e}_{h}(\mu)\right\|_{V}^{2}}{\alpha_{\mathrm{LB}}^{N_{h}}(\mu)}
$$

We next introduce the effectivity factors associated with these error estimators as

$$
\eta_{N}^{\mathrm{en}}(\mu)=\frac{\Delta_{N}^{\mathrm{en}}(\mu)}{\left\|u_{h}(\mu)-u_{N}(\mu)\right\|_{\mu}} \quad \text { and } \quad \eta_{N}^{s}(\mu)=\frac{\Delta_{N}^{s}(\mu)}{\left(s_{h}(\mu)-s_{N}(\mu)\right)}
$$

respectively. To achieve rigour, we shall insist on effectivity factors $\geq 1$; for sharpness, we desire them to be as close to one as possible. It has been demonstrated [RHP08, PR07] that

Proof. It follows directly from $(20.88)$ for $v=e_{h}(\mu)$, and from the Cauchy-Schwarz inequality that

$$
\left\|e_{h}(\mu)\right\|_{\mu}^{2} \leq\left\|\hat{e}_{h}(\mu)\right\|_{V}\left\|e_{h}(\mu)\right\|_{V}
$$

But $\left(\alpha^{N_{h}}(\mu)\right)^{\frac{1}{2}}\left\|e_{h}(\mu)\right\|_{V} \leq\left(a\left(e_{h}(\mu), e_{h}(\mu) ; \mu\right)\right)^{\frac{1}{2}}=\left\|e_{h}(\mu)\right\|_{\mu}$, thus from $(20.93)$ we obtain $\left\|e_{h}(\mu)\right\|_{\mu} \leq \Delta_{N}^{\mathrm{en}}(\mu)$ or $\eta_{N}^{\text {en }}(\mu) \geq 1$. We consider $(20.88)$ again $-$ but now for $v=\hat{e}_{h}(\mu)-$ and the Cauchy-Schwarz inequality to obtain

$$
\left\|\hat{e}_{h}(\mu)\right\|_{V}^{2} \leq\left\|\hat{e}_{h}(\mu)\right\|_{\mu}\left\|e_{h}(\mu)\right\|_{\mu}
$$

Thanks to continuity, $\left\|\hat{e}_{h}(\mu)\right\|_{\mu} \leq(\gamma(\mu))^{\frac{1}{2}}\left\|\hat{e}_{h}(\mu)\right\|_{V}$, hence from (20.94)

$$
\Delta_{N}^{\mathrm{en}}(\mu)=\left(\alpha_{\mathrm{LB}}^{N_{h}}(\mu)\right)^{-\frac{1}{2}}\left\|\hat{e}_{h}(\mu)\right\|_{V} \leq\left(\alpha_{\mathrm{LB}}^{N_{h}}(\mu)\right)^{-\frac{1}{2}}(\gamma(\mu))^{\frac{1}{2}}\left\|e_{h}(\mu)\right\|_{\mu}
$$

that is $\eta_{N}^{\text {en }}(\mu) \leq \sqrt{\gamma(\mu) / \alpha_{\mathrm{LB}}^{N_{h}}(\mu)}$. Next, from (20.33) we have $s_{h}(\mu)-s_{N}(\mu)=$ $\left\|e_{h}(\mu)\right\|_{\mu}^{2}$, and hence since $\Delta_{N}^{s}(\mu)=\left(\Delta_{N}^{\text {en }}(\mu)\right)^{2}:$

$$
\eta_{N}^{s}(\mu)=\frac{\Delta_{N}^{s}(\mu)}{s_{h}(\mu)-s_{N}(\mu)}=\frac{\left(\Delta_{N}^{\mathrm{en}}(\mu)\right)^{2}}{\|e(\mu)\|_{\mu}^{2}}=\left(\eta_{N}^{\mathrm{en}}(\mu)\right)^{2}
$$

in the end, (20.92) follows directly from (20.91) and (20.95).

A similar result can be obtained for the a posteriori error bound in the $V$ norm, which is defined as

$$
\Delta_{N}(\mu)=\frac{\left\|\hat{e}_{h}(\mu)\right\|_{V}}{\alpha_{\mathrm{LB}}^{N_{h}}(\mu)}
$$

In fact, by following the same argument as in Proposition $20.3$, it can be shown that:

The effectivity upper bounds, $(20.91),(20.92)$ and $(20.97)$, are independent of $N$, and hence stable with respect to $R B$ refinement.

Finally, we remark that the error bounds of the previous section are of little utility without an accompanying Offline-Online computational approach. The computationally crucial component of all the error bounds of the previous section is $\left\|\hat{e}_{h}(\mu)\right\|_{V}$, the dual norm of the residual $(20.85)$, whose expression can be expanded according to $(20.35)$ and $(20.6)$, thus yielding the possibility to pre-compute Offline the parameter-independent quantities and then evaluate, for any new value of $\mu$, just some parameter-dependent quantities. The interested reader can find further details for instance in [QRM11, RHP08].

An approach to the construction of lower bounds for the coercivity constant which is a generalized minimum eigenvalue $-$ is the Successive Constraint Method (SCM) introduced in [HRSP07]. The method - based on an Offline-Online strategy - reduces the Online (real-time) calculation to a small linear program for which the operation count is independent of $N_{h}$. See for instance [RHP08] for more details.

\section{$20.8$ Non-compliant problems}

For the sake of simplicity, we addressed in Sect. $20.7$ the RB approximation of affinely parametrized coercive problems in the compliant case. We now consider the elliptic case and the more general non-compliant problem: given $\mu \in \mathscr{D}$, find

$$
s(\mu)=J(u(\mu)),
$$

where $u(\mu) \in V$ satisfies

$$
a(u(\mu), v ; \mu)=f(v ; \mu) \quad \forall v \in V
$$

We assume that $a$ is coercive and continuous but not necessarily symmetric; we further suppose that both $J$ and $f$ are bounded linear functionals ${ }^{8}$, but we no longer require $J=f$. Moreover, we assume that both $a$ and $f$ are affine, see $(20.6)-(20.7)$.

Following the methodology (and the notation) of Sect. 20.7, we can readily find an a posteriori error bound for $s_{N}(\mu):$ by standard arguments we obtain

$$
\left|s_{h}(\mu)-s_{N}(\mu)\right| \leq\|J\|_{\left(V^{\left.N_{h}\right)^{\prime}}\right.} \Delta_{N}^{\text {en }}(\mu)
$$

where $\left\|u_{h}(\mu)-u_{N}(\mu)\right\| \mu \leq \Delta_{N}^{\text {en }}(\mu)$ and $\Delta_{N}^{\text {en }}(\mu)$ is defined in (20.90); see [RHP08, PR07] for further details. We denote this method "primal-only". Although for many outputs primal-only is perhaps the best approach (each additional output, and associated error bound, is a simple "add-on"), this approach has two drawbacks:

1. we loose the "quadratic convergence" effect $(20.33)$ for outputs (unless $J=f$ and $a$ is symmetric);

2. the effectivity factor $\Delta_{N}^{s}(\mu) /\left|s(\mu)-s_{N}(\mu)\right|$ may be unbounded: if $J=f$ then we know, from (20.33), that $\left|s_{h}(\mu)-s_{N}(\mu)\right| \sim\left\|\hat{e}_{h}(\mu)\right\|_{V}^{2}$ and hence

$$
\frac{\Delta_{N}^{s}(\mu)}{\left|s_{h}(\mu)-s_{N}(\mu)\right|} \sim \frac{1}{\left\|\hat{e}_{h}(\mu)\right\|_{V}} \rightarrow \infty \quad \text { as } N \rightarrow \infty
$$

i.e. the effectivity of the output error bound $\Delta_{N}^{s}$ defined in (20.90) tends to infinity as $(N \rightarrow \infty$ and $) u_{N}(\mu) \rightarrow u_{h}(\mu)$.

We may expect a similar behaviour for any $J$ "close" to $f:$ the problem is that $\Delta_{N}^{s}(\mu)$ does not reflect the contribution of the test space to the convergence of the output.

The introduction of RB primal-dual approximation allows to overcome the previous issues $-$ and ensure a stable limit as $N \rightarrow \infty$. Let us introduce the dual problem associated to $J$, that reads as follows: find $\psi(\mu) \in V$ such that

$$
a(v, \psi(\mu) ; \mu)=-J(v) \quad \forall v \in V
$$

$\psi$ is called the adjoint or dual field. Let us define the RB spaces for the primal and the dual problem, respectively:

$$
\begin{gathered}
V_{N_{p r}}=\operatorname{span}\left\{u_{h}\left(\mu^{k, p r}\right), 1 \leq k \leq N_{p r}\right\}, \quad 1 \leq N_{p r} \leq N_{p r, m a x} \\
V_{N_{d u}}=\operatorname{span}\left\{\Psi_{h}\left(\mu^{k, d u}\right), 1 \leq k \leq N_{d u}\right\}, \quad 1 \leq N_{d u} \leq N_{d u, \max } .
\end{gathered}
$$

${ }^{8}$ Typical output functionals correspond to the "integral" of the field $u(\mu)$ over an area or line (in particular, boundary segment) in $\bar{\Omega}$. However, by appropriate lifting techniques, "integrals" of the $f$ lux over boundary segments can also be considered. For our purposes a single FE space suffices for both the primal and dual, even if in the practice the FE primal and dual spaces may be different. The resulting $\mathrm{RB}$ approximations $u_{N_{p r}}(\mu) \in V_{N_{p r}}, \Psi_{N_{d u}}(\mu) \in V_{N_{d u}}$ solve

$$
\begin{aligned}
&a\left(u_{N_{p r}}(\mu), v ; \mu\right)=f(v ; \mu) \quad \forall v \in V_{N_{p r}}, \\
&a\left(v, \Psi_{N_{d u}}(\mu) ; \mu\right)=-J(v) \quad \forall v \in V_{N_{d u}}
\end{aligned}
$$

then, the RB output can be evaluated as

$$
s_{N_{p r}, N_{d u}}(\mu)=J\left(u_{N_{p r}}\right)-r^{p r}\left(\Psi_{N_{d u}} ; \mu\right)
$$

where

$$
\begin{aligned}
r^{p r}(v ; \mu) &=f(v ; \mu)-a\left(u_{N_{p r}}, v ; \mu\right) \\
r^{d u}(v ; \mu) &=-J(v)-a\left(v, \Psi_{N_{d u}} ; \mu\right)
\end{aligned}
$$

are the primal and the dual residuals, respectively. The term $r^{p r}\left(\Psi_{N_{d u}} ; \mu\right)$ allows to obtain a better convergence to the high-fidelity output $s_{h}(\mu)$, as remarked in [PG00].

Thus, the output error bound takes the following form:

$$
\Delta_{N}^{s}(\mu)=\frac{\left\|r^{\mathrm{pr}}(\cdot ; \mu)\right\|_{\left(V^{\left.N_{h}\right)}\right.}}{\left(\alpha_{\mathrm{LB}}^{N_{h}}(\mu)\right)^{1 / 2}} \frac{\left\|r^{\mathrm{du}}(\cdot ; \mu)\right\|_{\left(V^{\left.N_{h}\right)}^{\prime}\right.}}{\left(\alpha_{\mathrm{LB}}^{N_{h}}(\mu)\right)^{1 / 2}}
$$

in the non-compliant case, so that we are able to recover the "quadratic" output effect. Note that the Offline-Online procedure is very similar to the "primal-only" case, but now we need to do everything both for primal and dual; moreover, we need to evaluate both a primal and a dual residual for the a posteriori error bounds.

\subsection{Parametrized geometries and operators}

In this section we introduce the general class of scalar problems that fall under the abstract formulation of Sect. $20.1$, by extending the simple examples described in Sect. $20.1 .1$ to the case where the parameters might describe:

- physical properties (material coefficients, source terms, boundary data), possibly varying in different subregions of the computational domain;

- the geometrical configuration of the computational domain.

By following an increasing order of complexity, we first discuss the case of parametrized physical properties; then we describe a generalization to take into account variations of physical properties over different subregions; finally, we present the more difficult case dealing with parametrized geometries.

For the sake of exposition, let us distinguish between $d_{p}$ physical and $d_{g}$ geometrical parameter components: we denote the former by $\mu_{p}$ and the latter by $\mu_{g}$, respectively, so that $\mu=\left(\mu_{p}, \mu_{g}\right) \in \mathscr{D}_{p} \times \mathscr{D}_{g}=: \mathscr{D}$ and $\mathscr{D}_{p} \subset \mathbb{R}^{d p}, \mathscr{D}_{g} \subset \mathbb{R}^{d_{g}}$, with $p=d_{p}+d_{g}$. We focus on the following advection-diffusion-reaction problem:

$$
\begin{cases}-\frac{\partial}{\partial x_{i}}\left(v_{i j}\left(\mathbf{x} ; \mu_{p}\right) \frac{\partial}{\partial x_{j}} u(\mu)\right)+\left(b_{i}\left(\mathbf{x} ; \mu_{p}\right) \frac{\partial}{\partial x_{i}}\right) u(\mu) \\ u+\gamma\left(\mathbf{x} ; \mu_{p}\right) u(\mu)=F\left(\mathbf{x} ; \mu_{p}\right), & \mathbf{x} \in \Omega\left(\mu_{g}\right) \\ u(\mu)=0, & \mathbf{x} \in \Gamma_{D} \\ v_{i j}\left(\mathbf{x} ; \mu_{p}\right) \frac{\partial}{\partial x_{j}} u(\mu)=g_{N}\left(\mathbf{x} ; \mu_{p}\right), & \mathbf{x} \in \Gamma_{N}\end{cases}
$$

where we suppose that all coefficients (e.g. thermal conductivity, elastic coefficients, and so on) may depend on spatial coordinates and also a vector $\mu_{p}$ of physical parameters. The domain $\Omega\left(\mu_{g}\right) \subset \mathbb{R}^{d}, d=1,2,3$, may depend instead on a vector $\mu_{g}$ of geometrical parameters. For the sake of simplicity, in the following we consider problems defined over two-dimensional domains $(d=2)$.

Here we assume that only Neumann data imposed on $\Gamma_{N}$ are affected by the physical parameters, whereas on $\Gamma_{D}=\partial \Omega\left(\mu_{g}\right) \backslash \Gamma_{N}$ we impose homogeneous Dirichlet conditions. Parametrized Dirichlet data, although undergoing a similar treatment, involve suitable (possibly parametrized) lifting functions.

\subsubsection{Physical parameters}

In the simplest case, input parameters $\mu=\mu_{p}$ represent physical properties, whereas the computational domain $\Omega$ is parameter-independent. Moreover, we assume that the parametric dependence of the physical coefficients does not change on the computational domain. The situation where the domain is split in subregions associated with different physical properties will be covered later on.

The bilinear form and the linear form appearing in $(20.2)$ are given in this case by

$$
\begin{gathered}
a\left(w, v ; \mu_{p}\right)=\int_{\Omega} \frac{\partial w}{\partial x_{i}} v_{i j}\left(\mathbf{x} ; \mu_{p}\right) \frac{\partial v}{\partial x_{j}} d \Omega+\int_{\Omega} b_{i}\left(\mathbf{x} ; \mu_{p}\right) \frac{\partial w}{\partial x_{i}} v d \Omega+\int_{\Omega} \gamma\left(\mathbf{x} ; \mu_{p}\right) w v d \Omega \\
(20.101) \\
f(v ; \mu)=\int_{\Omega} F\left(\mathbf{x} ; \mu_{p}\right) v d \Omega+\int_{\Gamma_{N}} g_{N}\left(\mathbf{x} ; \mu_{p}\right) v d \Gamma
\end{gathered}
$$

where $V=H_{\Gamma_{D}}^{1}(\Omega)$; summation over repeated indices is understood.

In order to ensure an affine expansion of $(20.101)$, we assume that $\mu_{p}$ and $\mathbf{x}$ can be treated as separate variables, i.e. each coefficient shall be expressed as a product of two functions:

$$
\begin{gathered}
v_{i j}\left(\mathbf{x} ; \mu_{p}\right)=K_{i j}\left(\mu_{p}\right) \psi_{i j}(\mathbf{x}), \quad 1 \leq i, j \leq 2 \\
b_{i}\left(\mathbf{x} ; \mu_{p}\right)=K_{i 3}\left(\mu_{p}\right) \psi_{i 3}(\mathbf{x}), \quad i=1,2 \\
\gamma\left(\mathbf{x} ; \mu_{p}\right)=K_{33}\left(\mu_{p}\right) \psi_{33}(\mathbf{x})
\end{gathered}
$$

In this way, by taking the terms depending on the parameters $\mu_{p}$ out of the integrals, we can rewrite (20.101) as follows:

$$
a(w, v ; \mu)=\sum_{i, j=1}^{3} K_{i j}\left(\mu_{p}\right) \int_{\Omega}\left[\begin{array}{ccc}
\frac{\partial w}{\partial x_{1}} & \frac{\partial w}{\partial x_{2}} & w
\end{array}\right] \psi_{i j}(\mathbf{x})\left[\begin{array}{c}
\frac{\partial v}{\partial x_{1}} \\
\frac{\partial v}{\partial x_{2}} \\
v
\end{array}\right] d \Omega
$$

where $\mathbf{x}=\left(x_{1}, x_{2}\right)$ denotes a point in $\Omega$. Here $K: \mathscr{D}_{p} \rightarrow \mathbb{R}^{3 \times 3}$ and $\Psi: \Omega \rightarrow \mathbb{R}^{3 \times 3}$ are given matrices $-$ of components $K_{i j}\left(\mu_{p}\right)$ and $\psi_{i j}(\mathbf{x})$, respectively $-$ which are required to be symmetric and positive definite for any $\mathbf{x} \in \Omega, \mu_{p} \in \mathscr{D}_{p}$, in order to ensure the coercivity of the bilinear form.

In particular, the upper $2 \times 2$ principal submatrices of $K$ and $\Psi$ give the usual conductivity/diffusivity tensor; the $(3,3)$ term allows to consider a reaction (or mass) term. The $(3,1),(3,2)$ (and $(1,3),(2,3))$ elements of $K$ and $\Psi$ are related to first derivative (or convective) terms; in particular, in order to recover (20.101) we assume that $K_{31}\left(\mu_{p}\right)=K_{32}\left(\mu_{p}\right)=0$.

For the sake of simplicity, in the following we assume that $g_{N}=0$. We remark that, in the case of purely physical parametrization, the affinity assumption (20.6) is easy to fulfill; see Exercise $3$.

Let us now consider the case in which the domain

$$
\bar{\Omega}=\bigcup_{k=1}^{K_{\mathrm{reg}}} \mathscr{R}_{k}
$$

is split into regions $^{9} \mathscr{R}_{k}$, each associated with different physical properties. Instead of (20.101) we might therefore have

$$
\begin{aligned}
a\left(w, v ; \mu_{p}\right)=\sum_{k=1}^{K_{\text {reg }}} \int_{\mathscr{R}_{k}} & \frac{\partial w}{\partial x_{i}} v_{i j}^{k}\left(\mathbf{x} ; \mu_{p}\right) \frac{\partial v}{\partial x_{j}} d \Omega \\
&+\sum_{k=1}^{K_{\text {reg }}} \int_{\mathscr{R}_{k}} b_{i}^{k}\left(\mathbf{x} ; \mu_{p}\right) \frac{\partial w}{\partial x_{i}} v d \Omega+\sum_{k=1}^{K_{\text {reg }}} \int_{\mathscr{R}_{k}} \gamma^{k}\left(\mathbf{x} ; \mu_{p}\right) w v d \Omega
\end{aligned}
$$

with

$$
\begin{gathered}
v_{i j}^{k}\left(\mathbf{x} ; \mu_{p}\right)=K_{i j}^{k}\left(\mu_{p}\right) \psi_{i j}^{k}(\mathbf{x}), \quad 1 \leq i, j \leq 2 \\
b_{i}^{k}\left(\mathbf{x} ; \mu_{p}\right)=K_{i 3}^{k}\left(\mu_{p}\right) \psi_{i 3}(\mathbf{x}), \quad i=1,2, \quad \gamma^{k}\left(\mathbf{x} ; \mu_{p}\right)=K_{33}^{k}\left(\mu_{p}\right) \psi_{33}^{k}(\mathbf{x})
\end{gathered}
$$

${ }^{9}$ We make a distinction between $(i)$ a partition of the computational domain accounting for different physical properties over different subregions and (ii) a domain decomposition in different subdomains for the sake of geometrical representation, as shown in subsection 20.9.2. for any $1 \leq k \leq K_{\mathrm{reg}}$, and therefore

$$
a(w, v ; \mu)=\sum_{k=1}^{K_{\text {reg }}} \sum_{i, j=1}^{3} K_{i j}^{k}\left(\mu_{p}\right) \int_{\mathscr{R}_{k}}\left[\begin{array}{lll}
\frac{\partial w}{\partial x_{1}} & \frac{\partial w}{\partial x_{2}} & w
\end{array}\right] \psi_{i j}^{k}(x)\left[\begin{array}{c}
\frac{\partial v}{\partial x_{1}} \\
\frac{\partial v}{\partial x_{2}} \\
v
\end{array}\right] d \Omega
$$

where, for any $k=1, \ldots, K_{\mathrm{reg}}, K^{k}: \mathscr{D}_{p} \rightarrow \mathbb{R}^{3 \times 3}$ and $\Psi^{k}: \Omega \rightarrow \mathbb{R}^{3 \times 3}$ are given matrices. By proceeding as we did before, it is possible to recover the affine expansion also in this case.

\subsubsection{Geometrical parameters}

We now turn to the more difficult situation of geometrical parametrization. In this case, the original parametrized problem defined over a parametrized domain $\Omega_{\mathrm{o}}\left(\mu_{g}\right)$ needs to be transformed into an equivalent problem defined over a parameter-independent domain $\Omega$. This is a requirement of the $\mathrm{RB}$ method: if we wish to consider linear combinations of pre-computed solutions (the snapshots), the latter must refer to a common spatial configuration.

Thus, to permit geometric variation, our parameter-independent reference domain $\Omega$ must be regarded as the pre-image of $\Omega_{0}\left(\mu_{g}\right)$, the parameter-dependent "actual" or "original" domain of interest. The geometric transformation will yield variable (parameter-dependent) coefficients of linear and bilinear forms in the reference domain that, under suitable hypotheses to be discussed below, will take the requisite affine form $(20.6)$.

We shall first define an "original" problem (subscript $o$ ), over the parameterdependent domain $\Omega_{\mathrm{o}}=\Omega_{\mathrm{o}}\left(\mu_{g}\right) ;$ we denote $V_{\mathrm{o}}$ a suitable Hilbert space defined on $\Omega_{\mathrm{o}}\left(\mu_{g}\right)$. We shall also take into account possible physical parameters, so that $\mu=\left(\mu_{p}, \mu_{g}\right) \in \mathscr{D}=\mathscr{D}_{p} \times \mathscr{D}_{g}$ and $\mathscr{D}_{p} \subset \mathbb{R}^{d_{p}}, \mathscr{D}_{g} \subset \mathbb{R}^{d_{g}}$, with $p=d_{p}+d_{g}$.

In the elliptic case, the original problem reads as follows: given $\mu \in \mathscr{D}$, evaluate

$$
s_{\mathrm{o}}(\mu)=J_{\mathrm{o}}\left(u_{\mathrm{o}}(\mu)\right),
$$

where $u_{\mathrm{o}}(\mu) \in V_{\mathrm{o}}$ satisfies

$$
a_{\mathrm{o}}\left(u_{\mathrm{o}}(\mu), v ; \mu\right)=F_{\mathrm{o}}(v ; \mu) \quad \forall v \in V_{\mathrm{o}}
$$

The reference domain $\Omega$ is thus related to the original domain $\Omega_{\mathrm{o}}\left(\mu_{g}\right)$ through a parametric mapping $T\left(\cdot ; \mu_{g}\right)$, such that $\Omega_{\mathrm{o}}\left(\mu_{g}\right)=T\left(\Omega ; \mu_{g}\right) ;$ in particular, we may set $\Omega=\Omega_{\mathrm{o}}\left(\mu_{g}^{\text {ref }}\right)$, for a selected parameter value $\mu_{g}^{\text {ref }} \in \mathscr{D}_{g}$. In order to build a parametric mapping related to geometrical properties, we shall introduce a conforming domain decomposition of $\Omega_{\mathrm{o}}\left(\mu_{g}\right)$,

$$
\bar{\Omega}_{\mathrm{o}}\left(\mu_{g}\right)=\bigcup_{k=1}^{K_{\mathrm{dom}}} \bar{\Omega}_{\mathrm{o}}^{k}\left(\mu_{g}\right)
$$

consisting of mutually disjoint open subdomains $\Omega_{\mathrm{o}}^{k}\left(\mu_{g}\right)$,

$$
\Omega_{\mathrm{o}}^{k}\left(\mu_{g}\right) \cap \Omega_{\mathrm{o}}^{k^{\prime}}\left(\mu_{g}\right)=\emptyset, \quad 1 \leq k<k^{\prime} \leq K_{\mathrm{dom}}
$$

If related to geometrical properties used as input parameters (e.g. lengths, thicknesses, diameters or angles) parametric mappings can be defined in a quite intuitive fashion. We point out that, in the case of a combined physical/geometrical parametrization, the domain decomposition (20.111) can be used not only for algorithmic purposes, to ensure well-behaved mappings, but also to represent different physical coefficients, as in the case described in Sect. 20.9.1.

Hence, the original and reference subdomains are related under a map $T\left(\cdot ; \mu_{g}\right):$ $\Omega^{k} \rightarrow \Omega_{\mathrm{o}}^{k}\left(\mu_{g}\right), 1 \leq k \leq K_{\mathrm{dom}}$

$$
\Omega_{\mathrm{o}}^{k}\left(\mu_{g}\right)=T^{k}\left(\Omega^{k} ; \mu_{g}\right), \quad 1 \leq k \leq K_{\mathrm{dom}}
$$

these maps must be individually bijective, collectively continuous, and such that $T^{k}\left(\mathbf{x} ; \mu_{g}\right)=T^{k^{\prime}}\left(\mathbf{x} ; \mu_{g}\right) \forall \mathbf{x} \in \Omega^{k} \cap \Omega^{k^{\prime}}$, for $1 \leq k<k^{\prime} \leq K_{\text {dom }}$. Here we treat the simpler affine case, where the transformation is given, for any $\mu \in \mathscr{D}, \mathbf{x} \in \Omega^{k}$, by

$$
T_{i}^{k}\left(\mathbf{x}, \mu_{g}\right)=c_{i}^{k}\left(\mu_{g}\right)+\sum_{j=1}^{d} G_{i j}^{k}\left(\mu_{g}\right) x_{j}, 1 \leq i \leq d
$$

for given translation vectors $\mathbf{c}^{k}: \mathscr{D}_{g} \rightarrow \mathbb{R}^{d}$ and linear transformation matrices $\mathbb{G}^{k}$ : $\mathscr{D}_{g} \rightarrow \mathbb{R}^{d \times d}$, also known as "mapping coefficients". The linear transformation matrices can express rotation, scaling and/or shear. We can then define the associated Jacobians

$$
J^{k}\left(\mu_{g}\right)=\left|\operatorname{det}\left(\mathbb{G}^{k}\left(\mu_{g}\right)\right)\right|, \quad 1 \leq k \leq K_{\text {dom }}
$$

By assuming that the geometrical transformation is affine (i.e. given by $(20.113))$ the Jacobian is constant in space over each subdomain. We further define, for any $\mu_{g} \in \mathscr{D}_{g}$,

$$
\mathbb{D}^{k}\left(\mu_{g}\right)=\left(\mathbb{G}^{k}\left(\mu_{g}\right)\right)^{-1}, \quad 1 \leq k \leq K_{\mathrm{dom}}
$$

this matrix shall prove convenient in subsequent transformations involving derivatives. Under the assumption that the mapping is invertible, we know that the Jacobian $J\left(\mu_{g}\right)$ of $(20.114)$ is strictly positive, and that the derivative transformation matrix, $\mathbb{D}\left(\mu_{g}\right)=$ $\left(\mathbb{G}\left(\mu_{g}\right)\right)^{-1}$ of $(20.115)$, exists.

We recall that, in two dimensions, an affine transformation maps straight lines to straight lines, parallel lines to parallel lines and indeed parallel segments of equal length to parallel segments of equal length: it follows that a triangle maps to a triangle, a parallelogram to a parallelogram. We also recall that an affine transformation maps ellipses to ellipses. These properties are crucial for the description of domains relevant in engineering contexts, so that piecewise affine mappings, based on a domain decomposition in standard, elliptical, and curvy triangles [RHP08], can be employed to treat a much larger class of geometric variations, still satisfying the affinity assumption crucial for reducing the computational complexity. See for instance [RHP08] for further details.

Let us now place some conditions on $a_{0}$ and $F_{\mathrm{o}}$ so to ensure, combined with the affine geometry assumption, an affine expansion of the bilinear form. Similarly to what we have done in Sect. $20.9 .1$ in the case of physical parameters, we require $a_{0}(\cdot, \cdot ; \mu)$ : $V_{\mathrm{o}} \times V_{\mathrm{o}} \rightarrow \mathbb{R}$ to be expressed as

$$
a_{\mathrm{o}}(w, v ; \mu)=\sum_{k=1}^{K_{\mathrm{dom}}} \int_{\Omega_{0}^{k}\left(\mu_{g}\right)}\left[\begin{array}{lll}
\frac{\partial w}{\partial x_{\mathrm{ol}}} & \frac{\partial w}{\partial x_{\mathrm{o} 2}} & w
\end{array}\right] \mathscr{K}_{\mathrm{o}, i j}^{k}\left(\mathbf{x}_{\mathrm{o}} ; \mu_{p}\right)\left[\begin{array}{c}
\frac{\partial v}{\partial x_{\mathrm{ol}}} \\
\frac{\partial v}{\partial x_{\mathrm{o} 2}} \\
v
\end{array}\right] d \Omega_{\mathrm{o}}
$$

where $\mathbf{x}_{0}=\left(x_{\mathrm{ol}}, x_{\mathrm{o} 2}\right)$ denotes a point in $\Omega_{\mathrm{o}}\left(\mu_{g}\right)$. Here, for $1 \leq k \leq K_{\mathrm{dom}}, \mathscr{K}_{\mathrm{o}}^{k}: \Omega_{o}^{k} \times$ $\mathscr{D}_{p} \rightarrow \mathbb{R}^{3 \times 3}$ are given matrices, whose components depend on both spatial coordinates and physical parameters. In case the parametrization is concerned with both physical and geometrical elements, the matrices $\mathscr{K}_{\mathrm{o}}^{k}$ encode the properties related to the former; the meaning of the terms is the same as in Sect. 20.9.1.

Similarly, we require that $F_{\mathrm{o}}(\cdot ; \mu): V_{\mathrm{o}} \rightarrow \mathbb{R}$ can be expressed as

$$
F_{\mathrm{o}}(v ; \mu)=\sum_{k=1}^{K_{\mathrm{dom}}} \int_{\Omega_{0}^{k}\left(\mu_{g}\right)} \mathscr{F}_{\mathrm{o}}^{k}\left(\mathbf{x}_{0} ; \mu_{p}\right) v d \Omega_{\mathrm{o}}
$$

where, for $1 \leq k \leq K_{\text {dom }}, \mathscr{F}_{0}^{k}: \Omega_{o}^{k} \times \mathscr{D}_{p} \rightarrow \mathbb{R}$ are given functions, which might encode possible physical parametrizations related to source terms and/or boundary data.

We now transform the problem given on the original domain, into an equivalent problem over the reference domain: given $\mu \in \mathscr{D}$, evaluate

$$
s(\mu)=F(u(\mu))
$$

where $u(\mu) \in V$ satisfies

$$
a(u(\mu), v ; \mu)=F(v ; \mu) \quad \forall v \in V
$$

We may then identify $s(\mu)=s_{0}(\mu)$ and $u(\mu)=u_{\mathrm{o}}(\mu) \circ T\left(\cdot ; \mu_{g}\right)$, while

$$
a(w, v ; \mu)=\sum_{k=1}^{K_{\mathrm{dom}}} \int_{\Omega^{k}}\left[\begin{array}{lll}
\frac{\partial w}{\partial x_{1}} & \frac{\partial w}{\partial x_{2}} & w
\end{array}\right] \mathscr{K}_{i j}^{k}(\mathbf{x} ; \mu)\left[\begin{array}{c}
\frac{\partial v}{\partial x_{1}} \\
\frac{\partial v}{\partial x_{2}} \\
v
\end{array}\right] d \Omega
$$

is the transformed bilinear form. Here the $\mathscr{K}^{k}: \Omega^{k} \times \mathscr{D} \rightarrow \mathbb{R}^{3 \times 3}, 1 \leq k \leq K_{\mathrm{dom}}$, are symmetric positive definite matrices, whose components depend both on the spatial coordinates $\mathbf{x}=\left(x_{1}, x_{2}\right)$ and the parameters $\mu=\left(\mu_{p}, \mu_{g}\right) \in \mathscr{D}$

$$
\mathscr{K}^{k}(\mathbf{x} ; \mu)=J^{k}\left(\mu_{g}\right) \mathscr{G}^{k}\left(\mu_{g}\right) \mathscr{K}_{\mathrm{o}}^{k}\left(T^{k}\left(\mathbf{x} ; \mu_{g}\right) ; \mu_{p}\right)\left(\mathscr{G}^{k}\left(\mu_{g}\right)\right)^{\mathrm{T}}
$$

for $1 \leq k \leq K_{\mathrm{dom}}$. The matrices $\mathscr{G}^{k}: \mathscr{D}_{g} \rightarrow \mathbb{R}^{3 \times 3}, 1 \leq k \leq K_{\mathrm{dom}}$, are given by

$$
\mathscr{G}^{k}(\mu)=\left(\begin{array}{cc|c}
\mathbb{D}^{k}\left(\mu_{g}\right) & \begin{array}{l}
0 \\
0
\end{array} \\
\hline 0 & 0 & 1
\end{array}\right)
$$

$J^{k}\left(\mu_{g}\right)$ and $\mathbb{D}^{k}\left(\mu_{g}\right), 1 \leq k \leq K_{\mathrm{dom}}$, are given by $(20.114)$ and $(20.115)$, respectively.

Similarly, the new linear form can be expressed as

$$
F(v ; \mu)=\sum_{k=1}^{K_{\text {dom }}} \int_{\Omega^{k}} \mathscr{F}^{k}(\mathbf{x} ; \mu) v d \Omega
$$

where $\mathscr{F}^{k}: \Omega^{k} \times \mathscr{D} \rightarrow \mathbb{R}, 1 \leq k \leq K_{\mathrm{dom}}$, is given by

$$
\mathscr{F}^{k}(\mathbf{x} ; \mu)=J^{k}\left(\mu_{g}\right) \mathscr{F}_{\mathrm{o}}^{k}\left(T^{k}\left(\mathbf{x} ; \mu_{g}\right) ; \mu_{p}\right), \quad 1 \leq k \leq K_{\mathrm{dom}}
$$

We point out that, compared to a purely physical parametrization (20.110), a geometrical parametrization yields a similar but more involved structure of the tensor appearing in the bilinear form. In order to recover the affine expansion (20.6) for the bilinear form $a$ also in this case, we can proceed as in the case of physical parametrizations, once the original problem is read on the reference domain $\Omega$. In particular, we require that the geometrical mapping $T\left(\cdot ; \mu_{g}\right)$ is affine on each subdomain $\Omega^{k}$, i.e. it is of the form $(20.113)$, and that the parametrized physical coefficients depend affinely on the physical parameters $\mu_{p}$, i.e. they can be expressed as in (20.108)-(20.109). Under these conditions, the bilinear form satisfies the affine parametric dependence (20.6); $F$ undergoes a similar treatment. An example taking into account both geometrical and physical parameters will be explained thoroughly at the end of the chapter, see Sect. $20.10$.

We close by noting that the conditions we provide are sufficient but not necessary. For example, we can permit affine polynomial dependence on $\mathbf{x}_{0}$ in both $\mathscr{K}_{\mathrm{o}}^{k}\left(\mathbf{x}_{0} ; \mu\right)$ and $\mathscr{F}_{\mathrm{o}}^{k}\left(\mathbf{x}_{0} ; \mu\right)$ and still ensure an affine development under the form $(20.6)$; furthermore, in the absence of geometric variation, $\mathscr{K}_{\mathrm{o}}^{k}\left(\mathbf{x}_{0} ; \mu\right)$ and $\mathscr{F}_{\mathrm{o}}^{k}\left(\mathbf{x}_{0} ; \mu\right)$ can take on any "separable" form in $\mathbf{x}_{0}, \mu$. However, the affine expansion $(20.6)$ is by no means completely general: for more involved data parametric dependencies, non-affine techniques [BNMP04, GMNP07, Roz09] must be invoked. 

\subsection{A working example: a diffusion-convection problem}

In order to illustrate the previous concepts on a real example, in this section $^{10}$ we discuss a steady heat diffusion/convection problem. Precisely, we consider heat convection combined with heat diffusion in a straight duct, whose walls can be kept at uniform temperature insulated, or characterized by heat exchange.

This example describes a class of heat transfer problems in fluidic devices with a versatile configuration. In particular, the Péclet number, as a measure of axial transport velocity field (modelling the physics of the problem), and the length of the noninsulated portion of the duct, are only two of the possible parameters that may be varied in order to extract average temperatures. Also discontinuities in Neumann boundary conditions (different heat fluxes) and thermal boundary layers are interesting phenomena to be studied.

We consider the physical domain $\Omega_{\mathrm{o}}(\mu)$ shown in Fig. 20.6; all lengths are nondimensionalized with respect to a unit length $\tilde{h}$ (dimensional channel width); moreover, let us denote $\tilde{k}$ the dimensional (thermal) conductivity coefficient for the air flowing in the duct, $\tilde{\rho}$ its density and $\tilde{c}_{p}$ the specific heat capacity under constant pressure. We introduce the (thermal) diffusion coefficient $\tilde{D}=\tilde{k} / \tilde{\rho} \tilde{c}_{p}$, as well as the Péclet number $\mathbb{P e}=\tilde{U} \tilde{h} / \tilde{D}, \tilde{U}$ being the reference dimensional velocity for the convective field (see Chap. 12). We consider $P=2$ parameters: $\mu_{1}$ is the length of the noninsulated bottom portion of the duct (unit heat flux), while $\mu_{2}$ represents the Péclet number itself; the parameter domain is given by $\mathscr{D}=[1,10] \times[0.1,100]$.

The solution $u(\mu)$, defined as the non-dimensional temperature $u(\mu)=(\tau(\mu)-$ $\left.\tau_{i n}\right) / \tau_{i n}$ (where $\tau(\mu)$ is the dimensional temperature, $\tau_{i n}$ is the dimensional temperature of the air at the inflow and in the first portion of the duct), satisfies the following
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-269.jpg?height=174&width=682&top_left_y=854&top_left_x=110)

Fig. 20.6. Parametrized geometry and domain boundaries

10 Throughout the section, $\Omega_{0}(\mu)$ denotes the original (physical) domain, whose generic point is indicated by $\left(x_{1}, x_{2}\right)$; for the sake of simplicity, we formulate all problems in the original domain, but remove the subscripts o. Moreover, twiddle denotes dimensional quantities, while its absence signals a non-dimensional quantity. steady advection-diffusion problem:

$$
\left\{\begin{aligned}
-\frac{1}{\mu_{2}} \Delta u(\mu)+x_{2} \frac{\partial}{\partial x_{1}} u(\mu) &=0 & & \text { in } \Omega_{\mathrm{o}}(\mu) \\
\frac{1}{\mu_{2}} \frac{\partial u}{\partial n}(\mu) &=0 & & \text { on } \Gamma_{1} \cup \Gamma_{3} \\
\frac{1}{\mu_{2}} \frac{\partial u}{\partial n}(\mu) &=1 & & \text { on } \Gamma_{2} \\
u(\mu) &=0, & & \text { on } \Gamma_{4} \cup \Gamma_{5} \cup \Gamma_{6}
\end{aligned}\right.
$$

with summation $(i, j=1,2)$ over repeated indices; hence, we impose the temperature at the top walls and in the "inflow" zone of the duct $\left(\Gamma_{6}\right)$, while we consider an insulated wall (zero heat flux on $\Gamma_{1}$ and $\Gamma_{3}$ ) or heat exchange at a fixed rate (i.e. one on $\Gamma_{2}$ ) on other boundaries. We note that the forced convection field is given by a linear velocity profile $x_{2} \tilde{U}$ (Couette-type flow, [Pan05]).

The output of interest is the average temperature of the fluid on the non-insulated portion of the bottom wall of the duct, given by

$$
s(\mu)=T_{\mathrm{av}}(\mu)=\frac{1}{\mu_{1}} \int_{\Gamma_{2}} u(\mu)
$$

This problem is then mapped to the fixed reference domain $\Omega$ and discretized by the Galerkin FEM with piecewise linear elements; the dimension of the corresponding space is $N_{h}=5433$. Since we are in a noncompliant case, a further dual problem has to be solved in order to obtain better output evaluations and corresponding error bounds, see Sect. 20.8. In particular, we show in Fig. $20.7$ the lower bound of the coercivity constant of the bilinear form associated to our problem.

We plot in Fig. $20.8$ the convergence of the greedy algorithm for both the primal and the dual problem; with a fixed tolerance $\varepsilon_{\text {tol }}^{*}=10^{-2}, N_{p r, \max }=21$ and $N_{d u, \max }=30$ basis functions have been selected, respectively.

In Fig. $20.9$ the selected parameter values $S_{N_{p r}}$ for the primal problem and $S_{N_{d u}}$ for the dual problem are shown; in each case $\Xi_{\text {train }}$ is a uniform random sample of size $n_{\text {train }}=1000$.

Moreover, in Fig. $20.10$ some representative solutions (computed for $\left.N=N_{\max }\right)$ for selected values of parameters are reported.

The thermal boundary layer looks very different in the four cases. In particular, big variations of the temperature, as well as large gradients along the lower wall - are remarkable for high Péclet number, when forced convection dominates steady conduction; moreover, the standard behaviour of boundary layer width - usually given by $O(1 / \mathbb{P e})-$ is captured correctly.

In Fig. $20.11$ the RB evaluation (for $\left.N=N_{\max }\right)$ of the output of interest is reported as a function of the parameters, as well as the corresponding error bound. As we can see, for low values of $\mu_{2}$ (Péclet number) the dependence of the output on $\mu_{1}$ 

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-271.jpg?height=401&width=557&top_left_y=113&top_left_x=180)

Fig. 20.7. Lower bound of the coercivity constant $\alpha_{\mathrm{LB}}^{N_{h}}(\mu)$ as a function of $\mu$
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-271.jpg?height=390&width=718&top_left_y=598&top_left_x=94)

Fig. 20.8. Relative errors $\max _{\mu \in \mathbb{E}_{\text {train }}}\left(\Delta_{N_{p r}}(\mu) /\left\|u_{N_{p r}}(\mu)\right\|_{X}\right)$ and $\max _{\mu \in \Xi_{\text {train }}}\left(\Delta_{N_{d u}}(\mu) /\right.$ $\left.\left\|\psi_{N_{d u}}(\mu)\right\|_{X}\right)$ as functions of $N^{p r}$ and $N^{d u}$ for the $\mathrm{RB}$ approximations computed during the greedy procedure, for the primal (left) and the dual (right) problem, respectively. Here $\Xi_{\text {train }}$ is a uniform random sample of size $n_{\text {train }}=1000$ and the RB tolerance is $\varepsilon_{\mathrm{tol}}^{*}=10^{-2}$

(geometrical aspect) is rather modest; for high values of $\mu_{2}$, instead, the output shows a larger variation with respect to $\mu_{1}$. In the same way, for longer/shorter channels the dependence on the Péclet number is higher/lower. 
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-272.jpg?height=258&width=712&top_left_y=113&top_left_x=100)

Fig. 20.9. Selected parameter values $S_{N_{p r}}$ for the primal (left) and $S_{N_{d u}}$ for the dual (right) in the parameter space

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-272.jpg?height=239&width=351&top_left_y=454&top_left_x=94)
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-272.jpg?height=240&width=726&top_left_y=454&top_left_x=94) $(10,100)$ (bottom)

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-272.jpg?height=224&width=297&top_left_y=777&top_left_x=124)

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-272.jpg?height=224&width=297&top_left_y=774&top_left_x=494)
![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-272.jpg?height=226&width=666&top_left_y=774&top_left_x=124) in the parameter space

We conclude by collecting in Table $20.1$ all the data relevant to the RB approximation here considered.

Before closing this chapter, we warn the reader that reduced basis methods and, more in general, computational reduction techniques, are still a rapidly evolving research area. Table 20.1. Numerical details for the test case presented. RB spaces have been built by means of the greedy procedure, using a tolerance $\varepsilon_{\mathrm{tol}}^{\mathrm{RB}}=10^{-2}$ and a uniform $\mathrm{RB}$ greedy train sample of size $n_{\text {train }}=1000$. A comparison of the computational times between the Online $\mathrm{RB}$ evaluations and the corresponding FE simulations is reported. Here $t_{R B}^{\text {offline }}$ is the time of the Offline $\mathrm{RB}$ construction and storage, $t_{R B}^{\text {online }}$ is the time of an Online $\mathrm{RB}$ computation, while $t_{F E}$ is the time for an FE computation, once FE matrices are built

\begin{tabular}{lc}
\hline Approximation data & \\
\hline Number of parameters $P$ & 2 \\
Affine op. components $Q_{a}$ & 4 \\
Affine rhs components $Q_{f}$ & 1 \\
FE space dim. $\mathscr{N}$ & 5433 \\
RB primal space dim. $N_{\max }^{p r}$ & 21 \\
RB dual space dim. $N_{\max }^{d u}$ & 30 \\
\hline
\end{tabular}

\begin{tabular}{lc}
\hline Approximation data & \\
\hline RB construction $t_{R B}^{\text {offline }}$ (s) & $362.8 \mathrm{~s}$ \\
RB evaluation $t_{R B}^{\text {online }}(\mathrm{s})$ & $0.107 \mathrm{~s}$ \\
FE evaluation $t_{F E}(\mathrm{~s})$ & $14.3$ \\
Computational speedup $\mathscr{S}$ & 133 \\
Break-even point $\mathscr{Q}_{B E}$ & 26 \\
&
\end{tabular}

In this chapter we have provided a short introduction, pointed out all the essential ingredients required to set up a reduced basis method, even if only for the simple case of steady, elliptic, coercive PDEs. In the last decade RB methods and, more in general, computational reduction techniques, have been applied to several problems modelled by non-affinely parametrized and/or nonlinear, and/or noncoercive, and/or time-dependent problems (see e.g. [AF12, AZF12]), such as Stokes [RHM13] and Navier-Stokes [VP05, QR07] flows, elasticity problems and so on. A recent survey on these classes of problems, and on other computational reduction techniques, can be found for instance in [QR13].

\section{Exercises}

1. Prove Property 19.1; proceed according to the following steps (for simplicity, we omit the dependence on $\mu$ ):

(i) rewrite (19.63) as

$$
\left(\mathbb{A}_{h} \mathbb{V} \mathbf{u}_{N}, \mathbb{A}_{h} \mathbb{V} \mathbf{v}_{N}\right)_{2}=\left(\mathbf{f}_{h}, \mathbb{A}_{h} \mathbb{V} \mathbf{v}_{N}\right)_{2} \quad \forall \mathbf{v}_{N} \in \mathbb{R}^{N}
$$

(ii) by using bijection (19.44) and observing that $\left(\mathbb{V} \mathbf{v}_{N}\right)^{(p)}=\left(v_{N}, \varphi^{p}\right)_{h}$, show that

$$
\left(\mathbb{A}_{h} \mathbb{V} \mathbf{v}_{N}, \mathbf{z}_{h}\right)_{2}=a\left(v_{N}, z_{h}\right) \quad \forall \mathbf{v}_{N} \in \mathbb{R}^{N}, \mathbf{z}_{h} \in \mathbb{R}^{N_{h}}
$$

where $z_{h}=I_{h} \mathbf{z}_{h} \in V^{N_{h}}$, that is, $z_{h}(\mathbf{x})=\sum_{r=1}^{N_{h}} z_{h}^{(r)} \varphi^{r}(\mathbf{x})$ and $v_{N}=I_{N} \mathbf{v}_{N} \in V_{N}$,

that is $v_{N}(\mathbf{x})=\sum_{k=1}^{N} v_{N}^{(k)} \zeta_{k}(\mathbf{x})$;

(iii) define $\mathbf{y}_{h}=\mathbf{y}_{h}\left(\mathbf{v}_{N}\right)=\mathbb{A}_{h} \mathbb{V} \mathbf{v}_{N} \in \mathbb{R}^{N_{h}} ;$ then

$$
\left(\mathbf{f}_{h}, \mathbb{A}_{h} \mathbb{V} \mathbf{v}_{N}\right)_{2}=\left(\mathbf{f}_{h}, \mathbf{y}_{h}\left(\mathbf{v}_{N}\right)\right)_{2}
$$

note that $y_{h}\left(\mathbf{v}_{N}\right)=I_{h} \mathbf{y}_{h}\left(\mathbf{v}_{N}\right) \in V_{h}$ is such that

$$
\left(y_{h}\left(\mathbf{v}_{N}\right), z_{h}\right)_{h}=\left(\mathbf{y}_{h}\left(\mathbf{v}_{N}\right), \mathbf{z}_{h}\right)_{2}=\left(\mathbb{A}_{h} \mathbb{V} \mathbf{v}_{N}, \mathbf{z}_{h}\right)_{2} \quad \forall z_{h} \in V^{N_{h}}
$$

and by using (ii), show that $y_{h}\left(\mathbf{v}_{N}\right)$ satisfies (19.67);

(iv) show that

$$
\left(\mathbf{f}_{h}, \mathbf{y}_{h}\left(\mathbf{v}_{N}\right)\right)_{2}=\left(f_{h}, y_{h}\left(\mathbf{v}_{N}\right)\right)_{h}=f\left(y_{h}\left(\mathbf{v}_{N}\right)\right),
$$

where $f_{h}=I_{h} \mathbf{f}_{h} \in V^{N_{h}}$, that is $f_{h}(\mathbf{x})=\sum_{r=1}^{N_{h}} f_{h}^{(r)} \varphi^{r}(\mathbf{x})$, and $f_{h}^{(r)}=f\left(\varphi^{r}\right)$ (v) finally, recover the following variational form:

$$
a\left(u_{N}, y_{h}\left(v_{N}\right)\right)=f\left(y_{h}\left(v_{N}\right)\right) \quad \forall v_{N} \in V_{N}
$$

2. By introducing the Lagrangian function

$$
L\left(\mathbf{z}_{1}, \ldots, \mathbf{z}_{N}, \tilde{\lambda}_{11}, \ldots, \tilde{\lambda}_{i j}, \ldots, \tilde{\lambda}_{N N}\right)=E\left(\mathbf{z}_{1}, \ldots, \mathbf{z}_{N}\right)+\sum_{i, j=1}^{N} \tilde{\lambda}_{i j}\left(\mathbf{z}_{i}^{T} \mathbf{z}_{j}-\delta_{i j}\right)
$$

and writing the first-order necessary optimality conditions, show that the solution of problem (20.74) satisfies (20.72) and, in particular,

$$
\mathbb{U}^{T} \mathbb{U} \psi_{i}=\sigma_{i}^{2} \psi_{i} \quad i=1, \ldots, r
$$

Moreover, by denoting $\lambda_{i}=\tilde{\lambda}_{i i}$, show that

$$
\lambda_{i}=\sigma_{i}^{2}=\sum_{j=1}^{n_{\text {train }}}\left(\mathbf{u}_{j}^{T} \zeta_{i}\right)^{2}
$$

Furthermore, show that

$$
\sum_{i=1}^{n_{\text {train }}}\left\|\mathbf{u}_{i}-\sum_{j=1}^{k}\left(\mathbf{u}_{i}^{T} \zeta_{j}\right) \zeta_{j}\right\|_{2}^{2}=\sum_{i=1}^{n_{\text {train }}} \sum_{j=N+1}^{r}\left(\mathbf{u}_{i}^{T} \zeta_{j}\right)^{2}=\sum_{i=N+1}^{r} \lambda_{i}
$$

and thus $(20.75)$.

3. By considering the parametrized bilinear form $(20.106)$

$$
\begin{aligned}
&a\left(w, v ; \mu_{p}\right)=K_{11}\left(\mu_{p}\right) \int_{\Omega} \psi_{11}(x) \frac{\partial w}{\partial x_{1}} \frac{\partial v}{\partial x_{1}} d \Omega \\
&\quad+K_{12}\left(\mu_{p}\right) \int_{\Omega} \psi_{12}(x) \frac{\partial w}{\partial x_{1}} \frac{\partial v}{\partial x_{2}} d \Omega+\cdots+\mathscr{K}_{33}\left(\mu_{p}\right) \int_{\Omega} \psi_{33}(x) w v d \Omega
\end{aligned}
$$

recover the affine expansion (20.6). How many terms (at most) is the affine expansion made of? Now do the same but for the a domain split into $K_{\text {reg regions, each }}$ associated with different physical properties. 4. By referring to the problem addressed in Sect. 20.1.1 (the thermal block), consider an example of anisotropic conductivity. The associated bilinear form on each block reads

$$
a^{q}(w, v ; \mu)=\sum_{i, j=1}^{2} \mu_{x_{i} x_{j}}^{q} \int_{\mathscr{R}_{q}} \frac{\partial w}{\partial x_{i}} \frac{\partial v}{\partial x_{j}} d \Omega
$$

where $\left\{\mu_{x_{i} x_{j}}^{q}, i, j=1,2\right\}$ represent the conductivities modelling an anisotropic heat transfer in block $\mathscr{R}_{q}$. Provide a weak formulation of the parametrized problem; express $a(w, v ; \mu)$ by an affine decomposition like (20.13) and indicate the total number of parameters $P$ and the quantity $Q_{a}$ for the cases a) $B_{1}=B_{2}=3$, b) $B_{1}=$ $B_{2}=5$ and c) $B_{1}=3$ and $B_{2}=5$.

5. a) Consider a geometrical parametrization of the thermal block of Sect. $20.1 .1$ based on the formulation already provided in the text with $P=8$ physical parameters (i.e. $\mu_{s}$ for $\left.s=1, \ldots, 8\right)$, representing isotropic heat transfer in each sub-block. Consider the case of $3 \times 3$ sub-blocks with additional $P_{g}=6$ geometrical parameters (i.e. $\mu_{x_{1}}^{i}$ and $\mu_{x_{2}}^{i}$ for $\left.i=1,2,3\right)$. The length of the thermal block is $\mu_{x_{1}}^{1}+\mu_{x_{1}}^{2}+\mu_{x_{1}}^{3}=1$ in the $x_{1}$-direction and $\mu_{x_{2}}^{1}+\mu_{x_{2}}^{2}+\mu_{x_{2}}^{3}=1$ in the $x_{2}$-direction; in this way the first sub-block $\mathscr{R}_{1}$ (see Fig. 20.1) has dimension $\mu_{x_{1}}^{1} \times \mu_{x_{2}}^{1}$, while the sub-block $\mathscr{R}_{9}$ has dimension $\mu_{x_{1}}^{3} \times \mu_{x_{2}}^{3}$. Write the complete formulation for $a(w, v ; \mu)$ in the form given by $(20.13)$ and indicate the number $Q_{a}$ of forms $a^{q}(w, v ; \mu)$. Then report the complete formulation for all parameter-dependent functions $\Theta^{q}(\mu)$, in terms of physical parameters (i.e. $\mu_{s}$ for $\left.s=1, \ldots, 8\right)$ and geometrical parameters (i.e. $\mu_{x_{1}}^{i}$ and $\mu_{x_{2}}^{i}$ for $i=1,2,3)$.

b) Propose a range of variation for each geometrical parameter by respecting the given constraints (i.e. $\mu_{x_{1}}^{1}+\mu_{x_{1}}^{2}+\mu_{x_{1}}^{3}=1$ and $\left.\mu_{x_{2}}^{1}+\mu_{x_{2}}^{2}+\mu_{x_{2}}^{3}=1\right)$ and the consistency of the thermal block configuration, and by avoiding a geometrical degeneration of some elements $\mathscr{R}_{q}$

6. a) Consider the advection-diffusion example of Sect. $20.10$ and write the weak formulation of problem (20.120), defined on the original domain.

b) Propose a suitable domain decomposition of $\Omega$ and define a set of affine maps in order to describe the geometric deformation on each subdomain, by considering $\mu_{\mathrm{ref}}=(1,1)$, which in turn defines the reference domain $\Omega=\Omega_{\mathrm{o}}\left(\mu_{\mathrm{ref}}\right)$. Then write the corresponding weak formulation on the reference domain $\Omega$.

c) Characterize the expression of the a posteriori estimate $\Delta_{N}^{\text {en }}(\mu)$ for the error (in the energy norm) on the solution $\left\|u_{h}(\mu)-u_{N}(\mu)\right\| \mu$ and the expression of the a posteriori estimate $\Delta_{N}^{S}(\mu)$ on the output $\left|s_{h}(\mu)-s_{N}(\mu)\right|, s(\mu)$ being the quantity defined in $(20.120)$.

7. a) Consider the steady Stokes problem of Sect. $16.2$ (eq.(16.12)) and the lid driven cavity flow of Sect. 16.6. Take as possible geometrical parametrization for the cavity the aspect ratio $\mu=L / D$, where $L$ is the length and $D$ the height of the cavity, respectively. Write the parametrization and weak formulation of the problem in the reference domain. b) The reduced basis approximation spaces for a Stokes problem are given by $V_{N, \mathbf{u}}=\left\{\mathbf{u}_{h}\left(\mu^{n}\right), \sigma_{h}\left(\mu^{n}\right), 1 \leq n \leq N\right\} \subset V_{\mathbf{u}}^{N_{h}}$ for the velocity and $V_{N, p}=$ $\left\{p_{h}\left(\mu^{n}\right), 1 \leq n \leq N\right\} \subset V_{p}^{N_{h}}$ for the pressure, where, for $N$ selected $\mu^{n}, \mathbf{u}_{h}\left(\mu^{n}\right)$ and $p_{h}\left(\mu^{n}\right)$ represent the finite elements solutions for velocity and pressure, respectively, and $\sigma_{h}\left(\mu^{n}\right) \in V_{\mathbf{u}}^{N_{h}}$ is the solution of an auxiliary problem, called supremizer problem, which reads in the original domain

$$
\int_{\Omega_{0}} \nabla \sigma_{h} \cdot \nabla \mathbf{v} d \Omega=\int_{\Omega_{0}} p_{h}\left(\mu^{n}\right) \operatorname{div}(\mathbf{v}) d \Omega \quad \forall \mathbf{v} \in V_{\mathbf{u}}^{N_{h}}
$$

The enrichment of the velocity space by the supremizer guarantees the stability of the RB approximation and the fulfillment of an equivalent $\inf$-sup condition (see [RV07, RHM13]).

Write the reduced basis formulation for the Stokes problem; observe that the algebraic system obtained from the RB Galerkin projection features a block structure. Observe that this time the matrices are full, in contrast to what happens with the finite element method.

c) Do the same exercise considering the steady version of the Navier-Stokes equations (Sect. 16.1) by including also the affine transformation and the subsequent parametrization on the trilinear convective term (Sect. 16.7) $c(\mathbf{w}, \mathbf{z}, \mathbf{v} ; \mu)$, as considered, for example, in [VP05, QR07]. 

\section{References}

$\left[\mathrm{AAH}^{+} 98\right]$ Achdou Y., Abdoulaev G., Hontand $\mathrm{J}$., Kuznetsov Y., Pironneau O., and Prud'homme C. (1998) Nonmatching grids for fluids. In Domain decomposition methods, 10 (Boulder; $\mathrm{CO}$, 1997), volume 218 of Contemp. Math., pages 3-22. Amer. Math. Soc., Providence, RI.

[ABBS17] Antolin P., Bressan A., Buffa A., and Sangalli G. (2017) An isogeometric method for linear nearly-incompressible elasticity with local stress projection. Comput. Methods Appli. Mech. Eng. 316: 694-719.

$\left[\mathrm{ABC}^{+} 08\right] \quad$ Akkerman I., Bazilevs Y., Calo V., Hughes T., and Hulshoff S. (2008) The role of continuity in residual-based variational multiscale modeling of turbulence. Comput. Mech. 41(3): 371-378.

[ABCM02] Arnold D. N., Brezzi F., Cockburn B., and Marini L. D. (2001/02) Unified analysis of discontinuous Galerkin methods for elliptic problems. SIAM J. Numer. Anal. 39(5): 1749-1779.

$\left[\mathrm{ABH}^{+} 10\right] \quad$ Auricchio F., Beirão da Veiga L., Hughes T., Reali A., and Sangalli G. (2010) Isogeometric collocation methods. Math. Models Meth. Appl. Sci. 20(11): 2075 $2107$.

[ABKF11] Akkerman I., Bazilevs Y., Kees C., and Farting M. (2011) Isogeometric analysis of free-surface flow. J. Comput. Phys. 230(11): $4137-4152$.

[ABM09] Antonietti P. F., Brezzi F., and Marini L. D. (2009) Bubble stabilization of discontinuous Galerkin methods. Comput. Methods Appl. Mech. Engrg. 198(21-26): $1651-1659$.

$\left[\mathrm{ACH}^{+} 12\right] \quad$ Auricchio F., Calabrò F., Hughes T., Reali A., and Sangalli G. (2012) A simple algorithm for obtaining nearly optimal quadrature rules for NURBS-based Isogeometric Analysis. Comput. Methods Appli. Mech. Eng. 249-252: 15-27.

[Ada75]

Adams R. A. (1975) Sobolev Spaces. Academic Press, New York.

[AF03]

Adams R. A. and Fournier J. J. F. (2003) Sobolev Spaces Academic Press.

[AF12]  Amsallem D. and Farhat C. (2012) Stabilization of projection-based reducedorder models. Int. J. Numer. Methods Engr. 91(4): 358-377.

[AFG $\left.^{+} 00\right] \quad$ Almeida R. C., Feijóo R. A., Galeão A. C., Padra C., and Silva R. S. (2000) Adaptive finite element computational fluid dynamics using an anisotropic error estimator. Comput. Methods Appl. Mech. Engrg. 182: 379-400.

[Ago03] Agoshkov V. (2003) Optimal Control Methods and Adjoint Equations in Mathematical Physics Problems. Institute of Numerical Mathematics, Russian Academy of Science, Moscow.

[Aki94] Akin J. E. (1994) Finite Elements for Analysis and Design. Academic Press, London.

[AM09] Ayuso B. and Marini L. D. (2009) Discontinuous Galerkin methods for advectiondiffusion-reaction problems. SIAM J. Numer. Anal. 47(2): $1391-1420$.

[AMW99] Achdou Y., Maday Y., and Widlund O. (1999) Iterative substructuring preconditioners for mortar element methods in two dimensions. SIAM J. Numer. Anal. 36(2): $551-580$ (electronic).

[AO00] Ainsworth M. and Oden J. T. (2000) A posteriori error estimation in finite element analysis. Pure and Applied Mathematics. John Wiley and Sons, New York.

[Ape99] Apel T. (1999) Anisotropic Finite Elements: Local Estimates and Applications. Book Series: Advances in Numerical Mathematics. Teubner, Stuttgart.

[APV98] Achdou Y., Pironneau O., and Valentin F. (1998) Équations aux dérivées partielles et applications, chapter Shape control versus boundary control, pages 118. Éd. Sci. Méd. Elsevier, Paris.

[Arn82] Arnold D. N. (1982) An interior penalty finite element method with discontinuous elements. SIAM J. Numer. Anal. 19(4): 742-760.

[AS55] Allen D. N. G. and Southwell R. V. (1955) Relaxation methods applied to determine the motion, in two dimensions, of a viscous fluid past a fixed cylinder. Quart. J. Mech. Appl. Math. 8: 129-145.

[AS99] Adalsteinsson D. and Sethian J. A. (1999) The fast construction of extension velocities in level set methods. J. Comput. Phys. 148(1): 2-22.

[ATF87] Alekseev V., Tikhominov V., and Fomin S. (1987) Optimal Control. Consultants Bureau, New York.

[Aub67] Aubin J. P. (1967) Behavior of the error of the approximate solutions of boundary value problems for linear elliptic operators by Galerkin's and finite difference methods. Ann. Scuola Norm. Sup. Pisa 21: 599-637.

[Aub91] Aubry N. (1991) On the hidden beauty of the proper orthogonal decomposition. \mathrm{\{} T h e o r . ~ C o m p . ~ F l u i d . ~ D y n . ~ 2 : ~ 3 3 9 - 3 5 2 . ~

[AWB71] Aziz A., Wingate J., and Balas M. (1971) Control Theory of Systems Governed by Partial Differential Equations. Academic Press.

[AZF12]  Amsallem D., Zahr M., and Farhat C. (2012) Nonlinear model order reduction based on local reduced-order bases. Int. J. Numer. Methods Engr. $92(10): 891-$ $916$.

[Bab71] Babuška I. (1971) Error bounds for the finite element method. Numer. Math. 16: 322-333.

[BBC $\left.^{+} 06\right]$ Bazilevs Y., Beirão da Veiga L., Cottrell J., Hughes T., and Sangalli G. (2006) Isogeometric Analysis: approximation, stability, and error estimates for $h$-refined meshes. Math. Models Meth. Appl. Sci. 16(7): 1031-1090.

[BBF13]  Boffi D., Brezzi F. and Fortin M. (2013) Mixed Finite Element Methods and Applications, Springer.

[BBHH10] Benson D., Bazilevs Y., Hsu M.-C., and Hughes T. (2010) Isogeometric shell analysis: The Reissner-Mindlin shell. Comput. Methods Appli. Mech. Eng. 199(5): 276-289.

[BBHH11] Benson D., Bazilevs Y., Hsu M.-C., and Hughes T. (2011) A large deformation, rotation-free, isogeometric shell. Comput. Methods Appli. Mech. Eng. 200(13): 1367-1378.

[BBRS11] Beirão da Veiga L., Buffa A., Rivas J., and Sangalli G. (2011) Some estimates for h-p-k-refinement in Isogeometric Analysis. Numer. Mathematik $118(2)$ : $271-$ $305$. [BC13] Bornemann P. and Cirak F. (2013) A subdivision-based implementation of the hierarchical B-spline finite element method. Comput. Methods Appli. Mech. Eng. 253: 584-598.

[BCC $\left.^{+} 10\right]$ Bazilevs Y., Calo V., Cottrell J., Evans J., Hughes T., Lipton S., Scott M., and Sederberg T. (2010) Isogeometric analysis using T-splines. Comput. Methods Appli. Mech. Eng. 199(5-8): 229-263.

$\left[\mathrm{BCD}^{+} 11\right] \quad$ Binev P., Cohen A., Dahmen W., DeVore R., Petrova G., and Wojtaszczyk $\mathrm{P}$. (2011) Convergence rates for greedy algorithms in reduced basis methods. SIAM J. Math. Anal. 43(3): 1457-1472.

[BCS10] Buffa A., Cho D., and Sangalli G. (2010) Linear independence of the T-spline blending functions associated with some particular T-meshes. Comput. Methods Appli. Mech. Eng. 199(23-24): 1437-1445.

[BCZH06] Bazilevs Y., Calo V., Zhang Y., and Hughes T. (2006) Isogeometric fluid-structure interaction analysis with applications to arterial blood flow. Comput. Mech. 38(45): $310-322$.

[BDQ15] Bartezzaghi A., Dedè L., and Quarteroni A. (2015) Isogeometric Analysis for high order Partial Differential Equations on surfaces. Comput. Methods Appli. Mech. Eng. 295: 446-469.

[BDQ16] Bartezzaghi A., Dedè L., and Quarteroni A. (2016) Isogeometric Analysis of geometric partial differential equations. Comput. Methods Appli. Mech. Eng. 311: $625-647$

[BDR92] Babuška I., Durán R., and Rodríguez R. (1992) Analysis of the efficiency of an a posteriori error estimator for linear triangular finite elements. SIAM J. Numer. Anal. 29(4): 947-964.

[BE92] Bern M. and Eppstein D. (1992) Mesh generation and optimal triangulation. In Du D.-Z. and Hwang F. (eds) Computing in Euclidean Geometry. World Scientific, Singapore.

[Bec01] Becker R. (2001) Mesh adaptation for stationary flow control. J. Math. Fluid Mech. 3: 317-341.

[BHS12]  Bazilevs Y., Hsu M.-C., and Scott M. (2012) Isogeometric fluid-structure interaction analysis with emphasis on non-matching discretizations, and with application to wind turbines. Comput. Methods Appli. Mech. Eng. 249-252: 28-41.

[Bel99] Belgacem F. B. (1999) The mortar finite element method with Lagrange multipliers. Numer. Math. 84(2): 173-197.

[BEMS07] Burman E., Ern A., Mozolevski I., and Stamm B. (2007) The symmetric discontinuous Galerkin method does not need stabilization in 1D for polynomial orders $p \geq 2$. C. R. Math. Acad. Sci. Paris 345(10): 599-602.

[BF91a] Brezzi F. and Fortin M. (1991) Mixed and Hybrid Finite Element Methods. Springer-Verlag, New York.

[BF91b] Brezzi F. and Fortin M. (1991) Mixed and Hybrid Finite Element Methods, volume 15 of Springer Series in Computational Mathematics. Springer-Verlag, New York.

[BFHR97] Brezzi F., Franca L. P., Hughes T. J. R., and Russo A. (1997) $b=\int g$. Comput. Methods Appl. Mech. Engrg. 145: 329-339.

[BG87] Brezzi F. and Gilardi G. (1987) Functional Analysis and Functional Spaces. McGraw Hill, New York.

[BG98] Bernardi C. and Girault V. (1998) A local regularisation operator for triangular and quadrilateral finite elements. SIAM J. Numer. Anal. $35(5)$ : $1893-1916$.

[BGL05]  Benzi M., Golub G. H., and Liesen J. (2005) Numerical solution of saddle-point problems. Acta Numer. 14: 1-137. [BGL06a] Burkardt J., Gunzburger M., and Lee H. (2006) Centroidal voronoi tessellationbased reduced-order modeling of complex systems. SIAM J. Sci. Comput. $28(2)$ : 459-484.

[BGL06b] Burkardt J., Gunzburger M., and Lee H. (2006) POD and CVT-based reducedorder modeling of Navier-Stokes flows. Comput. Meth. Appl. Mech. Engrg. $196(1-3):$ 337-355.

[BGS96] Bjòrstad P., Gropp P., and Smith B. (1996) Domain Decomposition, Parallel Multilevel Methods for Elliptic Partial Differential Equations. Univ. Cambridge Press, Cambridge.

[BHL93] Berkooz G., Holmes P., and Lumley J. (1993) The proper orthogonal decomposition in the analysis of turbulent flows. Annu. Rev. Fluid Mech. $25(1): 539-575$.

[BHS06] Buffa A., Hughes T. J. R., and Sangalli G. (2006) Analysis of a multiscale discontinuous Galerkin method for convection-diffusion problems. SIAM J. Numer. Anal. 44(4): 1420-1440.

[BIL06] Berselli L. C., Iliescu T., and Layton W. J. (2006) Mathematics of Large Eddy Simulation of Turbulent Flows. Springer, Berlin Heidelberg.

[BLR12] Beirão da Veiga L., Lovadina C., and Reali A. (2012) Avoiding shear locking for the Timoshenko beam problem via isogeometric collocation methods. Comput. Methods Appli. Mech. Eng. 241: 38-51.

[BKR00] Becker R., Kapp H., and Rannacher R. (2000) Adaptive finite element methods for optimal control of partial differential equations: Basic concepts. SIAM, J. Control Opt. 39 (1): $113-132$.

[Bla02] Blanckaert K. (2002) Flow and turbulence in sharp open-channel bends. PhD thesis, École Polytechnique Fédérale de Lausanne.

[BLM14] Beirao da Veiga L., Lipnikov K. and Manzini G. (2014) The mimetic finite difference method for elliptic problems, MS\&A Series, volume 11, Springer.

[BM92] Bernardi C. and Maday Y. (1992) Approximations Spectrales de Problèmes aux Limites Elliptiques. Springer-Verlag, Berlin Heidelberg.

[BM94] Belgacem F. B. and Maday Y. (1994) A spectral element methodology tuned to parallel implementations. Comput. Methods Appl. Mech. Engrg. 116(1-4): 59-67. ICOSAHOM'92 (Montpellier, 1992).

[BM06] Brezzi F. and Marini L. D. (2006) Bubble stabilization of discontinuous Galerkin methods. In Advances in numerical mathematics, Proceedings International Conference on the occasion of the 60 th birthday of Y.A. Kuznetsov, September 16-17, 2005, pages 25-36. Institute of Numerical Mathematics of the Russian Academy of Sciences, Moscow.

[BMMP06] Bottasso C. L., Maisano G., Micheletti S., and Perotto S. (2006) On some new recovery based a posteriori error estimators. Comput. Methods Appl. Mech. Engrg. $195(37-40):$ 4794-4815.

[BMP94] Bernardi C., Maday Y., and Patera A. (1994) A new nonconforming approach to domain decomposition: the mortar element method. In Nonlinear partial differential equations and their applications. Collège de France Seminar; Vol. XI (Paris, 1989-1991), volume 299 of Pitman Res. Notes Math. Ser, pages 13-51. Longman Sci. Tech., Harlow.

[BMP $^{+}$12]  Buffa A., Maday Y., Patera A. T., Prud'homme C., and Turinici G. (2012) A priori convergence of the greedy algorithm for the parametrized reduced basis method. ESAIM Math. Model. Numer. Anal. $46(3)$ : $595-603$.

[BMS04] Brezzi F., Marini L. D., and Süli E. (2004) Discontinuos Galerkin methods for first-order hyperbolic problems. Math. Models Methods Appl. Sci. 14: $1893-$ $1903$. [BN83] Boland J. and Nicolaides R. (1983) Stability of finite elements under divergence constraints. SIAM J. Numer. Anal. 20: 722-731.

[BNMP04] Barrault M., Nguyen N. C., Maday Y., and Patera A. T. (2004) An "empirical interpolation" method: Application to efficient reduced-basis discretization of partial differential equations. C. R. Acad. Sci. Paris, Série I. 339: 667-672.

[BO06]  Benzi M. and Olshanskii M. (2006) An augmented lagrangian-based approach to the oseen problem. SIAM J. on Scientific Computing 28 (6): 2095-2113.

[Bre15] Brezzi F. (2015) The great beauty of VEMs, ProceedingsICM 1: 214-235 (2014). [BSV14] Buffa A., Sangalli G., and Vázquez R. (2014) Isogeometric methods for computational electromagnetics: B-spline and T-spline discretizations. J. Comput. Phys. 257, Part B: 1291-1320.

[BVS $^{+}$12] Borden M., Verhoosel C., Scott M., Hughes T., and Landis C. (2012) A phasefield description of dynamic brittle fracture. Comput. Methods Appli. Mech. Eng. 217-220: 77-95.

[Bre74] Brezzi F. (1974) On the existence, uniqueness and approximation of saddle-point problems arising from Lagrange multipliers. R.A.I.R.O. Anal. Numér. 8: 129-151. [Bre86] Brezis H. (1986) Analisi Funzionale. Liguori, Napoli.

[Bre00] Bressan A. (2000) Hyperbolic Systems of Conservation Laws: The Onedimensional Cauchy Problem. Oxford Lecture Series in Mathematics and its Applications. The Clarendon Press Oxford University Press, New York.

[BRM $^{+}$97]  Bassi F., Rebay S., Mariotti G., Pedinotti S., and Savini M. (1997) A high-order accurate discontinuous finite element method for inviscid and viscous turbomachinery flows. In Decuypere R. and Dibelius G. (eds) Proceedings of the $2 n d$ European Conference on Turbomachinery Fluid Dynamics and Thermodynamics, pages 99-108. Technologisch Instituut, Antwerpen, Belgium.

[BS94] Brenner S. C. and Scott L. R. (1994) The Mathematical Theory of Finite Element 訃 Methods. Springer-Verlag, New York.

[BS07] Brenner S. and Sung L. (2007) Bddc and feti-dp without matrices or vectors. Comput. Methods Appl. Mech. Engrg. 196: 1429-1435.

[BS08] Burman E. and Stamm B. (2008) Symmetric and non-symmetric discontinuous Galerkin methods stabilized using bubble enrichment. C. R. Math. Acad. Sci. Paris 346(1-2): 103-106.

[BS12] A. Borzi and V. Schultz, (2012) Computational Optimization of Systems Governed by Partial Differential Equations, SIAM, Philadelphia.

[Cab03] Caboussat A. (2003) Analysis and numerical simulation of free surface flows. PhD thesis, École Polytechnique Fédérale de Lausanne.

[CAC10]  Chinesta F., Ammar A., and Cueto E. (2010) Recent advances and new challenges in the use of the proper generalized decomposition for solving multidimensional models. Arch. Comput. Methods Engrg 17: 327-350.

[Ç07]  Çengel Y. (2007) Introduction to Thermodynamics and heat transfer. McGrawHill, New York.

[CH01]  Collis S. and Heinkenschloss M. (2001) Analysis of the streamline upwind/petrov galerkin method applied to the solution of optimal control problems. $C A A M$ report TR02-01.

[CHQZ06] Canuto C., Hussaini M., Quarteroni A., and Zang T. A. (2006) Spectral Methods. Fundamentals in Single Domains. Springer-Verlag, Berlin Heidelberg.

[CHQZ07] Canuto C., Hussaini M. Y., Quarteroni A., and Zang T. A. (2007) Spectral Methods. Evolution to Complex Geometries and Application to Fluid Dynamics. Springer-Verlag, Berlin Heidelberg. [CHB09] Cottrell J., Hughes T., and Bazilevs Y. (2009) Isogeometric Analysis. Toward Integration of $C A D$ and $F E A$. Wiley.

[CHR07] Cottrell J., Hughes T., and Reali A. (2007) Studies of refinement and continuity in Isogeometric structural analysis. Comput. Methods Appli. Mech. Eng. 196(4144): 4160-4183.

[Cia78] Ciarlet P. G. (1978) The Finite Element Method for Elliptic Problems. NorthHolland, Amsterdam.

[CFL28] Courant R., Friedrichs K., and Lewy H. (1928), Über die partiellen differenzengeichungen der mathematischen physik, Mathematische Annalen 100 (1): 32-74.

[CJRT01] Cohen G., Joly P., Roberts J. E., and Tordjman N. (2001) Higher order triangular finite elements with mass lumping for the wave equation. SIAM J. Numer. Anal. 38(6): 2047-2078 (electronic).

[CLC11] Chinesta F., Ladeveze P., and Cueto E. (2011) A short review on model order reduction based on proper generalized decomposition. Arch. Comput. Methods Engrg. 18: 395-404.

[Clé75]  Clément P. (1975) Approximation by finite element functions using local regularization. RAIRO, Anal. Numér 2 pages 77-84.

[Coc98] Cockburn B. (1998) An introduction to the discontinuous Galerkin method for convection-dominated problems. In Quarteroni A. (ed) Advanced Numerical Approximation of Nonlinesr Hyperbolic Equations, volume 1697 of $L N M$, pages 151-268. Springer-Verlag, Berlin Heidelberg.

[Coc99] Cockburn B. (1999) Discontinuous Galerkin methods for convection-dominated problems. In High-order methods for computational physics, volume 9 of Lect. Notes Comput. Sci. Eng., pages 69-224. Springer, Berlin.

[CFL28] Courant R., Friedrichs K., and Lewy H. (1928), Über die partiellen differenzengeichungen der mathematischen physik, Mathematische Annalen 100 (1): 32-74.

[CRBH06] Cottrell J., Reali A., Bazilevs Y., and Hughes T. (2006) Isogeometric analysis of structural vibrations. Comput. Methods Appli. Mech. Eng. 195(41-43): 5257 $5296$.

[dB01] de Boor C. (2001) A Practical Guide to Splines, volume 27 of Applied Mathematical Sciences. Springer-Verlag.

[DBH12] Dedè L., Borden M., and Hughes T. (2012) Isogeometric Analysis for topology optimization with a phase field model. Arch. Comput. Methods Engng. 19(3): $427-465$.

$\left[\mathrm{DCV}^{+} 16\right] \quad$ Dalcin L., Collier N., Vignal P., Côrtes A., and Calo V. (2016) PetIGA: A framework for high-performance isogeometric analysis. Comput. Methods Appli. Mech. Eng. 308: 151-181.

[Ded04] Dedé L. (2004) Controllo Ottimale e Adattività per Equazioni alle Derivate Parziali e Applicazioni. Tesi di Laurea, Politecnico di Milano.

[DG11] Demkowicz L. and Gopalakrishnan J. (2011) A class of discontinuous PetrovGalerkin methods. II. optimal test functions. Numer. Methods Partial Differential Equations 27(1): 70-105.

[dFRV11] de Falco C., Reali A., and Vázquez R. (2011) Geopdes: A research tool for isogeometric analysis of $\{$ PDEs $\}$. Advances Eng. Software 42(12): 1020-1034.

[DJQ15] Dedè L., Jäggli C., and Quarteroni A. (2015) Isogeometric numerical dispersion analysis for two-dimensional elastic wave propagation. Comput. Methods Appli. Mech. Eng. 284(320-348).

[DJS10] Dörfel M., Jüttler B., and Simeon B. (2010) Adaptive isogeometric analysis by local h-refinement with T-splines. Comput. Methods Appli. Mech. Eng. 199 (58): 264-275. [DQ15]  Dedè L. and Quarteroni A. (2015) Isogeometric Analysis for second order Partial Differential Equations on surfaces. Comput. Methods Appli. Mech. Eng. 284: 807-834.

[DS11]  Dedè L. and Santos H. (2011) B-spline goal-oriented error estimators for geometrically nonlinear rods. Comput. Mech. $49(1): 35-52$.

[Doh03] Dohrmann R. (2003) A preconditioner for substructuring based on constrained energy minimization. SIAM J. Sci. Comput. 25: 246-258.

[DPQ08] Detomi D., Parolini N., and Quarteroni A. (2008) Mathematics in the wind. MOX Reports (25). see the web page http: //mox.polimi. it.

[DQ05] Dedè L. and Quarteroni A. (2005) Optimal control and numerical adaptivity for advection-diffusion equations. M2AN Math. Model. Numer. Anal. $39(5)$ : 1019$1040$.

[DSW04] Dawson C., Sun S., and Wheeler M. F. (2004) Compatible algorithms for coupled flow and transport. Comput. Methods Appl. Mech. Engrg. 193(23-26): 2565 2580 .

[DT80]  Dervieux A. and Thomasset F. (1980) Approximation Methods for Navier-Stokes Problems, volume 771 of Lecture Notes in Mathematics, chapter A finite element method for the simulation of Rayleigh-Taylor instability, pages $145-158$. Springer-Verlag, Berlin.

[Dub91] Dubiner M. (1991) Spectral methods on triangles and other domains. J. Sci. Com-

䬴 put. 6: 345-390.

[DV02]  Darmofal D. L. and Venditti D. A. (2002) Grid adaptation for functional outputs:

銀 application to two-dimensional inviscid flows. J. Comput. Phys. 176: 40-69.

[DV09] Di Pietro D. A. and Veneziani A. (2009) Expression templates implementation of continuous and discontinuous Galerkin methods. Computing and Visualization in Science 12(8): 421-436.

[DZ06]  Dáger R. and Zuazua E. (2006) Wave Propagation, Observation and Control in 1-d Flexible Multi-Structures. Mathématiques et Applications. Springer, Paris.

[EBBH09] Evans J., Bazilevs Y., Babuška I., and Hughes T. (2009) $n$-widths, sup-infs, and optimality ratios for the $k$-version of the Isogeometric finite element method. Comput. Methods Appli. Mech. Eng. 198(21-26): 1726-1741.

[EG04] Ern A. and Guermond J. L. (2004) Theory and Practice of Finite Elements, volume 159 of Applied Mathematics Sciences. Springer-Verlag, New York.

[EGH00] Eynard R., Gallouët T. and Herbin R. (2000) Finite Volume Methods. In Ciarlet P.G., Lions J.L. (eds.) Handbook of Numerical Analysis, volume 7, 713-1018.

[EHS $\left.^{+} 06\right] \quad$ Elman H. C., Howte V. E., Shadid J., Shuttleworth R., and Tuminaro R. (2006) Block preconditioners based on approximate commutators. SIAM J. on Scientific Computing 27 (5): 1651-1668.

[EJ88] Eriksson E. and Johnson C. (1988) An adaptive method for linear elliptic problems. Math. Comp. 50: 361-383.

[Emb99] Embree M. (1999) Convergence of Krylov subspace methods for non-normal matrices. PhD thesis, Oxford University Computing Laboratories.

[Emb03] Embree M. (2003) The tortoise and the hare restart GMRES. SIAM Rev. 45 (2): 259-266.

[ESLT15] Evans E., Scott M., Li X., and Thomas D. (2015) Hierarchical T-splines: Analysis-suitability, Bézier extraction, and application as an adaptive basis for isogeometric analysis. Comput. Methods Appli. Mech. Eng. 284: 1-20.

[ESW05] Elman H., Silvester D., and Wathen A. (2005) Finite Elements and Fast Iterative Solvers. Oxford Science Publications, Oxford. [Eva98] Evans L. (1998) Partial differential equations. American Mathematical Society, Providence.

[FCZ03] Fernández-Cara E. and Zuazua E. (2003) Control theory: History, mathematical achievements and perspectives. Bol. Soc. Esp. Mat. Apl. 26: 79-140.

[FCZ04] Fernández-Cara E. and Zuazua E. (2004) On the history and perspectives of control theory. Matapli 74: 47-73.

[FLP00] Farhat C., Lesoinne M., and Pierson K. (2000) A scalable dual-primal domain decomposition method. Numer. Linear Algebra Appl. 7: 687-714.

[FLT $\left.^{+} 01\right] \quad$ Farhat C., Lesoinne M., Tallec P. L., Pierson K., and Rixen D. (2001) Feti-dp: a dual-primal unified feti method. i. a faster alternative to the two-level feti method. Intern. J. Numer. Methods Engrg. 50: 1523-1544.

[FMP04] Formaggia L., Micheletti S., and Perotto S. (2004) Anisotropic mesh adaptation in computational fluid dynamics: application to the advection-diffusion-reaction and the stokes problems. Appl. Numer: Math. 51(4): $511-533$.

[FMRT01] Foias C., Manley O., Rosa R., and Temam R. (2001) Navier-Stokes Equations and Turbulence. Cambridge Univ. Press, Cambridge.

[For77] Fortin M. (1977) An analysis of the convergence of mixed finite element methods. R.A.I.R.O. Anal. Numér. $11$.

[FP01] Formaggia L. and Perotto S. (2001) New anisotropic a priori error estimates. Numer. Math. 89: 641-667.

[FP02]  Ferziger J. H. and Peric M. (2002) Computational Methods for Fluid Dynamics. Springer, Berlino, III edition.

[FSV05] Formaggia L., Saleri F., and Veneziani A. (2005) Applicazioni ed esercizi di modellistica numerica per problemi differenziali. Springer Italia, Milano.

[FSV12]  Formaggia L., Saleri F., and Veneziani A. (2012) Solving Numerical PDEs: Problems, Applications, Exercises. Springer Italia, Milano.

[Fun92]  Funaro D. (1992) Polynomial Approximation of Differential Equations. SpringerVerlag, Berlin Heidelberg.

[Fun97] Funaro D. (1997) Spectral Elements for Transport-Dominated Equations. Springer-Verlag, Berlin Heidelberg.

[Fur97] Furnish G. (May/June 1997) Disambiguated glommable expression templates. Compuers in Physics 11(3): 263-269.

[GaAML04] Galeão A., Almeida R., Malta S., and Loula A. (2004) Finite element analysis of connection dominated reaction-diffusion problems. Applied Numerical Mathematics 48: 205-222.

[GB98] George P. L. and Borouchaki H. (1998) Delaunay Triangulation and Meshing. Editions Hermes, Paris.

[Ger08] Gervasio P. (2008) Convergence analysis of high order algebraic fractional step schemes for time-dependent Stokes equations. SIAM J. Numer. Anal. $\mathbf{4 6}(4)$, 1682-1703.

[GCBH08] Gómez H., Calo V., Bazilevs Y., and Hughes T. (2008) Isogeometric analysis of the Cahn-Hilliard phase-field model. Comput. Methods Appli. Mech. Eng. 197(49-50): 4333-4352.

[GHNC10] Gómez H., Hughes T., Nogúeira X., and Calo V. (2010) Isogeometric analysis of the Navier-Stokes-Korteweg equations. Comput. Methods Appli. Mech. Eng. $199(25-28): 1828-1840$.

[GJS12] Giannelli C., Jüttler B., and Speleers H. (2012) THB-splines: The truncated basis for hierarchical splines. Comp. Aided Geom. Design 29(7): 485-498.

[GL16] Gómez H. and Lorenzis L. D. (2016) The variational collocation method. Comput. Methods Appli. Mech. Eng. 309: 152-181. [GMNP07] Grepl M. A., Maday Y., Nguyen N. C., and Patera A. T. (2007) Efficient reducedbasis treatment of nonaffine and nonlinear partial differential equations. $M 2 A N$ (Math. Model. Numer: Anal.) 31(3): 575-605.

[GMS06] Guermond J. L. , Minev P. and Shen J. (2006), An overview of projection methods for incomprensible flows. Comput. Meth. Applied. Mech. Engnrng195: 6011 $6045$.

[GMSW89] Gill P., Murray W., Saunders M., and Wright M. (1989) Constrained nonlinear programming. Elsevier Handbooks In Operations Research And Management Science Optimization. Elsevier North-Holland, Inc., New York.

[GN12] Gomez H. and Nogueira X. (2012) An unconditionally energy-stable method for the phase field crystal equation. Comput. Methods Appli. Mech. Eng. 249-252: 52-61.

[GR96] Godlewski E. and Raviart P. A. (1996) Hyperbolic Systems of Conservations Laws, volume 118. Springer-Verlag, New York.

it

[Gri11] Grisvard P. (2011) Elliptic Problems in Nonsmooth Domains. SIAM.

[GRS07] Grossmann C., Ross H., and Stynes M. (2007) Numerical treatment of Partial

即 Differential Equations. Springer, Heidelberg, Heidelberg.

[GS05] Georgoulis E. H. and Süli E. (2005) Optimal error estimates for the $h p$-version interior penalty discontinuous Galerkin finite element method. IMA J. Numer. Anal. 25(1): 205-220.

[GSD14] Gahalaut K., Satyendra K., and Douglas C. (2014) Condition number estimates for matrices arising in NURBS based isogeometric discretizations of elliptic partial differential equations. arXiv [math.NA] $1406.6808$.

[GSV06] Gervasio P., Saleri F., and Veneziani A. (2006) Algebraic fractional-step schemes with spectral methods for the incompressible Navier-Stokes equations. J. Comput. Phys. 214(1): 347-365.

[Gun03] Gunzburger M. D. (2003) Perspectives in Flow Control and Optimization. Advances in Design and Control. SIAM.

[HAB11] Hsu M.-C., Akkerman I., and Bazilevs Y. (2011) High-performance computing of wind turbine aerodynamics using isogeometric analysis. Comput. \& Fluids $49(1): 93-100$.

[Hac]  Hackbush W.

[HB76] Hnat J. and Buckmaster J. (1976) Spherical cap bubbles and skirt formation. Phys. Fluids 19: 162-194.

[HCB05] Hughes T., Cottrell J., and Bazilevs Y. (2005) Isogeometric Analysis: CAD, finite elements, NURBS, exact geometry and mesh refinement. Comput. Methods Appli. Mech. Eng. 194(39-41): 4135-4195.

[HCSH17] Hiemstra R., Calabrò F., Schillinger D., and Hughes T. (2017) Optimal and reduced quadrature rules for tensor product and hierarchically refined splines in isogeometric analysis. Comput. Methods Appli. Mech. Eng. 316: $966-1004$.

[HER14] Hughes T., Evans J., and Reali A. (2014) Finite element and NURBS approximations of eigenvalue, boundary-value, and initial-value problems. Comput. Methods Appli. Mech. Eng. 272: 290-320.

[Hir88] Hirsh C. (1988) Numerical Computation of Internal and External Flows, volume 1. John Wiley and Sons, Chichester.

[HLB98] Holmes P., Lumley J., and Berkooz G. (1998) Turbulence, coherent structures, dynamical systems and symmetry. Cambridge Univ. Press.

[HN81] Hirt C. W. and Nichols B. D. (1981) Volume of fluid (VOF) method for the dynamics of free boundaries. J. Comp. Phys. 39: 201-225. [Hou95]  Hou T. Y. (1995) Numerical solutions to free boundary problems. ACTA Numerica 4: 335-415.

[HPUU09] Hinze M., Pinnau R., Ulbrich M. and Ulbrich S. (2009) Optimization with PDE Constraints. Springer.

[HRS08] Hughes T., Reali A., and Sangalli G. (2008) Duality and unified analysis of discrete approximations in structural dynamics and wave propagation: comparison of p-method finite elements with k-method nurbs. Comput. Methods Appli. Mech. Eng. 194(49-50): 4104-4124.

[HRS10] Hughes T., Reali A., and Sangalli G. (2010) Efficient quadrature for NURBSbased isogeometric analysis. Comput. Methods Appli. Mech. Eng. 199(5-8): 301 $313$.

[HRSP07] Huynh D. B. P., Rozza G., Sen S., and Patera A. T. (2007) A successive constraint linear optimization method for lower bounds of parametric coercivity and inf-sup stability constants. C. R. Acad. Sci. Paris, Analyse Numérique, Series I 345, pp. $473-478$.

[HRT96]  Heywood J. G., Rannacher R., and Turek S. (1996) Artificial boundaries and flux and pressure conditions for the incompressible Navier-Stokes equations. Internat. J. Numer. Methods Fluids $22(5)$ : 325-352.

[HSS02] Houston P., Schwab C., and Süli E. (2002) Discontinuous $h p$-finite element methods for advection-diffusion-reaction problems. SIAM J. Numer. Anal. $39(6)$ : 2133-2163 (electronic).

[Hug00] Hughes T. J. R. (2000) The Finite Element Method. Linear Static and Dynamic Finite Element Analysis. Dover Publishers, New York.

[HVZ97] Hamacher V. C., Vranesic Z. G., and Zaky S. G. (1997) Introduzione 6 all'architettura dei calcolatori. McGraw Hill Italia, Milano.

[HW65] Harlow F. H. and Welch J. E. (1965) Numerical calculation of time-dependent viscous incompressible flow of fluid with free surface. Physics of Fluids $8(12)$ : 2182-2189.

[HW08] Hesthaven J. S. and Warburton T. (2008) Nodal discontinuous Galerkin methods, volume 54 of Texts in Applied Mathematics. Springer, New York. Algorithms, analysis, and applications.

[HYR08] Hou T. Y., Yang D. P., and Ran H. (2008) Multiscale analysis and computation for the 3 d incompressible Navier-Stokes equations. SIAM Multiscale Modeling and Simulation 6 (4): 1317-1346.

[IK08] Ito K. and Kunisch K. (2008) On the Lagrange Multiplier Approach to Variational Problems and Applications. SIAM, Philadelphia.

[IR98] Ito K. and Ravindran S. S. (1998) A reduced-order method for simulation and control of fluid flows. Journal of Computational Physics 143(2): 403-425.

[IZ99] Infante J. and Zuazua E. (1999) Boundary observability for the space semidiscretizations of the 1 -d wave equation. M2AN Math. Model. Numer. Anal. $33(2):$ 407-438.

[Jam88] Jamenson A. (1988) Optimum aerodynamic design using cfd and control theory. AIAA Paper 95-1729-CP pages 233-260.

[JKD14] Johannessen K., Kvamsdal T., and Dokken T. (2014) Isogeometric analysis using \{LR \} B-Splines. Comput. Methods Appli. Mech. Eng. 269: 471-514.

[JK07] John V. and Knobloch P. (2007) On spurious oscillations at layers diminishing (SOLD) methods for convection-diffusion equations. I. A review. Comput. Methods Appl. Mech. Engrg. 196(17-20): 2197-2215.

[Joe05] Joerg M. (2005) Numerical investigations of wall boundary conditions for twofluid flows. Master's thesis, École Polytechnique Fédérale de Lausanne. [Joh87] Johnson C. (1987) Numerical Solution of Partial Differential Equations by the Finite Element Method. Cambridge University Press, Cambridge.

[KA00] Knabner P. and Angermann L. (2000) Numerical Methods for Elliptic and Parabolic Partial Differential Equations, volume 44 of $T A M$. Springer-Verlag, New York.

[KAJ02] Kim S., Alonso J., and Jameson A. (2002) Design optimization of hight-lift configurations using a viscous continuos adjoint method. AIAA paper, 40 th AIAA Aerospace Sciences Meeting and Exibit, Jan 14-17 $20020844$.

$\left[\mathrm{KBH}^{+} 10\right] \quad$ Kiendl J., Bazilevs Y., Hsu M.-C., Wüchner R., and Bletzinger K.-U. (2010) The bending strip method for isogeometric analysis of Kirchhoff-Love shell structures comprised of multiple patches. Comput. Methods Appli. Mech. Eng. 199(37-40): 2403-2416.

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-287.jpg?height=13&width=18&top_left_y=371&top_left_x=186)

[KF89] Kolmogorov A. and Fomin S. (1989) Elements of the Theory of Functions and Functional Analysis. V.M. Tikhominov, Nauka - Moscow.

[KMI $\left.^{+} 83\right] \quad$ Kajitani H., Miyata H., Ikehata M., Tanaka H., Adachi H., Namimatzu M., and Ogiwara S. (1983) Summary of the cooperative experiment on Wigley parabolic model in Japan. In Proc. of the 2nd DTNSRDC Workshop on Ship Wave Resistance Computations (Bethesda, USA), pages 5-35.

[KPTZ00] Kawohl B., Pironneau O., Tartar L., and Zolesio J. (2000) Optimal Shape Design. Springer-Verlag, Berlin.

[Kro97] Kroener D. (1997) Numerical Schemes for Conservation Laws. Wiley-Teubner, 即 Chichester.

[KS05] Karniadakis G. E. and Sherwin S. J. (2005) Spectral/hp Element Methods for Computational Fluid Dynamics. Oxford University Press, New York, II edition.

[KSWB14] Kiendl J., Schmidt R., Wüchner R., and Bletzinger K.-U. (2014) Isogeometric shape optimization of shells using semi-analytical sensitivity analysis and sensitivity weighting. Comput. Methods Appli. Mech. Eng. 274: 148-167.

[KVvdZvB14] Kuru G., Verhoosel C., van der Zee K., and van Brummelen E. (2014) Goaladaptive isogeometric analysis with hierarchical splines. Comput. Methods Appli. Mech. Eng. 270: 270-292.

[KWD02] Klawonn A., Widlund O., and Dryja M. (2002) Dual-primal feti methods for three-dimensional elliptic problems with heterogeneous coefficients. SIAM J. Numer. Anal. 40: $159-179$.

$\left[\mathrm{LDE}^{+} 13\right] \quad$ Liu J., Dedè L., Evans J., Borden M., and Hughes T. (2013) Isogeometric Analysis of the advective Cahn-Hilliard equation: spinodal decomposition under shear flow. J. Comput. Phys. 242: 321-350.

[Le 05] Le Bris C. (2005) Systèmes multiiéchelles: modélisation et simulation, volume 47 of Mathématiques et Applications. Springer, Paris.

[LEB $^{+}$10] Lipton S., Evans J., Bazilevs Y., Elguedj T., and Hughes T. (2010) Robustness of Isogeometric structural discretizations under severe mesh distortion. Comput. Methods Appli. Mech. Eng. 199(5-8): 357-373.

[LeF02] Lefloch Ph. G (2002), Hyperbolic Systems of Conservation Laws: the theory of classical and nonclassical shock waves, Springer.

[LeV02a] LeVeque R. J. (2002) Finite Volume Methods for Hyperbolic Problems. Cambridge Texts in Applied Mathematics.

[LeV02b] LeVeque R. J. (2002) Numerical Methods for Conservation Laws. Birkhäuser Verlag, Basel, II edition.

[LeV07] LeVeque R. J. (2007) Finite Difference Methods for Ordinary and Partial Differential Equations: Steady-State and Time-Dependent Problems. SIAM, Philadelphia. [Lio71] Lions J. (1971) Optimal Control of Systems Governed by Partial Differential Equations. Springer-Verlag, New York.

[Lio72] Lions J. (1972) Some Aspects of the Optimal Control of Distribuited Parameter Systems. SIAM, Philadelphia.

[Lio96] Lions P.-L. (1996) Mathematical topics in fluid mechanics. Vol. 1, volume 3 of Oxford Lecture Series in Mathematics and its Applications. The Clarendon Press Oxford University Press, New York. Incompressible models, Oxford Science Publications.

[LH16]  Liu J. and Hughes T. R. (2016) Isogeometric Phase-Field Simulation of Boiling, 䬴 pages 217-228. Springer International Publishing, Cham.

[LL59] Landau L. D. and Lifshitz E. M. (1959) Fluid mechanics. Translated from the Russian by J. B. Sykes and W. H. Reid. Course of Theoretical Physics, Vol. 6 . Pergamon Press, London.

[LL00] Lippman S. B. and Lajoie J. (2000) C++ Corso di Programmazione. Addison Wesley Longman Italia, Milano, III edition.

[LM68] Lions J. L. and Magenes E. (1968) Quelques Méthodes des Résolution des Problémes aux Limites non Linéaires. Dunod, Paris.

[LMQR13] Lassila T., Manzoni A., Quarteroni A., and Rozza G. (2013) Generalized reduced basis methods and $\mathrm{n}$-width estimates for the approximation of the solution manifold of parametric PDEs. In Brezzi F., Colli Franzone P., Gianazza U., and Gilardi G. (eds) Analysis and Numerics of Partial Differential Equations, volume 4 of Springer INdAM Series, pages 307-329. Springer, Milan.

[LR98] Lin S. P. and Reitz R. D. (1998) Drop and spray formation from a liquid jet. Annu. Rev. Fluid Mech. 30: 85-105.

[LvGM96] Lorentz G., von Golitschek M., and Makovoz Y. (1996) Constructive approximation: advanced problems. Springer-Verlag, New York.

[LW94] Li X. D. and Wiberg N. E. (1994) A posteriori error estimate by element patch post-processing, adaptive analysis in energy and $\mathrm{L}_{2}$ norms. Comp. Struct. 53: $907-919$.

[LW06]  Li J. and Widlund O. (2006) Feti-dp, bddc, and block cholesky methods. Internat.

銀 J. Numer. Methods Engrg. 66: 250-271.

[LWH14] Lorenzis L. D., Wriggers P., and Hughes T. (2014) Isogeometric contact: a review. GAMM-Mitteilungen 37(1): 85-123.

[LZZ13] Li X., Zhang J., and Zheng Y. (2013) NURBS-based Isogeometric Analysis of beams and plates using high order shear deformation theory. Math. Problems Eng. 2013: 1-9.

[Man93] Mandel J. (1993) Balancing domain decomposition. Comm. Numer. Methods Engrg. 9: 233-241.

[Mar95] Marchuk G. I. (1995) Adjoint Equations and Analysis of Complex Systems. Kluwer Academic Publishers, Dordrecht.

[Mau81] Maurer H. (1981) First and second order sufficient optimality conditions in mathematical programming and optimal control. Mathematical Programming Study 14: 163-177.

[Max76] Maxworthy T. (1976) Experiments on collisions between solitary waves. Journal of Fluid Mechanics 76: 177-185.

[MDT05] Mandel J., Dohrmann R., and Tezaur R. (2005) An algebraic theory for primal and dual substructuring methods by contraints. Appl. Numer. Math. 54: $167-193$.

[Mey00] Meyer C. D. (2000) Matrix Analysis and Applied Linear Algebra. SIAM.

[MOS92] Mulder W., Osher S., and Sethian J. (1992) Computing interface motion in compressible gas dynamics. Journal of Computational Physics 100(2): 209-228. [MP94] Mohammadi B. and Pironneau O. (1994) Analysis of the K-Epsilon Turbulence Model. John Wiley \& Sons, Chichester.

[MP97]  Muzaferija S. and Peric M. (1997) Computation of free-surface flows using finite volume method and moving grids. Numer. Heat Trans., Part B 32: 369-384.

[MP01] Mohammadi B. and Pironneau O. (2001) Applied Shape Optimization for Fluids. Clarendon Press, Oxford.

[MPT02a] Maday Y., Patera A. T., and Turinici G. (2002) Global a priori convergence theory for reduced-basis approximation of single-parameter symmetric coercive elliptic partial differential equations. C. R. Acad. Sci. Paris, Série I 335(3): 289-294.

[MPT02b] Maday Y., Patera A., and Turinici G. (2002) A Priori convergence theory for reduced-basis approximations of single-parameter elliptic partial differential equations. Journal of Scientific Computing 17(1-4): 437-446.

[MST17] Montardini M., Sangalli G., and Tamellini L. (2017) Optimal-order isogeometric collocation at Galerkin superconvergent points. Comput. Methods Appli. Mech.

![](https://cdn.mathpix.com/cropped/991f5db518e324d299c1ff4972bba9fb-289.jpg?height=17&width=21&top_left_y=442&top_left_x=181)
Eng. 316: $741-757$.

[MT01] Mandel J. and Tezaur R. (2001) On the convergence of a dual-primal substructuring method. Numerische Mathematik 88: 543-558.

[NAG10] Nagy A., Abdalla M., and Gürdal Z. (2010) Isogeometric sizing and shape optimisation of beam structures. Comput. Methods Appli. Mech. Eng. 199(17): 1216 1230 .

[NG13] Nörtoft P. and Gravesen J. (November 2013) Isogeometric shape optimization in fluid mechanics. Struct. Multidiscip. Optim. 48(5): 909-925.

[Nit68] Nitsche J. A. (1968) Ein kriterium für die quasi-optimalitat des Ritzchen Verfahrens. Numer. Math. 11: 346-348.

[Nit71] Nitsche J. (1971) Über ein Variationsprinzip zur Lösung von Dirichlet-Problemen bei Verwendung von Teilräumen, die keinen Randbedingungen unterworfen sind. Abh. Math. Sem. Univ. Hamburg 36: 9-15. Collection of articles dedicated to Lothar Collatz on his sixtieth birthday.

[Nou10] Nouy A. (2010) Proper generalized decompositions and separated representations for the numerical solution of high dimensional stochastic problems. Arch. Comput. Methods Engrg. 17: 403-434.

[NW06]  Nocedal J. and Wright S. (2006) Numerical Optimization. Springer Series in Operations Research and Financial Engineering. Springer, New York.

[NZ04] Naga A. and Zhang Z. (2004) A posteriori error estimates based on the polynomial preserving recovery. SIAM J. Numer. Anal. 42: 1780-1800.

[OBB98] Oden J. T., Babuška I., and Baumann C. E. (1998) A discontinuous $h p$ finite element method for diffusion problems. J. Comput. Phys. 146(2): $491-519$.

[OP07] Oliveira I. and Patera A. (2007) Reduced-basis techniques for rapid reliable optimization of systems described by affinely parametrized coercive elliptic partial differential equations. Optimization and Engineering 8(1): 43-65.

[OS88] Osher S. and Sethian J. A. (1988) Fronts propagating with curvature-dependent speed: algorithms based on Hamilton-Jacobi formulations. J. Comput. Phys. $79(1): 12-49$.

[Pan05]  Panton R. (2005) Incompressible Flow. Wiley \& Sons, 3rd edition.

[Pat80]  Patankar S. V. (1980) Numerical Heat Transfer and Fluid Flow. Hemisphere, Washington.

[Pat84] Patera A.T. (1984) A spectral element method for fluid dynamics: laminar flow in a channel expansion. J. Comput. Phys. 54: 468-488.

[Pet] www.mcs. anl.gov/petsc/. [PG00] Pierce N. and Giles M. B. (2000) Adjoint recovery of superconvergent functionals from PDE approximations. SIAM Review 42(2): 247-264.

[Pin85]  Pinkus A. (1985) $n$-Widths in Approximation Theory. Springer-Verlag, Ergebnisse.

[Pin08] Pinnau R. (2008) Model reduction via proper orthogonal decomposition. In Schilder W. and van der Vorst H. (eds) Model Order Reduction: Theory, Research Aspects and Applications, pages $96-109$. Springer.

[Pir84]  Pironneau O. (1984) Optimal Shape Design for Elliptic Systems. Springer-Verlag, New York.

[Por85] Porsching T. A. (1985) Estimation of the error in the reduced basis method solution of nonlinear equations. Mathematics of Computation 45(172): 487-496.

[PQ05] Parolini N. and Quarteroni A. (2005) Mathematical models and numerical simulations for the America's cup. Comput. Methods Appl. Mech. Engrg. 194(9-11): $1001-1026$.

[PQ07] Parolini N. and Quarteroni A. (2007) Modelling and numerical simulation for yacht engineering. In Proceedings of the 26th Symposium on Naval Hydrodynamics. Strategic Analysis, Inc., Arlington, VA, USA.

[PR07]  Patera A. T. and Rozza G. (2006-2007) Reduced Basis Approximation and A Posteriori Error Estimation for Parametrized Partial Differential Equations. Copyright MIT. To appear in MIT Pappalardo Monographs in Mechanical Engineering. [Pro97] Prohl A. (1997) Projection and Quasi-Compressibility Methods for Solving the Incompressible Navier-Stokes Equations. Advances in Numerical Mathematics. B.G. Teubner, Stuttgart.

[Pru06] Prud'homme C. (2006) A domain specific embedded language in $\mathrm{c}++$ for automatic differentiation, projection, integration and variational formulations. Scientific Programming 14(2): 81-110.

[PS03] Perugia I. and Schötzau D. (2003) The $h p$-local discontinuous Galerkin method for low-frequency time-harmonic Maxwell equations. Math. Comp. $72(243)$ : 1179-1214.

[PT97]  Piegl L. and Tiller W. (1997) The NURBS book. Springer-Verlag, New York.

[PWY90] Pawlak T. P., Wheeler M. J., and Yunus S. M. (1990) Application of the Zienkiewicz-Zhu error estimator for plate and shell analysis. Int. J. Numer. Methods Eng. 29: 1281-1298.

[Qia10] Qian X. (2010) Full analytical sensitivities in NURBS based isogeometric shape optimization. Comput. Methods Appli. Mech. Eng. 199(29-32): 2059-2071.

[QR07]  Quarteroni A. and Rozza G. (2007) Numerical solution of parametrized NavierStokes equations by reduced basis method. Num. Meth. PDEs 23: 923-948.

[QRDQ06] Quarteroni A., Rozza G., Dedè L., and Quaini A. (2006) Numerical approximation of a control problem for advection-diffusion processes. System modeling and optimization. IFIP Int. Fed. Inf. Process. 199: 261-273.

[QRM11]  Quarteroni A., Rozza G., and Manzoni A. (2011) Certified reduced basis approximation for parametrized partial differential equations in industrial applications. J. Math. Ind. 1(3).

[QR13]  Quarteroni A. and Rozza G. E. (2014, in press) Reduced Order Methods for Modeling and Computational Reduction. MS\&A Modeling, Simulation and Applications Series. Springer-Verlag Italia, Milano.

[QSS07]  Quarteroni A., Sacco R., and Saleri F. (2007) Numerical Mathematics. Springer, Berlin and Heidelberg, II edition. [QSV00]  Quarteroni A., Saleri F., and Veneziani A. (2000) Factorization methods for the numerical approximation of Navier-Stokes equations. Comput. Methods Appl. Mech. Engrg. 188(1-3): 505-526.

[Qu02]  Qu Z. (2002) Unsteady open-channel flow over a mobile bed. PhD thesis, École Polytechnique Fédérale de Lausanne.

[Qua93]  Quartapelle L. (1993) Numerical Solution of the Incompressible Navier-Stokes Equations. Birkhäuser Verlag, Basel.

[QV94]  Quarteroni A. and Valli A. (1994) Numerical Approximation of Partial Differential Equations. Springer, Berlin Heidelberg.

[QV99]  Quarteroni A. and Valli A. (1999) Domain Decomposition Methods for Partial Differential Equations. Oxford Science Publications, Oxford.

[Ran99] Rannacher R. (1999) Error control in finite element computations. An introduction to error estimation and mesh-size adaptation. In Error control and adaptivity in scientific computing (Antalya, 1998), pages 247-278. Kluwer Acad. Publ., Dordrecht.

[RC83] Rhie C. M. and Chow W. L. (1983) Numerical study of the turbulent flow past an airfoil with trailing edge separation. AIAA Journal $21(11): 1525-1532$.

[RH15]  Reali A. and Hughes T. (2015) An introduction to isogeometric collocation methods. Isogeometric Methods for Numerical Simulation pages 173-204.

[RHM13]  Rozza G., Huynh D.P.B., and Manzoni A. (2013) Reduced basis approximation and a posteriori error estimation for Stokes flows in parametrized geometries: roles of the inf-sup stability constants. Num. Math. 125(1), 115-152..

[RHP08] Rozza G., Huynh D.B.P., and Patera A. T. (2008) Reduced basis approximation and a posteriori error estimation for affinely parametrized elliptic coercive partial differential equations: Application to transport and continuum mechanics. Archives Computational Methods in Engineering 15(3): 229-275.

[Riv08] Rivière B. (2008) Discontinuous Galerkin methods for solving elliptic and parabolic equations, volume 35 of Frontiers in Applied Mathematics. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA. Theory and implementation.

[Rod94]  Rodríguez R. (1994) Some remarks on Zienkiewicz-Zhu estimator. Numer. Methods Part. Diff. Eq. 10: 625-635.

[Roz09]  Rozza G. (2009) Reduced basis method for Stokes equations in domains with non-affine parametric dependence. Comp. Vis. Science 12: 23-35.

[RR04]  Renardy M. and Rogers R. C. (2004) An Introduction to Partial Differential Equations. Springer-Verlag, New York, II edition.

[RST96] Ross H. G., Stynes M., and Tobiska L. (1996) Numerical Methods for Singularly Perturbed Differential Equations. Convection-Diffusion and Flow Problems. Springer-Verlag, Berlin Heidelberg.

[Rud91] Rudin W. (1991) Analyse Rèelle et Complexe. Masson, Paris.

[RV07]

Rozza G. and Veroy K. (2007) On the stability of reduced basis method for Stokes equations in parametrized domains. Comp. Meth. Appl. Mech. and Eng. 196: 1244-1260.

[RVS08] Rehman M., Vuik C., and Segal G. (2008) A comparison of preconditioners for incompressible Navier-Stokes solvers. Int. J. Numer. Meth. Fluids 57: 1731-1751. [RVS09] Rehman M., Vuik C., and Segal G. (2009) Preconditioners for the steady incompressible Navier-Stokes problem. Int. J. Applied Math. 38 (4).

[RWG99] Rivière B., Wheeler M. F., and Girault V. (1999) Improved energy estimates for interior penalty, constrained and discontinuous Galerkin methods for elliptic problems. I. Comput. Geosci. 3(3-4): 337-360 (2000). [RWG01] Rivière B., Wheeler M. F., and Girault V. (2001) A priori error estimates for finite element methods based on discontinuous approximation spaces for elliptic problems. SIAM J. Numer. Anal. 39(3): 902-931.

[Saa96] Saad Y. (1996) Iterative Methods for Sparse Linear Systems. PWS Publishing Company, Boston.

[Sag06] Sagaut P. (2006) Large Eddy Simulation for Incompressible Flows: an Introduction. Springer-Verlag, Berlin Heidelberg, III edition.

[Sal08]  Salsa S. (2008) Partial Differential Equations in Action - From Modelling to Theory. Springer, Milan.

[SBV $\left.^{+} 11\right] \quad$ Scott M., Borden M., Verhoosel C., Sederberg T., and Hughes T. (2011) Isogeometric finite element data structures based on Bézier extraction of $\mathrm{T}$-splines. Internat. J. Numer. Methods Eng. 88(2): 126-156.

[Sch69] Schwarz H. (1869) Über einige abbildungsdufgaben. J. Reine Agew. Math. 70: $105-120$.

[Sch98] Schwab C. (1998) $p$ and $h p$ - Finite Element Methods. Oxford Science Publication, Oxford.

[SDS $^{+}$12] Schillinger D., Dedè L., Scott M., Evans J., Borden M., Rank E., and Hughes T. (2012) An isogeometric design-through-analysis methodology based on adaptive hierarchical refinement of Nurbs, immersed boundary methods, and T-spline CAD surfaces. Comput. Methods Appli. Mech. Eng. 249-252: $116-150$.

[SER $\left.^{+} 13\right]$ Schillinger D., Evans J., Reali A., Scott M., and Hughes. T. (2013) Isogeometric collocation: cost comparison with Galerkin methods and extension to adaptive hierarchical NURBS discretizations. Comput. Methods Appli. Mech. Eng. 267: 170-232.

[SF73] Strang G. and Fix G. J. (1973) An Analysis of the Finite Element Method. Wellesley-Cambridge Press, Wellesley, MA.

[SGT01] S. Gottlieb C. S. and Tadmor E. (2001) Strong stability preserving high order time discretization methods. SIAM review 43 (1): 89-112.

[She] Shewchuk J. R.www.cs.cmu.edu/ quake/triangle.html.

[SHH14] Schillinger D., Hossain S., and Hughes T. (2014) Reduced Bézier element quadrature rules for quadratic and cubic splines in isogeometric analysis. Comput. Methods Appli. Mech. Eng. 277: 1-45.

[Shu88] Shu C. (1988) Total-variation-diminishing time discretizations. SIAM Journal on

" Scientific and Statistical Computing 9: 1073-1084.

[Sir87] Sirovich L. (1987) Turbulence and the dynamics of coherent structures, part i: Coherent structures. Quart. Appl. Math. $45(3)$ : $561-571$.

[SLSH12] Scott M., Li X., Sederberg T., and Hughes T. (2012) Local refinement of analysissuitable T-splines. Comput. Methods Appli. Mech. Eng. 213: 206-222.

[Smo01] Smolianski A. (2001) Numerical Modeling of Two-Fluid Interfacial Flows. PhD thesis, University of Jyväskylä.

[SO88] Shu C. and Osher S. (1988) Efficient implementation of essentially nonoscillatory shock-capturing schemes. Journal of Computational Physics 77: 439 471 .

[SO89] Shu C. and Osher S. (1989) Efficient implementation of essentially nonoscillatory shock-capturing schemes ii. Journal of Computational Physics 83: 32-78.

[Spi99] Spivak M. (1999) A comprehensive introduction to differential geometry. Vol. II. Publish or Perish Inc., Houston, Tex., iii edition. [Ste98] Stenberg R. (1998) Mortaring by a method of J. A. Nitsche. In Computational mechanics (Buenos Aires, 1998), pages CD-ROM file. Centro Internac. Métodos Numér. Ing., Barcelona.

$\begin{aligned}&\text { [Str71] } & \text { Stroud A. H. (1971) Approximate calculation of multiple integrals. Prentice-Hall, }\end{aligned}$ Inc., Englewood Cliffs, N.J.

[Str89] Strickwerda J. C. (1989) Finite Difference Schemes and Partial Differential Equations. Wadworth \& Brooks/Cole, Pacific Grove.

[Str00] Strostroup B. (2000) C++ Linguaggio, Libreria Standard, Principi di Programmazione. Addison Welsey Longman Italia, Milano, III edition.

[SV05] Saleri F. and Veneziani A. (2005) Pressure correction algebraic splitting methods for the incompressible Navier-Stokes equations. SIAM J. Numer. Anal. $43(1)$ : 174-194.

[SW10]  Stamm B. and Wihler T. P. (2010) $h p$-optimal discontinuous Galerkin methods 䬴 for linear elliptic problems. Math. Comp. 79(272): 2117-2133.

[SZ91] Sokolowski J. and Zolesio J. (1991) Introduction to Shape Optimization (Shape Sensitivity Analysis). Springer-Verlag, New York.

[SZBN03] Sederberg T., Zheng J., Bakenov A., and Nasri A. (2003) T-splines and TNURCCs. In ACM transactions on graphics $(T O G)$, volume 22:3, pages 477-484. ACM.

[Tan93] Tanaka N. (1993) Global existence of two phase nonhomogeneous viscous incompressible fluid flow. Comm. Partial Differential Equations $18(1-2)$ : 41-81.

[TDQ14] Tagliabue A., Dedè L., and Quarteroni A. (2014) Isogeometric Analysis and error estimates for high order partial differential equations in fluid dynamics. Comput. \& Fluids 102: 277-303.

[TE05] Trefethen L. and Embree M. (2005) Spectra and pseudospectra. The behavior of nonnormal matrices and operators. Princeton University Press, Princeton.

[Tem01] Temam R. (2001) Navier Stokes Equations. North-Holland, Amsterdam.

[TF88] Tsuchiya K. and Fan L.-S. (1988) Near-wake structure of a single gas bubble in a two-dimensional liquid-solid fluidized bed: vortex shedding and wake size variation. Chem. Engrg. Sci. $43(5)$ : $1167-1181$.

[Tho84] Thomee V. (1984) Galerkin Finite Element Methods for Parabolic Problems.

即 Springer, Berlin and Heidelberg.

[TI97] Trefethen L. and III D. B. (1997) Numerical Linear Algebra. SIAM.

[TL58] Taylor A. and Lay D. (1958) Introduction to Functional Analysis. J.Wiley \& Sons, New York.

[Tor09] Toro E. F. (2009) Riemann Solvers and Numerical Methods for Fluid Dynamics: A Practical Introduction. Springer-Verlag, Berlin Heidelberg.

[Tri]

㟍

software.sandia.gov/trilinos/.

[Tro10] F. Tröltzch (2010), Optimal Central of Partial Differential Equations: Theory, Methods and Applications. AMS, Providence.

[TSHH17] Toshniwal D., Speleers H., Hiemstra R., and Hughes T. (2017) Multi-degree smooth polar splines: A framework for geometric modeling and isogeometric analysis. Comput. Methods Appli. Mech. Eng. 316: 1005-1061.

[TSW99] Thompson J. F., Soni B. K., and Weatherill N. P. (eds) (1999) Handook of Grid Generation. CRC Press, Boca Raton.

[TW05] Toselli A. and Widlund O. (2005) Domain Decomposition Methods - Algorithms and Theory. Springer-Verlag, Berlin Heidelberg.

[TWM85] Thompson J. F., Warsi Z. U. A., and Mastin C. W. (1985) Numerical Grid Generation, Foundations and Applications. North-Holland, New York. [UMF]

www.cise.ufl.edu/research/sparse/umfpack/.

[Vas81] Vasiliev F. (1981) Methods for Solving the Extremum Problems. Nauka, Moscow.

[vdV03] van der Vorst H. A. (2003) Iterative Krylov Methods for Large Linear Systems. Cambridge University Press, Cambridge.

[Vel95] Veldhuizen T. (1995) Expression templates. C++ Report Magazine 7(5): 26-31. see also the web page http://osl.iu.edu/ tveldhui .

[Ven98]  Veneziani A. (1998) Mathematical and Numerical Modeling of Blood Flow Problems. PhD thesis, Università degli Studi di Milano.

[Ver84] Verfürth R. (1984) Error estimates for a mixed finite elements approximation of the stokes equations. R.A.I.R.O. Anal. Numér. 18: 175-182.

[Ver96] Verführth R. (1996) A Review of a Posteriori Error Estimation and Adaptive Mesh Refinement Techniques. Wiley-Teubner, New York.

[VGJS11] Vuong A.-V., Giannelli C., Jüttler B., and Simeon B. (2011) A hierarchical approach to adaptive local refinement in isogeometric analysis. Comput. Methods Appli. Mech. Eng. 200(49-52): 3554-3567.

[VM96] Versteeg H. and Malalasekra W. (1996) An Introduction to Computational Fluid Dynamics: the Finite Volume Method Approach. Prentice-Hall.

[Vol11] Volkwein S. (2011) Model reduction using proper orthogonal decomposition. Lecture Notes, University of Konstanz.

[VP05] Veroy K. and Patera A.T. (2005) Certified real-time solution of the parametrized steady incompressible Navier-Stokes equations: rigorous reduced-basis a posteriori error bounds. Int. J. Numer. Meth. Fluids 47(8-9): 773-788.

[Wes01] Wesseling P. (2001) Principles of Computational Fluid Dynamics. SpringerVerlag, Berlin Heidelberg New York.

[Whe78] Wheeler M. F. (1978) An elliptic collocation-finite element method with interior penalties. SIAM J. Numer. Anal. 15(1): 152-161.

[Wil98] Wilcox D. C. (1998) Turbulence Modeling in CFD. DCW Industries, La Cañada, CA, II edition.

[Win07] Winkelmann C. (2007) Interior penalty finite element approximation of NavierStokes equations and application to free surface flows. PhD thesis, École Polytechnique Fédérale de Lausanne.

[WFC08] Wall W., Frenzel M., and Cyron C. (2008) Isogeometric structural shape optimization. Comput. Methods Appli. Mech. Eng. 197(33-40): 2976-2988.

[WZLH17] Wei X., Zhang Y., Liu L., and Hughes T. (2017) Truncated t-splines: Fundamentals and methods. Comput. Methods Appli. Mech. Eng. 316: $349-372$.

[Woh01] Wohlmuth B. (2001) Discretization Methods and Iterative Solvers Based on Domain Decomposition. Springer.

[Wya00] Wyatt D. C. (2000) Development and assessment of a nonlinear wave prediction methodology for surface vessels. Journal of ship research 44: 96 .

[Yos74]  Yosida K. (1974) Functional Analysis. Springer-Verlag, Berlin Heidelberg.

[ZBG $\left.^{+} 07\right]$ Zhang Y., Bazilevs Y., Goswami S., Bajaj C., and Hughes T. (2007) Patientspecific vascular NURBS modeling for Isogeometric Analysis of blood flow. Comput. Methods Appli. Mech. Eng. 196(29): 2943-2959.

[Zie00] Zienkiewicz O. (2000) Achievements and some unsolved problems of the finite element method. Int. J. Numer. Meth. Eng. 47: 9-28.

[ZT00] Zienkiewicz O. C. and Taylor R. L. (2000) The Finite Element Method, Vol. 1, The Basis. Butterworth-Heinemann, Oxford, V edition.

[Zua03]  Zuazua E. (2003) Propagation, observation, control and numerical approximation of waves. Bol. Soc. Esp. Mat. Apl. 25: 55-126. [Zua05] Zuazua E. (2005) Propagation, observation, and control of waves approximated by finite difference methods. SIAM Review 47 (2): 197-243.

[Zua06]  Zuazua E. (2006) Controllability and observability of partial differential equations: Some results and open problems. In Dafermos C. and Feiteisl E. (eds) Handbook of Differential Equations: Evolutionary Differential Equations, volume 3 , pages 527-621. Elsevier Science.

[ZZ87] Zienkiewicz O. C. and Zhu J. Z. (1987) A simple error estimator and adaptive procedure for practical engineering analysis. Int. J. Numer. Meth. Engng. 24: 337-357.

[ZZ92] Zienkiewicz O. C. and Zhu J. Z. (1992) The superconvergent patch recovery and a posteriori error estimates. I: The recovery technique. Int. J. Numer. Meth. Engng. 33: $1331-1364$.