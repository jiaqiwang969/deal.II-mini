examples/step-17/doc/intro.dox

<a name="Intro"></a>

<h1>Introduction</h1>

<h3>Overview</h3>

这个程序没有引入任何新的数学思想；事实上，它所做的只是做与step-8已经做的完全相同的计算，但它以一种不同的方式来做：我们没有使用deal.II自己的线性代数类，而是在deal.II提供的类之上建立一切，这些类包裹着<a
href="http://www.mcs.anl.gov/petsc/" target="_top">PETSc</a>库的线性代数实现。由于PETSc允许将矩阵和向量分布在MPI网络中的几台计算机上，因此产生的代码甚至能够以%并行方式解决问题。如果你不知道PETSc是什么，那么这将是一个快速浏览其主页的好时机。

作为这个程序的先决条件，你需要安装PETSc，如果你想在一个集群上以%并行方式运行，你还需要<a
href="http://www-users.cs.umn.edu/~karypis/metis/index.html"
target="_top">METIS</a>来划分网格。在<a
href="../../readme.html" target="body">README</a>文件中描述了deal.II和这两个附加库的安装。

现在，关于细节：如前所述，该程序不计算任何新的东西，所以对有限元类等的使用与以前完全相同。与以前的程序不同的是，我们用几乎所有的类 <code>Vector</code> and <code>SparseMatrix</code> 代替了它们的近似值 <code>PETScWrappers::MPI::Vector</code> 和 <code>PETScWrappers::MPI::SparseMatrix</code> ，它们存储数据的方式使MPI网络中的每个处理器只存储矩阵或矢量的一部分。更具体地说，每个处理器将只存储与它 "拥有 "的自由度相对应的矩阵的那些行。对于向量，它们要么只存储与处理器拥有的自由度相对应的元素（这是右手边所需要的），要么也存储一些额外的元素，以确保每个处理器都能访问处理器拥有的单元（所谓 @ref GlossLocallyActiveDof "本地活动的自由度"）或邻近单元（所谓 @ref GlossLocallyRelevantDof "本地相关自由度"）上的解组件。

来自PETScWrapper命名空间的类所提供的接口与deal.II线性代数类的接口非常相似，但它们不是自己实现这一功能，而是简单地传递给它们相应的PETSc函数。因此，包装器只是用来给PETSc一个更现代的、面向对象的接口，并使PETSc和deal.II对象的使用尽可能地互换。使用PETSc的主要意义在于它可以在%并行状态下运行。我们将利用这一点，将域划分为与MPI网络中的进程一样多的块（"子域"）。同时，PETSc还提供了假的MPI存根，所以如果PETSc的配置中没有MPI，你可以在一台机器上运行这个程序。




<h3>Parallelizing software with MPI</h3>

开发软件以通过MPI在%parallel中运行，需要改变一下思维方式，因为我们通常必须分割所有的数据结构，使每个处理器只存储整个问题的一部分。因此，你通常不能在每个处理器上访问一个解决方案向量的所有组成部分 -- 每个处理器可能根本没有足够的内存来容纳整个解决方案向量。由于数据被分割或 "分布 "在各个处理器上，我们把MPI使用的编程模型称为 "分布式内存计算"（与 "共享内存计算 "相反，后者意味着多个处理器可以访问一个内存空间中的所有数据，例如，当一台机器的多个核心在一个共同任务上工作时）。分布式内存计算的一些基本原理在 @ref distributed "使用分布式内存的多处理器并行计算 "文档模块中讨论，该模块本身是 @ref Parallel "并行计算 "模块的一个子模块。

一般来说，为了真正能够扩展到大量的处理器，我们需要在可用的处理器之间分割出<i>every</i>数据结构，其大小随着整个问题的大小而扩展。关于程序 "扩展 "的定义，见 @ref GlossParallelScaling "本词汇表条目"）。这包括，例如，三角形、矩阵和所有全局向量（解决方案，右手边）。如果不拆分所有这些对象，其中一个对象将被复制到所有的处理器上，如果问题大小（和可用的处理器数量）变得很大，最终会简单地变得太大。另一方面，在每个处理器上保留大小与整个问题大小无关的对象是完全可以的。例如，可执行文件的每个副本将创建自己的有限元对象，或者我们在汇编中使用的局部矩阵）。)

在当前的程序中（以及相关的第18步），我们不会走得这么远，而是对MPI的使用做一个比较温和的介绍。更具体地说，我们要并行化的数据结构只有矩阵和向量。然而，我们并没有拆分Triangulation和DoFHandler类：每个进程仍然拥有这些对象的完整副本，而且所有进程都拥有其他进程所拥有的确切副本。然后，我们只需在每个处理器上的三角形的每个副本中，标记哪个处理器拥有哪些单元。这个过程被称为将网格 "分割 "为 @ref GlossSubdomainId "子域"。

对于较大的问题，必须在每个处理器上存储<i>entire</i>网格，显然会产生一个瓶颈。分割网格是稍微的，虽然没有多复杂（从用户的角度来看，虽然它<i>much</i>下更复杂）来实现，我们将展示如何在step-40和其他一些程序中这样做。在讨论这个程序的某个功能如何工作的过程中，我们会多次评论它不会扩展到大型问题，以及为什么不会。所有这些问题都将在第18步，特别是第40步中得到解决，它可以扩展到非常多的进程。

从哲学上讲，MPI的运作方式如下。你通常通过以下方式运行一个程序

@code
  mpirun -np 32 ./step-17
@endcode

这意味着在（比如）32个处理器上运行它。如果你是在一个集群系统上，你通常需要<i>schedule</i>程序在32个处理器可用时运行；这将在你的集群的文档中描述。但是在系统内部，每当这些处理器可用时，通常会执行上述相同的调用）。)这样做的目的是，MPI系统将启动32个<i>copies</i>的 <code>step-17</code> 的可执行文件。(这些正在运行的可执行文件中的每一个的MPI术语是，你有32个 @ref GlossMPIProcess "MPI进程"。)这可能发生在不同的机器上，甚至不能从对方的内存空间中读取，也可能发生在同一台机器上，但最终的结果是一样的：这32个副本中的每一个都将以操作系统分配给它的一些内存运行，而且它不能直接读取其他31个副本的内存。为了在一个共同的任务中进行协作，这32个副本就必须<i>communicate</i>相互协作。MPI是<i>Message Passing Interface</i>的缩写，通过允许程序<i>send messages</i>来实现这一点。你可以把它看作是邮件服务：你可以把一封写给特定地址的信放入邮件，它将被送达。但这是你能控制事物的程度。如果你想让收信人对信的内容做些什么，例如把你想要的数据从那边返回给你，那么需要发生两件事。(i)接收方需要实际去检查他们的邮箱里是否有东西，(ii)如果有的话，做出适当的反应，比如说发送数据回来。如果你等待这个返回信息，但原来的接收者却心不在焉，没有注意，那么你就不走运了：你只需要等待，直到你在那边的请求将被解决。在某些情况下，错误会导致原始接收者永远不检查你的邮件，在这种情况下，你将永远等待--这被称为<i>deadlock</i>。(  @dealiiVideoLectureSeeAlso{39,41,41.25,41.5}) 

在实践中，人们通常不在发送和接收单个消息的层面上编程，而是使用更高层次的操作。例如，在程序中，我们将使用函数调用，从每个处理器获取一个数字，将它们全部相加，然后将总和返回给所有处理器。在内部，这是用单个消息实现的，但对用户来说，这是透明的。我们称这种操作为<i>collectives</i>，因为<i>all</i>处理器参与其中。集合体允许我们编写程序，其中不是每个可执行文件的副本都在做完全不同的事情（这将是难以置信的编程难度），但实质上所有副本都在为自己做同样的事情（尽管是在不同的数据上），通过相同的代码块运行；然后他们通过集合体进行数据通信；然后再回到为自己做事情，通过相同的数据块运行。这是能够编写程序的关键部分，也是确保程序能够在任何数量的处理器上运行的关键部分，因为我们不需要为每个参与的处理器编写不同的代码。

这并不是说程序从来都是以不同的处理器在其可执行文件的副本中运行不同的代码块的方式来编写的。程序内部也经常以其他方式而不是通过集合体进行通信。但是在实践中，%并行有限元代码几乎总是遵循这样的方案：程序的每个副本在同一时间运行相同的代码块，中间穿插着所有处理器相互交流的阶段）。)

在现实中，即使是调用MPI集体函数的水平也太低了。相反，下面的程序根本不会包含对MPI的任何直接调用，而只包含对deal.II的用户隐藏这种通信的函数。这样做的好处是，你不需要学习MPI的细节和相当复杂的函数调用。也就是说，你确实必须理解上文所述的MPI背后的一般哲学。




<h3>What this program does</h3>

然后，这个程序演示的技术是。

- 如何使用PETSc封装类；这在本程序的主类的声明中已经可以看到，  <code>ElasticProblem</code>  。

- 如何将网格划分为子域；这发生在 <code>ElasticProblem::setup_system()</code> 函数。

- 如何对运行在MPI网络上的作业进行并行化操作；在这里，这是在很多地方都要注意的，最明显的是在 <code>ElasticProblem::assemble_system()</code> 函数中。

- 如何处理只存储向量项子集的向量，对于这些向量，我们必须确保它们在当前处理器上存储我们需要的东西。例如见 <code>ElasticProblem::solve()</code> and <code>ElasticProblem::refine_grid()</code> 函数。

- 如何处理同时在多个处理器上运行的程序的状态输出。这是通过程序中的 <code>pcout</code> 变量完成的，在构造函数中初始化。

由于这一切只能用实际的代码来证明，让我们直接进入代码，不再多说。


